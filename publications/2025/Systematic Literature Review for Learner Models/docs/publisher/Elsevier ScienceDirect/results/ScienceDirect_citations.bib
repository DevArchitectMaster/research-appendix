@article{SUN2023110969,
title = {A Metaverse text recognition model based on character-level contrastive learning},
journal = {Applied Soft Computing},
volume = {149},
pages = {110969},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110969},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623009870},
author = {Le Sun and Huiyun Li and Ghulam Muhammad},
keywords = {Metaverse, Lightweight model, Scene text recognition, Noise robustness, Time efficiency},
abstract = {Accurate and efficient text recognition can enhance the accuracy and time efficiency of human–computer interaction and information exchange in noise Metaverse scenarios. For example, it can improve the safety of digital twin-based intelligent transportation and the intelligence of Non-Player Characters. Robust features play a vital role in the performance of scene text recognition models in noise Metaverse situations. To extract robust features, and improve the accuracy and time efficiency of scene text recognition, we propose a Character level text recognition model in Metaverse applications, called MetaChara. It contains two main components: a lightweight text feature extraction module (LightFeature), and a robust character recognition module (RobChara). LightFeature leverages the advantage of global feature aggregation in the primitive representation learning network to handle irregular text images.RobChara incorporates the capability of contrastive learning from the momentum contrast method, improving the robustness of feature extraction in MetaChara. It structures a feature queue for organized storage. By optimizing the similarity of intra-character features and maximizing inter-character differences, it makes the model better adapted to scene text recognition tasks in Metaverse. Experiment results demonstrate that MetaChara is light with 29.14 million parameters and time efficient with an average recognition speed of 1.73 s. It also achieves excellent performance in terms of FLoating-point Operations (FLOPs), registering only 59.60 billion times for each operation. MetaChara achieves an average accuracy of 0.969 for character recognition. We present a case study where MetaChara quickly and accurately recognizes scene texts within the context of autonomous driving in the Metaverse. This demonstrates how MetaChara enhances safety and improves time efficiency for intelligent transportation systems.}
}
@article{HADDELER2023104512,
title = {Real-time terrain anomaly perception for safe robot locomotion using a digital double framework},
journal = {Robotics and Autonomous Systems},
volume = {169},
pages = {104512},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104512},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023001513},
author = {Garen Haddeler and Hari P. Palanivelu and Fabien Colonnier and Yung Chuen Ng and Albertus H. Adiwahono and Zhibin Li and Chee-Meng Chew and Meng Yee Michael Chuah},
keywords = {Robot sensing, Legged robot locomotion, Digital twins, Field robotics},
abstract = {Digital twinning systems are effective tools to test and develop new robotic capabilities before applying them in the real world. This work presents a real-time digital double framework that improves and facilitates robot perception of the environment. Soft or non-rigid terrains can cause locomotion failures, while visual perception alone is often insufficient to assess the physical properties of such surfaces. To tackle this problem we employ the proposed framework to estimate ground collapsibility through physical interactions while the robot is dynamically walking on challenging terrains. We extract discrepancy information between the two systems, a simulated digital double that is synchronized with a real robot, both using exactly the same physical model and locomotion controller. The discrepancy in sensor measurements between the real robot and its digital double serves as a critical indicator of anomalies between expected and actual motion and is utilized as input to a learning-based model for terrain collapsibility analysis. The performance of the collapsibility estimation was evaluated in a variety of real-world scenarios involving flat, inclined, elevated, and outdoor terrains. Our results demonstrate the generality and efficacy of our real-time digital double architecture for estimating terrain collapsibility.}
}
@article{ABDEEN2023102032,
title = {Citizen-centric digital twin development with machine learning and interfaces for maintaining urban infrastructure},
journal = {Telematics and Informatics},
volume = {84},
pages = {102032},
year = {2023},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2023.102032},
url = {https://www.sciencedirect.com/science/article/pii/S0736585323000965},
author = {Fathima Nishara Abdeen and Sara Shirowzhan and Samad M.E. Sepasgozar},
keywords = {City digital twin, Macro digital twin, Infrastructure, Machine learning, Deep learning, Sensors, Application processing interfaces},
abstract = {Serious interoperability challenges prevent the stakeholders of infrastructure projects and citizens as the final users, from interacting with each other and helping maintain a project over its lifetime. This paper focuses on macro-scale digital twins collecting stakeholders’ feedback as the end users of the infrastructure and its service buildings. The aim is to examine various technologies for developing a CCDT and use the information processed to maintain and manage infrastructure services. This involves a systematic review, investigating technologies for data acquisition, data processing, and interface development to improve CCDT capabilities. Among the 89 selected articles, 16% of the sample dataset directly focused on users’ engagement. When considering data acquisition technologies, the open data platforms (37% of the sample dataset), remote sensors (37%), and IoT sensors (8%) ensure the dynamic capabilities of the digital twin. Volunteered geographic information (VGI) and social sensing are two prominent technologies that encourage citizen engagement. The number of articles considering the use of segmentation and classification and object detection and tracking algorithms at city-scale digital twins is significant, accounting for 25% and 24% of all articles discussing various algorithms. Further, the study carried out a comprehensive analysis of application programming interfaces (APIs) while presenting their specifications, features, and applications.}
}
@article{LAKHAN2023101747,
title = {Secure-fault-tolerant efficient industrial internet of healthcare things framework based on digital twin federated fog-cloud networks},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {9},
pages = {101747},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101747},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823003014},
author = {Abdullah Lakhan and Ali Azawii {Abdul Lateef} and Mohd Khanapi {Abd Ghani} and Karrar Hameed Abdulkareem and Mazin Abed Mohammed and Jan Nedoma and Radek Martinek and Begoña Garcia-Zapirain},
keywords = {IoHT, Fault-tolerant, Digital twin, Industry 5.0, Blockchain, SFTS, Fog-cloud networks, CNN},
abstract = {The Industrial Internet of Healthcare Things (IIoHT) is the emerging paradigm in digital healthcare. Context-aware healthcare sensors, local intelligent watches, healthcare devices, wireless communication technologies, fog, and cloud computing are all parts of the IIoHT used in healthcare. The ubiquitous healthcare services it provides to its users in practice. However, the current IIoHT healthcare frameworks have security and failure issues in mobile fog and cloud networks where they are spread out. This paper presents the secure, fault-tolerant IIoHT Framework based on digital twin (DT) federated learning-enabled fog-cloud models. The DT is an effective technology that makes virtual copies of servers at different locations. DT integrated with federated learning inside the fog and cloud environments, where the failure of tasks and execution improved for healthcare sensor data. The study aims to reduce processing time and the risk of task failure. The study presents the Secure and Fault-Tolerant Strategies (SFTS)-enabled IIoHT framework that optimizes wearable sensor data and executes it with the minimum offloading and processing delays. Simulation results show that the proposed work minimized the security risk by 40%, failure risk of tasks risk by 50%, and the training and testing time by 39% for sensor data during the execution of mobile fog cloud networks.}
}
@article{SUN2023113991,
title = {On-line Milling Tool Wear Monitoring Under Practical Machining Conditions},
journal = {Measurement},
pages = {113991},
year = {2023},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.113991},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123015555},
author = {Yi Sun and Jigang He and Hongli Gao and Hongliang Song and Liang Guo},
keywords = {tool-wear monitoring, anodic or cathodic samples, marginal distribution, conditional distribution},
abstract = {Tool-wear monitoring plays a crucial role in high-speed cutting machining as it ensures the accuracy of the machining surface, improves tool utilization, and extends the life of machine tools. However, effectively selecting and processing data from each stage of the feed-path and leveraging the vast amount of unlabeled data associated with different processing parameters present considerable challenges in practical scenarios. To deal with the above problems, this study proposes the tool-wear monitoring method using unreliable pseudo-labels (TWM-U2PL), comprising both a teacher model and a student model. Within TWM-U2PL, the teacher model encompasses two independent classifiers that facilitate the extraction and categorization of tool-wear features, additionally assigning labels to certain unlabeled data as either anodic or cathodic samples. The student model achieves accurate tool-wear monitoring by dynamically adjusting marginal and conditional distribution. The experiments on a tool-wear dataset confirmed the effectiveness of TWM-U2PL, which achieved an outstanding accuracy of 87.25%, surpassing the well-known models such as CNN, CHMM, and FE-PCA-SA.}
}
@article{ZHONG2023103350,
title = {Advances in intelligent detection, monitoring, and control for preserving the quality of fresh fruits and vegetables in the supply chain},
journal = {Food Bioscience},
volume = {56},
pages = {103350},
year = {2023},
issn = {2212-4292},
doi = {https://doi.org/10.1016/j.fbio.2023.103350},
url = {https://www.sciencedirect.com/science/article/pii/S2212429223010015},
author = {Xiaolong Zhong and Min Zhang and Tiantian Tang and Benu Adhikari and Yamei Ma},
keywords = {Sensors, Detection, Imaging, Machine learning, Algorithm},
abstract = {The raw materials and information sharing of global fresh produce supply chains is happening every day, with the same time, losses of spoiled fruits and vegetables also exist, too. Most of the existing strategies have been successfully used in stop-loss practices such as refrigeration, atmosphere control and coating. They have achieved significant benefits but lack initiative. In this review, we sorted out and compiled data-driven networks and technical equipment reports, like emerging sensors, smart indicators and computer vision, etc. In addition, we also give suggestions on the system improvements of fruit and vegetable supply chain, including better applying cloud processing platforms such as IoT and blockchain to the supply chain. Finally, among the recommendations given, we highlight the importance of fencing strategies and real-time data management.}
}
@article{YOCKEY2023104960,
title = {Cyber threat assessment of machine learning driven autonomous control systems of nuclear power plants},
journal = {Progress in Nuclear Energy},
volume = {166},
pages = {104960},
year = {2023},
issn = {0149-1970},
doi = {https://doi.org/10.1016/j.pnucene.2023.104960},
url = {https://www.sciencedirect.com/science/article/pii/S0149197023003955},
author = {Patience Yockey and Anna Erickson and Christopher Spirito},
keywords = {Machine learning, Autonomous control systems, Digital twins, Cybersecurity},
abstract = {Advanced cyber-attacks against critical infrastructure and the energy sector are becoming more common. With the invention of autonomous control systems (ACS) within advanced nuclear reactor designs, system designers, reactor operators, and regulators must consider cybersecurity during the design and operational phases. This article provides a cyber threat assessment of machine learning (ML)-based digital twinning (DT) technologies in the context of advanced reactor ACS. A cyber–physical testbed was created to emulate nuclear reactor digital instrumentation and controls (I&C) and act as a basis for the ACS. The ACS was designed as two plant-level DTs predicting reactor malfunctions and determining control actions and two component-level DTs responsible for classifying component states and forecasting component inputs and outputs (I/O). Two duplicate ACS designs– one using a traditional ML framework and one using an automated ML (AutoML) framework– were created and tested against cyber-attacks on training data, real-time process data, and ML model architectures to determine their respective qualitative cyber-risk in terms of likelihood and impact. Both frameworks showed similar cyber-resilience against training, real-time, and ML architecture attacks, proving that neither is inherently more secure. Recommended safeguard and security measures are posed to system designers, reactor operators, and regulators to maintain the cybersecurity of ML-based DT technologies such as ACS, prompting a holistic view of shared responsibility for maintaining cyber-secure ML-based systems.}
}
@article{KUMAR2023111921,
title = {Digital twin-driven SDN for smart grid: A deep learning integrated blockchain for cybersecurity},
journal = {Solar Energy},
volume = {263},
pages = {111921},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.111921},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23005546},
author = {Prabhat Kumar and Randhir Kumar and Ahamed Aljuhani and Danish Javeed and Alireza Jolfaei and A. K. M. Najmul Islam},
keywords = {Blockchain, Deep learning, Digital twin, Internet of things, Smart grid, Software-defined networking},
abstract = {Internet of Things (IoT)-enabled Smart Grid (SG) network is envisioned as the next-generation network for intelligent and efficient electric power transmission. In SG environment, the Smart Meters (SMs) mostly exchange services and data from Service Providers (SPs) via insecure public channel. This makes the entire SG ecosystem vulnerable to various security threats. Motivated from the aforementioned challenges, we incorporate Digital Twin (DT) technology, Software-Defined Networking (SDN), Deep Learning (DL) and blockchain into the design of a novel SG network. Specifically, a secure communication channel is first designed using an authentication method based on blockchain technology that has the ability to withstand a number of well-known assaults. Second, a new DL architecture that includes a self-attention mechanism, a Bidirectional-Gated Recurrent Unit (Bi-GRU) model, fully connected layers, and a softmax classifier is designed to enhance the attack detection process in SG environments. To deliver low latency and real-time services, the SDN is next employed as the network’s backbone to send requests from SMs to a global SDN controller. DT technology is finally integrated into the SDN control plane, which stores the operating states and behavior models of SMs and communicates with SMs. The efficiency of the proposed framework is demonstrated by the blockchain implementation used in the SG network to assess computing time for the various numbers of transactions per block. Finally, the numerical results based on the N-BaIoT dataset shows better intrusion detection.}
}
@article{MA2023110718,
title = {Digital twin model for chiller fault diagnosis based on SSAE and transfer learning},
journal = {Building and Environment},
volume = {243},
pages = {110718},
year = {2023},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.110718},
url = {https://www.sciencedirect.com/science/article/pii/S036013232300745X},
author = {Xin Ma and Fan Chen and Zhihan Wang and Ke Li and Changbin Tian},
keywords = {Chiller, Fault diagnosis, Stacked sparse auto-encoder, Transfer learning, Digital twin},
abstract = {The equipment of chiller systems is characterized by a complex mechanical structure and operating environments that vary widely, resulting in high failure rates, energy waste, and costly maintenance. Traditional fault diagnosis methods suffer from low levels of digitalization and intelligence. Therefore, timely and accurate detection and diagnosis of the operating status of chiller systems are critical. To address the above issues, this study proposes a digital twin (DT) model for chiller system fault diagnosis based on stacked sparse auto-encoder (SSAE) and transfer learning (TL). First, a chiller system digital twin mapping model is constructed utilizing digital twin technology. Then, the SSAE model is constructed to provide real-time defect diagnosis and fault result validation. Finally, to address the limited chiller system data in practical applications, the knowledge of fault diagnosis acquired in the source domain is transferred to the target domain via TL. Experimental results demonstrate that the SSAE model outperforms back propagation (BP), recurrent neural network (RNN) and long short-term memory (LSTM) model in term of accuracy. Furthermore, using TL achieves a diagnostic accuracy of over 90% for different degrees of fault severity, with a significant decrease in cross-entropy loss compared to no TL, confirming the effectiveness of the TL method.}
}
@article{CHEN2023103439,
title = {Semi-supervised knowledge distillation framework for global-scale urban man-made object remote sensing mapping},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {122},
pages = {103439},
year = {2023},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2023.103439},
url = {https://www.sciencedirect.com/science/article/pii/S1569843223002637},
author = {Dingyuan Chen and Ailong Ma and Yanfei Zhong},
keywords = {Remote sensing, Knowledge distillation, Urban man-made object, Label diversity},
abstract = {Accurate mapping of global urban man-made objects such as buildings and roads is critical for monitoring urbanization. Remote sensing imagery provides a cost-effective way of mapping these objects, but the challenge of “knowledge forgetting” arises due to urban diversity and the continuous growth of global samples. Although the existing knowledge distillation approaches can transfer knowledge from a larger teacher model to a smaller student model by distilling the knowledge learned from reliable labels, they fail to work for global-scale mapping, which lies in two aspects: low-quality labeling and fixed-size models. In this paper, we propose GUMONet, which is a semi-supervised knowledge distillation framework for global-scale urban man-made object mapping. For the first phase, a label diversity progressive learning module is introduced for generating high-quality labels in a semi-supervised manner. Label diversity is used to measure the diverse urban patterns based on spatial-semantic uncertainty, where the diversified labels clustered in object boundaries and heterogeneous areas are attributed to high spatial uncertainty and semantic uncertainty, respectively. Based on the label diversity, the model decision boundary is progressively determined from coarse to fine. Specifically, at the early stage, instances away from the decision boundary are selected to ensure the stability of the model training. As the iteration progresses, instances close to the decision boundary are associated with a higher probability of further enhancing the quality of the uncertain labels by hard sample mining. For the second phase, a size-variable knowledge distillation module is adopted to optimize the data-model matching process. This module consists of a noise teacher model that prevents overfitting by injecting noise perturbations to increase the data distribution complexity and a size-variable student model that avoids underfitting by dynamically adjusting its size with the growth of global samples. We applied GUMONet to six study areas across four continents, with data from different sensors, achieving an 18.97% improvement in intersection over union, compared with the previous methods. Our results also demonstrate a positive correlation between urban development and urban diversity, with a correlation coefficient of 0.749. As urban development progresses, urban diversity stabilizes and building transformation becomes the primary means of promoting further development.}
}
@article{WANG2023110625,
title = {Digital twin modeling for structural strength monitoring via transfer learning-based multi-source data fusion},
journal = {Mechanical Systems and Signal Processing},
volume = {200},
pages = {110625},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110625},
url = {https://www.sciencedirect.com/science/article/pii/S0888327023005332},
author = {Bo Wang and Zengcong Li and Ziyu Xu and Zhiyong Sun and Kuo Tian},
keywords = {Digital twin modeling, Data fusion, Structural strength monitoring, Bagging, Transfer learning},
abstract = {Experimental measurement and numerical simulation are two typical methods to monitor the strength variation of structures. However, the former method is difficult to lay sufficient sensors on structures with large sizes or complex curved surfaces, and the latter one suffers from low prediction accuracy due to the simplification and idealization of the physical entity. How to efficiently utilize and combine these two kinds of methods remains a challenging problem. In this study, a novel digital twin modeling method via transfer learning-based multi-source data fusion (DTM-TL-MSDF) is proposed to make full use of the experiment data and the simulation data, aiming to establish an accurate digital twin model for structural strength monitoring in real-time. In the off-line stage, the clustering algorithm is used to pre-process the huge simulation data to relieve the computational burden, and the deep neural network (DNN) model is then pre-trained using the pre-processed simulation data. In the on-line stage, the pre-trained DNN model is fine-tuned using the experimental data to carry out transfer learning. Moreover, the bagging algorithm is employed in the fine-tuning process to improve the robustness and prediction accuracy due to its ability to address the dataset with only a small number of training points. To illustrate the effectiveness of the DTM-TL-MSDF method, a one-dimensional test function and an experimental study of a rectangular plate with hole under axial tension are studied. Results indicate that the DTM-TL-MSDF method can build an accurate digital twin model by integrating the simulation data and the experimental data with excellent global and local accuracy, providing a novel solution to monitor the variations of the structural full-field strength in real-time.}
}
@article{KILIC2023129118,
title = {Digital twin for Electronic Centralized Aircraft Monitoring by machine learning algorithms},
journal = {Energy},
volume = {283},
pages = {129118},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.129118},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223025124},
author = {Ugur Kilic and Gorkem Yalin and Omer Cam},
keywords = {Machine learning, Estimation, Turbofan, Primary engine parameters, Digital twin},
abstract = {Electronic Centralized Aircraft Monitoring (ECAM) parameters play a vital role in the operation of an aircraft to reduce the workload of the cockpit crew. A wide-body commercial aircraft with a triple-spool turbofan engine is examined within the scope of the study. This study is focused on the estimation of the ECAM primary engine parameters: Engine Pressure Ratio, Exhaust Gas Temperature, Fuel Flow, and Shaft Speeds without any additional measurement for data continuity. The recorded flight data obtained from a commercial aircraft is processed with machine learning methods, and the most suitable estimation method is tried to be determined. Correlation analysis is carried out for each data in the study to show strong predictor candidates. The modeling process is conducted by using MATLAB. Results indicate that the Fine Decision Tree is better at memorizing data, while the Wide Neural Network is better at generalizing data. Computational results show that the developed models are outstandingly precise and accurate to estimate aircraft's ECAM data to ensure flight safety for health and performance monitoring of an engine. Thus, when an unreliable situation occurs while performing the flight in practical conditions, the cockpit crew will be able to overcome this situation by Digital Twin.}
}
@article{BRAIK2023104020,
title = {A novel digital twin framework of electric power infrastructure systems subjected to hurricanes},
journal = {International Journal of Disaster Risk Reduction},
volume = {97},
pages = {104020},
year = {2023},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2023.104020},
url = {https://www.sciencedirect.com/science/article/pii/S2212420923005009},
author = {Abdullah M. Braik and Maria Koliou},
keywords = {Bayesian network, Community resilience, Digital twin, Electric power network, Hurricanes},
abstract = {The electric power network (EPN) is one of the most critical infrastructure systems as most lifeline, economic, and social systems depend heavily on it, and any disruption in the network may affect the well-being of modern societies. Being the most vulnerable to natural hazards, the resilience of the EPN has received plenty of attention in recent years, particularly considering the increasing frequency and severity of natural hazards associated with climate instabilities. The data revolution and the recent advances in the fields of artificial intelligence (AI), machine learning (ML), and the Internet of Things (IoT) have prompted researchers to take the next step and expand the available predictive models toward digital twins (DT). However, there is still a lack of an applicable framework for a DT of infrastructure systems in the face of disasters. In this paper, a novel DT framework of the EPN when subjected to hurricanes is proposed that combines physics-based and data-driven models while also employing a dynamic Bayesian network (DBN). The DBN can be updated in near real-time via data sensing to provide a DT that is simple, computationally feasible, scalable, and capable of modeling and estimating the failure and performance states of the various elements of the EPN. The proposed DT framework is applied to Galveston Island's EPN, and the results are validated using historical data, demonstrating that the DT can produce detailed and highly accurate estimations to be used in decision-making for community resilience planning.}
}
@article{BOUKREDERA2023107035,
title = {Enhancing the drilling efficiency through the application of machine learning and optimization algorithm},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {107035},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107035},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623012198},
author = {Farouk Said Boukredera and Mohamed Riad Youcefi and Ahmed Hadjadj and Chinedu Pascal Ezenkwu and Vahid Vaziri and Sumeet S. Aphale},
keywords = {Drilling optimization, Drilling vibrations, Artificial neural network, Machine learning, Rate of penetration},
abstract = {This article presents a novel Artificial Intelligence (AI) workflow to enhance drilling performance by mitigating the adverse impact of drill-string vibrations on drilling efficiency. The study employs three supervised machine learning (ML) algorithms, namely the Multi-Layer Perceptron (MLP), Support Vector Regression (SVR), and Regression Decision Tree (DTR), to train models for bit rotation (Bit RPM), rate of penetration (ROP), and torque. These models combine to form a digital twin for a drilling system and are validated through extensive cross-validation procedures against actual drilling parameters using field data. The combined SVR - Bit RPM model is then used to categorize torsional vibrations and constrain optimized parameter selection using the Particle Swarm Optimization block (PSO). The SVR-ROP model is integrated with a PSO under two constraints: Stick Slip Index (SSI<0.05) and Depth of Cut (DOC<5 mm) to further improve torsional stability. Simulations predict a 43% increase in ROP and torsional stability on average when the optimized parameters WOB and RPM are applied. This would avoid the need to trip in/out to change the bit, and the drilling time can be reduced from 66 to 31 h. The findings of this study illustrate the system's competency in determining optimal drilling parameters and boosting drilling efficiency. Integrating AI techniques offers valuable insights and practical solutions for drilling optimization, particularly in terms of saving drilling time and improving the ROP, which increases potential savings.}
}
@article{ZHU2023780,
title = {A reduced order model based on adaptive proper orthogonal decomposition incorporated with modal coefficient learning for digital twin in process industry},
journal = {Journal of Manufacturing Processes},
volume = {102},
pages = {780-794},
year = {2023},
issn = {1526-6125},
doi = {https://doi.org/10.1016/j.jmapro.2023.07.061},
url = {https://www.sciencedirect.com/science/article/pii/S1526612523007454},
author = {Xiaoyang Zhu and Yangjian Ji},
keywords = {Reduced order model, Adaptive proper orthogonal decomposition, Modal coefficient learning, Digital twin, Process industry},
abstract = {The digital twin (DT) technology provides a viable and promising direction for improving the level of the production status monitoring and the overall product quality in various fields. However, the accuracy of working condition identification, the timeliness of process adjustment, and the stability of product quality are put forward higher requirements in the process industry, which is characterized by nonlinear, large-scale, and dynamic complex systems. Therefore, it still remains a tricky challenge to construct and maintain an effective and accurate DT model in the process industry. A reduced order model (ROM) with the adaptive updating ability is proposed. The adaptive proper orthogonal decomposition (APOD) is adopted to achieve the continuous iteration and the adaptive optimization of the reduced basis set. Correspondingly, an adaptive learning algorithm based on the least squares support vector regression (LS-SVR) is developed to quickly obtain the modal coefficients and effectively circumvent the prohibitively high computational cost. In this way, the physical field of interest is expressed in a low-dimensional approximation with a high accuracy. The effectiveness of the method is verified by a case study in the process industry. Results show that the proposed model displays a high-precision fitting and a significant time saving for the full order model (FOM).}
}
@article{EUGENE2023108430,
title = {Learning and optimization under epistemic uncertainty with Bayesian hybrid models},
journal = {Computers & Chemical Engineering},
volume = {179},
pages = {108430},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108430},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423003009},
author = {Elvis A. Eugene and Kyla D. Jones and Xian Gao and Jialu Wang and Alexander W. Dowling},
keywords = {Bayesian inference, Optimization under uncertainty, Grey-box modeling, Digital twins},
abstract = {Hybrid (i.e., grey-box) models are a powerful and flexible paradigm for predictive science and engineering. Grey-box models use data-driven constructs to incorporate unknown or computationally intractable phenomena into glass-box mechanistic models. The pioneering work of statisticians Kennedy and O’Hagan introduced a new paradigm to quantify epistemic (i.e., model-form) uncertainty. While popular in several engineering disciplines, prior work using Kennedy–O’Hagan hybrid models focuses on prediction with accurate uncertainty estimates. This work demonstrates computational strategies to deploy Bayesian hybrid models for optimization under uncertainty. Specifically, the posterior distributions of Bayesian hybrid models provide a principled uncertainty set for stochastic programming, chance-constrained optimization, or robust optimization. Through two illustrative case studies, we demonstrate the efficacy of hybrid models, composed of a structurally inadequate glass-box model and Gaussian process bias correction term, for decision-making using limited training data. From these case studies, we develop recommended best practices and explore the trade-offs between different hybrid model architectures.}
}
@article{SARKAR2023,
title = {Cyber-agricultural systems for crop breeding and sustainable production},
journal = {Trends in Plant Science},
year = {2023},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2023.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1360138523002637},
author = {Soumik Sarkar and Baskar Ganapathysubramanian and Arti Singh and Fateme Fotouhi and Soumyashree Kar and Koushik Nagasubramanian and Girish Chowdhary and Sajal K. Das and George Kantor and Adarsh Krishnamurthy and Nirav Merchant and Asheesh K. Singh},
keywords = {cyber-agricultural systems, precision agriculture, smart farming, digital twins, ubiquitous sensing, cyberinfrastructure},
abstract = {The cyber-agricultural system (CAS) represents an overarching framework of agriculture that leverages recent advances in ubiquitous sensing, artificial intelligence, smart actuators, and scalable cyberinfrastructure (CI) in both breeding and production agriculture. We discuss the recent progress and perspective of the three fundamental components of CAS – sensing, modeling, and actuation – and the emerging concept of agricultural digital twins (DTs). We also discuss how scalable CI is becoming a key enabler of smart agriculture. In this review we shed light on the significance of CAS in revolutionizing crop breeding and production by enhancing efficiency, productivity, sustainability, and resilience to changing climate. Finally, we identify underexplored and promising future directions for CAS research and development.}
}
@article{MENG2023111870,
title = {Digital twin for intelligent probabilistic short term load forecasting in solar based smart grids using shark algorithm},
journal = {Solar Energy},
volume = {262},
pages = {111870},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.111870},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23005030},
author = {Fantuo Meng and Xianchang Wang},
keywords = {Probabilistic prediction model, Solar based smart grids, Real-time data prediction, Real-time digital twin simulation, Shark optimization},
abstract = {This article proposes a novel evolving based prediction model for the accurate short term load forecasting in solar based smart grids. The proposed method uses a probabilistic method for uncertainty quantization to make sure that the maximum modeling of the prediction interval would be achieved in a renewable based environment. In this regard, the innovative lower upper bound estimation method (LUBE) is trained using the real-time data of the smart grid gathered by the digital twin of the system. This would result in much higher results due to the avoidance of malfunction of the smart metering devices located within the smart grid. Digital twin can help predict the load demand in solar based smart grids by using machine learning algorithms to analyze the data from the smart grid. This data can be used to create a model of the system and predict how the load demand will change based on different factors such as weather, time of day, and season. By understanding the load demand, solar based smart grids can better manage their energy resources and optimize the performance of the system. In order to improve the model performance, white shark optimization algorithm (WSOA) is used as the trainer of the prediction model in a heuristic environment. The results advocate the high accuracy and reliability of the proposed method on practical dataset,}
}
@article{ZHAO2023100477,
title = {Statistical learning prediction of fatigue crack growth via path slicing and re-weighting},
journal = {Theoretical and Applied Mechanics Letters},
volume = {13},
number = {6},
pages = {100477},
year = {2023},
issn = {2095-0349},
doi = {https://doi.org/10.1016/j.taml.2023.100477},
url = {https://www.sciencedirect.com/science/article/pii/S209503492300048X},
author = {Yingjie Zhao and Yong Liu and Zhiping Xu},
keywords = {Fatigue crack growth, Structural health monitoring, Statistical noises, Rare events, Digital libraries},
abstract = {Predicting potential risks associated with the fatigue of key structural components is crucial in engineering design. However, fatigue often involves entangled complexities of material microstructures and service conditions, making diagnosis and prognosis of fatigue damage challenging. We report a statistical learning framework to predict the growth of fatigue cracks and the life-to-failure of the components under loading conditions with uncertainties. Digital libraries of fatigue crack patterns and the remaining life are constructed by high-fidelity physical simulations. Dimensionality reduction and neural network architectures are then used to learn the history dependence and nonlinearity of fatigue crack growth. Path-slicing and re-weighting techniques are introduced to handle the statistical noises and rare events. The predicted fatigue crack patterns are self-updated and self-corrected by the evolving crack patterns. The end-to-end approach is validated by representative examples with fatigue cracks in plates, which showcase the digital-twin scenario in real-time structural health monitoring and fatigue life prediction for maintenance management decision-making.}
}
@article{YU2023101599,
title = {Sensor-based indoor air temperature prediction using deep ensemble machine learning: An Australian urban environment case study},
journal = {Urban Climate},
volume = {51},
pages = {101599},
year = {2023},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2023.101599},
url = {https://www.sciencedirect.com/science/article/pii/S2212095523001931},
author = {Wenhua Yu and Bahareh Nakisa and Emran Ali and Seng W. Loke and Svetlana Stevanovic and Yuming Guo},
keywords = {Deep ensemble machine learning, Indoor temperature, Low-cost air quality sensors, Urban environment, Australia},
abstract = {Accurate prediction of indoor temperature is critical for climate change adaptation and occupant health. The aim of this study is to investigate an improved deep ensemble machine learning framework (DEML), by adjusting the model architecture with several machine learning (ML) and deep learning (DL) approaches to forecast the sensor-based indoor temperature in the Australian urban environment. We collected ambient station-based temperatures, satellite-based outdoor climate characteristics, and low-cost sensor-based indoor environmental metrics from 96 devices from August 2019 to November 2022, and established DEML with a rolling windows approach to assess the prediction stability over time. The DEML model was compared with several benchmark models, including Random Forest (RF), Support Vector Machine (SVM), eXtreme Gradient Boosting (XGboost), Long-short term memory (LSTM), and Super Learner model (SL). A total of 13,715 days [median: 341 days; IQR (the interquartile range): 221–977 days] of low-cost sensor-based indoor temperature were included in 25 commercial and residential buildings across eight cities. The prediction performance of DEML was superior to the other five benchmark models in most of the sensors [coefficients of determination (R2) of 0.861–0.990 and root mean square error (RMSE) of 0.125–0.886 °C], followed by RF and SL algorithms. DEML consistently achieved high accuracy across different climate zones, seasons, and building types, which could be used as a crucial tool for optimizing energy use, maintaining occupant comfort and health, and adapting to the impacts of climate change.}
}
@article{WANG2023101689,
title = {Security in defect detection: A new one-pixel attack for fooling DNNs},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101689},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101689},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002434},
author = {Pengchuan Wang and Qianmu Li and Deqiang Li and Shunmei Meng and Muhammad Bilal and Amrit Mukherjee},
keywords = {Industry 5.0, Endogenous safety, Zero-defect production, Defect detection, Network equipment, Deep neural network, TLMFO, One-pixel attack},
abstract = {The Industrial 5.0 Model integrates enabling technologies such as deep learning, digital twins, and the meta-universe with new development concepts. However, model and data security may pose challenges for developing zero-defect production and other industrial manufacturing industries. To address this issue, we generate adversarial examples using a one-pixel attack in adversarial machine learning, which can fool the defect detection classification model. The traditional one-pixel attack based on the Differential Evolution (DE) algorithm has limited global search ability. Therefore, we use a novel algorithm called Teaching and Learning-based Moth-Flame Optimization (TLMFO), which enhances the global search performance and improves the attack effectiveness. We evaluate TLMFO on benchmark functions and attacks on Cifar10 and ImageNet datasets, and compare it with MFO and DE. The results show that TLMFO outperforms both MFO and DE in terms of accuracy and speed of convergence. Moreover, TLMFO achieves notably better attack effectiveness than DE under targeted and untargeted attacks on the Cifar10 dataset and under-targeted attacks on the ImageNet dataset. Our research confirms that safety prevention is a link worth considering in developing Industry 5.0.}
}
@article{YANG2023104502,
title = {Intelligent optimization strategy for electrochemical removal of ammonia nitrogen by neural network embedded in a non-dominated sorting genetic algorithm},
journal = {Journal of Water Process Engineering},
volume = {56},
pages = {104502},
year = {2023},
issn = {2214-7144},
doi = {https://doi.org/10.1016/j.jwpe.2023.104502},
url = {https://www.sciencedirect.com/science/article/pii/S221471442301022X},
author = {Zhengwu Yang and Peng Chen and Guangyuan Meng and Xinwan Zhang and Yaqi Shi and Wenzhao Fu and Huawei Hu and Lehua Zhang},
keywords = {Ammonia nitrogen, Electrochemistry, Backpropagation neural network, Non-dominated sorted genetic algorithm-II, Multi-objective optimization},
abstract = {Electrochemical is a promising approach for the removal of ammonia nitrogen, but the challenge is to achieve better performance under lower energy consumption. In this study, a new electrochemical system hybrid with intelligent optimization algorithms was developed for the efficient removal of ammonia nitrogen and energy saving. Ammonia removal performance and energy consumption were recorded when the traditional electrochemical system operated at various parameters. As the premise of model training, the data were processed through Scatter diagram matrix, Box plot, Principal Component Analysis, Spearman correlation and Shapley Additive Explanations to evaluate the redundancy and independence of parameters. Backpropagation neural network based on deep learning was used as surrogate model of non-dominated sorted genetic algorithm-II, meanwhile, the range of electrochemical parameters was used as the constraint for multi-objective optimization. The optimized result is the Pareto front and the optimal solution was obtained by combining the Technique for Order Preference by Similarity to an Ideal Solution. This new hybrid system achieved an increase of 11.75 % ~ 13.61 % in ammonia removal and a reduction of 21.31 % ~ 36.84 % in energy consumption. The optimal solution represents a better ammonia removal performance at low energy consumption, which is meaningful for the concept of a real-time controlled electrochemical system.}
}
@article{ZHU2023,
title = {Mastering air combat game with deep reinforcement learning},
journal = {Defence Technology},
year = {2023},
issn = {2214-9147},
doi = {https://doi.org/10.1016/j.dt.2023.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S2214914723002349},
author = {Jingyu Zhu and Minchi Kuang and Wenqing Zhou and Heng Shi and Jihong Zhu and Xu Han},
keywords = {Air combat, MCLDPPO, Interruption mechanism, Digital twin, Distributed system},
abstract = {Reinforcement learning has been applied to air combat problems in recent years, and the idea of curriculum learning is often used for reinforcement learning, but traditional curriculum learning suffers from the problem of plasticity loss in neural networks. Plasticity loss is the difficulty of learning new knowledge after the network has converged. To this end, we propose a motivational curriculum learning distributed proximal policy optimization (MCLDPPO) algorithm, through which trained agents can significantly outperform the predictive game tree and mainstream reinforcement learning methods. The motivational curriculum learning is designed to help the agent gradually improve its combat ability by observing the agent's unsatisfactory performance and providing appropriate rewards as a guide. Furthermore, a complete tactical maneuver is encapsulated based on the existing air combat knowledge, and through the flexible use of these maneuvers, some tactics beyond human knowledge can be realized. In addition, we designed an interruption mechanism for the agent to increase the frequency of decision-making when the agent faces an emergency. When the number of threats received by the agent changes, the current action is interrupted in order to reacquire observations and make decisions again. Using the interruption mechanism can significantly improve the performance of the agent. To simulate actual air combat better, we use digital twin technology to simulate real air battles and propose a parallel battlefield mechanism that can run multiple simulation environments simultaneously, effectively improving data throughput. The experimental results demonstrate that the agent can fully utilize the situational information to make reasonable decisions and provide tactical adaptation in the air combat, verifying the effectiveness of the algorithmic framework proposed in this paper.}
}
@article{CASTRO2023107009,
title = {Digital twin framework using agent-based metaheuristic optimization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {107009},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107009},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623011934},
author = {Brenno Moura Castro and Marcelo de Miranda Reis and Ronaldo Moreira Salles and Ulisses A. Monteiro and Ricardo H.R. Gutiérrez},
keywords = {Catamaran, FE model updating, Multi-agent models, Vibration, Modal parameters},
abstract = {Despite advances in instrumentation and measurement techniques, it is still necessary to update numerical models to simulate or predict some structural responses, for example. Thus, this work proposes a metaheuristic framework based on hybrid agents, an approach within the Artificial Intelligence (AI) topics for updating Finite Element (FE) numerical models. This framework aims to provide flexible non-deterministic strategies to guide the updating process, ranging from simple local search procedures to complex learning processes. Two case studies are presented: (i) a free–free aluminium beam tested under laboratory conditions and; (ii) a catamaran tested during a sea trial under real operating conditions. The updating process aimed to optimize the stiffness matrix while maintaining the mass matrix unchanged. The objective function seeks to minimize the differences between numerical and experimental modal parameters, namely, natural frequencies and vibration modes. Results from the digital twin framework showed that the difference in natural frequencies significantly decreased, for example, 9% to 1% for the free–free aluminium beam and 15% to 4% for the catamaran’s main deck, when comparing the experimental with the updated FE model. As for the updated FE vibration modes, the Modal Assurance Criteria (MAC) values decreased slightly in both cases but within the acceptable MAC values (above 0.9), thus showing good consistency with the experimental vibration modes. In the end, the proposed framework was able to update the FE model directly using its respective reduced model, circumventing the”black box” of commercial packages.}
}
@article{KHAN2023100890,
title = {A precision-centric approach to overcoming data imbalance and non-IIDness in federated learning},
journal = {Internet of Things},
volume = {23},
pages = {100890},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100890},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523002135},
author = {Anam Nawaz Khan and Atif Rizwan and Rashid Ahmad and Qazi Waqas Khan and Sunhwan Lim and Do Hyeun Kim},
keywords = {Parameter aggregation, OOD data, Fair federated learning, Thermal comfort},
abstract = {Federated learning (FL) enables decentralized model training, but the distribution of data across devices presents significant challenges to global model convergence. Existing approaches risk losing the representativeness of local models after model aggregation, calling for a more efficient and robust solution. In this study, we address the model aggregation challenge within FL by focusing on elevating the performance of the global model amidst class imbalance and non-independent, non-identically distributed data. We aim to train a global model collaboratively that represents all participating nodes, promoting fairness and ensuring adequate representation of all classes in the model. We propose redistributing local model weights based on their precision-based contributions to each class to enhance the performance and communication efficiency of federated thermal comfort prediction. Our proposed method can assist in allocating more resources and attention to nodes with high precision for underrepresented classes, thereby improving the global model overall performance and fairness. Furthermore, our framework leverages the virtualization capability of digital-twin to enable the dynamic registration and participation of nodes in the federated learning process in real-time. The developed digital-twin framework allows for real-time monitoring and control of the decentralized training. Through our evaluation on a real dataset, we showcase noteworthy enhancements in accuracy and communication efficiency when compared to existing methods. Our evaluation shows that the proposed Class Precision-Weighted Aggregation technique (Fed-CPWA) outperforms Federated Averaging, with higher accuracy of 82.85% and lower communication costs by 25%. Our contribution represents a significant stride towards sustainable thermal comfort modeling, further advancing the development of equitable and resilient federated learning techniques.}
}
@article{LI2023109498,
title = {Dynamic scheduling of multi-memory process flexible job shop problem based on digital twin},
journal = {Computers & Industrial Engineering},
volume = {183},
pages = {109498},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109498},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223005223},
author = {Zhi Li and Yingjian Chen},
keywords = {Digital twin workshop, Multi-memory process, Flexible manufacturing system, Dynamic scheduling},
abstract = {Digital twin is one of the newly-emerged enabling technologies for achieving intelligent manufacturing. Based on the physical–digital convergence, digital twin provides manufacturing systems with a new model of collaboration between the workforce and industrial processes. With the characteristics of real-time communication and data-driven enablers, the digital twin scheduling strategy requires close cooperation between workers, systems and processes. However, in the process of digitization and intelligentization, industry will need to face the challenge of supporting new technologies and worker skills development. To this end, the paper considers workers’ multi-memory process (learning and forgetting) in the flexible job shop scheduling problem (MPFJSP). Meanwhile, the dynamic scheduling strategy of the digital twin-driven MPFJSP is proposed under machine breakdowns aiming at simultaneously minimizing the makespan, total carbon emissions, total production cost and product quality stability. A virtual workshop is adopted to simulate and optimize the dynamic scheduling scheme to realize intelligent workshop scheduling. Finally, a computational experiment is carried out to verify the effectiveness and advantages of the proposed intelligent scheduling strategy.}
}
@article{YUAN20232481,
title = {In situ characterization techniques and methodologies for high-temperature electrochemistry},
journal = {Chem},
volume = {9},
number = {9},
pages = {2481-2508},
year = {2023},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2023.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S2451929423003236},
author = {Rui Yuan and Handong Jiao and Hongmin Zhu and Daining Fang and Shuqiang Jiao},
keywords = {high-temperature electrochemistry,  study, methodology},
abstract = {Summary
High-temperature electrochemistry (HTE) plays an important role in basic industries, including metallurgy and energy and future frontier technologies such as deep space exploration and carbon neutralization. At present, traditional research methods based on ex situ, macroscale characterization, and theory analogy with room-temperature electrochemistry cannot meet the demands of researchers. In situ characterization techniques can monitor real-time information, which can establish a full spatiotemporal, multidimensional, cross-scale methodology with the support of theoretical calculations, numerical simulations, and so on. It is hence of urgency to summarize and outlook the in situ characterization techniques and analytical methods of HTE. This review focuses on the in situ characterization techniques in HTE and points to a future direction in order to construct a methodology for HTE research. We therefore call on more researchers to enter the field of HTE in order to drive HTE engineering to realize low-carbon, high efficiency, refinement, and intelligence.}
}
@article{JAMSHIDI2023110798,
title = {Metaverse and microorganism digital twins: A deep transfer learning approach},
journal = {Applied Soft Computing},
volume = {147},
pages = {110798},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110798},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623008165},
author = {Mohammad (Behdad) Jamshidi and Saleh Sargolzaei and Salimeh Foorginezhad and Omid Moztarzadeh},
keywords = {Artificial intelligence, Bacteria species, Convolutional neural network, Digital twin, Deep learning, Transfer learning, Image processing, Machine vision, Machine learning, Microorganism, Metaverse},
abstract = {Preparing the infrastructure for analyzing, recognizing, and characterizing microorganisms in the Metaverse can transform the fields of biology, medicine, and drug discovery. Accordingly, the realization of digital twins of microorganisms, such as viruses, fungi, algae, bacteria, protozoa, archaea, and multicellular animal parasites, can streamline the applicability of the Metaverse and similar emerging technologies like Cyber-Physical Healthcare Systems (CPHS). This is why a new approach to the digital twinning of bacteria has been presented in this research. This method of digital twinning can revolutionize the research and study of bacteria because it allows us to separate useful and harmful bacterial species, increasing the efficiency of treatment noticeably. This innovative method can easily be used by clinics, medical centers, or even by private users physically and virtually and can be adapted to every centralized or decentralized Metaverse Platform. To determine the proper treatment, biologists have always tried to identify the correct bacterial species that prompted a bacterial infection. They use various indicators, such as the bacterial cell’s shape and the size of the colony formed by the bacteria, to classify different types of bacteria with different biochemistries and shapes. However, it is challenging because of the extensive similarities between some species. For instance, such similarities exist between Staphylococcus aureus and Staphylococcus saprophyticus, which has caused numerous false diagnostic reports by operators. Wrong species reports bring about treatment failure and increase antibiotic resistance issues. Therefore, the digital twins of bacteria cover all of their identifiable characteristics and overcome many limitations to study them. In this approach, DTL techniques, including MobileNetV2, EfficientNetV2-S, and ResNet-50, have been employed to build digital twins of bacteria species. A hybrid dataset was used for training and evaluation. Among the models, EfficientNetV2-S exhibited the best performance, with a validation accuracy of 99.58% and a test accuracy of 99.33%. The results showed the ability of deep learning models to make bacteria digital twins based on image processing from different labs, thus assisting experts in speeding up the process and reducing diagnostic errors. In addition, in the mispredicted cases, the correct species was among the first three choices of the model. Therefore, not only can experts use DTL approaches to speed up the digital twin realization of microorganisms in the Metaverse, but they can also use these methods to reduce diagnostic errors.}
}
@article{HASSAN2023101728,
title = {SMFSOP: A semantic-based modelling framework for student outcome prediction},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101728},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101728},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002823},
author = {Yomna M.I. Hassan and Abeer Elkorany and Khaled Wassif},
keywords = {Semantic similarity, Semantic student modeling, Outcome prediction, Regression, Classification, Attribute reduction},
abstract = {Over the past two decades, studying the various factors affecting student performance became essential. Knowing these factors assist in enhancing student’s performance, teaching practices and policy decisions. This research proposes a framework named “Semantic-based Modeling Framework for Student Outcome Prediction” (SMFSOP), to automatically map students’ activities within their learning environment to a standardized behavioral model (Community of Inquiry model (CoI)). The generated student representation is utilized to cluster students and predict an outcome based on their cluster. The framework is divided into three phases: Data gathering and pre-processing, automated mapping, clustering and prediction. The automatic mapping uses semantic similarity between student attribute names/descriptions, and CoI model indicators. Path and BERT similarities were identified as the best performers compared to human annotators. K-means, DBSCAN, and Kernel K-means are used for the clustering step, followed by LassoCV for regression-based prediction, & K-nearest neighbors for classification-based prediction. In order to prove that the proposed framework is generally applicable, three real life datasets were used as a case study. Best-performing trials enhanced outcome prediction as follows: In StudentLife Dataset, Adjusted R2 is enhanced by 3% (95% to 98%), and MSE decreased by 2.375 % (0.126 to 0.031). In social network dataset, Adjusted R2 was enhanced by 17% (65% to 82%). The MSE decreased by 4.4% (0.164 to 0.12). For the “Open university learning Analytics dataset” (OULAD), accuracy is improved by 1.56%, F1-score enhanced by 0.014. Precision is enhanced by 3.1%.}
}
@article{ZHAO2023107284,
title = {MSKD: Structured knowledge distillation for efficient medical image segmentation},
journal = {Computers in Biology and Medicine},
volume = {164},
pages = {107284},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107284},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523007497},
author = {Libo Zhao and Xiaolong Qian and Yinghui Guo and Jiaqi Song and Jinbao Hou and Jun Gong},
keywords = {Knowledge distillation, Medical image segmentation, Lightweight neural networks, Deep learning, Teacher-student model, Feature filtering distillation, Region graph distillation},
abstract = {In recent years, deep learning has revolutionized the field of medical image segmentation by enabling the development of powerful deep neural networks. However, these models tend to be complex and computationally demanding, posing challenges for practical implementation in clinical settings. To address this issue, we propose an efficient structured knowledge distillation framework that leverages a powerful teacher network to assist in training a lightweight student network. Specifically, we propose the Feature Filtering Distillation method, which focuses on transferring region-level semantic information while minimizing redundant information transmission from the teacher to the student network. This approach effectively mitigates the problem of inaccurate segmentation caused by similar internal organ characteristics. Additionally, we propose the Region Graph Distillation method, which exploits the higher-order representational capabilities of graphs to enable the student network to better imitate structured semantic information from the teacher. To validate the effectiveness of our proposed methods, we conducted experiments on the Synapse multi-organ segmentation and KiTS kidney tumor segmentation datasets using various network models. The results demonstrate that our method significantly improves the segmentation performance of lightweight neural networks, with improvements of up to 18.56% in Dice coefficient. Importantly, our approach achieves these improvements without introducing additional model parameters. Overall, our proposed knowledge distillation methods offer a promising solution for efficient medical image segmentation, empowering medical experts to make more accurate diagnoses and improve patient treatment.}
}
@article{SEMERARO2023128699,
title = {Guidelines for designing a digital twin for Li-ion battery: A reference methodology},
journal = {Energy},
volume = {284},
pages = {128699},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.128699},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223020935},
author = {Concetta Semeraro and Haya Aljaghoub and Mohammad Ali Abdelkareem and Abdul Hai Alami and Michele Dassisti and A.G. Olabi},
keywords = {Digital twin, Lithium battery, Battery energy storage systems, Unsupervised machine learning, Formal concept analysis},
abstract = {The integration of digital technologies is causing a significant change in the energy sector. These innovations have transformed traditional energy grids into intelligent grids. As a result, the digital replica of Battery Energy Storage Systems (BESS) has become one of the most crucial components in the energy sector. Digital twin technology enables the seamless integration of BESS into intelligent grids and offers numerous benefits, such as easy identification and prediction of faults, real-time system monitoring, optimization, temperature regulation, and estimation of parameters. As a result, the overall performance of BESS is improved by the digital twin technology. Consequently, this paper discusses the general guidelines that must be followed to develop a digital twin for a Li-ion BESS successfully. The main function is to define how to design a digital twin able to optimize the system and facilitate early and predictive fault detection and diagnosis.}
}
@article{CARLO2023474,
title = {The importance of cybersecurity frameworks to regulate emergent AI technologies for space applications},
journal = {Journal of Space Safety Engineering},
volume = {10},
number = {4},
pages = {474-482},
year = {2023},
issn = {2468-8967},
doi = {https://doi.org/10.1016/j.jsse.2023.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468896723000678},
author = {Antonio Carlo and Nebile Pelin Mantı and Bintang Alam Semesta W．A．M and Francesca Casamassima and Nicolò Boschetti and Paola Breda and Tobias Rahloff},
keywords = {Cybersecurity, Artificial intelligence, Earth observation, Cyber risk, Emerging disruptive technologies},
abstract = {Over the past decades, industries and governments have progressively been relying upon space data-centric and data-dependant systems. This led to the emergence of malicious activities, also known as cyber-threats, targeting such systems. To counter these threats, new technologies such as Artificial Intelligence (AI) have been implemented and deployed. Today, AI is highly capable of delivering fast, precise, and reliable command-and-control decision-making, as well as providing reliable vulnerability analysis using well-proven cutting-edge techniques, at least when applied to terrestrial applications. In fact, this might not yet be the case when used for space applications. AI can also play a transformative and important role in the future of space cybersecurity, and it poses questions on what to expect in the near-term future. Challenges and opportunities deriving from the adoption of AI-based solutions to achieve cybersecurity and later cyber defence objectives in both civil and military operations require rethinking of a new framework and new ethical requirements. In fact, most of these technologies are not designed to be used or to overcome challenges in space. Because of the highly contested and congested environment, as well as the highly interdisciplinary nature of threats to AI and Machine Learning (ML) technologies, including cybersecurity issues, a solid and open understanding of the technology itself is required, as well as an understanding of its multidimensional uses and approaches. This includes the definition of legal and technical frameworks, ethical dimensions and other concerns such as mission safety, national security, and technology development for future uses. The continuous endeavours to create a framework and regulate interdependent uses of combined technologies such as AI and cybersecurity to counter “new” threats require the investigation and development of “living concepts” to determine in advance the vulnerabilities of networks and AI. This paper defines a cybersecurity risk and vulnerability taxonomy to enable the future application of AI in the space security field. Moreover, it assesses to what extent a network digital twins’ simulation can still protect networks against relentless cyber-attacks in space against users and ground segments. Both concepts are applied to the case study of Earth Observation (EO) operations, which allows for conclusions to be drawn based on the business impact (reputational, environmental, and social) of a cyber malicious activity. Since AI technologies are developing on a daily basis, a regulatory framework is proposed using ethical and technical approaches for this technology and its use in space.}
}
@article{LI2023107327,
title = {Cross-user gesture recognition from sEMG signals using an optimal transport assisted student-teacher framework},
journal = {Computers in Biology and Medicine},
volume = {165},
pages = {107327},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107327},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523007928},
author = {Xinhui Li and Xu Zhang and Xiang Chen and Xun Chen and Aiping Liu},
keywords = {Myoelectric interfaces, Gestural recognition, Cross-user variability, Domain adaptation, Student-teacher framework},
abstract = {The cross-user gesture recognition is a puzzle in the myoelectric control system, owing to great variability in muscle activities across different users. To address this problem, a novel optimal transport (OT) assisted student-teacher (ST) framework (termed OT-ST) was proposed in this paper to facilitate transfer across user domains in an unsupervised domain adaptation (UDA) manner. In this framework, the initial parameters of the ST models were trained with the labeled data from users in the source domain. In the model transfer stage for a new user in the target domain, the teacher model was utilized to generate pseudo labels for unlabeled testing samples, providing guidance to the adaptation of the student model. The OT algorithm was employed to optimize the pseudo labels generated from the teacher model, avoiding the model bias and further improving the effect of domain adaptation. The performance of the proposed OT-ST framework was evaluated via experiments of classifying seven hand gestures using high-density surface electromyogram (HD-sEMG) recordings from extensor digitorum muscles of eight intact-limbed subjects. The OT-ST framework yielded a high accuracy of 96.50 ± 2.88% for new users, and outperformed other common machine learning and UDA methods significantly (p < 0.01), demonstrating its effectiveness. The OT-ST framework does not require special repetitive training or any labeled data for calibration. In addition, it can incrementally learn from new testing samples and improve the recognition ability. This study provides a promising method for developing user-generic myoelectric pattern recognition, with wide applications in human-computer interaction, consumer electronics and prosthesis control.}
}
@article{DENG2023104955,
title = {Deep reinforcement learning for fuel cost optimization in district heating},
journal = {Sustainable Cities and Society},
volume = {99},
pages = {104955},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104955},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723005668},
author = {Jifei Deng and Miro Eklund and Seppo Sierla and Jouni Savolainen and Hannu Niemistö and Tommi Karhela and Valeriy Vyatkin},
keywords = {Deep reinforcement learning, Digital twin, District heating, Setpoint optimization},
abstract = {This study delves into the application of deep reinforcement learning (DRL) frameworks for optimizing setpoints in district heating systems, which experience hourly fluctuations in air temperature, customer demand, and fuel prices. The potential for energy conservation and cost reduction through setpoint optimization, involving adjustments to supply temperature and thermal energy storage utilization, is significant. However, the inherent nonlinear complexities of the system render conventional manual methods ineffective. To address these challenges, we introduce a novel learning framework with an expert knowledge module tailored for DRL techniques. The framework leverages system status information to facilitate learning. The training is performed by employing model-free DRL methods and a refined digital twin of the Espoo district heating system. The expert module, accounting for power plant capacities, ensures actionable directives aligned with operational feasibility. Empirical validation through comprehensive simulations demonstrates the efficacy of the proposed approach. Comparative analyses against manual methods and evolutionary techniques highlight the approach's superior ability to curtail fuel costs. This study advances the understanding of DRL in district heating optimization, offering a promising avenue for enhanced energy efficiency and cost savings.}
}
@article{CHEN2023103522,
title = {Building and road detection from remote sensing images based on weights adaptive multi-teacher collaborative distillation using a fused knowledge},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {124},
pages = {103522},
year = {2023},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2023.103522},
url = {https://www.sciencedirect.com/science/article/pii/S1569843223003461},
author = {Ziyi Chen and Liai Deng and Jing Gou and Cheng Wang and Jonathan Li and Dilong Li},
keywords = {Knowledge distillation, Remote sensing, Building extraction, Road extraction},
abstract = {Knowledge distillation is one effective approach to compress deep learning models. However, the current distillation methods are relatively monotonous. There are still rare studies about the combination of distillation strategies using multiple types of knowledge and employing multiple teacher models. Besides, how to optimize the weights among different teacher models is still an open problem. To address these issues, this paper proposes a novel approach for knowledge distillation, which effectively enhances the robustness of the distilled student model by a weights adaptive multi-teacher collaborative distillation. Moreover, the proposed method utilizes feature knowledge exchange guidance between teacher networks to transfer more comprehensive feature knowledge to the student model, which further improves the learning capability of hidden layers’ details. The extensive experimental results demonstrate that the proposed method achieves state-of-the-art performance on Massachusetts Roads Dataset, LRSNY Roads Dataset, and WHU Building Dataset. Specifically, under the guidance of the first ensemble of teacher networks, we obtained IoU scores of 47.33%, 78.15%, and 80.71%, respectively. Under the guidance of the second ensemble of teacher networks, we obtained IoU scores of 48.56%, 79.51%, and 81.35%, respectively.}
}
@article{SELVARAJAN2023,
title = {PUDT: Plummeting Uncertainties in Digital Twins for Aerospace Applications using Deep Learning Algorithms},
journal = {Future Generation Computer Systems},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2300448X},
author = {Shitharth Selvarajan and Hariprasath Manoharan and Achyut Shankar and Alaa O. Khadidos and Adil O. Khadidos and Antonino galletta},
keywords = {Digital twins, Aerospace applications, Deep learning, Uncertainty},
abstract = {Identifying objects in aircraft monitoring systems poses significant challenges due to the presence of extreme loading conditions. Despite the presence of several sensor units, the transmission of precise data to multiple data units is hindered by an increase in time intervals. Therefore, the suggested methodology is specifically developed for the purpose of generating digital replicas for aeronautical applications, wherein an aero transfer function is correlated with the digital twins. Mapping functions are utilized in the monitoring of diverse parameters that are associated with the identification of objects inside data transmission networks, with the aim of minimizing uncertainty. The suggested system model is enhanced by incorporating analytical representations and deep learning methods, resulting in the provision of zero point twin functionalities. The present study investigates the aforementioned integrated procedure through the analysis of four different situations. In these settings, an aero communication tool box is employed to transform the device configuration into simulation outputs. The results obtained from the comparison of these scenarios reveal that the projected model significantly enhances the maintenance period while minimizing data errors.}
}
@article{ALRBEAWI2023100098,
title = {A Review of Modern Approaches of Digitalization in Oil and Gas Industry},
journal = {Upstream Oil and Gas Technology},
volume = {11},
pages = {100098},
year = {2023},
issn = {2666-2604},
doi = {https://doi.org/10.1016/j.upstre.2023.100098},
url = {https://www.sciencedirect.com/science/article/pii/S2666260423000130},
author = {Salam Al-Rbeawi},
keywords = {Digitization, Data Science, machine learning, Artificial Intelligence},
abstract = {This paper introduces a review of the modern approaches of digitalization in the oil and gas industry. The objective is understanding the current applications of the traditional technologies used in the petroleum industry and studying the opportunities for implementing innovative digital tools and systems that could enhance the operational efficiency and reduce the cost, risk, and environmental impacts. The paper consists of several tasks starting from reviewing the present technologies used mainly in the upstream activities: exploration, field development, drilling and completion, and operation and production. The techniques used for gathering and analyzing the big data packages received from unlimited digitalized resources is the second task covered in this paper while the computing technologies such as cloud and cognitive computing as well as the advanced analytical and numerical solutions developed by real-time algorithms is the third task. Detail information about the short- and long-term digitalization technologies in both hardware and software parts is represented in this paper such as robotics and automation, powerful sensors and measuring tools, unmanned vehicles and drones, 3D printers and wearable gadgets as well as artificial inelegance, machine learning, and digital-twin computing. Real examples of currently used digitalized applications in different upstream sections of the petroleum industry are demonstrated while the expected added values created by the digitalization for the industry and communities and the possible changes either positively or negatively in the operational costs, maintenance patterns, workforce safety, environmental impacts, job positions, and the required skills and experiences during digitalization era are explained. The outcomes are summarized in the following points. The digitalization should be the priority for most upstream companies in order to enhance and optimize the production system. The big volume of data may require innovative computing tools that could develop powerful analytical models used for calibrating the production environment and maximizing the production capacity of oil and gas fields. Data transfer and data sharing are two key factors in the digital transformation where the integrated data platforms could enhance the collaboration among the participants of the oil and gas ecosystems and accelerate innovative solutions. The digital era may lead to a sharp decline in the workforce and significant changes in the job position description as the human intervention with the production system might be minimized. The expected added value of the digital transformation during the next decades will be very big wherein the upstream petroleum industry may have the biggest share that could be more than 60% of the total investments while the midstream industry may have only 10%. There will be also a lot of challenges represented by the fact that the petroleum industry has not yet made the necessary shift in mindset to embrace the digitalization potential. Moreover, the digitalization era may require new revolutionized regulations especially in terms of sharing the data and changing the structures of the organizations as well as the workforces.}
}
@article{GAO2023111872,
title = {An optimal management architecture based on digital twin for smart solar-based islands incorporating deep learning and modified particle swarm optimization},
journal = {Solar Energy},
volume = {262},
pages = {111872},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.111872},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23005054},
author = {LiPeng Gao and Zhe Gao},
keywords = {Optimal management, Deep learning, Interruptible load, Digital twin, Power shifting, solar unit},
abstract = {Smart island (SI) energy management is a type of energy management system used to ensure that energy is used efficiently on islands. It is designed to reduce energy consumption and costs while also improving the sustainability of the island. Smart island energy management systems use a combination of technologies such as solar, wind, and other renewable energy sources, energy storage systems, smart meters, and advanced analytics to monitor and manage energy usage. The system can be used to provide islanders with real-time energy usage data, allowing them to make informed decisions about their energy use. Using the effects on the environment and economics of the SI as a basis for fully modeling the optimum performance of the SI in grid-connected operations, the feasibility and balance of the SI power grid are ensured. In addition, taking into account the impact of interruptible loads (IL) on SI operation costs, a power shift for IL can be performed using storage batteries in a digital twin environment. A new ecology-driven optimization algorithm has been developed that continuously adjusts migration rates with habitat suitability indexes of normalized individuals and adds a differential perturbation to the migration operator of the migration process. The enhanced particle swarm optimization algorithm has been implemented as the SI optimization dispatching algorithm. In order to have a precise prediction of the renewable energy sources, their output power is predicted using the recurrent neural network (RNN) deep learning model. Based on the simulation outcomes, it is evident that the suggested power dispatching model could greatly decrease the overall price of the system by implementing the advanced and effective algorithm and model presented in the study.}
}
@article{GONG2023341758,
title = {RamanCMP: A Raman spectral classification acceleration method based on lightweight model and model compression techniques},
journal = {Analytica Chimica Acta},
volume = {1278},
pages = {341758},
year = {2023},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2023.341758},
url = {https://www.sciencedirect.com/science/article/pii/S0003267023009790},
author = {Zengyun Gong and Chen Chen and Cheng Chen and Chenxi Li and Xuecong Tian and Zhongcheng Gong and Xiaoyi Lv},
keywords = {Raman spectroscopy, RamanCMP, Lightweight model, Model compression},
abstract = {In recent years, Raman spectroscopy combined with deep learning techniques has been widely used in various fields such as medical, chemical, and geological. However, there is still room for optimization of deep learning techniques and model compression algorithms for processing Raman spectral data. To further optimize deep learning models applied to Raman spectroscopy, in this study time, accuracy, sensitivity, specificity and floating point operations numbers(FLOPs) are used as evaluation metrics to optimize the model, which is named RamanCompact(RamanCMP). The experimental data used in this research are selected from the RRUFF public dataset, which consists of 723 Raman spectroscopy data samples from 10 different mineral categories. In this paper, 1D-EfficientNet adapted to the spectral data as well as 1D-DRSN are proposed to improve the model classification accuracy. To achieve better classification accuracy while optimizing the time parameters, three model compression methods are designed: knowledge distillation using 1D-EfficientNet model as a teacher model to train convolutional neural networks(CNN), proposing a channel conversion method to optimize 1D-DRSN model, and using 1D-DRSN model as a feature extractor in combination with linear discriminant analysis(LDA) model for classification. Compared with the traditional LDA and CNN models, the accuracy of 1D-EfficientNet and 1D-DRSN is improved by more than 20%. The time of the distilled model is reduced by 9680.9s compared with the teacher model 1D-EfficientNet under the condition of losing 2.07% accuracy. The accuracy of the distilled model is improved by 20% compared to the CNN student model while keeping inference efficiency constant. The 1D-DRSN optimized with channel conversion method saves 60% inference time of the original 1D-DRSN model. Feature extraction reduces the inference time of 1D-DRSN model by 93% with 94.48% accuracy. This study innovatively combines lightweight models and model compression algorithms to improve the classification speed of deep learning models in the field of Raman spectroscopy, forming a complete set of analysis methods and laying the foundation for future research.}
}
@article{ZHANG2023108181,
title = {Digital twin perception and modeling method for feeding behavior of dairy cows},
journal = {Computers and Electronics in Agriculture},
volume = {214},
pages = {108181},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.108181},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923005690},
author = {Yi Zhang and Yu Zhang and Meng Gao and Baisheng Dai and Shengli Kou and Xinjie Wang and Xiao Fu and Weizheng Shen},
keywords = {Digital twin, Indoor positioning, IMUs, Dairy cow, Deep learning},
abstract = {The digital twin of cows holds significant promise for advancing animal welfare and production efficiency. This paper aims to propose an architecture for digital twins of cows that covers their entire lifecycle. A digital twin solution has been developed that utilizes indoor positioning data and inertial measurement unit (IMU) data to construct a cow’s digital shadow. As an example, the paper utilizes the classification of cow feeding and non-feeding behaviors to study the digital twin perception and modeling methods. A custom-made collar integrated with utilized ultra-wideband (UWB) chips and inertial measurement units (IMUs) was utilized to collect real-time location and neck movement data from five healthy non-lactating Holstein cows. The collected data was transmitted via UWB signals to the positioning anchor and subsequently forwarded to a local server. To classify the feeding and non-feeding behaviors of the cows, three methods were employed: Support Vector Machines (SVM), K-Nearest Neighbor (KNN), and Long Short-Term Memory (LSTM). According to the experimental results, all three classification methods were effective, however, LSTM outperformed the others. Employing solely IMU data and implementing the LSTM, the precision of identifying bovine foraging behavior reached 91.05%, with concomitant precision and recall rates of 92.23 and 91.35%, respectively. Through an integration of the data from indoor position detection and IMU devices and the employment of LSTM, the accuracy of identification increased to 94.97%, with a precision rate of 99.99% and a recall rate of 93.86%. The trial of the digital twin solution demonstrated the rationality and technical feasibility of the digital twin architecture, which holds significant reference value for the development of animal digital twins in the animal husbandry industry.}
}
@article{MAO2023108242,
title = {A teacher-to-student information recovery method toward energy-efficient animal activity recognition at low sampling rates},
journal = {Computers and Electronics in Agriculture},
volume = {213},
pages = {108242},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.108242},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923006300},
author = {Axiu Mao and Meilu Zhu and Endai Huang and Xi Yao and Kai Liu},
keywords = {Behavioral classification, Deep learning, Resampling, Reconstruction, Knowledge distillation},
abstract = {Automated animal activity recognition (AAR) has advanced greatly through recent advances in sensing technologies and deep learning, and improved livestock management efficiency, animal health, and welfare monitoring. In practical automated AAR systems where animals need to be monitored over a long period, the sampling rate dramatically affects the energy consumption and battery life of sensing devices due to continuous data collection and transmission. Considering real-world benefits, existing works have often lowered the sampling rate to reduce energy costs. However, when the sampling rate falls below a threshold, the AAR performance degrades rapidly due to many relevant signals being missed. Therefore, this study proposed a novel method, dubbed teacher-to-student information recovery (T2S-IR), to improve the performance of AAR at low sampling rates. This approach effectively leverages the knowledge obtained from high-sampling-rate data, to assist in recovering the missing information in features extracted by the classification network trained on low-sampling-rate data. The workflow of the T2S-IR contains two main steps. (1) we utilize high-sampling-rate data for training teacher classification and reconstruction networks sequentially. (2) Then, we train a student classification network using low-sampling-rate data, while promoting its performance by exploiting the knowledge learned by trained teacher networks via two novel modules, namely the reconstruction-based information recovery (RIR) module and the correlation-distillation-based information recovery (CDIR) module. Specifically, the RIR module employs the pretrained teacher reconstruction network to enforce the student classification network to learn complete and descriptive features. The CDIR module enforces the feature maps of student network to mimic internal correlations within feature maps of pretrained teacher classification network along temporal and sensor axes directions. To validate our proposed T2S-IR, we conducted experiments on two public datasets acquired for horses and goats using triaxial accelerometers and gyroscopes with an initial sampling rate of 100 Hz. Data having low sampling rates were obtained by downsampling the original data at different frequencies (i.e., 50, 25, 12.5, 10, 5, and 2 Hz). The results demonstrated that our method remarkably boosted the classification network trained on low-sampling-rate data (e.g., percentage-point increments in the precision, recall, F1-score, and accuracy of 3.33%, 3.58%, 3.45%, and 2.19%, respectively, for the 12.5-Hz horse data and 7.6%, 4.44%, 6.9%, and 0.79%, respectively, for the 5-Hz goat data) while outperforming existing knowledge distillation methods. The enhanced classification network can be directly applied in practical AAR tasks with low sampling rates, significantly beneficial for scenarios with constrained energy sources for wearable devices.}
}
@article{BRAHMBHATT2023100127,
title = {Digital twin assisted decision support system for quality regulation and leak localization task in large-scale water distribution networks},
journal = {Digital Chemical Engineering},
volume = {9},
pages = {100127},
year = {2023},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2023.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2772508123000455},
author = {Parth Brahmbhatt and Abhilasha Maheshwari and Ravindra D. Gudi},
keywords = {Water quality management, Leak detection, Digital twins, Optimization & control, Water distribution network, Neural Networks},
abstract = {Effective water resource management is essential in large metropolitan cities. Digital Twins (DT), supported by IIoT and machine learning technologies, provide opportunities for real-time prediction and optimization for effective decision-making in water distribution systems. A framework for the digital twin of the Water Distribution Network (WDN) is developed in this paper to achieve higher operational efficiency using ‘WNTR’, the Python-based library of EPANET. All computational experiments and methods were validated on the benchmark hydraulic C-TOWN network (Ostfeld et al., 2011). The hydraulic parameters and quality parameters of the DT model for the water network were calibrated using the Differential Evolution (DE) algorithm. The calibrated DT served as a real-time proxy to generate simulation data, which is used for two different applications in large-scale water networks: (i) Disinfectant dosage regulation task using booster stations and (ii) pipe leakage localization task. The calibrated DT was utilized to estimate the optimal disinfectant dosing rates, ensuring water quality control within an acceptable range using optimization. The results highlight the effectiveness of the neural network and real-time optimization strategy to achieve the optimal dosing rate. For the leakage localization task, the Graph Convolution Networks (GCN) based neural network trained on the DT was found to predict leakage location very accurately.}
}
@article{CHEN2023581,
title = {The advance of digital twin for predictive maintenance: The role and function of machine learning},
journal = {Journal of Manufacturing Systems},
volume = {71},
pages = {581-594},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S027861252300211X},
author = {Chong Chen and Huibin Fu and Yu Zheng and Fei Tao and Ying Liu},
keywords = {Digital twin, Predictive maintenance, Machine learning, Prognostic and health management},
abstract = {The recent advance of digital twin (DT) has greatly facilitated the development of predictive maintenance (PdM). DT for PdM enables accurate equipment status recognition and proactive fault prediction, enhancing reliability. This shift from reactive to proactive services optimizes maintenance schedules, minimizes downtime, and improves enterprise profitability and competitiveness. However, the research and application of DT for PdM are still in their infancy, probably because the role and function of machine learning (ML) in DT for PdM have not yet been fully investigated by the industry and academia. This paper focuses on a systematic review of the role of ML in DT for PdM and identifies, evaluates and analyses a clear and systematic approach to the published literature relevant to DT and PdM. Subsequently, the state-of-the-art applications of ML in various application areas of DT for PdM are introduced. Finally, the challenges and opportunities of ML for DT-PdM are revealed and discussed. The outcome of this paper can bring tangible benefits to the research and implementation of ML in DT-PdM.}
}
@article{ZOHDI2023116261,
title = {Machine-learning a perfect bending soccer goal shot},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {415},
pages = {116261},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116261},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523003857},
author = {T.I. Zohdi},
keywords = {Soccer, Kicks, Goal optimization, Machine-learning},
abstract = {The objective of this work is to ascertain the optimal bending kick velocity and spin that a player should impart to exactly hit a target within a soccer goal, using machine-learning optimization. Specifically, the work develops a model of a kicking player producing a high-velocity spinning soccer ball which interacts with the surrounding air that induces drag forces and the Magnus effect, both of which are functions of the Reynolds number. This yields a set of highly nonlinear, coupled, differential equations. The framework is designed to enable digital-twin type technologies, i.e. digital replicas that run in real time with the physical system on laptops or other mobile systems for rapid feedback. The overall guiding motivation is to provide a useful tool to assist coaches and to rapidly train players. Numerical examples are provided to illustrate the process.}
}
@article{MOTSA2023116912,
title = {A data-driven, machine learning scheme used to predict the structural response of masonry arches},
journal = {Engineering Structures},
volume = {296},
pages = {116912},
year = {2023},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2023.116912},
url = {https://www.sciencedirect.com/science/article/pii/S0141029623013275},
author = {Siphesihle Mpho Motsa and Georgios Ε. Stavroulakis and Georgios Α. Drosopoulos},
keywords = {FEM, Machine Learning, Artificial Neural Network, Multi-hinge failure, Damage Prediction, Masonry Arches, Data-driven Mechanics, Digital Twin},
abstract = {A data-driven methodology is proposed, for the investigation of the ultimate response of masonry arches. Aiming to evaluate their structural response in a computationally efficient framework, machine learning metamodels, in the form of artificial neural networks, are adopted. Datasets are numerically built, integrating Matlab, Python and commercial finite element software. Heyman’s assumptions are adopted within non-linear finite element analysis, incorporating contact-friction laws between adjacent stones, to capture failure in the arch. The artificial neural networks are trained, validated, and tested using the least square minimization technique. It is shown that the proposed scheme can be used to provide a fast and accurate prediction of the deformed geometry, the collapse mechanism and the ultimate load. Cases studies demonstrate the efficiency of the method in random, new arch geometries. Relevant Matlab/Python scripts and datasets are provided. The method can be extended towards structural health monitoring and the concept of digital twin.}
}
@article{CHEN2023102196,
title = {Improving completeness and accuracy of 3D point clouds by using deep learning for applications of digital twins to civil structures},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102196},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102196},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003245},
author = {Shihong Chen and Gao Fan and Jun Li},
keywords = {Deep learning, Depth completion, Digital twin, Structural modeling, 3D reconstruction},
abstract = {In the Architecture, Engineering, and Construction (AEC) sector, digital twins rely on precise 3D models to convey digital information about physical structures in a virtual space. However, due to the vulnerability to measurement errors in weak-textured regions, 3D point clouds generated by conventional photometric consistency or deep learning-based Multi-View Stereo (MVS) algorithms are often incomplete or inaccurate. Therefore, this paper integrates the consistency constraint across multiple views and the inferential capacity of deep learning to propose a novel approach for refining the missing regions of the depth maps generated by photo-consistency based MVS algorithms. The proposed solution involves a cost volume pyramid-based depth completion (CVP-DC) network with three multi-level pyramid structures, which sequentially estimates and completes depth maps in a coarse-to-fine manner. A dataset that consists of input images and the corresponding depth maps generated by photo-consistency based MVS algorithms, along with output ground truth depth maps, is developed using an open DTU MVS dataset. CVP-DC demonstrates competitive performance when tested on the public DTU MVS dataset, outperforming existing MVS algorithms in terms of both completeness and accuracy. Additionally, experimental studies are conducted utilizing UAV-collected RTK (Real-Time Kinematic) images of an outdoor bridge pier to reconstruct point clouds with absolute scales. Experimental validations demonstrate the effectiveness and applicability of the proposed approach in filling uneven and incomplete depth maps, thereby enhancing the completeness of the generated point clouds. The proposed approach holds promise for establishing precise 3D models for the digital twin of the AEC sector.}
}
@article{ESDERS2023108574,
title = {Scaling up machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points},
journal = {Computers & Chemical Engineering},
pages = {108574},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108574},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423004441},
author = {Malte Esders and Gimmy Alex Fernandez Ramirez and Michael Gastegger and Satya Swarup Samal},
keywords = {Machine learning, Surrogate model, Flowsheet simulation, End-to-end training, Cycle solving, Fixed point iteration, Model initialization, Digital twin},
abstract = {Idealized first-principles models of chemical plants can be inaccurate. An alternative is to fit a Machine Learning (ML) model directly to plant sensor data. We use a structured approach: Each unit within the plant gets represented by one ML model. After fitting the models to the data, the models are connected into a flowsheet-like directed graph. We find that for smaller plants, this approach works well, but for larger plants, the complex dynamics arising from large and nested cycles in the flowsheet lead to instabilities in the solver during model initialization. We show that a high accuracy of the single-unit models is not enough: The gradient can point in unexpected directions, which prevents the solver from converging to the correct stationary state. To address this problem, we present a way to fine-tune ML models such that initialization, even with very simple solvers, becomes robust.}
}
@article{ZHAO2023,
title = {Mixed noise-guided mutual constraint framework for unsupervised anomaly detection in smart industries},
journal = {Computer Communications},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423004723},
author = {Qing Zhao and Yan Wang and Yuxuan Lin and Shaoqi Yan and Wei Song and Boyang Wang and Jun Huang and Yang Chang and Lizhe Qi and Wenqiang Zhang},
keywords = {Anomaly detection, Mixed noise, Mutual constraint},
abstract = {Large-scale sensor and data acquisition systems, integrated with deep learning methodologies, play a pivotal role in enhancing the sustainability and security of smart city environments, exemplifying the critical significance of anomaly detection techniques. Anomaly detection in complex industrial scenarios presents various challenges, such as intricate working environments, limited anomaly samples, and lack of a priori information. Unsupervised anomaly detection based on knowledge distillation enables anomaly detection using only normal samples. However, the similarity in structure between teacher and student models, along with identical input data flow, hampers accurate anomaly detection and localization. To address these issues, we propose MNMC, an unsupervised anomaly detection model consisting of a mixed noise generation module emulating real defects, a mutual constraint module, and an anomaly segmentation module. Firstly, to enhance the student network’s ability to learn robust features, we construct a hybrid noise model comprising dead-leaves noise and perlin noise. This generates features with structural texture and distributional characteristics closer to real anomalies. Secondly, we design a mutual constraint framework to further improve the learning ability of the student network for normal features by constraining representations containing only a single noise. Lastly, for the detection of anomalies at different scales, we propose a new evaluation metric based on equal importance of normal and anomalous regions. Through ablation experiments, we demonstrate the effectiveness of the simulated real defect generation module and the mutual constraints module. Performance experiments on the MVTec dataset show that our method achieves competitive results compared to the current state-of-the-art anomaly detection methods.}
}
@article{MARU2023107520,
title = {Improved building facade segmentation through digital twin-enabled RandLA-Net with empirical intensity correction model},
journal = {Journal of Building Engineering},
volume = {78},
pages = {107520},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107520},
url = {https://www.sciencedirect.com/science/article/pii/S235271022301700X},
author = {Michael Bekele Maru and Yusen Wang and Hansun Kim and Hyungchul Yoon and Seunghee Park},
keywords = {Digital twin, RandLA-Net, Building facade, Intensity, Lidar},
abstract = {The Architectural Engineering and Construction (AEC) industry can benefit from accurate building facade segmentation, which can provide valuable insights into building maintenance, urban planning, and security efforts. Light Detection and Ranging (LiDAR) sensors are effective in recognizing building facade components from (three-dimensional) 3D point clouds by registering 3D spatial coordinates and radiometric information, which reveal the spectral property of a scanned surface. Although the radiometric information (i.e., intensity feature) can be used for segmentation, its accuracy may be reduced by factors such as scanning geometry and external factors that affect the object’s radiometric information. To address this issue, this study proposes a robust and automated method for segmenting building facade components using LiDAR point cloud data and an empirical-based intensity correction model to ensure proper segmentation. The proposed method employs RandLA-Net, a deep learning model capable of effectively processing large-scale point cloud data, to classify building facade components based on their spatial features combined with corrected intensity features. By incorporating the proposed method into a digital twin, it is possible to perform accurate building facade segmentation and generate valuable insights into the building’s physical condition, energy efficiency, and aesthetic value in real-time. The effectiveness of the proposed method was experimentally validated using a school building facade, which demonstrated significant improvements in the recognition of facade components and highlighted the potential of digital twin-enabled building facade segmentation for the AEC industry.}
}
@article{CHOWDHURY2023166432,
title = {Climate change and coastal morphodynamics: Interactions on regional scales},
journal = {Science of The Total Environment},
volume = {899},
pages = {166432},
year = {2023},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2023.166432},
url = {https://www.sciencedirect.com/science/article/pii/S004896972305057X},
author = {Piyali Chowdhury and Naresh Kumar Goud Lakku and Susana Lincoln and Jaya Kumar Seelam and Manasa Ranjan Behera},
keywords = {Coastal morphodynamics, Climate change, Coastal zone management, Coastal resilience framework, Digital Twin},
abstract = {Climate change and its impacts, combined with unchecked human activities, intensify pressures on coastal environments, resulting in modification of the coastal morphodynamics. Coastal zones are intricate and constantly changing areas, making the monitoring and interpretation of data a challenging task, especially in remote beaches and regions with limited historical data. Traditionally, remote sensing and numerical methods have played a vital role in analysing earth observation data and supporting the monitoring and modelling of complex coastal ecosystems. However, the emergence of artificial intelligence-based techniques has shown promising results, offering the additional advantage of filling data gaps, predicting data in data-scarce regions, and analysing multidimensional datasets collected over extended periods of time and larger spatial scales. The main objective of this study is to provide a comprehensive review of the existing literature, discussing both traditional methods and various emerging artificial intelligence-based approaches used in studying the coastal dynamics, shoreline change analysis, and coastal monitoring. Ultimately, the study proposes a climate resilience framework to enhance coastal zone management practices and policies, fostering resilience among coastal communities. The outcome of this study aligns with and supports particularly SDG 13 of the UN (Climate Action) and advances it by identifying relevant methods in coastal erosion studies and proposing integrated management plans informed by real-time data collection and analysis/modelling using physics-based models.}
}
@article{WILSDON2023104813,
title = {Autonomous control of heat pipes through digital twins: Application to fission batteries},
journal = {Progress in Nuclear Energy},
volume = {163},
pages = {104813},
year = {2023},
issn = {0149-1970},
doi = {https://doi.org/10.1016/j.pnucene.2023.104813},
url = {https://www.sciencedirect.com/science/article/pii/S0149197023002482},
author = {Katherine Wilsdon and Joshua Hansel and M. Ross Kunz and Jeren Browning},
keywords = {Fission battery, Digital twin, Unattended operation},
abstract = {Fission batteries are envisioned as nuclear energy systems and associated technology that can be fully utilized in a battery-like operation, where the system is delivered as a ‘plug-and-play’ service. Several key attributes define the desired functionality of these systems: economic, standardized, installed, unattended, and reliable. The construction and operation of unattended, plug-and-play fission batteries will require sufficiently robust hardware and software technologies. Using a digital twin (DT) may reduce costs and risk associated with employing fission batteries through the integration of the disparate systems used in the design, construction, and operation of these nuclear energy systems. A DT employing machine-learning (ML) and physics-based representations to forecast future performance could potentially be used for anticipatory control. Before application of a DT in the fission domain, the DT technology should be validated in a non-fission environment. A DT of a single-heat-pipe test article in the Microreactor AGile Non-nuclear Experimental Testbed (MAGNET) was demonstrated with predictive, self-adjusting capability on 30 March 2022. The test plan stated that: (1) the operators will manually change the temperature set point of the heat pipe to an upper limit or lower limit; and (2) the DT will predict the temperature will go beyond this temperature threshold, and then will update the temperature set point to the baseline temperature without any human intervention. The DT used a two-step process including a least absolute shrinkage and selection operator (LASSO) for variable selection between the sensors followed by vector autoregressive (VAR) models for multivariate forecasting to predict future performance of the heat pipe within MAGNET. Additionally, a physics model was created within the Sockeye framework that would be applied in future tests. With controlled rates of temperature change, the DT successfully self-adjusted the heat pipe before reaching the lower limit under expected conditions. Ultimately, this DT could be leveraged as a foundational framework in future fission battery applications for continuously monitoring and actively self-adjusting a heat pipe.}
}
@article{ZHENG2023108926,
title = {Artificial intelligence-driven rechargeable batteries in multiple fields of development and application towards energy storage},
journal = {Journal of Energy Storage},
volume = {73},
pages = {108926},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2023.108926},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X23023241},
author = {Li Zheng and Shuqing Zhang and Hao Huang and Ruxiang Liu and Mian Cai and Yinghui Bian and Long Chang and Huiping Du},
keywords = {Rechargeable batteries, Machine learning, Artificial intelligence, Material discovery, Status prediction},
abstract = {Rechargeable batteries are vital in the domain of energy storage. However, traditional experimental or computational simulation methods for rechargeable batteries still pose time and resource constraints. Artificial intelligence (AI), especially machine learning (ML) technology, has experienced rapid growth in recent years. The excellent classification and regression abilities of ML have been successfully applied to various fields of rechargeable battery research, resulting in numerous outstanding achievements. Herein, it is worthwhile to summarize the work of AI in rechargeable battery technology. This review aims to present a comprehensive account of the multiple fields where AI has been utilized for rechargeable battery research. First, the concept of ML and the key steps of processing are summarized. We then discuss how AI enables prediction of battery states and parameters in battery management systems, mainly including state of charge, state of health. Following this, the applications of AI to the discovery of key materials for rechargeable batteries, including cathodes, anodes, and electrolytes, are stated. We subsequently provide illustrations of how rechargeable batteries are utilized in charging protocols for energy storage. Additionally, we briefly outline the potential for developing AI’s new elements of machine vision and digital twins in battery research. Finally, we conclude by addressing challenges and perspectives for ML to drive innovations in battery technology.}
}
@article{ROUHOLLAHI2023102289,
title = {CardioVision: A fully automated deep learning package for medical image segmentation and reconstruction generating digital twins for patients with aortic stenosis},
journal = {Computerized Medical Imaging and Graphics},
volume = {109},
pages = {102289},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2023.102289},
url = {https://www.sciencedirect.com/science/article/pii/S0895611123001076},
author = {Amir Rouhollahi and James Noel Willi and Sandra Haltmeier and Alireza Mehrtash and Ross Straughan and Hoda Javadikasgari and Jonathan Brown and Akinobu Itoh and Kim I. {de la Cruz} and Elena Aikawa and Elazer R. Edelman and Farhad R. Nezami},
keywords = {Aortic stenosis, Automated AI platform, Calcium distribution, Digital twin},
abstract = {Aortic stenosis (AS) is the most prevalent heart valve disease in western countries that poses a significant public health challenge due to the lack of a medical treatment to prevent valve calcification. Given the aging population demographic, the prevalence of AS is projected to rise, resulting in a progressively significant healthcare and economic burden. While surgical aortic valve replacement (SAVR) has been the gold standard approach, the less invasive transcatheter aortic valve replacement (TAVR) is poised to become the dominant method for high- and medium-risk interventions. Computational simulations using patient-specific models, have opened new research avenues for optimizing emerging devices and predicting clinical outcomes. The traditional techniques of generating digital replicas of patients’ aortic root, native valve, and calcification are time-consuming and labor-intensive processes requiring specialized tools and expertise in anatomy. Alternatively, deep learning models, such as the U-Net architecture, have emerged as reliable and fully automated methods for medical image segmentation. Two-dimensional U-Nets have been shown to produce comparable or more accurate results than trained clinicians’ manual segmentation while significantly reducing computational costs. In this study, we have developed a fully automatic AI tool capable of reconstructing the digital twin geometry and analyzing the calcification distribution on the aortic valve. The developed automatic segmentation package enables the modeling of patient-specific anatomies, which can then be used to simulate virtual interventional procedures, optimize emerging prosthetic devices, and predict clinical outcomes.}
}
@article{WANG2023105071,
title = {Automatic high-level motion sequencing methods for enabling multi-tasking construction robots},
journal = {Automation in Construction},
volume = {155},
pages = {105071},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105071},
url = {https://www.sciencedirect.com/science/article/pii/S092658052300331X},
author = {Xi Wang and Shuoqi Wang and Carol C. Menassa and Vineet R. Kamat and Wes McGee},
keywords = {Human-robot collaboration, Learning from demonstration, Construction robot, Digital twin, Building information modeling, Robot sequential motions},
abstract = {Robots are expected to play an important role in future construction work. However, they are not yet widely adopted by the industry because it is difficult and expensive to program robots to conduct a variety of construction tasks. This paper presents a method for intuitively and flexibly teaching robots to perform various construction tasks through demonstrations. Robots are first programmed with basic skill primitives and then learn the sequencing of these primitive skills to perform different types of construction work under the guidance of human supervisors. The construction workflow and the interaction processes are enabled by a process-level digital twin system. Case studies with three assembly scenarios and a wooden frame construction experiment are used to present and verify the proposed method. The proposed approach enables automatic robot motion sequencing abilities through Learning from Demonstration and has the potential to enable the widespread adoption of robots on construction sites.}
}
@article{WANG2023109368,
title = {Digital twin based multi-objective energy management strategy for energy internet},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {154},
pages = {109368},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.109368},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523004258},
author = {Danlu Wang and Ruyi Fan and Yushuai Li and Qiuye Sun},
keywords = {Digital twin, Parallel system, Multi-objective energy management, Deep reinforcement learning},
abstract = {Energy management problem (EMP) has been a widely researched topic in optimal operation of Energy Internet (EI). However, the rapid growth in energy network scale and penetration of distributed renewable generations (DRGs) bring new challenges to energy management. Therefore, a digital twin (DT) based parallel energy management strategy is proposed for the large-scale EI which consists of We-energy (WE). Firstly, a parallel energy management framework is proposed. By establishing this triple parallel structure, states of energy networks can be observed realtimely, which enables flexible responses to fluctuations of DRGs and energy plug-and-play. Abandoned renewable energy is taken into account in the optimization model, which promotes the utilization of renewable energy. Then, a multi-timescale optimization strategy is proposed to handle different timescales of multi-energy networks. Furthermore, for better obtaining and processing information and avoiding dimensional curse, a DT based deep Q-learning algorithm (DQN) is proposed. Eventually, compared with the traditional benefit consensus based strategy, the simulation verifies the effectiveness of the DT based parallel energy management strategy.}
}
@article{YUCESAN2023110921,
title = {Physics-informed digital twin for wind turbine main bearing fatigue: Quantifying uncertainty in grease degradation},
journal = {Applied Soft Computing},
volume = {149},
pages = {110921},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110921},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623009390},
author = {Yigit A. Yucesan and Felipe A.C. Viana},
keywords = {Physics-informed neural network, Wind turbines, Digital twins, Uncertainty quantification},
abstract = {In the field of prognostics and health management for industrial equipment, digital twins stand out as essential tools. Wind park operators can harness the potential of digital twins to monitor component health, enabling proactive measures to optimize energy production while reducing maintenance costs. This study introduces a novel hybrid digital twin application tailored for monitoring wind turbine main bearing fatigue. It combines physics-based and data-driven kernels to address variable grease quality and biased observations. Our study covers model initialization, cross-validation, fleet management, and the influence of sampled turbines on prediction. Our approach offers two key advantages: (a) It reduces the need for extensive datasets, typical in other machine learning methods, by incorporating physics-based knowledge into the network architecture, and (b) it quantifies output uncertainties using tailored network layers and loss functions. Our findings conclude that even under compound uncertainty scenario, our hybrid model can estimate fleet unreliability only off by 4.7 weeks. However, it is essential to note that computational costs are tied to data processing, similar to other recurrent neural networks. A limitation is that the physics-based kernels must align with common machine learning linear algebra practices.}
}
@article{WU2023110824,
title = {ADCL: Adversarial Distilled Contrastive Learning on lightweight models for self-supervised image classification},
journal = {Knowledge-Based Systems},
volume = {278},
pages = {110824},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110824},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123005749},
author = {Ran Wu and Huanyu Liu and Jun-Bao Li},
keywords = {Adversarial distillation, Lightweight models, Self-supervised learning},
abstract = {With the development of modern sensors, numerous images are collected in edge application scenarios; however, their utilization is quite expensive because a massive effort is required to label them for further usage. Self-supervised learning, with no need for labeled data, shows great potential in this context; however, notable performance degradations can be observed when training lightweight networks which are essential in edge implementation. We propose an effective distillation method called Adversarial Distilled Contrastive Learning (ADCL) to mitigate this issue. Specifically, we introduced knowledge distillation into self-supervised learning to transfer underlying feature clustering relations from teacher models to shallow models. We adopted an online-updated rather than a pretrained teacher model to realize convenient implementation to specific data domains. An adversarial loss item was introduced to alleviate unstable optimization caused by an online trained teacher by forcing the teacher model to find feature relations beyond the recognition of the student model. Compared with other self-supervised knowledge distillation methods that maintain data queues consisting of positive and negative examples, the asymmetric contrastive learning method was employed to further relieve the memory bottleneck during training. The experimental results prove the effectiveness of our method. When ResNet-50 is used as a teacher to teach ResNet-18 on ImageNet, ADCL achieves top-1 accuracies of 60.3% , which surpasses other knowledge distillation methods with online teachers and is comparable to approaches using pretrained teachers and data queues.}
}
@article{DESAI2023103525,
title = {Enhanced multi-fidelity modeling for digital twin and uncertainty quantification},
journal = {Probabilistic Engineering Mechanics},
volume = {74},
pages = {103525},
year = {2023},
issn = {0266-8920},
doi = {https://doi.org/10.1016/j.probengmech.2023.103525},
url = {https://www.sciencedirect.com/science/article/pii/S0266892023001145},
author = {Aarya Sheetal Desai and Navaneeth N. and Sondipon Adhikari and Souvik Chakraborty},
keywords = {Multi-fidelity, Deep-H-PCFE, Uncertainty quantification, Surrogate models, Digital twin},
abstract = {The increasing significance of digital twin technology across engineering and industrial domains, such as aerospace, infrastructure, and automotive, is undeniable. However, the lack of detailed application-specific information poses challenges to its seamless implementation in practical systems. Data-driven models play a crucial role in digital twins, enabling real-time updates and predictions by leveraging data and computational models. Nonetheless, the fidelity of available data and the scarcity of accurate sensor data often hinder the efficient learning of surrogate models, which serve as the connection between physical systems and digital twin models. To address this challenge, we propose a novel framework that begins by developing a robust multi-fidelity surrogate model, subsequently applied for tracking digital twin systems. Our framework integrates polynomial correlated function expansion (PCFE) with the Gaussian process (GP) to create an effective surrogate model called H-PCFE. Going a step further, we introduce deep-HPCFE, a cascading arrangement of models with different fidelities, utilizing nonlinear auto-regression schemes. These auto-regressive schemes effectively address the issue of erroneous predictions from low-fidelity models by incorporating space-dependent cross-correlations among the models. To validate the efficacy of the multi-fidelity framework, we first assess its performance in uncertainty quantification using benchmark numerical examples. Subsequently, we demonstrate its applicability in the context of digital twin systems.}
}
@article{LIANG2023102543,
title = {Trained teacher: Who is good at teaching},
journal = {Displays},
volume = {80},
pages = {102543},
year = {2023},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2023.102543},
url = {https://www.sciencedirect.com/science/article/pii/S0141938223001762},
author = {Xingzhu Liang and Feilong Bi and Wen Liu and Xinyun Yan and Chunjiong Zhang and Chenxing Xia},
keywords = {Knowledge distillation, Trained teacher, Knowledge transfer, Teacher-student model},
abstract = {Knowledge distillation is an emerging method for acquiring efficient, small-scale networks. The main idea is to transfer knowledge from a complex teacher model with high learning capacity to a simple student model. To this end, various approaches to knowledge distillation have been proposed in the past few years, focusing mainly on modifications to student learning styles and less on changes to teacher teaching styles. Therefore, our new approach to knowledge distillation teacher training involves adapting the trained teachers to the knowledge distillation model in order to minimize the gap between the student model and the teacher model. We introduced the idea of a “Trained Teacher”: Our approach involves using a specially trained teacher network that, by incorporating knowledge distillation constraints during its own training, adapts to the teaching model in advance and performs nearly identically to a typical teacher network. This allows students to absorb the teacher's knowledge more effectively, thereby increasing their competence. In addition, the methods of mainstream knowledge distillation currently in use are equally appropriate to our educated teachers. Extensive tests on numerous datasets reveal that our technique outperforms the original knowledge distillation in accuracy on standard KD by 2%. Our code and pre-trained models can be found at https://github.com/JSJ515-Group/Trained_teacher.}
}
@article{HIELSCHER2023105248,
title = {A neural network based digital twin model for the structural health monitoring of reinforced concrete bridges},
journal = {Structures},
volume = {57},
pages = {105248},
year = {2023},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2023.105248},
url = {https://www.sciencedirect.com/science/article/pii/S235201242301336X},
author = {T. Hielscher and S. Khalil and N. Virgona and S.A. Hadigheh},
keywords = {Structural health monitoring, Digital twin, Machine learning, Fibre-optic sensors, Fibre Bragg grating, Artificial neural network},
abstract = {Developments in Structural Health Monitoring (SHM) research over the past few decades have demonstrated potential in optimising maintenance solutions for degrading infrastructure. The scale of structural deterioration worldwide and the inadequacy of current non-destructive evaluation techniques necessitate the adoption of accessible, quantitative, continuous SHM technology into mainstream asset management practice. This paper seeks to address this significant demand by proposing a robust, end-to-end, fibre-optic sensor (FOS) monitoring prototype which utilises deep neural networks to convert FOS strain output into an interactive digital twin (DT) visualisation. Finite-element validation demonstrated that the prototype was capable of capturing reliable structural analytics, recording an average error of less than 2kNm and an absolute error of less than 0.15 mm for bending moment and deflection respectively. Furthermore, the predictive mean absolute error of the integrated artificial neural network was less than 1με during testing, demonstrating the accuracy of the digital twin when generating baseline strain data for structural analysis.}
}
@article{TAY2023103701,
title = {Artificial neural network framework for prediction of hydroelastic response of very large floating structure},
journal = {Applied Ocean Research},
volume = {139},
pages = {103701},
year = {2023},
issn = {0141-1187},
doi = {https://doi.org/10.1016/j.apor.2023.103701},
url = {https://www.sciencedirect.com/science/article/pii/S0141118723002420},
author = {Zhi Yung Tay},
keywords = {Feed-forward neural network, Very large floating structure, Hydroelastic response, Irregular wave, Machine learning, Surrogate model},
abstract = {The response of a very large floating structure (VLFS) must take into consideration the elastic deformation of the structure (commonly termed hydroelastic response) under wave action. Conventionally, the hydroelastic response could be computed by using the coupled finite element-boundary element (FE-BE) method, where the mat-like structure is modelled using plate theory and the water modelled using the potential theory. The FE-BE method requires the structure to be discretised into finer elements and the wetted surface boundary to be represented by smaller panels to accurately capture the hydroelastic response of the structure. Thus, the coupled FE-BE method could be computationally expensive when the structure gets larger or when subjected to waves of smaller wavelengths. To accelerate the computational time in predicting the hydroelastic response of the VLFS, a surrogate model trained using the feed-forward neural network is proposed. The hydroelastic responses under different wavelengths, structural stiffnesses and wave directions are first generated where these data are split into three groups for training, validation, and testing (prediction) purposes. The accuracy of the prediction in terms of correlation coefficient R is compared for the different train datasets, the number of neurons and hidden layers as well as the optimisation techniques. The finding shows that an accuracy of close to 99% to the ground truth could be achieved with only 80% of the train dataset. The hydroelastic response under irregular wave conditions predicted using the feed-forward neural network framework is also presented.}
}
@article{GUO2023200070,
title = {Design and Optimization of an Open Personalized Human-Computer Interaction System for Yearbook Painting Based on the Learner's Model},
journal = {Systems and Soft Computing},
pages = {200070},
year = {2023},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2023.200070},
url = {https://www.sciencedirect.com/science/article/pii/S2772941923000236},
author = {Zaozao Guo and Muhamad Firdaus Ramli and Wenpeng Zhang},
keywords = {Learner modeling, human-computer interaction, emotion, database, yearbooks},
abstract = {Abstracts
With the rapid development of information technology such as big data and learning analytics, intelligent systems, a product of the deep integration of technology and education, have emerged. In this paper, a human-computer interaction teaching system for traditional art yearbooks is proposed based on the learner model. Firstly, the attention mechanism based long and short term memory network is used to mine the emotion from the course review text of learners, and the association rule algorithm and ID3 algorithm are used to initialize and dynamically update the text. Constructing a personalized HCI teaching system with the learner as the center. Based on the smart learning model, the functional modules of the human-computer interaction teaching system are analyzed and designed in detail, including online learning, online testing and educational information. The design of the database of the intelligent teaching system is proposed, and the design process of the database is fully demonstrated in terms of both database relationship design and database table structure design, taking into account the security of the database. Finally, the learner model and personalized human-computer interaction system that incorporate the emotions of this paper are tested for performance, and the results show that the prediction accuracy of this paper's model is about 3% higher than the standard model DKT on the 2009 dataset, about 3% higher than the standard model DKT on the AUC index, and about 4% lower than the standard model DKT on the RMSE index. Students learn through the personalized human-computer interaction system, and their mastery of the traditional art of New Year's Paintings is more thorough, and the learning effect is significantly improved.}
}
@article{ZOHDI2023116220,
title = {Rapid machine-learning enabled design and control of precise next-generation cryogenic surgery in dermatology},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {417},
pages = {116220},
year = {2023},
note = {A Special Issue in Honor of the Lifetime Achievements of T. J. R. Hughes},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116220},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523003444},
author = {Tarek I. Zohdi and Mona Zohdi-Mofid},
keywords = {Cryogenics, Dermatology, Digital-twin, Machine-learning},
abstract = {In the field of dermatology, the use of cryogenic processes, such as cryoablation, cryotherapy, etc., have grown dramatically over the last decade. This usually entails using a cryoprobe to freeze and destroy unwanted tissue, such as cancer cells. The focus of this work is to develop a digital-twin (a digital replica) of the performance of a cryogenic probe, which can be used to pre-plan and optimize surgical procedures, in order to maximize successful outcomes. Specifically, we model the optimal cryoprobe-induced cooling protocol needed to eliminate cells/tissue in specific regions, while minimizing damage to nearby tissue. The modeling approach is to develop mathematical surface point-source heat extraction kernels and then to create optimal surface patterns that the cryoprobe induces, by arranging the point-sources accordingly. Spatial and temporal control of the heat extraction is modeled. The entire subdermal thermal field is then constructed by superposing the solutions, enabling precise cryogenic treatment. Finally, a Machine Learning Algorithm (MLA) is then applied to optimize the set of parameters to deliver a precise response, making it an ideal real-time surgical tool.}
}
@article{KOPROV20231009,
title = {Systems and methods for authenticating manufacturing Machines through an unobservable fingerprinting system},
journal = {Manufacturing Letters},
volume = {35},
pages = {1009-1018},
year = {2023},
note = {51st SME North American Manufacturing Research Conference (NAMRC 51)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.08.051},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323001086},
author = {Pavel Koprov and Shyam Gadhwala and Aniket Walimbe and Xiaolei Fang and Binil Starly},
keywords = {Cybersecurity, Connected Manufacturing, Authentication, Physical Unclonable Function, Digital Twin, Vibration},
abstract = {Digital transformation leads to the inevitable change in the security paradigm for machines on a factory production floor. A unified namespace for machines in an Industrial Internet of Things (IIoT) network is only reliable when machine assets can trust and verify the identity of assets connected to the IIoT system. Current methods of asset authentication do not consider physical unclonable functions (PUFs) and can easily be spoofed or misused. Our work proposes using PUFs for industrial equipment such as CNC machines, robots, and 3D printers for identifying machines on a network and providing authentication procedures. In this work, we chose to use the vibration associated with machines and its embedded moving parts as a means to identify machine assets on a network. It is hypothesized that the vibrations associated with specific machine movements will be unique to each machine even when machines look exactly the same. The moving parts within a machine may produce a unique vibration pattern that can be used for machine identification throughout the working cycle. Our method requires light computing and relatively cheap measuring devices to capture the ‘fingerprints’ of machines and verify the signal's integrity. An adequate number of equipment has been tested for the worst-case scenario, i.e. when two machines look exactly the same with the same moving parts and produce exactly similar motion to generate the vibration signal. Data preprocessing and standard machine learning techniques like RF, LASSO, and SVM show great performance on raw time series data, enabling 100% TPR and more than 94% TNR in detecting the false class of the machines.}
}
@article{SHI2023102973,
title = {A deep weakly semi-supervised framework for endoscopic lesion segmentation},
journal = {Medical Image Analysis},
volume = {90},
pages = {102973},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102973},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523002335},
author = {Yuxuan Shi and Hong Wang and Haoqin Ji and Haozhe Liu and Yuexiang Li and Nanjun He and Dong Wei and Yawen Huang and Qi Dai and Jianrong Wu and Xinrong Chen and Yefeng Zheng and Hongmeng Yu},
keywords = {Endoscopic lesion segmentation, Weakly semi-supervised learning, Regularization consistency},
abstract = {In the field of medical image analysis, accurate lesion segmentation is beneficial for the subsequent clinical diagnosis and treatment planning. Currently, various deep learning-based methods have been proposed to deal with the segmentation task. Albeit achieving some promising performances, the fully-supervised learning approaches require pixel-level annotations for model training, which is tedious and time-consuming for experienced radiologists to collect. In this paper, we propose a weakly semi-supervised segmentation framework, called Point Segmentation Transformer (Point SEGTR). Particularly, the framework utilizes a small amount of fully-supervised data with pixel-level segmentation masks and a large amount of weakly-supervised data with point-level annotations (i.e., annotating a point inside each object) for network training, which largely reduces the demand of pixel-level annotations significantly. To fully exploit the pixel-level and point-level annotations, we propose two regularization terms, i.e., multi-point consistency and symmetric consistency, to boost the quality of pseudo labels, which are then adopted to train a student model for inference. Extensive experiments are conducted on three endoscopy datasets with different lesion structures and several body sites (e.g., colorectal and nasopharynx). Comprehensive experimental results finely substantiate the effectiveness and the generality of our proposed method, as well as its potential to loosen the requirements of pixel-level annotations, which is valuable for clinical applications.}
}
@article{VASILIKIS2023115927,
title = {A digital twin approach for maritime carbon intensity evaluation accounting for operational and environmental uncertainty},
journal = {Ocean Engineering},
volume = {288},
pages = {115927},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115927},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823023119},
author = {Nikolaos Vasilikis and Rinze Geertsma and Andrea Coraddu},
keywords = {Digital twin, Carbon intensity, Operational uncertainties, Environmental uncertainties, Data-driven methods, Hybrid propulsion},
abstract = {Maritime industry has set ambitious goals to drastically reduce its greenhouse gas emissions through stipulating and enforcing a number of energy assessment measures. Unfortunately, measures like the EEDI, EEXI, SEEMP and CII do not account for the operational and environmental uncertainty of operations at sea, even though they do provide a first means of evaluating the carbon footprint of ships. The increasing availability of high-frequency operational data offers the opportunity to quantify and account for this uncertainty in energy performance predictions. Current methods to evaluate and predict energy performance at a whole energy system level do not sufficiently account for operational and environmental uncertainty. In this work, we propose a digital twin that accurately predicts the fuel consumption and carbon footprint of the hybrid propulsion system of an Ocean-going Patrol Vessel (OPV) of the Royal Netherlands Navy under the aggregate effect of operational and environmental uncertainty. It combines first-principle steady-state models with machine learning algorithms to reach an accuracy of less than 5% MAPE on both mechanical and electrical propulsion, while bringing a 40% to 50% improvement over a model that does not utilise machine learning algorithms. Results over actual voyage intervals indicate a prediction accuracy of consumed fuel and carbon intensity within 2.5% accounting for a confidence interval of 95%. Finally, the direct comparison between mechanical and electrical propulsion showed no clear energy-saving benefits and a strong dependency of the results on each voyage’s specific operational and environmental conditions.}
}
@article{KUMAR2023102465,
title = {Improving public school productivity: Evidence from model schools in India},
journal = {Economics of Education Review},
volume = {97},
pages = {102465},
year = {2023},
issn = {0272-7757},
doi = {https://doi.org/10.1016/j.econedurev.2023.102465},
url = {https://www.sciencedirect.com/science/article/pii/S0272775723001127},
author = {G. Naveen Kumar},
keywords = {Education quality, Education policy, School productivity, Public school, India},
abstract = {This paper studies the impact of India’s “model” school program which aimed to provide high quality education to economically disadvantaged students. Model schools combine better infrastructure with more accountability, contract teachers, and lower per-pupil spending than regular public schools. Using a fuzzy Regression Discontinuity Design based on entrance exam cutoffs, I find attending a model school for five years increases test scores in math by 0.38 standard deviations, in science by 0.26 sd, and in social science by 0.26 sd on average. Furthermore, model schools increase the probability of joining pre-university by 11.5 percentage points. The results suggest it is possible to deliver substantial improvement of outcomes in public schools at a slightly lower level of school spending through a package of reforms.}
}
@article{LI2023206,
title = {Digital twin model-based smart assembly strategy design and precision evaluation for PCB kit-box build},
journal = {Journal of Manufacturing Systems},
volume = {71},
pages = {206-223},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001929},
author = {Xurui Li and Guangshuai Liu and Si Sun and Wenyu Yi and Bailin Li},
keywords = {3D point cloud, Printed circuit board, 6-DoF pose, Robotic assembly, Digital twin},
abstract = {Research concerning microelectronics assembly has attracted increasing attention from the manufacturing industry. In this context, achieving precise placement of the printed circuit board (PCB) inside the enclosure is the most critical aspect of the kit-box build assembly task. However, automating the PCB kit-box build assembly (PKBA) process remains a challenging work. This study presents a Digital Twin (DT) model that enables accurate perception of object pose for intelligent PKBA assembly, facilitating real-time monitoring and evaluation of its service status. In our system, a symmetry-drive method is proposed to optimize the initial pose in 6-DoF matching during DT assembly. Based on the developed technology, a three-stage learning method is established to achieve grasping point localization and robotic assembly trajectory planning. To ensure accurate and robust robot assembly, we developed a PKBA quality prediction model based on the DT system, which predicts the uncertainty of actual PCB assembly positioning by small displacement torsor (SDT) theory and Monte Carlo methods. Particularly, the assembly quality of the actual product is effectively monitored when the state of the virtual simulation model corresponds to the physical assembly object. Finally, a prototype system and a case study involving dexterous assembly tasks are conducted to verify the effectiveness and feasibility of the proposed method. The results indicate that the proposed PKBA strategy achieves an 82% assembly success rate. By employing well-designed strategies, our method ensures that the majority of errors are below 0.8 mm and 0.6 degrees.}
}
@article{ZHANG2023103454,
title = {DMSC-Net: A deep Multi-Scale context network for 3D object detection of indoor point clouds},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {122},
pages = {103454},
year = {2023},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2023.103454},
url = {https://www.sciencedirect.com/science/article/pii/S1569843223002789},
author = {Zhenxin Zhang and Dixiang Xu and P. Takis Mathiopoulos and Qiang Wang and Liqiang Zhang and Zhihua Xu and Jincheng Jiang and Zhen Li},
keywords = {Indoor point cloud, Object detection, Multi-head attention mechanism, Deep multi-scale contextual feature, Deep learning},
abstract = {Indoor object detection has emerged as one of the key technologies for the success of numerous indoor system applications, such as autonomous navigation, accurate modeling of indoor environments, digital twin and terra Hertz (THz) communications. This paper first proposes a flexible and inter-operational detection module, termed deep multi-scale context (DMSC) module, aiming at the development of efficient indoor object detection techniques using the point clouds. More specifically, by combining the deep contextual information of indoor objects and multi-scale features, a novel deep multi-scale contextual feature is designed. Furthermore, we introduce the decoder part of the vision transformer into the indoor object proposal generation by means of a multi-head attention (MHA) module from a three-dimensional (3D) point cloud to accurately extract object proposals generating high-quality bounding boxes. Extensive experiments have shown that, the effective interoperability of the proposed DMSC module with three object detection networks, namely VoteNet, GroupFree 3D and RBGNet, leads to improvements in their mAP@0.25 by 6.5%, 0.9% and 0.4% on the ScanNetV2 datasets, respectively. The proposed end-to-end network, termed as DMSC-Net, consists of an indoor point cloud feature learning backbone (FLB) unit, and three modules, namely the DMSC, a voting decision (VD) module, and an MHA module. Extensive experiments have shown that the DMSC-Net outperforms other advanced indoor 3D detection networks, such as RBGNet, by 1.1% and 0.9% of mAP@0.25 when applied on ScanNet and SUN RGB-D datasets, respectively. The developed code is publicly available at: https://github.com/CNU-DLandCV-lab/MHA_DMSC.}
}
@article{ROBLES2023104007,
title = {OpenTwins: An open-source framework for the development of next-gen compositional digital twins},
journal = {Computers in Industry},
volume = {152},
pages = {104007},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001574},
author = {Julia Robles and Cristian Martín and Manuel Díaz},
keywords = {Digital twin composition, Open-source digital twin framework, Kafka-ML, 3D visualizations, Industry 4.0},
abstract = {Although digital twins have recently emerged as a clear alternative for reliable asset representations, most of the solutions and tools available for the development of digital twins are tailored to specific environments. Furthermore, achieving complex digital twins often requires the orchestration of technologies and paradigms such as machine learning, the Internet of Things, and 3D visualization, which are rarely seamlessly aligned in open-source solutions. In this paper, we present an open-source framework for the development of compositional digital twins, i.e., advanced digital twins that link individual entities or subsystems to create a higher degree digital twin, allowing knowledge sharing and data relationships. In this open framework, digital twins can be easily developed and orchestrated with 3D-connected visualizations, IoT data streams, and real-time machine-learning predictions. To demonstrate the feasibility of the framework, a use case in the Petrochemical Industry 4.0 has been developed.}
}
@article{LIU2023106961,
title = {A novel seminar learning framework for weakly supervised salient object detection},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106961},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106961},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623011454},
author = {Yan Liu and Yunzhou Zhang and Zhenyu Wang and Fei Yang and Feng Qiu and Sonya Coleman and Dermot Kerr},
keywords = {Salient object detection, Weakly supervised, Seminar learning framework, Cross attention guided network, Consistency transformation mechanism, Multiple pseudo labels},
abstract = {Weakly supervised salient object detection (SOD) is a challenging task and has drawn much attention from several research perspectives, it has revealed two problems while driving the rapid development of saliency detection. (1) Large divergence in the characteristics of saliency regions in terms of location, shape and size makes them difficult to recognize. (2) The properties of convolutional neural networks dictate that it is insensitive to various transformations, which will lead to hardly balance the application of various disturbances. To tackle these limitations, this paper proposes a novel seminar learning framework with consistent transformation ensembling (SLF-CT) for scribble supervised SOD. The framework consists of the teacher–student model and the student–student model for segmenting the salient objects. Specifically, we first design a cross attention guided network (CAGNet) as a baseline model for saliency prediction. Then we assign CAGNet to the teacher–student model, where the teacher network is based on the exponential moving average and guides the training of the student network. Moreover, we adopt multiple pseudo labels to transfer the information among students from different conditions. To further enhance the regularization of the network, a consistency transformation mechanism is also incorporated, which encourages the saliency prediction and input image of the network to be consistent. The experimental results demonstrate that the proposed approach performs favorably comparable with the state-of-the-art weakly supervised methods. As far as we know, the proposed approach is the first application of seminar learning in the SOD area.}
}
@article{GHIENNE2023107155,
title = {Learning structural stress virtual sensors from on-board instrumentation of a commercial aircraft},
journal = {Computers & Structures},
volume = {289},
pages = {107155},
year = {2023},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2023.107155},
url = {https://www.sciencedirect.com/science/article/pii/S0045794923001852},
author = {Martin Ghienne and Alexandre Limare},
keywords = {Structural stress virtual sensors, Time series regression, Ensemble model learning, Challenge AI for industry, Aircraft Digital Twin},
abstract = {This work aims to predict the mechanical stress on the structure of a business jet in service phase from flight instrument only. A significant database obtained from test flights using aircraft instrumented with strain gauges has been provided as part of an Artificial Intelligence challenge organized by the French Ile-de-France region and the aircraft manufacturer Dassault Aviation. Learning techniques are considered to train a prediction model of the aircraft structural stress. The proposed baseline includes a clustering step for phase identification in time series and an ensemble model with two stacked regressors. The model is trained on a dataset of 117 flights and its overall performance is evaluated on a validation set of flight sequences from 186 flights. The main advantages of the proposed learning approach are its prediction accuracy, its training frugality and its interpretability. This paper presents a global data science workflow applied to a problem of structural stress prediction. Despite a development in a constrained time period with no direct access to the data, the proposed approach demonstrates the feasibility of the concept of learned virtual sensor of aircraft structural stress and paves the way for applications to other structures.}
}
@article{LIU2023127,
title = {A novel bionic decision-making mechanism for digital twin-based manufacturing system},
journal = {Manufacturing Letters},
volume = {35},
pages = {127-131},
year = {2023},
note = {51st SME North American Manufacturing Research Conference (NAMRC 51)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.08.119},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323001803},
author = {Shimin Liu and Pai Zheng and Suiyan Shang},
keywords = {Digital twin, Manufacturing system, Bionic decision-making, Smart manufacturing},
abstract = {As an innovative smart manufacturing system, the virtual entities-based decision-making process is the most typical difference between the digital twin-based manufacturing system (DTMS) and other smart manufacturing systems. Therefore, the accuracy of virtual entity-driven decision-making is the key to affecting the system reliability of the DTMS. Normally, the manufacturing process is often accompanied by complex state changes, which are collected by the perception module of the DTMS in the form of high-dimensional information. Then, the decision-making model needs to respond to these state changes in real-time and give reasonable decision results back to physical space, which has become an important scientific issue of DTMS. To fill this gap, a novel bionic decision-making mechanism for DTMS is put forward by introducing the biological sequential learning mechanism into the decision-making process. Subsequently, the systematic decision-making process imitates biological instinct and learning behavior mechanisms to explore the short-term and long-term process of decision-making. The bionic decision-making mode formed by combining the above two modes provides adaptive decision-making in different scenarios. It is believed that the bionic decision-making mechanism can help to quickly and accurately give decision-making feedback to guide on-site manufacturing and ensure product quality and manufacturing efficiency.}
}
@article{JEUNG2023129973,
title = {Data assimilation for urban stormwater and water quality simulations using deep reinforcement learning},
journal = {Journal of Hydrology},
volume = {624},
pages = {129973},
year = {2023},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2023.129973},
url = {https://www.sciencedirect.com/science/article/pii/S0022169423009150},
author = {Minhyuk Jeung and Jiyi Jang and Kwangsik Yoon and Sang-Soo Baek},
keywords = {Data assimilation, Deep reinforcement learning, Stormwater management model, Extreme rainfall event, Sensitivity analysis},
abstract = {Hydrological models have been used to understand the transportation of water quantity and quality in drainage systems, and the stormwater management model (SWMM) is one of the most widely-used models for runoff quantity and quality simulations in urban areas. Although significant efforts have been made to identify the appropriate input parameters of the SWMM model in various watersheds, it is difficult to reflect the variability of the real environment using a fixed input parameter. Data assimilation (DA) is a compatibility method that improves prediction accuracy by merging observations and simulation outputs. It is useful to reduce the forecast error from temporal transferability with information interaction between the model output and observation data. However, conventional DA approaches cannot completely overcome their unrealistic assumptions such as linearity, normality, and error covariances. To address these challenges, we used deep reinforcement learning (RL) to develop an automatic assimilation model that interactively optimizes the SWMM parameters in a real environment (SWMM-RL). In the SWMM-RL model, the agent is trained by rewarding and/or punishing the action (modulated SWMM input parameter) in real time according to the state changes. The model was constructed to minimize the error of runoff and pollutant load simulation of suspended solids (SS), total nitrogen (TN), and total phosphorus (TP) in each stormwater monitoring event. The results demonstrated that the SWMM-RL model primarily outperformed the fixed input parameter SWMM model (SWMM-PS) in simulating the runoff and the three different pollutant loads (the median value of Nash–Sutcliffe efficiency (NSE) for 10 stormwater events was increased by 0.11, 0.31, 0.36, and 0.07 for runoff, SS, TN, and TP, respectively). The SWMM-PS has disadvantages in simulating low rainfall events because of the high sensitivity of runoff peaks to heavy rainfall conditions. Furthermore, the effects of extreme rainfall events were estimated using the SWMM-RL model. This study showed how SWMM-RL combined with the DA method increased forecast accuracy by providing sensitivity and temporal transitions of input parameters.}
}
@article{ISICHEI2023e00321,
title = {Cybersecurity and privacy in smart bioprinting},
journal = {Bioprinting},
volume = {36},
pages = {e00321},
year = {2023},
issn = {2405-8866},
doi = {https://doi.org/10.1016/j.bprint.2023.e00321},
url = {https://www.sciencedirect.com/science/article/pii/S2405886623000647},
author = {Joan C. Isichei and Sajad Khorsandroo and Salil Desai},
keywords = {3D printing, Artificial intelligence, Bioprinting, Cybersecurity, Digital twin (DT), Internet of things (IoT)},
abstract = {Bioprinting is a versatile technology gaining rapid adoption in healthcare fields such as tissue engineering, regenerative medicine, drug delivery, and surgical planning. Although the current state of the technology is in its infancy, it is envisioned that its evolution will be enabled by the integration of the following technologies: Internet of Things (IoT), Cloud computing, Artificial Intelligence/Machine Learning (AI/ML), NextGen Networks, and Blockchain. The product of this integration will eventually be a smart bioprinting ecosystem. This paper presents the smart bioprinting ecosystem as a multilayered architecture and reviews the cyber security challenges, vulnerabilities, and threats in every layer. Furthermore, the paper presents privacy preservation solutions and provides a purview of the open research challenges in the smart bioprinting ecosystem.}
}
@article{CHEN2023422,
title = {Novel learning framework for optimal multi-object video trajectory tracking},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {5},
pages = {422-438},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000220},
author = {Siyuan Chen and Xiaowu Hu and Wenying Jiang and Wen Zhou and Xintao Ding},
keywords = {Web3D, Virtual evacuation, Multi-object tracking, Trajectory extraction, Trajectory optimization},
abstract = {Background
With the rapid development of Web3D, virtual reality, and digital twins, virtual trajectories and decision data considerably rely on the analysis and understanding of real video data, particularly in emergency evacuation scenarios. Correctly and effectively evacuating crowds in virtual emergency scenarios are becoming increasingly urgent. One good solution is to extract pedestrian trajectories from videos of emergency situations using a multi-target tracking algorithm and use them to define evacuation procedures.
Methods
To implement this solution, a trajectory extraction and optimization framework based on multi-target tracking is developed in this study. First, a multi-target tracking algorithm is used to extract and preprocess the trajectory data of the crowd in a video. Then, the trajectory is optimized by combining the trajectory point extraction algorithm and Savitzky–Golay smoothing filtering method. Finally, related experiments are conducted, and the results show that the proposed approach can effectively and accurately extract the trajectories of multiple target objects in real time.
Results
In addition, the proposed approach retains the real characteristics of the trajectories as much as possible while improving the trajectory smoothing index, which can provide data support for the analysis of pedestrian trajectory data and formulation of personnel evacuation schemes in emergency scenarios.
Conclusions
Further comparisons with methods used in related studies confirm the feasibility and superiority of the proposed framework.}
}
@article{FANG2023107645,
title = {A digital twin modeling method based on multi-source crack growth prediction data fusion},
journal = {Engineering Failure Analysis},
volume = {154},
pages = {107645},
year = {2023},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2023.107645},
url = {https://www.sciencedirect.com/science/article/pii/S135063072300599X},
author = {Xin Fang and Guijie Liu and Honghui Wang and Xiaojie Tian},
keywords = {Digital twin, Crack growth prediction, Theoretical model, Machine learning, Consistency retention, Data fusion},
abstract = {This paper proposes a digital twin method based on multi-source crack growth prediction data fusion. In this method, two different prediction methods based on theoretical model correction and machine learning model correction are constructed, which avoids the inapplicability of a single method in practical applications. Meantime, based on the consistency retention method corresponding to each model, the influence of uncertainty factors on crack growth prediction is gradually reduced by inputting crack detection data. Subsequently, by fusing the historical data and prediction data, the crack growth prediction result with the smallest deviation and higher reliability is output. The verification results show that the digital twin model proposed in this paper can effectively reduce the influence of uncertainty factors on crack growth prediction and realize the dynamic prediction of crack growth.}
}
@article{FAN2023111867,
title = {Energy management of renewable based power grids using artificial intelligence: Digital twin of renewables},
journal = {Solar Energy},
volume = {262},
pages = {111867},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.111867},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23005005},
author = {Xuezhou Fan and Yajuan Li},
keywords = {Renewable power grids, Cloud-fog computing, Artificial intelligence, Server broker policies, Digital twin},
abstract = {This paper proposes fog computing as a method of minimizing latency and maximizing performance by storing the information in the cloud to give a wide range of services to users using cloud computing. Also, consumer requests have been handled by several cloud Data Centers (DCs). Due to the fog's ability to attract more users and provide more services, load balancing has become increasingly crucial. It is therefore necessary to enhance the method that balances the fog's load. The study proposes a three-layer model comprising a cloud layer, a fog layer, and a user layer for optimal energy management in renewable power grids. The renewable energy sources are all modeled within the digital twin environment to make sure that very accurate monitoring is made. It is proposed for balancing the fog load using an artificial intelligence based optimization algorithm called the Whale optimization algorithm (WOA). Services are provided by fog servers in response to requests from users. In case of failure of the fog layer, all consumers' records are saved and services are provided to them through the cloud. Moreover, the service broker policies (SBP) have been applied to select the DCs efficiently. This paper compares the suggested algorithm to three previously developed algorithms including the particle swarm optimization (PSO), differential devolution (DE), and teaching–learning-based optimization (TLBO). It uses the three SBPs of closest data center, optimize response time (RT), reconfigure dynamically with load, and an improved SBP. It also minimizes RT and process time to improve the efficiency. The results show that there is an improvement in WOA's performance by approximately 5% compared to that of other algorithms.}
}
@article{YANG2023114692,
title = {Identification of industrial exhaust based on an electronic nose with an interleaved grouped residual convolutional compression network},
journal = {Sensors and Actuators A: Physical},
volume = {363},
pages = {114692},
year = {2023},
issn = {0924-4247},
doi = {https://doi.org/10.1016/j.sna.2023.114692},
url = {https://www.sciencedirect.com/science/article/pii/S0924424723005411},
author = {Shuangjing Yang and Huisheng Zhang and Zhe Li and Shukai Duan and Jia Yan},
keywords = {E-nose, Convolutional neural network, Interleaved grouped residual convolution, Knowledge distillation, Industrial exhaust identification},
abstract = {Industrial exhaust detection helps to identify pollution sources, assess the distribution of environmental pollutants, and reduce disease. In this paper, an electronic nose (E-nose) system for industrial exhaust detection is established, and an interleaved grouped residual convolutional compression network (IGRCCN) based on knowledge distillation (KD) is proposed to achieve industrial exhaust identification. First, a detection system based on 15 cross-sensitive gas sensors is constructed to detect 10 common industrial exhaust gases. Second, the KD-based IGRCCN, which couples a novel teacher model and a lightweight student model, is designed. Specifically, combining the working principle of the mammalian olfactory system and the characteristics of E-nose data, we propose using two interleaved grouped residual convolution block at the sensor level and channel level to construct the teacher model, which extracts signal features while preventing feature degradation. In addition, the KD framework enables the lightweight student model to efficiently classify gases, with a performance comparable to that of the teacher model, thus realizing model compression. The experimental results show that the classification accuracy of the IGRCCN is 98.33%, which vastly outperforms other deep learning models. In addition, IGRCCN obtained promising classification accuracies of 88.66% and 92.92% for small sample training and early identification, respectively, which showed the advantages of the proposed method in practical applications.}
}
@article{ALIMAM2023101846,
title = {The resurrection of digital triplet: A cognitive pillar of human-machine integration at the dawn of industry 5.0},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {10},
pages = {101846},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101846},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823004007},
author = {Hassan Alimam and Giovanni Mazzuto and Nicola Tozzi and Filippo {Emanuele Ciarapica} and Maurizio Bevilacqua},
keywords = {Digital Triplet , Cognitive Digital Twin , Artificial Intelligence , Human-Machine Integration, Brain-Computer Interface , Industry 5.0},
abstract = {The integration of AI technology with digital transformation has profoundly shaped the evolution towards digital triplet architecture, grounded in human-centric methodologies. By infusing human intellectual activities into both physical and cyberspace, innovative links between humans and machines are established. Despite limitations in transitioning from tangible human presence to the digital realm in cyberspace, extensive efforts are underway to harness emotional, visual, and oral responses, thereby enhancing the reasoning and predictive capabilities of digital twins. These advancements aim to elevate real-time human interactions with physical and virtual systems by integrating intelligent AI algorithms and cognitive computing systems into digital twins. This paper meticulously analyses recent trends in digital twins, tracing their evolution from traditional concepts and applications to a nuanced digital triplet hierarchy that incorporates human intuition, knowledge, and creativity within cyberspace. we delve into the hierarchical framework of the digital triplet, resonating with maturity, domination, and volition levels, enhances cognitive and perceptual capabilities in cyberspace. The study provides a systematic overview of the development of ultra-realistic digital models, incorporating real-time data-driven artefacts that integrate intelligent activities with multidomain, multiphysics, and multiscale simulations. The research scope is focused on augmenting the perceptive and heuristic capabilities of the digital triplet framework by utilizing AI in data analytics, retrieving heterogeneous data from virtual entities using semantic artificial intelligence technologies, and amalgamating AI and machine learning with human insight and perceptual knowledge. The proposed digital triplet hierarchy aims to enhance cyberspace's capacity for learning, cognitive skills, and knowledge transfer. It can be a guideline for the researcher to promote cognitive augmentation of the human brain through brain-machine/computer interface, virtual, augmented, and extended reality, fostering a symbiotic relationship between humans and machines in the industrial metaverse and industry 5.0. The paper discusses future directions for research and the challenges involved in developing intelligent digital twins towards the digital triplet paradigm, aiming to embody intelligent activities and cognitive capabilities within the framework of human–machine symbiosis.}
}
@article{YASSIN2023100039,
title = {Digital twin in power system research and development: Principle, scope, and challenges},
journal = {Energy Reviews},
volume = {2},
number = {3},
pages = {100039},
year = {2023},
issn = {2772-9702},
doi = {https://doi.org/10.1016/j.enrev.2023.100039},
url = {https://www.sciencedirect.com/science/article/pii/S2772970223000263},
author = {Mohammed A.M. Yassin and Ashish Shrestha and Suhaila Rabie},
keywords = {Communication channel, Cyber-physical systems, Digital twin, Internet of things, Machine learning},
abstract = {In order to address the issues that arise in modern power systems, such as system dynamics, stability, control, efficiency, reliability, economy, planning and policy, and so on, efforts have been made to develop new tools and techniques, components, methodologies, and scientific innovations in a variety of fields. These efforts have been undertaken to address these issues. The term “digital twin” (DT) refers to one of the most reliable and rapidly developing technologies that have recently been incorporated into a variety of applications, platforms, and real-time projects. The authors of this study offered a scoping review of DT technologies with a primary emphasis on power systems. It has been established that the underlying notion behind this technology, as well as its operating principle, types, communication channels and protocols, and standards, have all been thoroughly examined. In addition, the possibility of integrating other technologies with DT has also been considered, along with the potential benefits of doing so and the potential difficulties that may arise. Based on the information gained from the current projects, the finished projects, the research publications, as well as the research and industry insights, a critical discussion has been made.}
}
@article{LI2023109590,
title = {Digital twin-driven focal modulation-based convolutional network for intelligent fault diagnosis},
journal = {Reliability Engineering & System Safety},
volume = {240},
pages = {109590},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109590},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023005045},
author = {Sheng Li and Qiubo Jiang and Yadong Xu and Ke Feng and Yulin Wang and Beibei Sun and Xiaoan Yan and Xin Sheng and Ke Zhang and Qing Ni},
keywords = {Rolling bearing, Fault diagnosis, Digital twin, Lightweight, Focal modulation},
abstract = {Rolling bearings are essential components of various rotating machinery and are critical in ensuring safe and reliable industrial production. Deep learning techniques have demonstrated outstanding potential for real-time monitoring of bearings, contributing to the safe operation of machinery and equipment. However, deep learning-based fault diagnosis methods typically rely on training datasets comprising samples of all potential failure modes that may not be acquirable in specific industrial settings. To tackle the challenge above, this paper introduces a digital twin approach to generate synthetic data to supplement and enhance the quality and availability of training data in deep learning methods. Specifically, the main contributions of this research are: (1) constructing a digital twin model of rolling bearings to generate an approximation of the physical entity bearing status data. (2) investigating the efficient combination of CNNs and focal modulation mechanism, and proposing a novel lightweight architecture, FM-LCN, aims to learn local-global representations of simulated data to improve diagnostic performance. Experiments demonstrate that FM-LCN outperforms five state-of-the-art competitive models by a large margin in accuracy with lower computational cost.}
}
@article{LI2023120441,
title = {Establishing boundary conditions in sewer pipe/soil heat transfer modelling using physics-informed learning},
journal = {Water Research},
volume = {244},
pages = {120441},
year = {2023},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2023.120441},
url = {https://www.sciencedirect.com/science/article/pii/S0043135423008813},
author = {Jiuling Li and Nur Nabilah Naina Mohamad and Keshab Sharma and Zhiguo Yuan},
keywords = {Heat transfer, Boundary conditions, Temperature modelling, Physics-informed model, Digital twin, Sewer system, Pipe-soil heat transfer},
abstract = {Modelling heat transfer in sewers and the surrounding soil is important for effective sewer maintenance, and for heat recovery from wastewater. The boundary conditions, including both the thickness of the soil layer to be modelled and the temperature distribution around the boundary of the soil layer, directly determine both the efficiency and accuracy of the models. Yet there is no systematic method to establish these conditions. This study presents a novel and generic approach to establishing efficient boundary conditions for sewer heat transfer modelling. Fourier transform is applied to identify the dominant frequencies of the temperatures of the heat sources/sinks, namely the atmosphere, sewer air and wastewater. A simple data-driven model for determining the thickness of the soil-layer to be included, and three physics-informed models for predicting the temperatures at the soil-layer boundary are then learnt from mechanistic models for sewer heat transfer, taking into consideration the frequency spectra. The methodology achieved high fidelity to the mechanistic models in predicting the soil-layer boundary temperatures and sewer wall temperatures for real-life sewers. This approach offers an easy yet reliable way to obtain efficient boundary conditions that significantly improve both the accuracy and speed of sewer heat transfer modelling.}
}
@article{STEED2023436,
title = {Deep active-learning based model-synchronization of digital manufacturing stations using human-in-the-loop simulation},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {436-450},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001619},
author = {Clint Alex Steed and Namhun Kim},
keywords = {Human-centric manufacturing, Digital twin, Virtual reality (VR), AI and machine learning, Virtual manufacturing, Digital transformation},
abstract = {The effective and accurate modeling of human performance is one of the key technologies in virtual/smart manufacturing systems. However, a significant challenge lies in acquiring sufficient data for such modeling. Virtual Reality (VR) emerges as a promising solution, making human manufacturing experiments more practical and accessible. In this paper, we present a novel framework that efficiently models human assembly duration by leveraging VR to prototype data-acquisition systems for assembly tasks. Central to the framework is an active learning model, which intelligently selects experimental conditions to yield the most informative results, effectively reducing the number of experiments required. As a result, the system demands fewer experimental trials and operates on an automated basis. In VR experiments involving throughput rate, the active model significantly reduces the data requirement, thereby expediting the experiment and modeling process. While this framework demonstrates remarkable efficiency, it does exhibit sensitivity to non-constant noise and may necessitate prior data from similar assembly tasks to identify high-noise. Notably, this proposed method extends beyond manufacturing, allowing the quick generation of human performance models in virtual systems and enhancing experiment scalability across various fields. With its potential to revolutionize human performance modeling, our framework represents a promising avenue for advancing virtual/smart manufacturing systems and other related applications.}
}
@article{ANTONELLO2023,
title = {Surrogate model-based calibration of a flying Earth observation satellite},
journal = {Advances in Space Research},
year = {2023},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2023.10.043},
url = {https://www.sciencedirect.com/science/article/pii/S0273117723008633},
author = {Federico Antonello and Daniele Segneri and Vemund Reggestad},
keywords = {Spacecraft, European Space Agency (ESA), Calibration, Artificial Intelligence, Surrogate Model, Digital Twin},
abstract = {At the European Space Agency (ESA), Modeling and Simulation (M&S) plays a fundamental role during the lifetime of a spacecraft, being used from the design phase to the testing and during operations in space. M&S tools embed general physics-based models and disciplines characterized by configurable parameters which have to be calibrated in order to mimic the behavior of the actual flying spacecraft. However, their calibration requires a large number of simulations which are unfeasible to be obtained through computationally expensive high-fidelity simulation models. Thus, the inability to calibrate the high-fidelity simulation models poses limitations for the use of M&S tools during spacecraft operations. In this light, the present work proposes the use of a surrogate model-based approach for the calibration of simulation models of spacecraft. The approach integrates a computationally inexpensive deep-learning-based surrogate model, which mimics the high-fidelity simulation model without requiring the same computational burden, and a metaheuristic optimization algorithm, to identify the optimal values of the simulation model configurable parameters. This enhances the capabilities of M&S tools and allows their use in operations. The approach’s effectiveness is shown by its application to real flying Earth observation satellite data and simulation models.}
}
@article{DING2023107416,
title = {Intelligent emergency digital twin system for monitoring building fire evacuation},
journal = {Journal of Building Engineering},
volume = {77},
pages = {107416},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107416},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223015966},
author = {Yifei Ding and Yuxin Zhang and Xinyan Huang},
keywords = {Fire emergency, Computer vision, Human behaviour, Digital twin, Building evacuation},
abstract = {The provision of real-time and detailed evacuation information feedback is vitally significant for the formulation and adaptation of the onsite evacuation strategy. The conventional surveillance system relying on surveillance cameras is limited to processing video and fails to extract human behaviour or provide privacy protection. This work proposes an Intelligent Emergency Digital Twin system based on computer vision and deep learning. The system comprises (1) CCTV network, (2) YOLOv4 evacuee detector, (3) DeepSORT evacuee tracker, (4) Perspective transformer, and (5) Digital Twin interface. It enables the detection and tracking of evacuees, the calculation of their egress speed, and the protection of their privacy in a digital interface. The proposed system was evaluated in a staircase of an office building through two types of tests with positive results: ratio of successfully detecting is 100% in individual objects test and about 90% in multiple objects test. The evacuation data generated by the digital twin system would be useful for guiding evacuations out of fire scenarios. This proposed digital twin framework can lay the foundation for the implementation of smart human monitoring in fire scenarios for buildings.}
}
@article{ELADLY2023101544,
title = {Enhancing circular economy via detecting and recycling 2D nested sheet waste using Bayesian optimization technique based-smart digital twin},
journal = {Results in Engineering},
volume = {20},
pages = {101544},
year = {2023},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2023.101544},
url = {https://www.sciencedirect.com/science/article/pii/S2590123023006710},
author = {Amira M. Eladly and Ahmed M. Abed and Moustafa H. Aly and Wessam M. Salama},
keywords = {Bayesian optimization technique, Circular economy, Classification, Deep learning, Segmentation, 2D nested waste, Sustainability, Waste management},
abstract = {The recycling process is controversial on a worldwide scale since it is based on the concept of sustainability and minimizing the environmental footprint. From this perspective, a practical framework for managing the 2D nesting waste is presented in this paper, which is a crucial phase in many production processes. Deep Learning (DLs) models are used to detect (i.e., segmentation) and categorize (i.e., classification) waste area sizes to enhance the circular economy via recycling procedures. Moreover, the apparel industry for having huge 2D nesting waste is implemented in this paper. Furthermore, data augmentation is performed in this paper to overcome the lack of datasets. The segmentation stage is applied based on the SegNet deep convolutional neural network (DCNN). In addition to, the classification stage based on the DLs, ResNet34, InceptionV3, and DenseNet121 is implemented to classify our datasets. The experimental results demonstrate that the proposed framework outperforms other existing techniques, where the Area Under the Curve (AUC) of 99.87 %, detection success rate (DSR) of 99.88 %, sensitivity of 99.98 %, precision of 99.98 %, F1-score of 98.99 %, mean square error (MSE) from 0.01 % to 2.49 %, efficiency of 97.41 %, and computational time of 3.9 s. Therefore, our proposed framework achieves the best performance compared with the literature. The future work will focus on enhancing the circular economy by utilizing the 3D nested sheet waste.}
}
@article{GUO2023102965,
title = {A digital twin to quantitatively understand aging mechanisms coupled effects of NMC battery using dynamic aging profiles},
journal = {Energy Storage Materials},
volume = {63},
pages = {102965},
year = {2023},
issn = {2405-8297},
doi = {https://doi.org/10.1016/j.ensm.2023.102965},
url = {https://www.sciencedirect.com/science/article/pii/S2405829723003434},
author = {Wendi Guo and Yaqi Li and Zhongchao Sun and Søren Byg Vilsen and Daniel Ioan Stroe},
keywords = {Digital twin, Aging mechanisms, NMC battery, Dynamic aging profiles, Charging protocols},
abstract = {Traditional lithium-ion battery modeling does not provide sufficient information to accurately verify battery performance under real-time dynamic operating conditions, particularly when considering various aging modes and mechanisms. To improve the current methods, this paper proposes a lithium-ion battery digital twin that can capture real-time data and integrate the strong coupling between SEI layer growth, anode crack propagation, and lithium plating. It can be utilized to estimate aging behavior from macroscopic full-cell level to microscopic particle level, including voltage-current profiles in dynamic aging conditions, predict the degradation behavior of Nickel-Manganese-Cobalt-Oxide (NMC) based lithium-ion batteries, and assist in electrochemical analysis. This model can improve the root cause analysis of cell aging, enabling a quantitative understanding of aging mechanism coupled effects. Three charging protocols with dynamic discharging profiles are developed to simulate real vehicle operation scenarios and used to validate the digital twin, combining operando impedance measurements, post-mortem analysis, and SEM to further prove the conclusions. The digital twin can accurately predict battery capacity fade within 0.4% MAE. The results indicate that SEI layer growth is the primary contributor to capacity degradation and resistance increase. Based on the analysis of the model, it is concluded that one of the proposed multi-step charging protocols, in comparison to a standard continuous charging protocol, can reduce the degradation of NMC-based lithium-ion batteries. This paper represents a firm physical foundation for future physics-informed machine learning development.}
}
@article{KHAN2023S127,
title = {10526 The World's First Proof of Concept of the Practical Potential in Artificially Intelligent Digital Twins in Advanced Laparoscopic Training},
journal = {Journal of Minimally Invasive Gynecology},
volume = {30},
number = {11, Supplement },
pages = {S127},
year = {2023},
note = {SI: Abstracts of the 52nd AAGL Global Congress on Minimally Invasive Gynecology},
issn = {1553-4650},
doi = {https://doi.org/10.1016/j.jmig.2023.08.404},
url = {https://www.sciencedirect.com/science/article/pii/S1553465023007355},
author = {ZR Khan},
abstract = {Study Objective
To develop the world's first digital twins of healthcare educators and trainers in advanced laparoscopic gynaecological surgery, utilising deep machine learning to produce a human-like interface which can lead tutorials and training on an online platform autonomously.
Design
Using a third generation neural network-based language prediction model a purpose coded artificial intelligence (AI) system was prompted to develop training modules for a AI-powered advanced laparoscopic gynaecological surgery training programme. The avatars for the training modules were custom built, based on the lead trainer's facial features and voice. The AI-powered modules were made part of a larger hybrid training programme with an online / webinar component, alongside live training by the same lead trainers.
Setting
The bespoke AI-generated training modules were added to an already existing advanced laparoscopic training programme for Consultants and Residents in Gynaecology.
Patients or Participants
10 Consultants and Residents in Gynaecology with an interest in laparoscopic surgery were taught by the digital twins of the lead trainers. The subject matter of the training modules was similar but the content was not the same.
Interventions
N/A.
Measurements and Main Results
Feedback was collected after the training programme with particular interest in the performance of the digital twins. The feedback was overwhelmingly positive. The digital twins have now also been purposed to carry out assessments in the form of multiple choice questions and collect trainee feedback with subjective responses.
Conclusion
For a fraction of the true cost of healthcare education, digital twins can enable remote learning, in multiple languages, allowing educators and trainers to reach a wider audience of students without the need for physical classroom space. This can be particularly important in healthcare education, where access to specialized training can be limited in certain geographic regions, not to mention the advantage of objectively tracking and analyzing every nuance of learning performance in the realm of advanced laparoscopic skills.}
}
@article{TAO2023113461,
title = {A digital twin-based fault diagnostic method for subsea control systems},
journal = {Measurement},
volume = {221},
pages = {113461},
year = {2023},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.113461},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123010254},
author = {Haohan Tao and Peng Jia and Xiangyu Wang and Xi Chen and Liquan Wang},
keywords = {Digital twin, Fault diagnosis, Hydraulic system, Subsea control system},
abstract = {A digital twin (DT) based framework is proposed for data-driven fault diagnosis in a subsea control system (SCS). A novel modeling technique, the physics informed temporal convolution network (PITCN), is first developed by combining a traditional physics-based simulation with collected sensor signals (e.g., pressure and flowrate). The DT is then used to generate simulated signals under different operation and fault conditions, for the purpose of training the convolutional neural network (CNN) based data-driven fault diagnostic model. In addition, an online model modification technique is proposed to label the SCS real-time data used for continuously training the PITCN and CNN during the SCS production period. Experimental results showed the proposed diagnostic framework is superior to traditional CNN based diagnostic methods, as measured by diagnostic accuracy, particularly when labeled sample volumes are limited. The proposed online model modification improved diagnostic accuracy from 91.87% to 97.5% using real-time collected data.}
}
@article{ZHU2023116444,
title = {A super-real-time three-dimension computing method of digital twins in space nuclear power},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {417},
pages = {116444},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116444},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523005686},
author = {Enping Zhu and Tao Li and Jinbiao Xiong and Xiang Chai and Tengfei Zhang and Xiaojing Liu},
keywords = {Digital twin, GPU and multi-core CPU, Machine learning, Super-real-time, Space nuclear reactor},
abstract = {Digital twins (DTs) have attracted widespread attention in academia and industry in recent years. It can accurately reflect the physical world in real-time, enabling online monitoring, control, and prediction operations. Their foundation is super-real-time computing and high data representation capabilities. However, current DTs do not achieve 3D super-real-time computing. This study proposes a novel 3D computational method for solving fluid–solid coupling problems in a super-real-time. The method is based on a mixed solution framework that combines traditional numerical methods with deep learning operators. Specifically, the method employs multi-core CPU parallel acceleration to solve the solid equations while leveraging the computing power of GPU to solve the fluid equations. The fluid–solid coupling is achieved through information exchange between the GPU and the multi-core CPU. In addition, the proposed method introduces a new deep learning operator framework based on the DeepONET. The framework is accompanied by a database structure that facilitates model training and validation and a loss function that guides the training. The space nuclear reactor, an improved TOPAZ-II system, was selected to demonstrate its feasibility. Four non-training transient conditions were simulated to test the generalization performance. The results show that the proposed method achieves an average error between the calculated results and reference values below 2.5%, with the average error of thermodynamic parameters below 1.5%. The average deviation between system parameter peak values during the transient process and the reference value was less than 5 s. The result meets the acceptable error level and satisfies the super-real-time requirements with a time acceleration ratio of approximately 1.17, which is 60 times faster than traditional numerical methods. The results demonstrate the accuracy and efficiency of the proposed method for DT.}
}
@article{CAO2023111868,
title = {Fault detection and classification in solar based distribution systems in the presence of deep learning and social spider method},
journal = {Solar Energy},
volume = {262},
pages = {111868},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.111868},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23005017},
author = {Hanhua Cao and Huanping Zhang and Changle Gu and Yuhuai Zhou and Xiu He},
keywords = {Fault classification and detection, Generative adversarial networks (GANs), Social spider optimization algorithm, Digital twin, Solar based distribution systems},
abstract = {This research proposes an intelligent method for fault detection and classification (FDC) in solar based distribution systems using Generative Adversarial Networks (GANs) and Social Spider method. The suggested method is constructed using the combination of GANs and Social Spider method, which is a hybrid system to increase its capability for the classification purposes. The GANs model is used to detect fault signatures in the system, while the Improved Social Spider method is used to reinforce its training process. The proposed method is evaluated on the big data gathered using the digital twin of a solar based distribution system for different situations of operation. The results show that GANs can detect fault signatures with high accuracy and the Improved Social Spider method can classify the fault types with high accuracy. The proposed method compared with other existing methods and the results show that the suggested method outperforms the existing methods considering accuracy, recall, precision and speed. The proposed method can be used for FDC in solar based distribution systems, and can be developed to other distribution systems.}
}
@article{MARTINEZGUTIERREZ2023103136,
title = {Convergence of Virtual Reality and Digital Twin technologies to enhance digital operators’ training in industry 4.0},
journal = {International Journal of Human-Computer Studies},
volume = {180},
pages = {103136},
year = {2023},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2023.103136},
url = {https://www.sciencedirect.com/science/article/pii/S1071581923001453},
author = {Alberto Martínez-Gutiérrez and Javier Díez-González and Paula Verde and Hilde Perez},
keywords = {Industry 4.0, Digital twin, Virtual reality, Training},
abstract = {Industry 4.0 technologies enable the generation of added value throughout the production process. Among them, Digital Twins (DT) allow the modelization of cyber–physical systems in the virtual world and Virtual Reality (VR) allows an immersive perspective of the behavior of industrial equipment in a digitized environment. The combination of DT and VR can generate a digital platform for operators’ training where the industrial operator can perceive a more realistic environment for digital learning. In this paper, we introduce the convergence of DT and VR to enhance the digital learning process of driving an industrial mobile robot. To validate this proposal, an experimental methodology looking for measuring the transfer of skills from digital training into the real world has been set. This experiment consists of handling a mobile robot in a predefined course looking for committing the lowest number of failures in the minimum possible time. The experiment has been carried out by defining three different training methods: training with real equipment as the control group and two different experimental groups following digital training (VR and computer application-supported techniques). The abilities of their subjects have been measured in the initial and final stages of the experiment showing an improvement of 47% through real training, 38% through VR and 28% through the computer application. Results demonstrate the utility of using DT to attain significant digital learning and validate the initial hypothesis demonstrating the enhancement of digital learning through VR-supported training.}
}
@article{CAO2023116156,
title = {Flow field distribution and structural strength performance evaluation of fixed offshore wind turbine based on digital twin technology},
journal = {Ocean Engineering},
volume = {288},
pages = {116156},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.116156},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823025404},
author = {Yu Cao and Xiaobo Tang and Jie Li and Wenhua Chu and Fang Wang},
keywords = {Offshore wind turbine, Digital twin, Rapid monitoring, Flow field distribution, Structural strength},
abstract = {Due to the adverse impact of the offshore environment, the cost of on-site monitoring, operation and maintenance of offshore wind turbines is greatly increased. Here, a digital twin (DT) method based on digital model and computational fluid dynamics (CFD) simulation database is proposed, which is used to rapidly predict and synchronously display the distribution of the wake field, structural deformation and stress of fixed OWTs. First, a large number of data have been calculated by using CFD method. Second, the three-dimensional finite element model is reduced to a digital model by the proper orthogonal decomposition method, all data is stored in a multi-source heterogeneous database. Furthermore, the anisotropic inverse distance weighted interpolation and particle swarm optimization algorithm methods are used to obtain uncalculated data results and supplement database. Then the Bayesian regularization-back propagation neural network method is developed to correct the data with large errors. The results show that the learning error of the flow field and structural strength is less than 10% and 4% compared with the CFD method, respectively. This study could provide a rapid monitoring engineering reference for the safety assessment of turbine flow field distribution and structural strength.}
}
@article{CHENGULA2023100510,
title = {Improving road safety with ensemble learning: Detecting driver anomalies using vehicle inbuilt cameras},
journal = {Machine Learning with Applications},
volume = {14},
pages = {100510},
year = {2023},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2023.100510},
url = {https://www.sciencedirect.com/science/article/pii/S2666827023000634},
author = {Tumlumbe Juliana Chengula and Judith Mwakalonge and Gurcan Comert and Saidi Siuhi},
keywords = {Driver anomaly detection, Road safety, XGBoost, Learner models, Vehicle cameras},
abstract = {The adoption of Advanced Driver Assistance Systems (ADAS) has expanded dramatically in recent years, with the goal of improving road safety and driving comfort. Driver monitoring is important to ADAS since it identifies abnormalities such as sleepiness, distraction, and impairment to guarantee safe vehicle operation. Traditional methods of detecting driver anomalies rely on intrusive physiological measures, while ADAS with built-in cameras offers a non-intrusive and cost-effective option. This study investigates the application of ensemble model learning for driver anomaly detection in automobiles employing ADAS and in-vehicle cameras. Deep learning models such as ResNet50, DenseNet201, and Inception V3 were deployed as learner models to classify driving behavior. The raw dataset used in this study was in the form of videos obtained from the National Tsinghua Driver Drowsiness Detection (NTHUDD) dataset. Amongst the two ensemble models used, the eXtreme Gradient Boost (XGBoost) classifier pooled predictions from the learner models. It attained a remarkable average accuracy and precision of 99% on the validation dataset. Classes such as laugh_talk and yawning were properly and separately distinguished. The ensemble technique capitalized on the strengths of various models while mitigating their weaknesses, resulting in robust and trustworthy forecasts. The findings highlight the potential of ensemble modeling to enhance driver anomaly detection systems, providing valuable insights for improving road safety. By continually monitoring driver behavior and detecting abnormalities, ADAS can provide timely warnings and interventions to prevent accidents and save human lives.}
}
@article{ZHANG2023117507,
title = {Digital twin of wind farms via physics-informed deep learning},
journal = {Energy Conversion and Management},
volume = {293},
pages = {117507},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2023.117507},
url = {https://www.sciencedirect.com/science/article/pii/S0196890423008531},
author = {Jincheng Zhang and Xiaowei Zhao},
keywords = {Digital twin, Lidar, NS equations, Physics-informed machine learning, Wind farm wake},
abstract = {The spatiotemporal flow field in a wind farm determines the wind turbines’ energy production and structural fatigue. However, it is not obtainable by the current measurement, modeling, and prediction tools in wind industry. Here we propose a novel data and knowledge fusion approach to create the first digital twin for onshore/offshore wind farm flow system, which can predict the in situ spatiotemporal wind field covering the entire wind farm. The digital twin is developed by integrating the Lidar measurements, the Navier–Stokes equations, and the turbine modeling using actuator disk method, via physics-informed neural networks. The design enables the seamless integration of Lidar measurements and turbine operating data for real-time flow characterization, and the fusion of flow physics for retrieving unmeasured wind field information. It thus addresses the limitations of existing wind prediction approaches based on supervised machine learning, which cannot achieve such prediction because the training targets are not available. Case studies of a wind farm under typical operating scenarios (i.e. a greedy case, a wake-steering case, and a partially-operating case) are carried out using high-fidelity numerical experiments, and the results show that the developed digital twin achieves very accurate mirroring of the physical wind farm, capturing detailed flow features such as wake interaction and wake meandering. The prediction error for the flow fields, on average, is just 4.7% of the value range. With the accurate flow field information predicted, the digital twin is expected to enable brand new research across wind farm lifecycle including monitoring, control, and load assessment.}
}
@article{SHI2023217,
title = {Multi-UAV-assisted computation offloading in DT-based networks: A distributed deep reinforcement learning approach},
journal = {Computer Communications},
volume = {210},
pages = {217-228},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.07.041},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423002748},
author = {Junling Shi and Chunyu Li and Yunchong Guan and Peiyu Cong and Jie Li},
keywords = {Mobile edge computing, Computation offloading, Unmanned aerial vehicle, Digital twin, Deep reinforcement learning, Federated learning},
abstract = {In the industrial Internet, Mobile Edge Computing (MEC) can provide the ability to transfer a large number of delay-sensitive and compute-intensive tasks to MEC servers, thus improving Quality of Service (QoS). Considering Unmanned Aerial Vehicles (UAVs) have the advantages of wide communication coverage and low deployment cost, UAVs have great potential to be employed as aerial base stations to provide computation resources for Intelligent Mobile Devices (IMDs). Due to the limited computation resources and energy of IMDs, we designed a multi-UAV-assisted MEC system in ground cells. To minimize the weighted sum of task completion delay and energy consumption, and ensure the QoS requirements of IMDs, we jointly consider the dynamic channel state, renewable energy utilization, UAVs trajectory, and tasks offloading ratio. To solve the non-convexity problem of complex high-dimensional states, we propose a model-free Deep Reinforcement Learning (DRL) offloading scheme based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm. Moreover, we adopt Federated Learning (FL) to train DRL models to enhance the robustness of the model and the security of IMDs data. Meanwhile, the real environment is modeled as Digital Twin (DT) to monitor network changes and train the local DRL model, and the central cloud server can obtain the local model in real-time to aggregate the global model. Extensive experimental numerical results show that the proposed algorithm improves the system energy efficiency and reduces task completion delay.}
}
@article{RENTERIADELTORO2023,
title = {Digitalization as an aggregate performance in the energy transition for nuclear industry},
journal = {Nuclear Engineering and Technology},
year = {2023},
issn = {1738-5733},
doi = {https://doi.org/10.1016/j.net.2023.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S1738573323005296},
author = {Florencia de los Angeles {Renteria del Toro} and Chen Hao and Akira Tokuhiro and Mario Gomez-Fernandez and Armando Gomez-Torres},
keywords = {Digitalization, New-energy transition era (NETE), Analytical network process (ANP), Nuclear power infrastructure development (NPID), Nuclear-technologies},
abstract = {The emerging technologies at the industrial level have deployed rapidly within the energy transition process innovations. The nuclear industry incorporates several technologies like Artificial Intelligence (AI), Machine Learning (ML), Digital Twins, High-Performance-Computing (HPC) and Quantum Computing (QC), among others. Factors identifications are explained to set up a regulatory framework in the digitalization era, providing new capabilities paths for nuclear technologies in the forthcoming years. The Analytical Network Process (ANP) integrates the quantitative-qualitative decision-making analysis to assess the implementation of different aspects in the digital transformation for the New-Energy Transition Era (NETE) with a Nuclear Power Infrastructure Development (NPID). 2023 Elsevier Ltd. All rights reserved.}
}
@article{SCHIRMANN2023115608,
title = {A comparison of physics-informed data-driven modeling architectures for ship motion predictions},
journal = {Ocean Engineering},
volume = {286},
pages = {115608},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115608},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823019923},
author = {Matthew L. Schirmann and James W. Gose and Matthew D. Collette},
keywords = {Machine learning, Ship motions, Digital twin, Neural network, Gaussian process, Research vessel},
abstract = {How to select the optimal formulations for building blended physics-machine learning models for ship motions is not currently clear. This work compares and contrasts two approaches to this problem: (1) A black-box deep learning approach based on a new neural network architecture that can better handle varying wave conditions, and (2) a clear-box model based on updates to linear response amplitude operators via a Gaussian process regression. Both models are trained and evaluated on a dataset consisting of more than 15,000 30-minute-long motion observation windows from two research vessels at sea in the Atlantic and Pacific oceans. Three different hindcast weather services are used, including two models from the EU’s Copernicus system and NOAA’s WAVEWATCH III. The evaluation shows that a tradeoff exists between the formulations, with the black-box formulation offering higher accuracy and the cost of less transparency. The weather hindcast used has a small impact on the results, and the ability of both models to generalize predictions between near-sister ships is also encouraging for the practical application of these techniques.}
}
@article{MCMANUS2023110000,
title = {Digital twin-enabled domain adaptation for zero-touch UAV networks: Survey and challenges},
journal = {Computer Networks},
volume = {236},
pages = {110000},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110000},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004450},
author = {Maxwell McManus and Yuqing Cui and Josh (Zhaoxi) Zhang and Jiangqi Hu and Sabarish Krishna Moorthy and Nicholas Mastronarde and Elizabeth Serena Bentley and Michael Medley and Zhangyu Guan},
keywords = {UAV, Digital twin, Domain adaptation, Network softwarization, AI/ML},
abstract = {In existing wireless networks, the control programs have been designed manually and for certain predefined scenarios. This process is complicated and error-prone, and the resulting control programs are not resilient to disruptive changes. Data-driven control based on Artificial Intelligence and Machine Learning (AI/ML) has been envisioned as a key technique to automate the modeling, optimization and control of complex wireless systems. However, existing AI/ML techniques rely on sufficient well-labeled data and may suffer from slow convergence and poor generalizability. In this article, focusing on digital twin-assisted wireless unmanned aerial vehicle (UAV) systems, we provide a survey of emerging techniques that can enable fast-converging data-driven control of wireless systems with enhanced generalization capability to new environments. These include simultaneous localization and sensing (SLAM)-based sensing and network softwarization for digital twin construction, robust reinforcement learning and system identification for domain adaptation, and testing facility sharing and federation. The corresponding research opportunities are also discussed.}
}
@article{UDUGAMA202363,
title = {Digital tools in chemical engineering education: The needs and the desires},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {63-70},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000192},
author = {Isuru A. Udugama and Martin Atkins and Christoph Bayer and James Carson and Duygu Dikicioglu and Krist V. Gernaey and Jarka Glassey and Matthew Taylor and Brent R. Young},
keywords = {Digitalisation, Digital tools in education, Non-technical barriers, Digital twins},
abstract = {Educators in chemical engineering have a long and rich history of employing digital tools to solve fundamental engineering problems. Today, with the megatrend of digitalisation, there is a growing set of tools that can be used for chemical engineering education. However, identifying which tool is ideally suited to support teaching a given chemical engineering concept can be challenging. To answer this question a survey was distributed to Heads of Departments at IChemE institutions and members of the IChemE committees focused on digitalisation. The survey respondents rated Microsoft Excel (VBA), commercial simulators, and scripting tools as ideal for teaching core subjects such as mass and energy balances, mass transfer and reaction engineering while respondents found 3D Models, and Virtual/Augmented Reality models as being most suited for teaching subjects such as process design, safety and sustainability. Mathematical/programming simplicity, ease of maintenance, and low initial investment costs were identified as key non-technical aspects that will hinder the adoption of a given digital tool. Weighing the benefits of education and non-technical hurdles, the respondents preferred the use of simpler digitalisation platforms such as Excel and scripting languages over the more advanced platforms such as Virtual/Augmented Reality where possible. It was identified that the widespread adoption of more advanced digitalisation tools will require removal of the above mentioned non-technical barriers as well as other barriers such as tool shareability.}
}
@article{ZHOU2023120327,
title = {Synthetic data generation method for data-free knowledge distillation in regression neural networks},
journal = {Expert Systems with Applications},
volume = {227},
pages = {120327},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120327},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423008291},
author = {Tianxun Zhou and Keng-Hwee Chiam},
keywords = {Data-free knowledge distillation, Knowledge distillation, Neural network, Regression, Machine learning},
abstract = {Knowledge distillation is the technique of compressing a larger neural network, known as the teacher, into a smaller neural network, known as the student, while still trying to maintain the performance of the larger neural network as much as possible. Existing methods of knowledge distillation are mostly applicable for classification tasks. Many of them also require access to the data used to train the teacher model. To address the problem of knowledge distillation for regression tasks in the absence of original training data, the existing method uses a generator model trained adversarially against the student model to generate synthetic data to train the student model. In this study, we propose a new synthetic data generation strategy that directly optimizes for a large but bounded difference between the student and teacher model. Our results on benchmark experiments demonstrate that the proposed strategy allows the student model to learn better and emulate the performance of the teacher model more closely.}
}
@article{ZHANG2023102571,
title = {A deep learning-enabled human-cyber-physical fusion method towards human-robot collaborative assembly},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {83},
pages = {102571},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102571},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000479},
author = {Chao Zhang and Guanghui Zhou and Dongxu Ma and Rui Wang and Jiacheng Xiao and Dan Zhao},
keywords = {Human-cyber-physical system, Human-robot collaboration, Deep learning, Smart assembly, Augmented reality, Digital twin},
abstract = {Human-robot collaborative (HRC) assembly has become popular in recent years. It takes full advantage of the strength, repeatability and accuracy of robots and the high-level cognition, flexibility and adaptability of humans to achieve an ergonomic working environment with better overall productivity. However, HRC assembly is still in its infancy nowadays. How to ensure the safety and efficiency of HRC assembly while reducing assembly failures caused by human errors is challenging. To address the current challenges, this paper proposes a novel human-cyber-physical assembly system (HCPaS) framework, which combines the powerful perception and control capacity of digital twin with the virtual-reality interaction capacity of augmented reality (AR) to achieve a safe and efficient HRC environment. Based on the framework, a deep learning-enabled fusion method of HCPaS is proposed from the perspective of robot-level fusion and part-level fusion. Robot-level fusion perceives the pose of robots with the combination of PointNet and iterative closest point (ICP) algorithm, where the status of robots together with their surroundings could be registered into AR environment to improve the human's cognitive ability of complex assembly environment, thus ensuring the safe HRC assembly. Part-level fusion recognizes the type and pose of parts being assembled with a parallel network that takes an extended Pixel-wise Voting Network (PVNet) as the base architecture, on which assembly sequence/process information of the part could be registered into AR environment to provide smart guidance for manual work to avoid human errors. Eventually, experimental results demonstrate the effectiveness and efficiency of the approach.}
}
@article{LI2023110016,
title = {Constructing a probability digital twin for reactor core with Bayesian network and reduced-order model},
journal = {Annals of Nuclear Energy},
volume = {193},
pages = {110016},
year = {2023},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2023.110016},
url = {https://www.sciencedirect.com/science/article/pii/S0306454923003353},
author = {Wenhuai Li and Jiejin Cai and Haoliang Lu and Junling Wang and Li Cai and Zhihong Tang and Jinggang Li and Chao Wang},
keywords = {Data assimilation, Machine learning, Bayesian neural network, Model order reduction, Digital twins, Reactor core},
abstract = {In constructing a digital twin for a nuclear reactor core, it is important to consider the influence of randomness from various sources. Data assimilation (DA) can combine time distribution observations with dynamic models to approximate the real state of a physical system. Machine learning (ML) and DA share similarities under the Bayesian framework, and using probabilistic ML may provide a way to improve or replace current DA techniques. This paper proposes using a probabilistic ML as Bayesian neural network (BNN) to solve an inverse problem of core monitoring and demonstrates its feasibility through a pressurized water reactor core simulation analysis. Model order reduction technology is also analyzed, and the feasibility and benefit of using it to achieve core monitoring under steady-state conditions is preliminarily verified and discussed. Future work will focus on improving estimation and prediction models under transient operating conditions by unifying DA and ML under the Bayesian framework.}
}
@article{JIANG2023106370,
title = {Graph Neural Networks (GNNs) based accelerated numerical simulation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106370},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106370},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623005547},
author = {Chunhao Jiang and Nian-Zhong Chen},
keywords = {Surrogate model, Graph neural networks, Machine learning, Numerical simulation},
abstract = {Finite element method (FEM) based high-fidelity simulation can be computationally demanding and time-consuming as engineering problems become more complicated. It is thus necessary to develop a surrogate model that only requires a small amount of computational time but retains sufficient accuracy. A graph neural network (GNN) based framework is proposed as a general surrogate model for FEM to simulate the Von Mises stress distribution. The mesh body is embedded to a graph and a novel global attribute representation is introduced to capture the geometry and boundary conditions while overcoming the common problem of over smoothing in graph deep learning. The challenge to deal with varying geometry and boundary conditions is overcome by the proposed model and thus it outperforms existing methods such as proper orthogonal decomposition (POD) and greedy algorithm in terms of generalization. Numerical experiments are given to demonstrate the capability of the model developed and the results show that the proposed model not only accurately predicts the stress distribution but also speed-ups hundreds of times faster compared to a FEM-based simulator, enabling real-time structural response analysis for the application of digital twin and structural health monitoring. It is indicated that GNNs can be a powerful tool for resolving complex physical problems, thereby assisting in advancing science and enhancing engineering productivity.}
}
@article{PERNO2023103987,
title = {A machine learning digital twin approach for critical process parameter prediction in a catalyst manufacturing line},
journal = {Computers in Industry},
volume = {151},
pages = {103987},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103987},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001379},
author = {Matteo Perno and Lars Hvam and Anders Haug},
keywords = {Digital twin, Machine learning, Industry 4.0, Catalyst, Virtual reality, Process industry},
abstract = {Digital twins (DTs) are rapidly changing how manufacturing companies leverage the large volumes of data they generate daily to gain a competitive advantage and optimize their supply chains. When coupled with recent developments in machine learning (ML), DTs have the potential to generate invaluable insights for process manufacturing companies to help them optimize their manufacturing processes. However, this potential has yet to be fully exploited due to the challenges that process manufacturing companies face in developing and implementing DTs in their organizations. Although DTs are receiving increasing attention in both industry and academia, there is limited literature on how to apply them in the process industry. To address this gap, this paper presents a framework for developing ML-based DTs to predict critical process parameters in real time. The proposed framework is tested through a case study at an international process manufacturing company in which it was used to collect and process plant data, build accurate predictive models for two critical process parameters, and develop a DT application to visualize the models’ predictions. The case study demonstrated the usefulness of the proposed DT–ML framework in the sense that it provided the company with more accurate predictions than the models it previously applied. The study provides insights into the value of applying ML-based DT in the process industry and sheds light on some of the challenges associated with the application of this technology.}
}
@article{ZHAO2023103903,
title = {Super learner ensemble model: A novel approach for predicting monthly copper price in future},
journal = {Resources Policy},
volume = {85},
pages = {103903},
year = {2023},
issn = {0301-4207},
doi = {https://doi.org/10.1016/j.resourpol.2023.103903},
url = {https://www.sciencedirect.com/science/article/pii/S0301420723006141},
author = {Jue Zhao and Shahab Hosseini and Qinyang Chen and Danial {Jahed Armaghani}},
keywords = {Metal prices, Copper prices, Super-learner, SVR, XGBoost, ANN, Cubist},
abstract = {Companies and governments dependent on copper mining need to be able to predict copper prices in order to make important decisions. Despite the nonlinear and nonstationary nature of copper prices, their periods may vary as they fluctuate due to potential growth, cyclical fluctuations and errors. A trend-cycle refers to the combination of trend and cyclical components. Trend-cycles are characterized by different characteristics, which are crucial to making predictions. Therefore, this study focuses on developing and proposing a novel model based on an ensemble machine learning technique to predict monthly copper prices in the future by using different soft computing methods, including multi-layer perception (MLP) neural network, support vector regression (SVR), and extreme gradient boosting (XGBoost). The monthly copper price dataset from August 2001 to August 2021 was gathered for this aim based on the 14 effective parameters. These parameters were selected based on the suggestions of previous research. The main novelty of this study is the development most accurate model to predict monthly copper prices using Cubist algorithm-based super learner model as the new predictive system. The results indicated that the proposed super learner models outperformed of MLP, SVR, and XGBoost models based on the determination coefficient (R-squared), value account for (VAF), root mean square of errors (RMSE), Accuracy (Acc) and Mean Absolute Relative Error (MARE). A comprehensive comparison of different artificial intelligence models demonstrates that MLP neural network is the best model for predicting monthly copper prices. However, the standalone models involving MLP, SVR, and XGBoost, presented higher error with an RMSE in the interval of [278.3826 - 502.6946], and MARE in the interval of [0.056 - 0.1277]. Hence, Cubist-based super learner can be employed as a reliable system to predict monthly copper prices in the future. Besides, this study presents a rational mathematical model based on gene expression programming (GEP) for copper price prediction in future and for use by other researchers. Noteworthy, the final step of the study was sensitivity analysis conducting, which the results revealed that “lead price” and “euro to USD” parameters have respectively the lowest and highest impact on the monthly copper price.}
}
@article{WANG2023107200,
title = {A lightweight crack segmentation network based on knowledge distillation},
journal = {Journal of Building Engineering},
volume = {76},
pages = {107200},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107200},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223013803},
author = {Wenjun Wang and Chao Su and Guohui Han and Heng Zhang},
keywords = {Crack segmentation, Deep learning, Knowledge distillation, Lightweight network, Channel-wise distillation},
abstract = {This paper presents a novel approach for addressing the challenges of large parameter volumes and high computational complexity in existing deep learning models for crack detection. This method involves training a student model using a pretrained teacher model to guide the learning process. The novelty of the method is the use of channel-wise knowledge distillation to normalize activation maps between the teacher and student models, followed by the minimization of the asymmetric Kullback–Leibler divergence to achieve optimal model performance. By focusing on imitating regions with prominent activation values, the student model achieves accurate crack localization. Test results show that the method improves crack segmentation, based on improvements in the F1_score and intersection over union by 2.17% and 3.55%, respectively, and outperforms other compared knowledge distillation methods. A lightweight crack segmentation model that ensures accuracy and efficiency is established in this study, which can provide an efficient solution for crack segmentation in real-world scenarios.}
}
@article{DEKONING2023916,
title = {Digital twins: dynamic model-data fusion for ecology},
journal = {Trends in Ecology & Evolution},
volume = {38},
number = {10},
pages = {916-926},
year = {2023},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2023.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169534723000903},
author = {Koen {de Koning} and Jeroen Broekhuijsen and Ingolf Kühn and Otso Ovaskainen and Franziska Taubert and Dag Endresen and Dmitry Schigel and Volker Grimm},
keywords = {digital twins, biodiversity conservation, evidence-based conservation, model-data integration, real-time monitoring, digital conservation},
abstract = {Digital twins (DTs) are an emerging phenomenon in the public and private sectors as a new tool to monitor and understand systems and processes. DTs have the potential to change the status quo in ecology as part of its digital transformation. However, it is important to avoid misguided developments by managing expectations about DTs. We stress that DTs are not just big models of everything, containing big data and machine learning. Rather, the strength of DTs is in combining data, models, and domain knowledge, and their continuous alignment with the real world. We suggest that researchers and stakeholders exercise caution in DT development, keeping in mind that many of the strengths and challenges of computational modelling in ecology also apply to DTs.}
}
@article{LIANG2023108833,
title = {Data-driven digital twin method for leak detection in natural gas pipelines},
journal = {Computers and Electrical Engineering},
volume = {110},
pages = {108833},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.108833},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623002574},
author = {Jing Liang and Li Ma and Shan Liang and Hao Zhang and Zhonglin Zuo and Juan Dai},
keywords = {Digital twin, Leak detection, Natural gas pipeline, Deep learning},
abstract = {Leak detection in natural gas pipelines is an extremely important and persistent problem in the oil and gas industry. The construction of accurate physical models of pipelines is limited by the high complexity, unavailable closed-form solution, and strict experienced personnel requirements. Moreover, industrial automation has the common problems of large amount of data with little information. This paper proposes a data-driven digital twin (DT) method for leak detection as a new paradigm solution to these challenges. From the perspective of knowledge-based data-driven, a DT pipeline learning and updating scheme based on normal data directly from operational data during the entity pipelines life-cycle is proposed to enhance DT adaptability. A DT-driven leak detection method is proposed, making effective use of data interaction and fusion of DT. The effectiveness and performance of the proposed approach is illustrated by deploying the DT pipeline in a simulated leak scenario of a real running natural gas pipeline. © 2012 Published by Elsevier Ltd. Selection and/or peer-review under responsibility of Global Science and Technology Forum Pte Ltd}
}
@article{SOMMER2023100462,
title = {Automated generation of digital twin for a built environment using scan and object detection as input for production planning},
journal = {Journal of Industrial Information Integration},
volume = {33},
pages = {100462},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100462},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X23000353},
author = {Markus Sommer and Josip Stjepandić and Sebastian Stobrawa and Moritz von Soden},
keywords = {Digital twin, Digital factory, Object recognition, Indoor object acquisition, Simulation, Artificial intelligence},
abstract = {The simulation of production processes using a digital twin can be utilized for prospective planning, analysis of existing systems or process-parallel monitoring. In all cases, the digital twin offers manufacturing companies room for improvement in production and logistics processes leading to cost savings. However, many companies, especially small and medium-sized enterprises, do not apply the technology, because the generation of a digital twin in a built environment is cost-, time- and resource-intensive and IT expertise is required. These obstacles will be overcome by generating a digital twin using a scan of the shop floor and subsequent object recognition. This paper describes the approach with multiple steps, parameters, and data which must be acquired in order to generate a digital twin automatically. It is also shown how the data is processed to generate the digital twin and how object recognition is integrated into it. An overview of the entire process chain is given as well as results in an application case.}
}
@article{DHAKANE2023100264,
title = {A Graph Dynamical neural network approach for decoding dynamical states in ferroelectrics.},
journal = {Carbon Trends},
volume = {11},
pages = {100264},
year = {2023},
issn = {2667-0569},
doi = {https://doi.org/10.1016/j.cartre.2023.100264},
url = {https://www.sciencedirect.com/science/article/pii/S2667056923000196},
author = {Abhijeet Dhakane and Tian Xie and Dundar E. Yilmaz and Adri C.T. van Duin and Bobby G. Sumpter and P. Ganesh},
keywords = {Ferroelectrics, Machine-Learning, Neural-Networks, Digital Twin, Phase-transitions, Domain walls},
abstract = {Ferroelectric materials such as BaTiO3 show tremendous potential for emerging advances in memory devices, particular neuromorphic type devices. High density of memory can be obtained by stabilising polar domain walls at the nanoscale, regions of discontinuity between the well-defined polarization order parameter, but little is known about what controls their structure and dynamics in real nanoscale materials. Indeed, chiral polar domain walls have been observed in heterogeneous ferroelectrics, such as oxygen-deficient BaTiO3, but very little is known about how such polar-domains walls interact with defects. Indeed, a critical understanding of how dynamics of domain-walls depend on point-defects is crucial to create engineered ferroelectric memory devices. We perform large-scale simulations of nansocale domain-wall dynamics in pristine and defective BaTiO3 using reactive force-field developed by us earlier (Phys. Chem. Chem. Phys., 2019, 21, 18240–18249), and capture their dynamical dependence on point defects using a graph dynamical neural-network approach, which we adapted to interrogate solids with well-defined order-parameters, and implemented using Pytorch based libraries. Our machine learning (ML) approach goes beyond the traditional post-processing methods to capture both spatial and temporal heterogeneities of large-scale molecular dynamics simulations of complex defective ferroelectric oxide materials. We crucially find that isolated oxygen vacancies introduce very localized spatial regions (∼ 1–2 unit-cell in length) that show slow dipole relaxation due to formation of defect-dipoles, and that these defect-dipoles in turn slow the intrinsic dynamics of domain walls. Further, the roughness of domain walls, also influenced by vacancies, introduce dynamic heterogeneity along the domain-wall [1]. As such we find a novel mechanism by which quenched disorder due to defects introduce dynamic heterogeneity thereby influencing response to external fields (particularly time varying fields) in a ferroelectric. Our study also emphasizes the need for creating digital twins of dynamical quantities to achieve autonomous in operando control of nanoscale switching.}
}
@article{SHAO2023109781,
title = {Conditional pseudo-supervised contrast for data-Free knowledge distillation},
journal = {Pattern Recognition},
volume = {143},
pages = {109781},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109781},
url = {https://www.sciencedirect.com/science/article/pii/S003132032300479X},
author = {Renrong Shao and Wei Zhang and Jun Wang},
keywords = {Model compression, Knowledge distillation, Representation learning, Contrastive learning, Privacy protection},
abstract = {Data-free knowledge distillation (DFKD) is an effective manner to solve model compression and transmission restrictions while retaining privacy protection, which has attracted extensive attention in recent years. Currently, the majority of existing methods utilize a generator to synthesize images to support the distillation. Although the current methods have achieved great success, there are still many issues to be explored. Firstly, the outstanding performance of supervised learning in deep learning drives us to explore a pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods cannot distinguish the distributions of different categories of samples, thus producing ambiguous samples that may lead to an incorrect evaluation by the teacher. Besides, current methods cannot optimize the category-wise diversity samples, which will hinder the student model learning from diverse samples and further achieving better performance. In this paper, to address the above limitations, we propose a novel learning paradigm, i.e., conditional pseudo-supervised contrast for data-free knowledge distillation (CPSC-DFKD). The primary innovations of CPSC-DFKD are: (1) introducing a conditional generative adversarial network to synthesize category-specific diverse images for pseudo-supervised learning, (2) improving the modules of the generator to distinguish the distributions of different categories, and (3) proposing pseudo-supervised contrastive learning based on teacher and student views to enhance diversity. Comprehensive experiments on three commonly-used datasets validate the performance lift of both the student and generator brought by CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git}
}
@article{GALEAZZI2023108252,
title = {Development of a surrogate model of an amine scrubbing digital twin using machine learning methods},
journal = {Computers & Chemical Engineering},
volume = {174},
pages = {108252},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108252},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423001229},
author = {Andrea Galeazzi and Kristiano Prifti and Carlo Cortellini and Alessandro {Di Pretoro} and Francesco Gallo and Flavio Manenti},
keywords = {Machine-learning, Surrogate modeling, Digital twin, Amine scrubbing, Design of experiments, Latin hypercube},
abstract = {Advancements in the process industry require building more complex simulations and performing computationally intensive operations like optimization. To overcome the numerical limit of conventional process simulations a surrogate model is a viable strategy. In this work, a surrogate model of an industrial amine scrubbing digital twin has been developed. The surrogate model has been built based on the process simulation created in Aspen HYSYS and validated as a digital twin against real process data collected during a steady-state operation. The surrogate relies on an accurate Design of Experiments procedure. In this case, the Latin-Hypercube method has been chosen and several nested domains have been defined in ranges around the nominal steady state operative condition. Several machine learning models have been trained using cross-validation, and the most accurate has been selected to predict each target. The resulting surrogate model showed a satisfactory performance, given the data available.}
}
@article{BRAHMA2023100754,
title = {Learning impact of recent ICT advances based on virtual reality IoT sensors in a metaverse environment},
journal = {Measurement: Sensors},
volume = {27},
pages = {100754},
year = {2023},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2023.100754},
url = {https://www.sciencedirect.com/science/article/pii/S2665917423000909},
author = {Mahul Brahma and M. Anline Rejula and Bhavana Srinivasan and S.N. Kumar and W. Aisha Banu and K. Malarvizhi and S. Sharon Priya and Abhishek Kumar},
keywords = {ICT, Women in technology, Digital twin, IoMT},
abstract = {The use of avatars in the Metaverse and virtual reality technology in education is a rapidly developing field that has the potential to revolutionize the way students learn. In the context of the Indian EdTech market, virtual reality could play a significant role in enhancing the learning experience for students, although more research is needed to explore its full potential.ICT has already made a significant impact on education, with academic programming, technical instruction, and theoretical concepts being taught through digital platforms. Virtual reality has the potential to take this a step further, allowing students to experience immersive and interactive learning environments that simulate real-world scenarios.However, as mentioned in the paragraph, there is still limited research on the use of virtual reality in education, particularly in the Indian context. Further studies are needed to assess the effectiveness of this technology and ensure that it can be integrated effectively into the educational system.Overall, the use of virtual reality in education is an exciting development in the ICT industry and has the potential to make a significant impact on the learning experience for students. Various non-governmental organizations (NGOs) seek to improve women's lives and provide them with self-sufficiency training. The government is likewise concerned about this issue and has established a number of programs to assist women. Information and communication technology (ICT) is significant in this context. ICT has improved the security, knowledge, education, employability, confidence, and popularity of women. The significance of ICT for the empowerment of women is highlighted in this study. To illuminate the various facets of ICT's impact on society, various examples have been addressed. Because of ICT, new generations of women all over the globe have defied all expectations and proven themselves in every aspect of life, even the most complicated and time-consuming realm of enterprise. In this regard, ICT may possibly become a magic wand for improving women's current situations. In this research, the eight factors that influence how ICT affects rural women's empowerment were found.}
}
@article{XU2023110139,
title = {Deep-learning-enhanced digital twinning of complex composite structures and real-time mechanical interaction},
journal = {Composites Science and Technology},
volume = {241},
pages = {110139},
year = {2023},
issn = {0266-3538},
doi = {https://doi.org/10.1016/j.compscitech.2023.110139},
url = {https://www.sciencedirect.com/science/article/pii/S0266353823002324},
author = {Xiaoyao Xu and Guowen Wang and Han Yan and Laibin Zhang and Xuefeng Yao},
keywords = {Digital twin, Machine learning, Fabric rubber composites, Anisotropy, Mechanical properties},
abstract = {Digital twins are undergoing growth that enables highly informative and scaleable interaction between physical objects and virtual twins, which is of great significance to the life cycle analysis of composites. Real-time fine evolution is challenging due to vast combinations of input features and high-resolution calculated variables. Here, we systematically demonstrate an AI-based methodology for digital twinning of complex composite structures. First, three types of deep neural networks are created with optionally used autoencoders as surrogate models, with architectures and data processing inspired by the rule-of-mixture of composites. Second, the prediction accuracy and efficiency are evaluated quantitatively and qualitatively, demonstrating the feasibility of predicting 3D displacement and stress fields directly from sensing data of temperature, pressure and loading displacement, and the optimal architecture is selected to be the evolving digital twin. Finally, the real-time interactive experiments are relayed to the digital twin and demonstrated that it can interact with physical objects and evolve online with high accuracy. These results indicate that the computational time can be reduced by 3∼6 orders of magnitude with high information intensity and scalability compared with conventional numerical and experimental methods, which opens up the avenues for the cost-effective and efficient development of digital twin services for composites.}
}
@article{KLASS2023233308,
title = {Lifelong performance monitoring of PEM fuel cells using machine learning models},
journal = {Journal of Power Sources},
volume = {580},
pages = {233308},
year = {2023},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2023.233308},
url = {https://www.sciencedirect.com/science/article/pii/S0378775323006845},
author = {Lukas Klass and Alexander Kabza and Frank Sehnke and Katharina Strecker and Markus Hölzle},
keywords = {Fuel cells, Performance monitoring, Artificial intelligence, Machine learning, Digital twin, LSTM},
abstract = {The development of fuel cells highly depends on the reliable operation of fuel cells on test benches for testing purposes. Even though the test bench’s control software contains an alarm module, it is only able to detect the most extreme failures due to the widespread operating parameter range of a fuel cell. This paper presents a novel machine learning based approach to monitor the operation of fuel cell stacks on a test bench and thereby ensuring the proper conduction of the tests. Methods for monitoring the operating conditions set by the test bench using clustering as well as methods to monitor the fuel cell’s performance using digital twins are proposed. The developed methods are applied on real testing data to demonstrate their ability to detect even slight deviations that remained undiscovered to the state of the art monitoring system of the test bench. After automating, the proposed methods allow a more sensitive monitoring of the fuel cell operation on test benches leading to more usable data and clearer test results and thereby speeding up the development of fuel cells.}
}
@article{MULLERZHANG2023103933,
title = {Towards live decision-making for service-based production: Integrated process planning and scheduling with Digital Twins and Deep-Q-Learning},
journal = {Computers in Industry},
volume = {149},
pages = {103933},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103933},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000830},
author = {Zai Müller-Zhang and Thomas Kuhn and Pablo Oliveira Antonino},
keywords = {Digital Twin, Reinforcement Learning, Deep-Q-Network integrated process planning and scheduling, Smart manufacturing},
abstract = {Production flow is becoming increasingly complex since manufacturers must react quickly to changing markets demands and diverse customer requirements. In order to ensure production efficiency, it is essential to have an adequate scheduling system capable of managing diverse process flows and handling unforseen changes. In this paper, we present an approach leveraging Digital Twins (DTs) and Deep-Q-Learning to perform integrated process planning and scheduling for service-based production. DTs of production assets provide live information about their physical entities for our approach to perform live decision-making based on the current operation conditions. We use Deep-Q-Learning which is a deep Reinforcement Learning (RL) algorithm to perform integrated process planning and scheduling. We present two RL-designs that deal with different situations of live decision-making. We have evaluated the learning efficiency and scalability of the RL-designs on a virtual aluminum cold rolling mill developed by the SMS Group,11https://www.sms-group.com/plants/cold-rolling-mills-for-aluminum. in the context of the BaSys 4.2 project.22https://www.eclipse.org/research/projects/basys_42/. The results show that the first RL-design is suitable for deriving schedules for individualized production with small lots where process plans must be re-calculated frequently, while the second RL-design is optimal for production with large job quantities where jobs arrive continuously.}
}
@article{WANG2023115027,
title = {Lifetime prediction of electronic devices based on the P-stacking machine learning model},
journal = {Microelectronics Reliability},
volume = {146},
pages = {115027},
year = {2023},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2023.115027},
url = {https://www.sciencedirect.com/science/article/pii/S0026271423001270},
author = {Fei Wang and Ye Yang and Tao Huang and Yang Xu},
keywords = {Stacking algorithm, Pearson correlation analysis, Machine learning, IGBT device, Lithium-ion battery, Lifetime prediction},
abstract = {Nowadays the data-driven artificial intelligence (AI) and machine learning (ML) provide novel approaches for the effective lifetime prediction of the electronic devices with complicated mechanisms and multiple controlling factors. However, the existing ML prediction models cannot process both high accuracy and high efficiency at the meantime. To address this problem, this paper proposes the new P-Stacking ML algorithm model, which combines the Pearson correlation analysis and the Stacking multi-model fusion. The Pearson correlation analysis is first applied to select the multiple base learner models with weak correlations for the following Stacking multi-model fusion. Two neutral network layers are built to perform the Stacking algorithm. In this work, two distinct types of electronic devices are utilized to verify the effectiveness of the P-Stacking ML model, which are the IGBT devices and the lithium-ion batteries respectively. For the IGBT lifetime prediction, the results have shown that compared with the long-term and short-term memory neural network (LSTM) model in the previous literature, the mean square error (MSE) of the P-Stacking ML model is improved by 6 %, and the average model training time is reduced by 90 %. Moreover, for the lithium batteries, the lifetime prediction accuracy of the P-Stacking model is increased by 65 %, and the training time is decreased by 90 %. The results demonstrated that the P-Stacking ML model can significantly improve both the prediction accuracy and efficiency simultaneously.}
}
@article{CURRIE2023108337,
title = {The emerging role of artificial intelligence and digital twins in pre-clinical molecular imaging},
journal = {Nuclear Medicine and Biology},
volume = {120-121},
pages = {108337},
year = {2023},
issn = {0969-8051},
doi = {https://doi.org/10.1016/j.nucmedbio.2023.108337},
url = {https://www.sciencedirect.com/science/article/pii/S0969805123000240},
author = {Geoffrey M. Currie},
keywords = {Molecular imaging, Deep learning, Artificial intelligence, Digital twin, Mouse twin},
abstract = {Introduction
Pre-clinical molecular imaging, particularly with mice, is an essential part of drug and radiopharmaceutical development. There remain ethical challenges to reduce, refine and replace animal imaging where possible.
Method
A number of approaches have been adopted to reduce the use of mice including using algorithmic approaches to animal modelling. Digital twins have been used to create a virtual model of mice, however, exploring the potential of deep learning approaches to digital twin development may enhance capabilities and application in research.
Results
Generative adversarial networks produce generated images that sufficiently resemble reality that they could be adapted to create digital twins. Specific genetic mouse models have greater homogeneity making them more receptive to modelling and suitable specifically for digital twin simulation.
Conclusion
There are numerous benefits of digital twins in pre-clinical imaging including improved outcomes, fewer animal studies, shorter development timelines and lower costs.}
}
@article{RATHNASIRI2023102085,
title = {Data-driven approaches to built environment flood resilience: A scientometric and critical review},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102085},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102085},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002136},
author = {Pavithra Rathnasiri and Onaopepo Adeniyi and Niraj Thurairajah},
keywords = {Built assets, Data-driven, Computational methods, Community, Environment, Flood, Resilience, Society},
abstract = {Environmental hazards such as floods significantly frustrate the functionality of built assets. In addressing flood-induced challenges, data usage has become important. Despite existing vast flood-related research, no research has presented a comprehensive insight into global studies on data-driven built environment flood resilience. Hence, this study conducted a comprehensive review of data-driven approaches to flood resilience. Scientometric analysis revealed emerging countries, authorships, keywords, and research hotspots. The critical review revealed data-centric approaches such as Machine Learning (ML), Artificial Intelligence (AI), Flood Simulations, Bayesian Modelling, Building Information Modelling (BIM) and Geographic Information Systems (GIS). However, they were mainly deployed in hydraulic flood simulations for prediction, monitoring, risk, and damage assessments. Further, the potentials of computational methods in tackling built environment resilience challenges were identified. Deploying the approaches in the future requires a better understanding of the status quo. These methods include hybrid data-driven approaches, ontology-based knowledge representation, multiscale modelling, knowledge graphs, blockchain technology, convolutional neural networks, automated approaches integrated with social media data, data assimilation, BIM models linked with sensors and satellite imagery and ML and AI-based digital twin models. Nevertheless, reference to data-informed built-asset resilience decisions and clear-cut implications on built-asset resilience improvement remain indistinct in many studies. This suggests that more opportunities exist to contextualise data for built environment flood resilience. This study concluded with a conceptual map of flood context, methodologies, data types engaged, and future computational methods with directions for future research.}
}
@article{DRAKOULAS2023116155,
title = {FastSVD-ML–ROM: A reduced-order modeling framework based on machine learning for real-time applications},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {414},
pages = {116155},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116155},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523002797},
author = {G.I. Drakoulas and T.V. Gortsas and G.C. Bourantas and V.N. Burganos and D. Polyzos},
keywords = {Reduced order modeling, Machine learning, Parameterized PDEs, Digital twins},
abstract = {Digital twins have emerged as a key technology for optimizing the performance of engineering products and systems. High-fidelity numerical simulations constitute the backbone of engineering design, providing insight into the performance of complex systems. However, large-scale, dynamic, non-linear models require significant computational resources and are prohibitive for real-time digital twin applications. To this end, reduced order models (ROMs) are employed, to approximate the high-fidelity solutions while accurately capturing the dominant aspects of the physical behavior. The present work proposes a new machine learning (ML) platform for the development of ROMs to handle large-scale numerical problems dealing with transient nonlinear partial differential equations. Our framework, named as FastSVD-ML-ROM, utilizes (i) a singular value decomposition (SVD) update methodology, to compute a linear subspace of the multi-fidelity solutions during the simulation process, (ii) convolutional autoencoders for nonlinear dimensionality reduction, (iii) feed-forward neural networks to map the input parameters to the latent spaces, and (iv) long–short term memory networks to predict and forecast the dynamics of parametric solutions. The efficiency of the FastSVD-ML-ROM framework is demonstrated for a 2D linear convection–diffusion benchmark, the problem of fluid flow around a cylinder, the 2D lid-driven cavity problem at high Reynolds numbers, and the 3D blood flow inside an arterial segment. The accuracy of the reconstructed results indicates the robustness of the proposed approach.}
}
@article{YAZICI2023101455,
title = {A survey of applications of artificial intelligence and machine learning in future mobile networks-enabled systems},
journal = {Engineering Science and Technology, an International Journal},
volume = {44},
pages = {101455},
year = {2023},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2023.101455},
url = {https://www.sciencedirect.com/science/article/pii/S2215098623001337},
author = {İbrahim Yazici and Ibraheem Shayea and Jafri Din},
keywords = {Cyber security, Deep learning, Digital twin, Intelligent transportation systems, Reinforcement learning, Smart energy, Smart healthcare, Supervised learning, Unsupervised learning, Unmanned Aerial Vehicle (UAV), 5G, 6G},
abstract = {Different fields have been thriving with the advents in mobile communication systems in recent years. These fields reap benefits of data collected by Internet of Things (IoT) in next generation (5G and 5BG) mobile networks. The IoT concept transforms different fields by providing large amount of data to be used in their operations. This is achieved by massively utilized sensors and mobile devices that acquire data from internet connected devices to keep track of physical systems. Hence, different use cases benefit from the data generated thanks to future mobile network systems. Intelligent Transportation Systems, Smart Energy, Digital Twins, Unmanned Aerial Vehicles (UAVs), Smart Health, Cyber Security are of significant use cases that big data plays an important role for them. Large amount of data entails more intelligent systems with respect to conventional methods, and it also entails highly reduced response time for use cases. Artificial intelligence and machine learning models are adept in satisfying the requirements of this big data situations for different use cases. In this sense, this paper provides a survey of machine learning and artificial intelligence applications for different use cases enabled by future mobile communication systems. An overview of machine learning types and artificial intelligence is presented to provide insights into the intelligent method concepts. Available studies are extensively summarized, and they are also grouped to provide a complete overview of the study. Discussions on the reviewed papers based on artificial intelligence and machine learning concepts are made, and some descriptive figures about the results of the discussions are also given in the paper. Finally, research challenges for artificial intelligence and machine learning applications in the use cases are introduced, future research directions and concluding remarks are presented accordingly.}
}
@article{DEWILDE2023113171,
title = {Building performance simulation in the brave new world of artificial intelligence and digital twins: A systematic review},
journal = {Energy and Buildings},
volume = {292},
pages = {113171},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2023.113171},
url = {https://www.sciencedirect.com/science/article/pii/S0378778823004012},
author = {Pieter {de Wilde}},
keywords = {Digital twin, Machine learning, Artificial intelligence, Cyber-physical system, Internet of things, Data mining, Building performance simulation},
abstract = {In an increasingly digital world, there are fast-paced developments in fields such as Artificial Intelligence, Machine Learning, Data Mining, Digital Twins, Cyber-Physical Systems and the Internet of Things. This paper reviews and discusses how these new emerging areas relate to the traditional domain of building performance simulation. It explores the boundaries between building simulation and these other fields in order to identify conceptual differences and similarities, strengths and limitations of each of these areas. The paper critiques common notions about these new domains and how they relate to building simulation, reviewing how the field of building performance may evolve and benefit from the new developments.}
}
@article{ZHU2023106735,
title = {A review of distributed energy system optimization for building decarbonization},
journal = {Journal of Building Engineering},
volume = {73},
pages = {106735},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106735},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223009142},
author = {Xiaoyu Zhu and Xingxing Zhang and Pu Gong and Yu Li},
keywords = {DES, Building decarbonization, Energy optimization, Digital twin, Smart cities},
abstract = {Building energy consumption has increased rapidly in the past decade, in particular for heat demand and electric vehicles, owning to the development of economy and improvement of living standard. Distributed Energy Systems (DESs), which can effectively improve the share of renewable energy in the energy mix, lower the energy cost and reduce environmental impact, is a promising approach to meet the increased energy demand. This paper presents a review of the system architecture of DESs for building decarbonization, including hybrid energy systems, energy storage technologies, building flexible loads, and electric vehicles. The uncertainties from both the environment and human interventions challenge the energy management due to the asynchrony between energy generation and energy consumption. Thus, the system should be optimally designed and operated to enhance the reliability, affordability, and flexibility of the DES. The paper highlights the adoption of optimization approaches. Finally, future trends and challenges are discussed. It is concluded that the digital transformation featured with IoT, AI, advanced machine learning, sophisticated optimization approaches, and Blockchain is the enabler for future smart cities.}
}
@article{ABDI2023107376,
title = {Modeling of capacitance for carbon-based supercapacitors using Super Learner algorithm},
journal = {Journal of Energy Storage},
volume = {66},
pages = {107376},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2023.107376},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X23007739},
author = {Jafar Abdi and Tahereh Pirhoushyaran and Fahimeh Hadavimoghaddam and Seyed Ali Madani and Abdolhossein Hemmati-Sarapardeh and Seyyed Hamid Esmaeili-Faraj},
keywords = {Carbon-based materials, Capacitance prediction, Supercapacitors, Super Learner model, Sensitivity analysis},
abstract = {Due to some specifications such as high capacitance and power density, electrostatic double-layer capacitors (EDLCs) are more noticeable than other supercapacitors. Some physical and chemical properties, surface functional groups, and testing conditions affect the efficiency and in particular the capacitance of EDLCs with carbon-based electrodes. In this study, four machine learning models, including Super Learner (SL), Extremely Randomized Trees (Extra trees), Extreme learning machine (ELM), and Multivariate adaptive regression splines (MARS) were implemented to predict the EDLCs' capacitance based on different impressive properties. A large dataset was assigned to the 121 different carbonaceous electrodes collected under various conditions, including 13 physical and chemical properties: voltage window (V), specific surface area (SSA) and SSA of micropore, pore volume (PV), and micropore volume, the ratio of D-band and G-band (Id/Ig) and doping elements (inputs parameters). The results indicated that the SL model with the R2 values of 0.9781, 0.9717, and 0.9768 for training, testing, and total dataset, respectively, and the RSME value of 18.099 was the most accurate model in comparison with the others. Indeed, the sensitivity analysis results exhibited that SSA with the relevance factor of 0.323 is the most important feature in the capacitance of carbon-based electrodes, while the presence of boron, sulfur, fluorine, phosphorus doping elements and pore size can be ignored.}
}
@article{XIE2023100053,
title = {Federated selective aggregation for on-device knowledge amalgamation},
journal = {Chip},
volume = {2},
number = {3},
pages = {100053},
year = {2023},
issn = {2709-4723},
doi = {https://doi.org/10.1016/j.chip.2023.100053},
url = {https://www.sciencedirect.com/science/article/pii/S2709472323000163},
author = {Donglin Xie and Ruonan Yu and Gongfan Fang and Jiaqi Han and Jie Song and Zunlei Feng and Li Sun and Mingli Song},
keywords = {Federated learning, Knowledge amalgamation, Model reusing},
abstract = {ABSTRACT
In the current work, we explored a new knowledge amalgamation problem, termed Federated Selective Aggregation for on-device knowledge amalgamation (FedSA). FedSA aims to train an on-device student model for a new task with the help of several decentralized teachers whose pre-training tasks and data are different and agnostic. The motivation to investigate such a problem setup stems from a recent dilemma of model sharing. Due to privacy, security or intellectual property issues, the pre-trained models are, however, not able to be shared, and the resources of devices are usually limited. The proposed FedSA offers a solution to this dilemma and makes it one step further, again, the method can be employed on low-power and resource-limited devices. To this end, a dedicated strategy was proposed to handle the knowledge amalgamation. Specifically, the student-training process in the current work was driven by a novel saliency-based approach which adaptively selects teachers as the participants and integrated their representative capabilities into the student. To evaluate the effectiveness of FedSA, experiments on both single-task and multi-task settings were conducted. The experimental results demonstrate that FedSA could effectively amalgamate knowledge from decentralized models and achieve competitive performance to centralized baselines.}
}
@article{CHEN2023102581,
title = {Multisensor fusion-based digital twin for localized quality prediction in robotic laser-directed energy deposition},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {84},
pages = {102581},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102581},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000571},
author = {Lequn Chen and Guijun Bi and Xiling Yao and Chaolin Tan and Jinlong Su and Nicholas Poh Huat Ng and Youxiang Chew and Kui Liu and Seung Ki Moon},
keywords = {Additive Manufacturing, In-situ defect detection, Laser-directed energy deposition, Machine learning, Multisensor fusion},
abstract = {Early detection of defects, such as keyhole pores and cracks is crucial in laser-directed energy deposition (L-DED) additive manufacturing (AM) to prevent build failures. However, the complex melt pool behaviour cannot be adequately captured by conventional single-modal process monitoring approaches. This study introduces a multisensor fusion-based digital twin (MFDT) for localized quality prediction in the robotic L-DED process. The data used in multisensor fusion includes features extracted from a coaxial melt pool vision camera, a microphone, and an off-axis short wavelength infrared thermal camera. The key novelty of this work is a spatiotemporal data fusion method that synchronizes multisensor features with the real-time robot motion data to achieve localized quality prediction. Optical microscope (OM) images of the printed part are used to locate defect-free and defective regions (i.e., cracks and keyhole pores), which serve as ground truth labels for training supervised machine learning (ML) models for quality prediction. The trained ML model is then used to generate a virtual quality map that registers quality prediction outcomes within the 3D volume of the printed part, thus eliminating the need of physical inspections by destructive methods. Experiments show that the virtual quality map closely matches the actual quality observed by OM. Compared to traditional single-sensor-based quality prediction, the MFDT has achieved a significantly higher quality prediction accuracy (96%), a higher ROC-AUC score (99%), and a lower false alarm rate (4.4%). As a result, the MFDT is a more reliable method for defect prediction. The proposed MFDT also lays the groundwork for our future development of a self-adaptive hybrid processing strategy that combines machining with AM for defect removal and quality improvement.}
}
@article{CEVALLOS2023105510,
title = {Towards a more accurate characterization of granular media 2.0: Involving AI in the process},
journal = {Computers and Geotechnics},
volume = {160},
pages = {105510},
year = {2023},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2023.105510},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X23002677},
author = {Stefano Buitrón Cevallos and Alex X. Jerves and Utkarsh Mital and David A. Medina and V. Santiago Quinteros and Maurizio Mulas and Øyvind Torgersrud},
keywords = {Granular materials, x-ray-CT scanning, Image processing, Level set, Convolutional neural network, Virtual laboratory testing},
abstract = {We introduce a Convolutional Neural Network (CNN) to reduce grains’ manual inspection time after image processing on raw 3D x-ray computed tomography (3DXRCT) images from a sample of granular material to obtain level-set function-based digital twins of individual grains. The CNN automatically distinguishes properly segmented digital grains with up to 90% of accuracy. This algorithm is trained using, ground-truth, level set-based digital grain representations from a natural soil sampled at Jaramijó (Ecuador). The implemented convolutional neural network provides groundbreaking processing power, reducing the, otherwise, manual inspection time expended for a small sample, e.g., 200 000 grains, from approximately a couple of weeks to only a few hours. Furthermore, transfer learning and training from scratch are compared for artificially graded granular materials such as Øysand (Norway) and Hostun sand (France). The CNN’s learning process is interpreted by means of grain morphological parameters, i.e., sphericity, roundness, grain diameter, and volume-surface ratio. Hence, being able to automatically segment a greater amount of grains from 3DXRCT images of natural and artificial soils in a short period of time, enables us, for first time, to perform actual 3DLS-DEM-based virtual laboratory testing (a plug-and-play one-stop shop). Providing unprecedented and unique data for engineering applications.}
}
@article{SITAPURE2023108339,
title = {CrystalGPT: Enhancing system-to-system transferability in crystallization prediction and control using time-series-transformers},
journal = {Computers & Chemical Engineering},
volume = {177},
pages = {108339},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108339},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423002090},
author = {Niranjan Sitapure and Joseph Sang-Il Kwon},
keywords = {Time-series-transformers (TST), Transfer learning, System-to-system transferability, Digital twins, Model predictive controller (MPC)},
abstract = {For prediction and real-time control tasks, machine-learning (ML)-based digital twins are frequently employed. However, while these models are typically accurate, they are custom-designed for individual systems, making system-to-system (S2S) transferability difficult. This occurs even when substantial similarities exist in the process dynamics across different chemical systems. To address this challenge, we developed a novel time-series-transformer (TST) framework that exploits the powerful transfer learning capabilities inherent in transformer algorithms. This was demonstrated using readily available process data obtained from different crystallizers operating under various operational scenarios. Using this extensive dataset, we trained a TST model (CrystalGPT) to exhibit remarkable S2S transferability not only across all pre-established systems, but also to an unencountered system. CrystalGPT achieved a cumulative error across all systems, which is eight times superior to that of existing ML models. Additionally, we coupled CrystalGPT with a model predictive controller to reduce the variance in setpoint tracking to just 1%.}
}
@article{SUN2023109404,
title = {Deep learning framework for gas turbine performance digital twin and degradation prognostics from airline operator perspective},
journal = {Reliability Engineering & System Safety},
volume = {238},
pages = {109404},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109404},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023003186},
author = {Jianzhong Sun and Zichen Yan and Ying Han and Xinyun Zhu and Caiqiong Yang},
keywords = {Gas turbine, Performance degradation, Digital twin, Prognostics, N-CMAPSS},
abstract = {Digital twin technology has emerged as a research hotspot in the field of intelligent operation and maintenance of gas turbines. This paper proposes a data-driven Digital Twin approach for gas turbine performance monitoring and degradation prognostics from an airline operator perspective. The framework adopts a semi-supervised deep learning method to construct a data-driven Performance Digital Twin (PDT) rather than a physics-based performance model. The PDT-derived multi-dimensional health features is used to characterize the performance degradation and enhance the input features for the prognostics network. Specifically, domain knowledge from the asset operator perspective is incorporated into the prognostics model to improve the performance. The proposed approach is evaluated on real-world turbofan engines and the NCMAPSS dataset, achieving promising results compared to the state-of-art approaches. The developed data-driven prognostics framework provides a low-cost alternative to an expensive physics-based prognostics approach for gas turbine operators. It enables asset users to implement their own data-driven prognostics and maintenance strategies.}
}
@article{CANCEMI2023112502,
title = {Unsupervised anomaly detection in pressurized water reactor digital twins using autoencoder neural networks},
journal = {Nuclear Engineering and Design},
volume = {413},
pages = {112502},
year = {2023},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2023.112502},
url = {https://www.sciencedirect.com/science/article/pii/S0029549323003515},
author = {S.A. Cancemi and R. {Lo Frano} and C. Santus and T. Inoue},
keywords = {Neural Network, Autoencoder, Predictive Maintenance, Safety, NPP, Unsupervised Anomaly Detection, LOCA},
abstract = {Deep learning (DL), that is becoming quite popular for prediction and analysis of complex patterns in large amounts of data is used to investigate the safety behaviour of the nuclear plant items. This is achieved by using multiple layers of artificial neural networks to process and transform input data, allowing for the creation of highly accurate predictive models. Particularly to the aim the unsupervised machine learning approach and the digital twin concept in form of pressurized water reactor 2-loop simulator are used. This innovative methodology is based on neural network algorithm that makes capable to predict failures of plant structure, system, and components earlier than the activation of safety and emergency systems. Moreover, to match the objective of the study several scenarios of loss of cooling accident (LOCA) of different break size were simulated. To make the acquisition platform realistic, Gaussian noise was added to the input signals. The neural network has been fed by synthetic dataset provide by PCTRAN simulator and the efficiency in event identification was studied. Further, due to the very limited studies on the unsupervised anomaly detection by means of autoencoder neural networks applied for plant monitoring and surveillance, the methodology has been validated with experimental data from resonant test rig designed for fatigue testing of tubular components. The obtained results demonstrate the reliability and the efficiency of the methodology in detecting anomalous events prior the activation of safety system. Particularly, if the difference between the expected readings and the collected data goes beyond the predetermined threshold, then the anomalous event is identified, e.g., the model detected anomalies up to 38 min before the reactor scram intervention.}
}
@article{ZAKIRHOSSAIN2023128646,
title = {Modeling of microbial fuel cell power generation using machine learning-based super learner algorithms},
journal = {Fuel},
volume = {349},
pages = {128646},
year = {2023},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2023.128646},
url = {https://www.sciencedirect.com/science/article/pii/S0016236123012590},
author = {S.M. {Zakir Hossain} and Nahid Sultana and Shaker Haji and Shaikha {Talal Mufeez} and Sara {Esam Janahi} and Noof {Adel Ahmed}},
keywords = {Electricity generation, Fuel cell, Bayesian algorithm, Response surface methodology, Support vector regression, Boosted regression tree},
abstract = {Electricity generation from microbial fuel cells (MFCs) is a potential environment-friendly technology. This study provides Bayesian Algorithm (BA) based Support Vector Regression (SVR) and Boosted Regression Tree (BRT) as prospective super learner modeling tools (BA-SVR, BA-BRT) for predictions of electricity production from MFCs. The membrane thickness, external resistance, and anode area were considered independent variables, while power generation was taken as a response variable. The key novelties of this study include (i) hybridization of BA with SVR and BRT (separately) for forecasting power generation from fuel cells for the first time, (ii) performance comparison of the developed models (BA-SVR and BA-BRT) with the existing Response Surface Methodology (RSM) based on the coefficient of determination (R2), relative error (RE), mean absolute error (MAE), mean absolute percentage error (MAPE), root mean square error (RMSE), and computing efficiency, and the (iii) analysis of the models’ robustness by utilizing Gaussian white noise. Based on the performance indicators, the proposed super leaner models showed excellent performance compared to the existing M.J. Salar-García et al. RSM model. The BA-SVR model provided the lowest errors (MAE of 2.94, RSME of 7.2926, MAPE of 13.8341) with the highest R2 of 0.9981, compared to the BA-BRT and RSM models. The proposed BA-SVR model showed superior performance to the RSM and BA-BRT models in predicting the MFCs’ power generation, with a performance improvement of more than 90% regarding MAPE, as an example. The future prediction and high robustness of the proposed super learner model would ensure quick estimation for maximization of electricity generation that may lead to reducing massive lab trials and saving resources.}
}
@article{KEKIC2023100739,
title = {Evaluating vaccine allocation strategies using simulation-assisted causal modeling},
journal = {Patterns},
volume = {4},
number = {6},
pages = {100739},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100739},
url = {https://www.sciencedirect.com/science/article/pii/S266638992300079X},
author = {Armin Kekić and Jonas Dehning and Luigi Gresele and Julius {von Kügelgen} and Viola Priesemann and Bernhard Schölkopf},
keywords = {COVID-19, causality, vaccine, SEIR model, vaccine allocation, modeling},
abstract = {Summary
We develop a model to retrospectively evaluate age-dependent counterfactual vaccine allocation strategies against the coronavirus disease 2019 (COVID-19) pandemic. To estimate the effect of allocation on the expected severe-case incidence, we employ a simulation-assisted causal modeling approach that combines a compartmental infection-dynamics simulation, a coarse-grained causal model, and literature estimates for immunity waning. We compare Israel’s strategy, implemented in 2021, with counterfactual strategies such as no prioritization, prioritization of younger age groups, or a strict risk-ranked approach; we find that Israel’s implemented strategy was indeed highly effective. We also study the impact of increasing vaccine uptake for given age groups. Because of its modular structure, our model can easily be adapted to study future pandemics. We demonstrate this by simulating a pandemic with characteristics of the Spanish flu. Our approach helps evaluate vaccination strategies under the complex interplay of core epidemic factors, including age-dependent risk profiles, immunity waning, vaccine availability, and spreading rates.}
}
@article{CHE20231405,
title = {Opportunities for battery aging mode diagnosis of renewable energy storage},
journal = {Joule},
volume = {7},
number = {7},
pages = {1405-1407},
year = {2023},
issn = {2542-4351},
doi = {https://doi.org/10.1016/j.joule.2023.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S2542435123002647},
author = {Yunhong Che and Xiaosong Hu and Remus Teodorescu},
abstract = {Lithium-ion batteries are key energy storage technologies to promote the global clean energy process, particularly in power grids and electrified transportation. However, complex usage conditions and lack of precise measurement make it difficult for battery health estimation under field applications, especially for aging mode diagnosis. In a recent issue of Nature Communications, Dubarry et al. shed light on this issue by investigating the solution based on machine learning and battery digital twins. They achieved aging modes diagnosis of photovoltaics-connected batteries working for 2 years with more than 10,000 degradation paths under different seasons and cloud shading conditions.}
}
@article{LANDERS2023113738,
title = {TEMGYM Advanced: Software for electron lens aberrations and parallelised electron ray tracing},
journal = {Ultramicroscopy},
volume = {250},
pages = {113738},
year = {2023},
issn = {0304-3991},
doi = {https://doi.org/10.1016/j.ultramic.2023.113738},
url = {https://www.sciencedirect.com/science/article/pii/S0304399123000554},
author = {David Landers and Ian Clancy and Rafal E. Dunin-Borkowski and Dieter Weber and Andrew Stewart},
keywords = {Ray Tracing, Parallelisation, Differential Algebra, Aberration Integral, NanoMi},
abstract = {Characterisation of the electron beams trajectory in an electron microscope is possible in a few select commercial software packages, but these tools and their source code are not available in a free and accessible manner. This paper introduces the free and open-source software TEMGYM Advanced, which implements ray tracing methods that calculate the path of electrons through a magnetic or electrostatic lens and allow evaluation of the first-order properties and third-order geometric aberrations. Validation of the aberration coefficient calculations is performed by implementing two independent methods – the aberration integral and differential algebra (DA) methods and by comparing the results of each. This paper also demonstrates parallelised electron ray tracing through a series of magnetic components, which enables near real-time generation of a physically accurate beam-spot including aberrations and brings closer the realisation of a digital twin of an electron microscope. TEMGYM Advanced represents a valuable resource for the electron microscopy community, providing an accessible and open source means of characterising electron lenses. This software utilises the Python programming language to complement the growing ecosystem of free and open-source software within the electron microscopy community, and to facilitate the application of machine learning to an electron microscope digital twin for instrument automation. The software is available under GNU Public License number Three (GPL 3).}
}
@article{CUI2023100732,
title = {Long-sequence voltage series forecasting for internal short circuit early detection of lithium-ion batteries},
journal = {Patterns},
volume = {4},
number = {6},
pages = {100732},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100732},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923000727},
author = {Binghan Cui and Han Wang and Renlong Li and Lizhi Xiang and Jiannan Du and Huaian Zhao and Sai Li and Xinyue Zhao and Geping Yin and Xinqun Cheng and Yulin Ma and Hua Huo and Pengjian Zuo and Guokang Han and Chunyu Du},
keywords = {lithium-ion battery, internal short circuit detection, voltage prediction, power prediction, time-series forecasting, deep learning, encoder-decoder, attention model},
abstract = {Summary
Accurate early detection of internal short circuits (ISCs) is indispensable for safe and reliable application of lithium-ion batteries (LiBs). However, the major challenge is finding a reliable standard to judge whether the battery suffers from ISCs. In this work, a deep learning approach with multi-head attention and a multi-scale hierarchical learning mechanism based on encoder-decoder architecture is developed to accurately forecast voltage and power series. By using the predicted voltage without ISCs as the standard and detecting the consistency of the collected and predicted voltage series, we develop a method to detect ISCs quickly and accurately. In this way, we achieve an average percentage accuracy of 86% on the dataset, including different batteries and the equivalent ISC resistance from 1,000 Ω to 10 Ω, indicating successful application of the ISC detection method.}
}
@article{CHEN2023120519,
title = {Consistency- and dependence-guided knowledge distillation for object detection in remote sensing images},
journal = {Expert Systems with Applications},
volume = {229},
pages = {120519},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120519},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423010217},
author = {Yixia Chen and Mingwei Lin and Zhu He and Kemal Polat and Adi Alhudhaif and Fayadh Alenezi},
keywords = {Deep learning, Object detection, Remote sensing, Knowledge distillation},
abstract = {As one of the challenging tasks in the remote sensing (RS), object detection has been successfully applied in many fields. Convolution neural network (CNN) has recently attracted extensive attention and is widely used in the natural image processing. Nevertheless, RS images have cluttered scenes compared with natural images. As a result, the existing detectors perform poorly in RS images, especially with the complicated backgrounds. Moreover, the detection inference time and model volume of detectors in RS images often go unrecognized. To address the above issues, this study proposes a novel method for object detection in RS images, which is called the consistency- and dependence-guided knowledge distillation (CDKD). To this end, the spatial- and channel-oriented structure discriminative modules (SCSDM) are put forward to extract the discriminative spatial locations and channels to which the teacher model pays attention. SCSDM improves the feature representation of the student model by effectively eliminating the influence of noises and the complicated backgrounds. Then, the consistency and dependence of the features between the teacher model and the student model are constructed under the guidance of SCSDM. Experimental results over public datasets for RS images demonstrate that our CDKD method surpasses the state-of-the-art methods effectively. Most of all, on the RSOD dataset, our CDKD method achieves 92% mean average precision with 3.3 M model volume and 588.2 frames per second.}
}
@article{MOINGEON2023103605,
title = {Virtual patients, digital twins and causal disease models: Paving the ground for in silico clinical trials},
journal = {Drug Discovery Today},
volume = {28},
number = {7},
pages = {103605},
year = {2023},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2023.103605},
url = {https://www.sciencedirect.com/science/article/pii/S1359644623001216},
author = {Philippe Moingeon and Marylore Chenel and Cécile Rousseau and Emmanuelle Voisin and Mickael Guedj},
keywords = {Artificial intelligence, causal disease model, computational precision medicine, digital twins,  trial simulation, machine learning, virtual patient},
abstract = {Computational models are being explored to simulate in silico the efficacy and safety of drug candidates and medical devices. Disease models that are based on patients’ profiling data are being produced to represent interactomes of genes or proteins and to infer causality in the pathophysiology, which makes it possible to mimic the impact of drugs on relevant targets. Virtual patients designed from medical records as well as digital twins are generated to simulate specific organs and to predict treatment efficacy at the individual patient level. As the acceptance of digital evidence by regulators grows, predictive artificial intelligence (AI)-based models will support the design of confirmatory trials in humans and will accelerate the development of efficient drugs and medical devices.}
}
@article{LIU2023120122,
title = {Intelligent digital-twin prediction and reverse control system architecture for thermal errors enabled by deep learning and cloud-edge computing},
journal = {Expert Systems with Applications},
volume = {225},
pages = {120122},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120122},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423006243},
author = {Jialan Liu and Chi Ma and Hongquan Gui and Shilong Wang},
keywords = {Intelligent system, Precision machine tools, Digital twin, LSTM neural network, Error prediction model, Edge computing},
abstract = {The heat generation is significant in the machining process, leading to thermal errors, and finally the geometric precision of machined parts is reduced. So the precision machine tool is a key factor in determining the geometric precision of complex parts. In recent years, the error control method is applied. But the method fails in reducing thermal errors because it cannot effectively process large-volume data, resulting from its low executing efficiency. To solve above issues, a new intelligent digital-twin prediction and reverse control system is designed for thermally induced errors based on the user-edge-cloud architecture to expedite the executing efficiency. The data-driven error modeling method is augmented by an error mechanism-based modeling to express the thermal error as a function with the temperature, armature current, rotational speed, and ambient temperature as independent variables, and then the long-term memorizing behavior of thermal errors is demonstrated. The error model is established based on an improved wavelet threshold denoising (IWTD) and a (long short-term memory) LSTM network to describe the memorizing behavior, and IWTD-LSTM network error prediction model is embedded into the digital-twin system. The digital-twin system and IWTD-LSTM network model were verified on a precision machine tool. With the implementation of the digital-twin system, the thermal error and the volume of the transferred data are reduced by 88.72% and 56.36%, respectively.}
}
@article{LUO2023106855,
title = {Optimal sensor placement for reconstructing wind pressure field around buildings using compressed sensing},
journal = {Journal of Building Engineering},
volume = {75},
pages = {106855},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106855},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223010343},
author = {Xihaier Luo and Ahsan Kareem and Shinjae Yoo},
keywords = {Sensor placement, Compressed sensing, Pressure measurements, Wind pressure field reconstruction},
abstract = {Deciding how to optimally deploy sensors in a large, complex, and spatially extended structure is critical to ensure that the surface pressure field is accurately captured for subsequent analysis and design. In some cases, reconstruction of missing data is required in downstream tasks such as the development of digital twins. This paper presents a data-driven sparse sensor selection algorithm, aiming to provide the most information contents for reconstructing aerodynamic characteristics of wind pressures over tall building structures parsimoniously. The algorithm first fits a set of basis functions to the training data, then applies a computationally efficient QR algorithm that ranks existing pressure sensors in order of importance based on the state reconstruction to this tailored basis. The findings of this study show that the proposed algorithm successfully reconstructs the aerodynamic characteristics of tall buildings from sparse measurement locations, generating stable and optimal solutions across a range of conditions. As a result, this study serves as a promising first step toward leveraging the success of data-driven and machine learning algorithms to supplement traditional genetic algorithms currently used in wind engineering.}
}
@article{MARDANPOUR2023119073,
title = {Human activity recognition based on multiple inertial sensors through feature-based knowledge distillation paradigm},
journal = {Information Sciences},
volume = {640},
pages = {119073},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119073},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523006588},
author = {Malihe Mardanpour and Majid Sepahvand and Fardin Abdali-Mohammadi and Mahya Nikouei and Homeyra Sarabi},
keywords = {Human activity recognition, Knowledge distillation, Edge device, Deep learning, Tensor decomposition},
abstract = {In recent years, numerous high accuracy methods have been developed for classifying activities using multi inertial sensors. Despite their reliability and precision, they suffer from high computational cost and which make them improper for deploying in edge devices that are limited resources. This paper addresses this drawback by employing a knowledge distillation (KD) paradigm which maps tri-axial multi signals into single axis signals, thus; it can recognize activities with fewer number of signals and consequently less computation. In this method, a big teacher model is trained in advanced with three IMU sensors each of which have tri-axial signals. Then, a small student model is trained with just one of the axes of these sensors under monitoring of teacher which reduces the number of signals. Tucker decomposition is also exploited in order to improve KD performance by separating a core tensor from feature maps that has more informative knowledge. Evaluation of our method on REALDISP dataset demonstrates that the student model could achieve accuracy of 92.90% with much less complexity making it suitable for embedded devices. Moreover, it outperforms in comparison to other state-of-the-art KD approaches.}
}
@article{NAGARAJ2023e251,
title = {Augmenting digital twins with federated learning in medicine},
journal = {The Lancet Digital Health},
volume = {5},
number = {5},
pages = {e251-e253},
year = {2023},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(23)00044-4},
url = {https://www.sciencedirect.com/science/article/pii/S2589750023000444},
author = {Divya Nagaraj and Priya Khandelwal and Sandra Steyaert and Olivier Gevaert}
}
@article{XU2023,
title = {Comparative study on landslide susceptibility mapping based on different ratios of training samples and testing samples by using RF and FR-RF models},
journal = {Natural Hazards Research},
year = {2023},
issn = {2666-5921},
doi = {https://doi.org/10.1016/j.nhres.2023.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666592123000732},
author = {Ke Xu and Zhou Zhao and Wei Chen and Jianquan Ma and Fei Liu and Yihao Zhang and Zijun Ren},
keywords = {Comparative study, Landslide susceptibility, Sample , RF model, FR-RF model},
abstract = {Evaluation of landslide susceptibility is essential to planning of land and space utilization. For this purpose, the paper presents a case study from Fugu County, Shaanxi Province, China. Firstly, the geological environment and current state of landslides in Fugu County were investigated. Then, slope, aspect, terrain relief, curvature, lithology, land type, and normalized difference vegetation index (NDVI) were considered as the landslide susceptibility condition factors, and the correlation between these carried out by using Multicollinearity Analysis method. Next, landslide and non-landslide samples were divided into training samples and testing samples according to the sample ratios of 8/2, 7/3, 6/4, and 5/5, respectively. The landslide susceptibility mapping was carried out by using Random Forest (RF) model and Frequency Ratio coupled with Random Forest (FR-RF) model, respectively. Lastly, the landslide density (LD), landslide frequency ratio (LFR), the area under the curve (AUC) of the receiver operator, and other indicators were used to validate the rationality, accuracy, and performance of the landslide susceptibility maps produced from different models and ratios. The results indicated that all maps are reasonable, except the map when ratio is 5/5. For each map, regardless of ratios, the LD and LFR are the greatest in the zones classed as having a very high susceptibility, followed by those with a high, moderate, low, and very low classes. In the Random Forest (RF) model, when the training test set is not at the same time its in the area of extremely high sensitivity of LD and the size of the FR value respectively 7/3 (201.026) > 8/2 (154.440) > 6/4 (93.696) >5/5 (136.364) and 7/3 (4.806) > 8/2 (3.692) > 6/4 (3.260) > 5/5 (2.240); in the Frequency Ratio coupled with Random Forest (FR-RF) model, Inall the training test sets the size of the proportion of LD and FR value respectively 7/3 (145.693) > 6/4 (127.151) > 5/5 (122.857) > 8/2 (113.263) and 7/3 (3.334) > 6/4 (3.073) > 5/5 (2.811) > 8/2 (2.592). What else, from the comparison of ROC curves, when ratio is 7/3, the accuracy of the two models is higher than that of other ratios. Similarly, the results of the ensemble model (A combination of two models with different learning abilities.) are not more reasonable than the results of the single model, which reflects that the combination of a weaker learner model (Frequency Ratio model here) with a stronger learner model (Random Forest model here) can diminish the performance of the stronger model.}
}
@article{LAI202376,
title = {Digital twin-based structural health monitoring by combining measurement and computational data: An aircraft wing example},
journal = {Journal of Manufacturing Systems},
volume = {69},
pages = {76-90},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001085},
author = {Xiaonan Lai and Liangliang Yang and Xiwang He and Yong Pang and Xueguan Song and Wei Sun},
keywords = {Digital twin, Structural health monitoring, Load identification, Multi-fidelity modeling, Fatigue damage estimation},
abstract = {Digital twin is a concept that utilizes digital technologies to mirror the real-time states of physical assets and extract the hidden yet valuable information of physical assets for optimization, decision-making or scheduling. By combining measurement and computational data, this paper presents a digital twin-based structural health monitoring framework of physical assets. The process for building the measurement-computation combined digital twin (MCC-DT) involves four steps. First, an artificial intelligence-driven load identification method combining measurement and computational data is employed to recognize the loads applied on physical assets. Two approaches were proposed to realize load identification, based on single fidelity surrogate models and deep learning techniques, respectively. Second, multi-fidelity surrogate (MFS) models are applied to improve the accuracy in the MCC-DT. Two routes for implementing the MFS models are introduced and the advantages and shortcomings of both are analyzed. Third, an online rainflow counting algorithm is developed to calculate the degradation of the physical assets. The main advantage of the algorithm is that it can provide a near real-time estimation for the damage accumulated of physical assets. Finally, the data generated from the first three steps can be fused into a three-dimensional scene using Web graphics library to provide an intuitive view of the MCC-DT. To describe the implementation details of the framework and verify its applicability and effectiveness, the MCC-DT was established using an aircraft model as an example.}
}
@article{HASHMI2023101070,
title = {Consensus based phase connectivity identification for distribution network with limited observability},
journal = {Sustainable Energy, Grids and Networks},
volume = {34},
pages = {101070},
year = {2023},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2023.101070},
url = {https://www.sciencedirect.com/science/article/pii/S2352467723000784},
author = {Md Umar Hashmi and David Brummund and Rickard Lundholm and Arpan Koirala and Dirk {Van Hertem}},
keywords = {Data-driven, Distribution network, Machine learning, Phase identification, Voltage time series},
abstract = {The mitigation of distribution network (DN) unbalance and the use of single-phase flexibility for congestion mitigation requires accurate phase connection information, which is often not available. For a large DN, the naïve phase identification proposed in the majority of the prior works using a single voltage reference, this does not scale well for a multi-feeder DN. We present a consensus algorithm-based phase identification mechanism which uses multiple three-phase reference points to improve the prediction of phases. Due to the absence of real measurements for a real-suburban German DN, the algorithms are developed and evaluated over synthetic data using a digital twin. To utilize strongly correlated measurements, the DN is clustered into zones. We observe those reference measurements located in the same zone as the single-phase consumer, with unknown phase connectivity, leads to accurate prediction of DN phases. Four consensus algorithms are developed and compared. Using numerical results, we recommend the most robust phase identification mechanism. In our evaluation, measurement error, and the impact of the neutral conductor are also assessed. We assume limited DN observability and apply our findings to a German DN without smart meters, but only less than 8% of nodes have measurement boxes along with single-phase consumers with a home energy management system. Voltage time series for 1 month (hourly sampled) is utilized. The numerical results indicate that for 1% accuracy class measurement, the phase connectivity of 308 out of 313 single-phase consumers in a German DN can be identified. Further, we also propose metrics quantifying the goodness of the phase identification. The phase identification framework based on consensus algorithms for DN zones is scalable for large DN and robust towards measurement errors as the estimation is not dependent on a single measurement point.}
}
@article{MAITY2023113278,
title = {Real-time temperature monitoring of weld interface using a digital twin approach},
journal = {Measurement},
volume = {219},
pages = {113278},
year = {2023},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.113278},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123008424},
author = {D. Maity and R. Premchand and M. Muralidhar and V. Racherla},
keywords = {Dissimilar metal joining, Friction processing, Weld interface temperature prediction, Finite element simulations, Machine learning model, Digital twin},
abstract = {A new friction processing technique is developed for lap welding of Al–Cu sheets using inter diffusion of copper and aluminium at their interface. The metals selectively melt at the interface at a temperature near their eutectic point. This paper proposes a digital twin approach for real-time temperature monitoring at the joint interface using the machine’s real-time current data. The real time temperature data is used to predict exact instance of interface melting and to control the resulting weld microstructure. The digital twin model is calibrated using a finite element model which is in turn calibrated using experiments. Moving average of machine current and temperature history are used to predict real time interface temperature using a linear regression based recursive machine learning model with high precision. The model predictions have an R2 value of 99.5%. The digital twin approach resulted in significant increase in joint strength and fracture energy.}
}
@article{RUMIN2023113157,
title = {Utilization of measurements, machine learning, and analytical calculation for preventing belt flip over on conveyor belts},
journal = {Measurement},
volume = {218},
pages = {113157},
year = {2023},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.113157},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123007212},
author = {Przemysław Rumin and Janusz Kotowicz and Daniel Hogg and Anna Zastawna-Rumin},
keywords = {Anomaly detection, Belt conveyor, Machine learning, Predictive maintenance},
abstract = {The use of information technology in modern industry is becoming increasingly important to ensure the failure-free operation of devices. Innovatively, this article describes methods to predict belt flip over. The article describes the tested installation and the acquisition and processing of a unique data set gathered from hilly and very long belt conveyors. Values are predicted using novel analytical algorithms with a combination of machine learning algorithms. A simulation study is presented to demonstrate the effectiveness of the proposed method. The simulation results and case studies show that the proposed method generates accurate results and can be used to predict belt flip over for large-scale belt conveyors.}
}
@article{DHARMADHIKARI2023103556,
title = {A reinforcement learning approach for process parameter optimization in additive manufacturing},
journal = {Additive Manufacturing},
volume = {71},
pages = {103556},
year = {2023},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2023.103556},
url = {https://www.sciencedirect.com/science/article/pii/S2214860423001690},
author = {Susheel Dharmadhikari and Nandana Menon and Amrita Basak},
keywords = {Reinforcement learning, Q-learning, Additive manufacturing, Directed energy deposition, Process optimization},
abstract = {Process optimization for metal additive manufacturing (AM) is crucial to ensure repeatability, control microstructure, and minimize defects. Despite efforts to address this via the traditional design of experiments and statistical process mapping, there is limited insight on an on-the-fly optimization framework that can be integrated into a metal AM system. Additionally, most of these methods, being data-intensive, cannot be supported by a metal AM alloy or system due to budget restrictions. To tackle this issue, the article introduces a Reinforcement Learning (RL) methodology transformed into an optimization problem in the realm of metal AM. An off-policy RL framework based on Q-learning is proposed to find optimal laser power (P)- scan velocity (v) combinations with the objective of maintaining steady-state melt pool depth. For this, an experimentally validated Eagar–Tsai formulation is used as a digital twin emulating the laser-directed energy deposition (L-DED) environment, where the laser operates as the agent across the P−v space such that it maximizes rewards for a melt pool depth closer to the optimum. The culmination of the training process yields a Q-table where the state (P,v) with the highest Q-value corresponds to the optimized process parameters. For a desired melt pool depth of 1 mm for SS316L, the proposed algorithm predicts an optimal P−v combination of 888.9 W - 566.7 mm/min that yields a melt pool depth within 50 μm of the experimental observation. The framework, therefore, provides a model-free approach to learning without any prior.}
}
@article{YANG2023102595,
title = {Meta-model-based shop-floor digital twin architecture, modeling and application},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {84},
pages = {102595},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102595},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000716},
author = {Xiaolang Yang and Xuemei Liu and Heng Zhang and Ling Fu and Yanbin Yu},
keywords = {Shop-floor digital twin, Meta-model, MBSE, RAMI 4.0, Intelligent manufacturing},
abstract = {Digital twin is regarded as the virtual counterpart of physical entities, which can mirror the physical behavior and performance. Digital twin technology provides strong support for the achievement of cyber-physical system and intelligent manufacturing. Many investigations have been carried out for the digital twin of specific products. However, there are less researches on digital twin in the shop-floor domain, and there is a lack of model-driven digital twin comprehensive architecture. The modeling approach to the full lifecycle of digital twin is not considered enough. This paper proposes a meta-model-based shop-floor digital twin construction approach and a comprehensive architecture. A meta-model based on RAMI 4.0 is constructed, which provide a novel idea for the description of manufacturing resources and their status. The proposed shop-floor digital twin architecture consists of three key implementation elements: the meta-model construction, data modeling (including data interaction between cyber-physical spaces) and constructing different integration level models of shop-floor digital twin based on iteration feedback between the demands and models. The proposed approach is validated through a case study of the fischer learning factory 4.0.}
}
@article{ZHANG2023120542,
title = {Towards deep probabilistic graph neural network for natural gas leak detection and localization without labeled anomaly data},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120542},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120542},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423010448},
author = {Xinqi Zhang and Jihao Shi and Xinyan Huang and Fu Xiao and Ming Yang and Jiawei Huang and Xiaokang Yin and Asif {Sohail Usmani} and Guoming Chen},
keywords = {Variation Bayesian Inference, Graph deep learning, Leakage detection, Leakage localization, Digital twin},
abstract = {Deep learning has been widely applied to automated leakage detection and location of natural gas pipe networks. Prevalent deep learning approaches do not consider the spatial dependency of sensors, which limits leakage detection performance. Graph deep learning is a promising alternative to prevailing approaches as it can model spatial dependency. However, the challenge of collecting real-world anomaly data for training limits the accuracy and robustness of currently used graph deep learning approaches. This study proposes a deep probabilistic graph neural network in which attention-based graph neural network is built to model spatial sensor dependency. Variational Bayesian inference is integrated to model the posterior distribution of sensor dependency so that the leakage can be localized. An urban natural gas pipe network experiment is employed to construct the benchmark dataset, in which normal time-series data is applied to develop our proposed model while anomaly leakage data is used for performance comparison between our model and other state-of-the-art models. The results demonstrate that our model exhibits competitive detection accuracy (AUC) = 0.9484, while the additional uncertainty interval provides more comprehensive leakage detection information compared to state-of-the-art deep learning models. In addition, our model’s posterior distribution enhances the leakage localization with the accuracy of positioning (PAc) = 0.8, which is higher than that of other state-of-the-art graph deep learning models. This study provides a comprehensive and robust alternative for subsequent decision-making to mitigate natural gas leakage from pipe networks.}
}
@article{MARTENS2023102076,
title = {Cross domain matching for semantic point cloud segmentation based on image segmentation and geometric reasoning},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102076},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102076},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002045},
author = {Jan Martens and Timothy Blut and Jörg Blankenbach},
keywords = {Infrastructure, Machine learning, Cross domain matching, Point clouds, Semantic segmentation, BIM},
abstract = {Many infrastructure assets in transportation such as roads and bridges represent challenges for inspection and maintenance due to advanced age, structural deficiencies and modifications. Concepts such as Building Information Modelling (BIM) aim to alleviate the problem of health monitoring and asset management by providing digital building models constructed from survey data to all stakeholders. Ageing and oftentimes poorly-documented infrastructure objects such as bridges in particular benefit from a continuous integration of changes to form a digital twin which reflects the asset’s as-is state. However, the process of reconstructing geometric–semantic models from survey data is a manual and labour-intensive process and makes continuously updating the models a difficult task. To automate this process, a cross-domain approach using an artificial neural network is presented which performs semantic segmentation in the image domain and transfers the results over to the point cloud. For the following fine segmentation, geometric knowledge in the 3D domain is used for post-processing and filtering via geometric reasoning. Using this method, a 3D semantic segmentation is achieved which does not require any 3D point cloud training data and only a low amount of image training data.}
}
@article{HOSSEINI2023116019,
title = {Single-track thermal analysis of laser powder bed fusion process: Parametric solution through physics-informed neural networks},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {410},
pages = {116019},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116019},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523001433},
author = {E. Hosseini and P. Scheel and O. Müller and R. Molinaro and S. Mishra},
keywords = {Physics informed neural networks, Additive manufacturing, Thermal simulation, Parametric analysis},
abstract = {Modelling the highly localised and rapid phenomena occurring during metal additive manufacturing (MAM) processes such as the laser powder bed fusion (LPBF) demands the adoption of very fine time- and space-discretisation and therefore high computational cost for the classical simulation approaches, namely the finite element method (FEM). Particularly, when the solution is required for a range of scenarios, e.g. in sensitivity or optimisation analyses, computation costs of such simulations are not affordable. As an alternative strategy, this study explores the application of physics informed neural networks (PINNs) as a low-cost physics-based simulation approach for the thermal analysis of the LPBF process, through which reliable transient and steady-state temperature profiles for single-track LPBF depositions are achieved. An unsupervised learning strategy is employed for PINNs to parametrically solve the heat transfer equation for the LPBF process. The trained PINNs calculate the temperature profiles and the melt-pool dimensions evolving during the LPBF process for any given set of material’s thermal properties and process conditions at practically zero computational cost. The reliability of the PINNs outcomes is verified through ground-truth data generated based on several benchmark equivalent finite element simulations.}
}
@article{CHOU2023113611,
title = {A perceptron-based learning method for solving the inverse problem of the brain model via poroelastodynamics},
journal = {Chaos, Solitons & Fractals},
volume = {172},
pages = {113611},
year = {2023},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2023.113611},
url = {https://www.sciencedirect.com/science/article/pii/S096007792300512X},
author = {Dean Chou and Po-Yen Chen},
keywords = {Brain model, Inverse problem, Neural networks, Perceptron-based model, Water transfer coefficients},
abstract = {Computer simulations and in silico models are currently the best tools for understanding complex biological processes. However, the complexity of biological tissues, with multiple cellular mechanisms in response to changing physical and chemical external stimuli, makes the corresponding mathematical models highly nonlinear with numerous parameters. These parameters are crucial to the models but are often fitted for specific conditions, making the conclusions drawn difficult to generalize. Moreover, some of these parameters will be hard to obtain through either clinical measurements or experiments. Hence, in this study, we introduced a perceptron-based method to determine unknown parameters of water transfer coefficients in the cerebral multi-compartmental poroelasticity model. Based on the nature and conditions of the available data, we designed a straightforward and functional model to solve a steady-state inverse problem. Moreover, we added an analytical solution to restrict the learning tendency of the model. It is to be noted that we only evaluated the unknown parameters without fitting the solution of PDEs. We believe that this study presents a functional perceptron-based approach for investigating and demonstrating unknown parameters using the cerebral multi-compartmental poroelasticity model. Besides, the algorithm was fully presented since we believed that our scheme has the ability to utilise in various field for those who need to estimate unknown parameters in PDEs. Furthermore, we tested the efficiency and effectiveness of the proposed method and demonstrated how the framework can help estimate the parameters rapidly. Finally, we discussed the unmet needs and forecasted future tasks of this framework.}
}
@article{XIONG2023102912,
title = {Ability-aware knowledge distillation for resource-constrained embedded devices},
journal = {Journal of Systems Architecture},
volume = {141},
pages = {102912},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.102912},
url = {https://www.sciencedirect.com/science/article/pii/S1383762123000917},
author = {Yi Xiong and Wenjie Zhai and Xueyong Xu and Jinchen Wang and Zongwei Zhu and Cheng Ji and Jing Cao},
keywords = {Embedded devices, Model compression, Knowledge distillation},
abstract = {Deep Neural Network (DNN) models have notably improved the efficiency of machine learning tasks. However, their high storage and computational costs restrict their deployment on resource-limited embedded devices. Knowledge distillation (KD) has emerged as a promising approach for compressing DNN models. However, two challenges in KD, namely the capacity gap problem and the time-consuming redundancy problem, have hindered its performance and efficiency in compression. To alleviate these challenges, this paper proposes a novel framework, called Ability-Aware Knowledge Distillation (AAKD). AAKD introduces a knowledge sample selection strategy and an adaptive teacher switching strategy based on the dynamic awareness of the student’s ability. This enables the framework to automatically select suitable knowledge samples and teacher networks according to the increasing representation ability of students. Extensive experiments on different datasets and models have demonstrated that AAKD can enhance the performance of compact student models, significantly improve the efficiency of distillation, and lead to higher compression rates.}
}
@article{EATY2023120444,
title = {Digital twin for electric vehicle battery management with incremental learning},
journal = {Expert Systems with Applications},
volume = {229},
pages = {120444},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120444},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423009466},
author = {Naga Durga Krishna Mohan Eaty and Priyanka Bagade},
keywords = {Digital Twin, SoH (State of Health), SoC (State of Charge), Continual learning, Internet of things, Cloud computing, Microsoft Azure},
abstract = {The current Industry 4.0 revolution promotes the use of cyber–physical systems to enhance manufacturing and other industrial processes via automation, real-time analysis, etc. Data communication between individual systems plays an important role in this revolution’s success. As defined by researchers, Digital Twin is the digital representation of a physical system that enables predictive maintenance. Due to the increase in environmental pollution, battery-powered electric vehicles (EVs) are regarded as the urgent solution to internal combustion engines in the transportation business, despite obstacles such as safety concerns and range estimation. State of Health (SoH) and State of Charge (SoC) are two battery metrics that, when precisely anticipated, permit safer and longer battery use. Predicting these parameters online is computationally and financially expensive. Alternately, some of these factors could be predicted in the cloud rather than on the vehicle, hence cutting costs. Consequently, the EV business is one example where cloud-to-vehicle data connection saves total costs. A digital twin for an EV battery would aid in the estimate of battery parameters for predictive maintenance. This paper presents a Digital Twin paradigm for EV battery management in which SoH is predicted in the cloud and SoC is estimated on-vehicle. A continuous learning method is also proposed for forecasting SoH, whereas the Kalman filter is used to estimate SoC. The proposed framework predicts the SoH with a mean square error of 0.022.}
}
@article{HUANG2023,
title = {DTAIS: Distributed Trusted Active Identity Resolution Systems for the Industrial Internet},
journal = {Digital Communications and Networks},
year = {2023},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2023.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352864823001128},
author = {Tao Huang and Renchao Xie and Yuzheng Ren and F. Richard Yu and Zhuang Zou and Lu Han and Yunjie Liu and Demin Cheng and Yinan Li and Tian Liu},
keywords = {Industrial Internet, NFT, IPFS, trust, identity resolution system},
abstract = {In recent years, the Industrial Internet and Industry 4.0 came into being. With the development of modern industrial intelligent manufacturing technology, digital twins, Web3 and many other digital entity applications are also proposed. These applications apply architectures such as distributed learning, resource sharing, and arithmetic trading, which make high demands on identity authentication, asset authentication, resource addressing, and service location. Therefore, an efficient, secure, and trustworthy Industrial Internet identity resolution system is needed. However, most of the traditional identity resolution systems follow DNS architecture or tree structure, which has the risk of a single point of failure and DDoS attack. And they cannot guarantee the security and privacy of digital identity, personal assets, and device information. So we consider a decentralized approach for identity management, identity authentication, and asset verification. In this paper, we propose a distributed trusted active identity resolution system based on the inter-planetary file system (IPFS) and non-fungible token (NFT), which can provide distributed identity resolution services. And we have designed the system architecture, identity service process, load balancing strategy and smart contract service. In addition, we use Jmeter to verify the performance of the system, and the results show that the system has good high concurrent performance and robustness.}
}
@article{ASANTEOKYERE2023100089,
title = {Estimating total organic carbon (TOC) of shale rocks from their mineral composition using stacking generalization approach of machine learning},
journal = {Upstream Oil and Gas Technology},
volume = {11},
pages = {100089},
year = {2023},
issn = {2666-2604},
doi = {https://doi.org/10.1016/j.upstre.2023.100089},
url = {https://www.sciencedirect.com/science/article/pii/S266626042300004X},
author = {Solomon Asante-Okyere and Solomon Adjei Marfo and Yao Yevenyo Ziggah},
keywords = {Total organic carbon, Stacking, Mineral composition, Multivariate adaptive regression spline, Machine learning},
abstract = {A fundamental parameter in the exploration and development of unconventional shale reservoirs is total organic carbon (TOC). To achieve reliable TOC values, it requires a labour intensive and time-consuming laboratory experiment. On the other hand, models have been proposed using geophysical well logs as input variables with little attention paid to the contribution of mineralogical parameters in the evaluation of TOC. In this paper, a novel stacking machine learning technique is examined to generate accurate TOC predictions from the mineral content of the shale rock in the Sichuan Basin. The stacking machine learning model involves first-level models of multivariate adaptive regression spline (MARS), random forest (RF) and gradient boosted machine (GBM) known as base learners, while MARS was further used in the next step as the meta learner model. The research result indicated that the stacking TOC model outperformed the single applied models of MARS, GBM and RF. The proposed stacking TOC model generated estimates having the least error statistics of 0.29, 0.54 and 0.54 for MSE, RMSE and MAPE respectively while producing the highest correlation of 0.86 during the model validation stage. Therefore, stacking machine learning approach permits an improved estimation of TOC from the mineralogy of the rock.}
}
@article{HUA2023121128,
title = {Digital twin based reinforcement learning for extracting network structures and load patterns in planning and operation of distribution systems},
journal = {Applied Energy},
volume = {342},
pages = {121128},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121128},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923004920},
author = {Weiqi Hua and Bruce Stephen and David C.H. Wallom},
keywords = {Digital twin, Distribution network, Fitted Q-iteration, Load pattern, Network configuration, Reinforcement learning},
abstract = {Low voltage distribution networks deliver power to the last mile of the network, but are often legacy assets from a time when low carbon technologies, e.g., electrified heat, storage, and electric vehicles, were not envisaged. Furthermore, exploiting emerging data from distribution networks to provide decision support for adapting planning and operational strategies with system transitions presents a challenge. To overcome these challenges, this paper proposes a novel application of digital twins based reinforcement learning to improve decision making by a distribution system operator, with key metrics of predictability, responsiveness, interoperability, and automation. The power system states, i.e., network configurations, technological combinations, and load patterns, are captured via a convolutional neural network, chosen for its pattern recognition capability with high-dimensional inputs. The convolutional neural networks are iteratively trained through the fitted Q-iteration algorithm, as a batch mode reinforcement learning, to adapt the planning and operational decisions with the dynamic system transitions. Case studies demonstrate the effectiveness of the proposed model by reducing 50% of the investment cost when the system transitions towards the winter and maintaining the power loss and loss of load within 5% compared to the benchmark optimisation. Doubled power consumption was observed in winter under future energy scenarios due to the electrification of heat. The trained model can accurately adapt optimal decisions according to the system changes while reducing the computational time of solving optimisation problems, for a range of scales of distribution systems, demonstrating its potential for scalable deployment by a system operator.}
}
@article{RAKIC2023144836,
title = {Liquid organic hydrogen carrier hydrogenation–dehydrogenation: From ab initio catalysis to reaction micro-kinetics modelling},
journal = {Chemical Engineering Journal},
volume = {472},
pages = {144836},
year = {2023},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2023.144836},
url = {https://www.sciencedirect.com/science/article/pii/S1385894723035672},
author = {Emilija Rakić and Miha Grilc and Blaž Likozar},
keywords = {Liquid organic hydrogen carrier (LOHC) molecules, Multiscale modelling chemical reaction kinetics, Density functional theory (DFT), Computational fluid dynamics (CFD), Machine learning (ML), Catalysis, H storage},
abstract = {The continued selective focus on the exploitation of fossil fuel chemicals as one of the main environment-depleting sources of energy is one of the reasons for the carbon dioxide emissions, severe air pollution and market crisis of today. The related easy transition to efficient renewable resources also brings challenges, such as the storing of performance generated year round. Consistent application option is to convert the formed transferred electricity produced into the hydrogen through electrolysis, store gaseous H2, and reversibly proceed with reforming or cracking. This routine way is also referred to in literature as evolving green H2. A relatively new method of storage is liquid organic carriers (LOHCs). These are molecules that are in a (l) state at room temperature measurements, contain unsaturated covalent bonds, and can be hydrogenated/dehydrogenated in the many loading cycles without catalytic decomposition products. Paper presents possible structure systems that have been previously investigated as an alternative to conventional. The process of catalysis, reduction and coking, catalysts, and the most commonly used elementary groups, interactions, and reaction condition analyses are listed, while an overview of studies that have assessed technical transfer phenomena is also provided. Reports, dealing with derived micro-kinetic modelling/computational fluid dynamics (CFD), which is a direction for further research activities, are few. As for multiscale, review ranges from the density functional theory (DFT) to CFD. The review paper also addresses the latest studies on LOHCs in the field of artificial intelligence (AI), machine learning (ML), and artificial neural networks (ANN). The progress within the area with approaches is highlighted. Mesoscale surface–selectivity relationships, the robustness towards deactivation and techno-economics are dominant in linking the digital twin design to operation.}
}
@article{HUANG2023103697,
title = {Collective reinforcement learning based resource allocation for digital twin service in 6G networks},
journal = {Journal of Network and Computer Applications},
volume = {217},
pages = {103697},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103697},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523001169},
author = {Zhongwei Huang and Dagang Li and Jun Cai and Hua Lu},
keywords = {Digital twin, Resource allocation, Internet of Things, Collective reinforcement learning},
abstract = {The 6th generation (6G) mobile communications technology will realize the interconnection of humans, machines, things as well as virtual space. The development of digital twins (DTs) and 6G has accelerated the Internet of Things (IoT) in an unprecedented way. The combination of DTs and edge intelligence (EI) enables powerful digital space synchronized with the real world constructed in the intelligent edge, bringing real-time, and adaptive services delivery of IoT. However, the dynamic features and heterogeneous resources in 6G-enabled IoT make the resource allocation for computation-intensive and delay-sensitive DTs services more challenging. In this paper, we first define the DTs implementation process as a DT service function chain (DTSFC) and address the resource allocation problem of DTs-empowered networks in form of dynamic DTSFCs orchestration. We further propose a novel collective reinforcement learning (CRL) method which is inspired by human collaboration, to realize the effective resource allocation of DTSFCs. Numerical results verify that the proposed CRL algorithm improves the learning efficiency and generalization ability compared with the benchmarks.}
}
@article{XIE2023115098,
title = {Probabilistic real-time natural gas jet fire consequence modeling of offshore platforms by hybrid deep learning approach},
journal = {Marine Pollution Bulletin},
volume = {192},
pages = {115098},
year = {2023},
issn = {0025-326X},
doi = {https://doi.org/10.1016/j.marpolbul.2023.115098},
url = {https://www.sciencedirect.com/science/article/pii/S0025326X23005301},
author = {Weikang Xie and Junjie Li and Jihao Shi and Xinqi Zhang and Asif Sohail Usmani and Guoming Chen},
keywords = {Real-time jet fire modeling, Offshore platform, Deep learning, Variational Bayesian inference, Digital twin},
abstract = {Natural gas jet fire induced by igniting blowouts has the potential to cause critical structure damage and great casualties of offshore platforms. Real-time natural gas jet fire plume prediction is essential to support the emergency planning to mitigate subsequent damage consequence and ocean pollution. Deep learning based on a large amount of Computational fluid dynamics (CFD) simulations has recently been applied to real-time fire modeling. However, existing approaches based on point-estimation theory are ‘over-confident’ when prediction deficiency exists, which reduce robustness and accuracy for emergency planning support. This study proposes probabilistic deep learning approach for real-time natural gas jet fire consequence modeling by integrating variational Bayesian inference with deep learning. Numerical model of natural gas jet fire from offshore platform is built and the natural gas jet fire scenarios are simulated to construct the benchmark dataset. Sensitivity analysis of pre-defined parameters such as MC (Monte Carlo) sampling number m and dropout probability p is conducted to determine the trade-off between model's accuracy and efficiency. The results demonstrated our model exhibits competitive accuracy with R2 = 0.965 and real-time capacity with an inference time of 12 ms. In addition, the predicted spatial uncertainty corresponding to spatial jet fire flame plume provides more comprehensive and reliable support for the following mitigation decision-makings compared to the state-of-the-art point-estimation based deep learning model. This study provides a robust alternative for constructing a digital twin of fire and explosion associated emergency management on offshore platforms.}
}
@article{SON2023102035,
title = {A novel physics-informed neural network for modeling electromagnetism of a permanent magnet synchronous motor},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102035},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102035},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623001635},
author = {Seho Son and Hyunseung Lee and Dayeon Jeong and Ki-Yong Oh and Kyung {Ho Sun}},
keywords = {Physics-informed neural network, Electromagnetics, Governing equation, Domain decomposition, Interface loss, Adaptive weight},
abstract = {This study presents a novel physics-informed neural network (PINN) architecture designed to address the challenges of replicating an electric motor. The proposed architecture has three key features. First, it uses three partial differential equations with rotational coordinate transformation to supervise the neural network during training with limited data, which improves the accuracy of the solution. One of the differential equations is expressed in variational form to effectively compute the numerical integration. Second, separate networks are proposed for the rotor and stator domains due to their distinct characteristics during operation, namely, that the rotor rotates while the stator remains fixed. An interface loss is included in the entire loss function to compensate for the significant discontinuity and incompatibility between the separate networks when estimating the results of both domains. Third, a learning rate annealing method is introduced to update the adaptive weights of each loss term, thus improving the accuracy and robustness during the training of the neural network. The performance of the proposed PINN was validated using electromagnetic response datasets obtained from both measurements and finite element analyses. Systematic analysis demonstrated that the three features significantly improved the accuracy and robustness of the neural network when estimating the electromagnetic responses of an electric motor. Furthermore, the inference time of the PINN is ten times faster than that of a finite element analysis with a similar level of accuracy, making it suitable for control and design purposes in various real-world applications. Consequently, the versatility of the proposed PINN can accelerate the development of digital twins for intelligent systems by deploying an electric motor, and it could also be used for prognostics and health management because it can estimate electromagnetic responses under both normal and failure conditions.}
}
@article{TONG2023110611,
title = {Two-stage reverse knowledge distillation incorporated and Self-Supervised Masking strategy for industrial anomaly detection},
journal = {Knowledge-Based Systems},
volume = {273},
pages = {110611},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110611},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123003611},
author = {Guoxiang Tong and Quanquan Li and Yan Song},
keywords = {Anomaly detection, Self-supervised mask training, Anomaly feature diffusion, Normalized embedding memory bank},
abstract = {In recent years, unsupervised anomaly detection based on knowledge distillation has gained special attention and some promising results have been reported in the literature. However, there is still room to improve the sensitivity of the model to anomalies. To do so, in this paper, a novel two-stage training method in terms of reverse knowledge distillation is proposed for anomaly detection and localization. Firstly, self-supervised mask training is introduced after the initial training of reverse knowledge distillation, which contributes greatly to the model detection against random unknown anomalies by self-simulating anomalies and forcing repair so as to reinforce learning single-category prototype patterns. Then, with the aim to facilitate the anomaly localization, an anomaly feature diffusion module is employed, which strengthens the correlation between pixels and helps spread the anomaly information to the surrounding area by covering the central pixel and reconstructing the representation for features after diffused. Furthermore, inspired by the human memory mechanism, an innovative normalized embedding memory bank is adopted to regulate the low-dimensional representations after embedding the encoding, inhibit the flow of anomalous information to the student decoder, and encourage the high-quality reconstruction of the model. Finally, the contextual similarity loss is used to guide the student model to learn knowledge representations from a contextual perspective, capture higher-order similarities between teachers and students, and delicately evaluate the differences between teachers and students. The empirical experiments conducted on the MVTec dataset show that the proposed SSMRKD method can achieve the best performance compared to other state-of-the-art methods, meanwhile extensive experiments of the ablation study validate the contribution of each component of the model. In addition, the advanced performance achieved on four commonly used datasets verifies the generalizability of the model in the industrial domain. Overall, the proposed SSMRKD method has significant advantages over the state-of-the-art anomaly detection methods.}
}
@article{GEURTSEN2023170,
title = {Deep reinforcement learning for optimal planning of assembly line maintenance},
journal = {Journal of Manufacturing Systems},
volume = {69},
pages = {170-188},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000845},
author = {M. Geurtsen and I. Adan and Z. Atan},
keywords = {Scheduling, Maintenance, Deep reinforcement learning, Simulation, Case-study, Flexibility},
abstract = {Discovering the optimal maintenance planning strategy can have a substantial impact on production efficiency, yet this aspect is often overlooked in favor of production planning. This is a missed opportunity as maintenance and production activities are deeply intertwined. Our study sheds light on the significance of maintenance planning, particularly in the dynamic setting of an assembly line. By maximizing the average production rate and incorporating flexible planning windows, buffer content, and machine production states, a unique problem is addressed in which a policy for planning maintenance on the final machine of a serial assembly line is developed. To achieve this, novel average-reward deep reinforcement learning techniques are employed and pitted against generic dispatching methods. Using a digital twin with real-world data, experiments demonstrate the immense potential of this new deep reinforcement learning technique, producing policies that outperform generic dispatching strategies and practitioner policies.}
}
@article{CHAI2023105251,
title = {Incremental learning model for dynamical identification and classification of abnormal vibration in operational underground facilities},
journal = {Tunnelling and Underground Space Technology},
volume = {140},
pages = {105251},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2023.105251},
url = {https://www.sciencedirect.com/science/article/pii/S0886779823002717},
author = {Fu Chai and Biao Zhou and Xiongyao Xie and Zixin Zhang and Chen Wang},
keywords = {Underground infrastructure, Monitoring, Incremental learning, Variational autoencoder, Metric function},
abstract = {Underground infrastructures are rapidly growing in size and complexity. However, their operations are affected by several hazards, including hidden structural deterioration and effects of random external constructions. Dynamic monitoring of these hazards is essential to provide early warning. We propose a vibration-based self-supervised incremental learning model for dynamic monitoring of emerging operational threats by recognizing abnormal responses. The model comprises teacher and student models based on a variational autoencoder (VAE) with a metric function. When the teacher model detects a new category of abnormal vibration, the student model is trained to recognize this abnormality through sample rehearsals and knowledge distillations. Subsequently, it becomes the teacher model for the next round of incremental learning. We demonstrate through a case study that catastrophic forgetting can be avoided and memory consumption can be reduced during dynamic network updates. Moreover, the use of a metric function in the VAE increases the vibration identification accuracy.}
}
@article{S2023100186,
title = {MUD enabled deep learning framework for anomaly detection in IoT integrated smart building},
journal = {e-Prime - Advances in Electrical Engineering, Electronics and Energy},
volume = {5},
pages = {100186},
year = {2023},
issn = {2772-6711},
doi = {https://doi.org/10.1016/j.prime.2023.100186},
url = {https://www.sciencedirect.com/science/article/pii/S2772671123000815},
author = {Mirdula S and Roopa M},
keywords = {Smart building, Deep learning, Manufacturer usage description, Anomaly detection},
abstract = {Nowadays, many Internet of Things (IoT) devices of different types are used in creating smart applications like smart cities, smart industries, smart environments, and the applications of industry-4.0. IoT devices are used for different purposes, such as security, remote monitoring, resource allocation, threats, ecosystems, and vulnerabilities. This paper proposed a deep learning algorithm-based solution to tighten the security level in the IoT-Smart environment network. The Intrusion Detection System (IDS) considered in this paper is Network IDS, which investigates the manufacturer usage description, digital twins, and deep learning-based user behavior information. IoT devices' communication and the users in smart buildings are automatically connected in the Intelligent Communication system. Since many devices and users are interconnected in smart buildings, the probability of cyber-attack is high. Thus, better security is needed in smart buildings and smart environments. It should focus on securing IoT devices, users, and their communication. Hence, this paper developed a deep learning-based anomaly detection framework to dynamically monitor the issues and problems with MUD profiles and detect the anomaly behavior. The Manufacturer Usage Description (MUD) profiles, dynamic user behavior, IoT devices' traffic data the pattern of abnormal/anomaly traffic at the device level is predicted while traffic occurs. The MUD-ML-based model is implemented in Python software, verifying the results.}
}
@article{WESCOAT202357,
title = {Redefining the digital triplet for surrogate system integration},
journal = {Manufacturing Letters},
volume = {36},
pages = {57-61},
year = {2023},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323000159},
author = {Ethan Wescoat and Matthew Krugh and Vinita Jansari and Laine Mears},
keywords = {Purposeful failure twin, Purposeful failure methodology, Digital twin, Digital triplet},
abstract = {Predictive Maintenance (PdM) requires methods and tools to convey process information to maintenance planners allowing for data-driven repair decisions. Cyber-Physical Systems (CPS) and Digital Twins (DT) are current tools that transform data for informed decision making; however, the successful deployment of these tools is hampered by missing or low levels of training data for machine specific events such as failure. This paper proposes a standardized framework for adapting data from offline environments to train online systems without real world failure training data. This novel process, the Surrogate Digital Triplet (SDTr) framework, incorporates a third system, the surrogate triplet, to transfer data between the lab (offline) and production (online) environment. SDTr standardizes the data, information, and knowledge interfaces between systems to pass offline learning to the real world in a traceable manner.}
}
@article{DAI2023164858,
title = {Achieving better indoor air quality with IoT systems for future buildings: Opportunities and challenges},
journal = {Science of The Total Environment},
volume = {895},
pages = {164858},
year = {2023},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2023.164858},
url = {https://www.sciencedirect.com/science/article/pii/S0048969723034812},
author = {Xilei Dai and Wenzhe Shang and Junjie Liu and Min Xue and Congcong Wang},
keywords = {Indoor air quality, Data-driven modeling, Machine learning, Internet of things, Occupant-centric control, Digital twins},
abstract = {With the development of IoT technology and low-cost indoor air quality (IAQ) sensors, the IoT-based IAQ monitoring platform has garnered significant research interest and demonstrated its potential in enhancing IAQ management. This study presents a comprehensive review of previous research on the development and application of IoT-based IAQ platforms in different built environments. It offers detailed insights into the design and implementation of recent IoT-based IAQ platforms. The findings indicate that the IoT-based IAQ platforms are able to provide reliable information for IAQ monitoring. To ensure quality control of the IoT-based IAQ platform, it is suggested to replace the sensors every 4–6 months for reliable monitoring. In another aspect, integrating data-driven technology into the platform is crucial for IAQ prediction and efficient control of ventilation systems, leveraging the wealth of data available from the IoT platform. According to recent studies that applied data-driven algorithms for IAQ management, it can be confirmed that the data-driven algorithms are able to prompt IAQ by providing either more information or a control strategy. However, it should be noted that only 9.1 % of the developed platforms integrated data-driven models for IAQ management. Based on our findings, current challenges and further opportunities are discussed. Future studies should focus on integrating data-driven algorithms into IoT-based IAQ platforms and developing digital twins that can be used for real building IAQ management. However, there is obvious tension between controlling ventilation for energy efficiency versus better air quality. It is important to make a balance between energy efficiency and better air quality according to the current situations of specific built environments. Also, the next generation of IoT-based IAQ platforms should include occupants in the loop to create a more occupant-centric IAQ management approach.}
}
@article{ZOHDI2023115991,
title = {A machine-learning digital-twin for rapid large-scale solar-thermal energy system design},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {412},
pages = {115991},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.115991},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523001147},
author = {T.I. Zohdi},
keywords = {Solar-thermal, Energy management systems, Digital-twin, Machine-learning},
abstract = {In many industrialized regions of the world, large-scale photovoltaic systems now contribute a significant part to the energy portfolio during daylight operation. However, as energy demands peak shortly before sunset and persist for several hours afterwards, the integration of solar-thermal systems is extremely advantageous as a green “bridge” energy source. Accordingly, this work develops a digital-twin model to track and optimize the flow of incoming solar power through a complex solar-thermal storage system, consisting of a large array of adaptable mirrors, an optical-receiver and a power distribution system for customers to extract energy. Specifically, the solar power flow is rapidly computed with a reduced order model of Maxwell’s equations, based on a high-frequency decomposition of the irradiance into multiple rays that experience mirror reflections, losses and ultimately receiver absorption and customer delivery. The method allows for rapid testing (in microseconds) of the performance of large numbers of mirror-receiver layout configurations in design space, over extremely long time periods, such as weeks, months and years, using a genetic-based machine-learning digital-twin framework, which integrates submodels for: •optics and tracking of the Fresnel multi-mirror system,•thermal absorption of the optical energy by the receiver and•optimal operating temperatures balancing radiative losses with heat storage. The overall machine-learning digital-twin optimizes the configuration layout to balance meeting customer demands and operational efficiency. Numerical examples are provided to illustrate the approach. Finally, a deep-learning algorithm is developed and applied to the create an Artificial Neural-Net representation, which allows for even further simulation speedup.}
}
@article{ERDOGDU2023101042,
title = {Mathematical modeling of food thermal processing: current and future challenges},
journal = {Current Opinion in Food Science},
volume = {51},
pages = {101042},
year = {2023},
issn = {2214-7993},
doi = {https://doi.org/10.1016/j.cofs.2023.101042},
url = {https://www.sciencedirect.com/science/article/pii/S2214799323000565},
author = {Ferruh Erdogdu},
abstract = {Food industry still relies on conventional canning and aseptic processing for thermal applications, while novel approaches have been introduced for process and energy efficiency with their environmentally-friendly features. Designing and optimizing a thermal process are the main concerns of the food industry where equipment design with manufacturing and process control features are also considered. Therefore, current and future challenges of mathematical approaches and their benefits for the upcoming challenges were introduced. Evolution of modeling studies was also presented, and new enabling technologies combined with mathematical modeling for the food processing were explained for the view of a sustainable and efficient processing.}
}
@article{ZHANG2023102601,
title = {Digital twin-enabled grasp outcomes assessment for unknown objects using visual-tactile fusion perception},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {84},
pages = {102601},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102601},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000777},
author = {Zhuangzhuang Zhang and Zhinan Zhang and Lihui Wang and Xiaoxiao Zhu and Huang Huang and Qixin Cao},
keywords = {Grasp outcomes assessment, Visual-tactile perception, Deep learning, Multimodal fusion, Digital twin},
abstract = {Humans can instinctively predict whether a given grasp will be successful through visual and rich haptic feedback. Towards the next generation of smart robotic manufacturing, robots must be equipped with similar capabilities to cope with grasping unknown objects in unstructured environments. However, most existing data-driven methods take global visual images and tactile readings from the real-world system as input, making them incapable of predicting the grasp outcomes for cluttered objects or generating large-scale datasets. First, this paper proposes a visual-tactile fusion method to predict the results of grasping cluttered objects, which is the most common scenario for grasping applications. Concretely, the multimodal fusion network (MMFN) uses the local point cloud within the gripper as the visual signal input, while the tactile signal input is the images provided by two high-resolution tactile sensors. Second, collecting data in the real world is high-cost and time-consuming. Therefore, this paper proposes a digital twin-enabled robotic grasping system to collect large-scale multimodal datasets and investigates how to apply domain randomization and domain adaptation to bridge the sim-to-real transfer gap. Finally, extensive validation experiments are conducted in physical and virtual environments. The experimental results demonstrate the effectiveness of the proposed method in assessing grasp stability for cluttered objects and performing zero-shot sim-to-real policy transfer on the real robot with the aid of the proposed migration strategy.}
}
@article{LIU202395,
title = {An effective energy management Layout-Based reinforcement learning for household demand response in digital twin simulation},
journal = {Solar Energy},
volume = {258},
pages = {95-105},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23002967},
author = {Huafeng Liu and Qine Liu and Chaoping Rao and Fei Wang and Fahad Alsokhiry and Alexey V. Shvetsov and Mohamed A. Mohamed},
keywords = {Fuzzy reasoning, Reinforcement learning, Solar-based smart home, Home energy management, Demand response, digital twin},
abstract = {With the growth in energy consumption, demand response (DR) programs in the power network have gained popularity and can be expected to become more widespread in the future. Through DR programs, users are encouraged for utilizing renewable energy and reducing their power consumption at peak times, thereby helping to balance supply and demand on the grid, as well as generating revenue from the sale of excess power. This paper presents an effective energy management layout (EML) for household DR employing Reinforcement Learning (RL) and Fuzzy Reasoning (FR). RL would be a model-free control method that consists of doing measures and assessing the outcomes as it interacts with the environments. Through direct integration of customer feedback into its control logic, the suggested method takes into account user satisfaction by utilizing FR as a reward function. Through the shift of controllable devices from peak hours, whenever energy cost is higher, to off-peak periods, whenever energy cost is low, Q-learning, an RL method according to a reward scheme, has been applied for scheduling the execution of smart home devices. With the suggested method, 14 home devices can be controlled by one agent, and many status-action pairs as well as fuzzy logic for the reward function are used to assess the actions taken for a particular status. Simulations are implemented in the digital twin environment and demonstrate that the suggested device planning method smooths the energy usage and minimizes the energy price by taking into account the consumers' satisfaction, the consumers' feedback, and their satisfaction settings. The Home EML has been presented with a consumer interface in MATLAB/Simulink for demonstrating the suggested DR approach. The simulation tools include smart devices, energy price signals, smart meters, solar photovoltaics, batteries, electric vehicle, and grid supply.}
}
@article{RACHMAWATI2023106430,
title = {Digital twin-enabled 3D printer fault detection for smart additive manufacturing},
journal = {Engineering Applications of Artificial Intelligence},
volume = {124},
pages = {106430},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106430},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623006140},
author = {Syifa Maliah Rachmawati and Made Adi Paramartha Putra and Jae Min Lee and Dong Seong Kim},
keywords = {Additive manufacturing, Deep learning, Digital twin, Fault detection, FDM printer},
abstract = {Early failure detection is required for Fused Deposition Modelling (FDM) 3D printers to reduce material waste. Typically, such systems are created based on images captured during printing or sensor data for tracking the extruder’s movement. This work presents a novel approach to sensor data-driven fault diagnosis, utilizing Artificial Intelligence (AI) technology to investigate the temperature imbalance in the extruder and printing surface. First, a Lightweight Convolutional Neural Network (LCNN) is proposed to detect faults from sensory data. The model’s architecture concatenates the CNN layer to extract additional features, improving the model’s performance while maintaining a lightweight configuration suitable for real-time monitoring systems. Second, the concept of Digital Twin (DT) technology for FDM 3D printer fault detection is introduced. The DT creates a virtual representation of a physical object, and its functionality is validated by examining the network’s latency and System Overhead (SO) as the number of clients increases. The simulation results show that the proposed LCNN with a DT environment can effectively monitor, detect, and control the physical workplace with an F1-Score of 0.9981 and an average latency of 995.4253ms. Additionally, this research contributes to the development of future technologies for virtual condition monitoring of 3D printer abnormalities, which will be essential for intelligent and autonomous factories.}
}
@article{IVANOV2023108938,
title = {Intelligent digital twin (iDT) for supply chain stress-testing, resilience, and viability},
journal = {International Journal of Production Economics},
volume = {263},
pages = {108938},
year = {2023},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2023.108938},
url = {https://www.sciencedirect.com/science/article/pii/S0925527323001706},
author = {Dmitry Ivanov},
keywords = {Supply chain resilience, Intelligent digital twin, Data analytics, Stress-test, Ripple effect, anyLogistix},
abstract = {A large variety of models have been developed in the last two decades aiming at supply chain (SC) stress-testing and resilience. New digital and artificial intelligence (AI) technologies allow to develop novel approaches and tools in this area for the transition from standalone models to intelligent decision-support systems (DSSs). However, the literature lacks concepts and guidelines for the design of such systems. In this paper, we offer a generalized decision-making framework for using digital twins in SC stress-testing and resilience analysis as well as delineate how digital twins can contribute to theory development in SC resilience and viability. We position our proposed approach as an intelligent digital twin (iDT) – a human–AI system which visualizes physical SCs in digital form, collects and processes data for modelling using analytics methods, mimics human decision-making rules, and creates new knowledge and decision-making algorithms through human–AI collaboration. We conclude that the iDT supports monitoring, disruption prediction (early signals), event-driven responses, learning, and proactive thinking, integrating proactive and reactive approaches to SC resilience. The iDT helps to make the unknown known and so contributes to the development of a proactive, adaptation-based view on SC resilience and viability. This research can be used to solve existing problems in the industry, and it develops new methods and infrastructures for solutions to future problems.}
}
@article{NTEFUASAAH2023106609,
title = {Blockchain technology in the AEC industry: Scientometric analysis of research activities},
journal = {Journal of Building Engineering},
volume = {72},
pages = {106609},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106609},
url = {https://www.sciencedirect.com/science/article/pii/S235271022300788X},
author = {Alvina Ekua {Ntefua Saah} and Jae-ho Choi},
keywords = {Blockchain technology, Smart contract, Digital technology, AEC industry, Scientometric review},
abstract = {The architectural, engineering and construction (AEC) industry is plagued with complex and difficult problems such as late payments, inadequate information sharing and supply chain issues. Thus, advances in blockchain technology are increasingly investigated as a viable digital technology to address such problems. This advantage has led to blockchain gaining significant traction within the AEC industry thus piquing the interests of researchers. Although prior reviews on blockchain have been well appreciated, those studies do not give a complete picture of blockhain in the AEC industry as a whole. For example, whereas some researchers focused their studies on the construction industry, others did so across multiple domains. And researchers who attempted to review blockchain in the AEC industry employed a qualitative methodology which have been criticized for its lack of reproducibility and susceptibility to subjective biases. Moreover, those studies which utilized a quantitative approach employed a single science mapping tool for its analysis, which did not meet the criteria for a robust science mapping research. Therefore, there is the need for further review research efforts to supplement the limitations of the previous works. Thus, this study presents a robust science-mapping based analysis of the state-of-the art research on blockchain technology in the AEC industry, by combining three scientometric tools to analyze quantitatively, 12,549 relevant bibliographic data retrieved from the Web of Science (WoS) database. The results revealed that blockchain can be used as an optimization technology to optimize processes, systems, activities, and decision-making in the AEC industry. The findings confirmed that blockchain is being utilized to resolve AEC management problems in supply chain, projects, risk, cost, privacy, and security. The study further disclosed Industry 4.0 technologies that have presently been integrated with blockchain to improve its practicability in the AEC industry, as well as the understudied research areas. Within the existing corpus of literature on blockchain, the research findings are informative in identifying and comprehending trends and patterns, including core research topics, countries, and institutions and their interconnection. This study contributes to the global body of knowledge in blockchain by providing a holistic view of the state-of-the-art development of blockchain and proposes the directions of future research efforts, while promoting the consciousness of blockchain in the AEC industry.}
}
@article{PAN2023100135,
title = {Building energy simulation and its application for building performance optimization: A review of methods, tools, and case studies},
journal = {Advances in Applied Energy},
volume = {10},
pages = {100135},
year = {2023},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2023.100135},
url = {https://www.sciencedirect.com/science/article/pii/S2666792423000148},
author = {Yiqun Pan and Mingya Zhu and Yan Lv and Yikun Yang and Yumin Liang and Ruxin Yin and Yiting Yang and Xiaoyu Jia and Xi Wang and Fei Zeng and Seng Huang and Danlin Hou and Lei Xu and Rongxin Yin and Xiaolei Yuan},
keywords = {Building performance simulation, Performance-driven design, Operational optimization, Digital twin, Building-to-grid},
abstract = {As one of the most important and advanced technology for carbon-mitigation in the building sector, building performance simulation (BPS) has played an increasingly important role with the powerful support of building energy modelling (BEM) technology for energy-efficient designs, operations, and retrofitting of buildings. Owing to its deep integration of multi-disciplinary approaches, the researchers, as well as tool developers and practitioners, are facing opportunities and challenges during the application of BEM at multiple scales and stages, e.g., building/system/community levels and planning/design/operation stages. By reviewing recent studies, this paper aims to provide a clear picture of how BEM performs in solving different research questions on varied scales of building phase and spatial resolution, with a focus on the objectives and frameworks, modelling methods and tools, applicability and transferability. To guide future applications of BEM for performance-driven building energy management, we classified the current research trends and future research opportunities into five topics that span through different stages and levels: (1) Simulation for performance-driven design for new building and retrofit design, (2) Model-based operational performance optimization, (3) Integrated simulation using data measurements for digital twin, (4) Building simulation supporting urban energy planning, and (5) Modelling of building-to-grid interaction for demand response. Additionally, future research recommendations are discussed, covering potential applications of BEM through integration with occupancy and behaviour modelling, integration with machine learning, quantification of model uncertainties, and linking to building monitoring systems.}
}
@article{KAY2023107848,
title = {Foundations for Human-AI teaming for self-regulated learning with explainable AI (XAI)},
journal = {Computers in Human Behavior},
volume = {147},
pages = {107848},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107848},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223001991},
author = {Judy Kay},
keywords = {Self-regulated learning, Learner control, Explainable AI (XAI), Scrutability, Open learner model (OLM)},
abstract = {This discussion takes a human-centred perspective of the contributions of the collection. Its papers explore diverse, new uses of AI with rich, multimedia sensor data towards new ways to measure and understand self-regulated learning. This work can contribute to the learning sciences. It can also provide a foundation for future personalised teaching and learning systems with explainable AI (XAI) and learner control. I will discuss the papers from that perspective with a focus on an important form of XAI in education – the Open Learner Models (OLM). When suitably designed, OLMs can empower a learner to: (1) contribute data about themself and their self-regulated learning processes, complementing conventional and multimedia data; (2) scrutinise and control learner data collection and use in AI-based systems and (3) be the controlling partner in AI-teaming that scaffolds their self-regulated learning processes.}
}
@article{RODRIGUEZ2023103958,
title = {Updating digital twins: Methodology for data accuracy quality control using machine learning techniques},
journal = {Computers in Industry},
volume = {151},
pages = {103958},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103958},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001082},
author = {Fabio Rodríguez and William D. Chicaiza and Adolfo Sánchez and Juan M. Escaño},
keywords = {Adaptive digital twin, Neural network (NN), Fuzzy inference system, Fault detection, Adaptive decision making},
abstract = {The Digital Twin (DT) constitutes an integration between cyber and physical spaces and has recently become a popular concept in smart manufacturing and Industry 4.0. The related literature provides a DT characterisation and identifies the problem of updating DT models throughout the product life cycle as one of the knowledge gaps. The DT must update its performance by analysing the variable data in real time of the physical asset, whose behaviour is constantly changing over time. The automatic update process involves a data quality problem, i.e., ensuring that the captured values do not come from measurement or provoked errors. In this work, a novel methodology has been proposed to achieve data quality in the interconnection between digital and physical spaces. The methodology is applied to a real case study using the DT of a real solar cooling plant, acting as a learning decision support system that ensures the quality of the data during the update of the DT. The implementation of the methodology integrates a neurofuzzy system to detect failures and a recurrent neural network to predict the size of the errors. Experiments were carried out using historical plant data that showed great results in terms of detection and prediction accuracy, demonstrating the feasibility of applying the methodology in terms of computation time.}
}
@article{WANG202337,
title = {Mutual mentor: Online contrastive distillation network for general continual learning},
journal = {Neurocomputing},
volume = {537},
pages = {37-48},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.03.066},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223003132},
author = {Qiang Wang and Zhong Ji and Jin Li and Yanwei Pang},
keywords = {General continual learning, Online contrastive distillation, Knowledge distillation, Supervised contrastive learning, Image classfication},
abstract = {The goal of General Continual Learning (GCL) is to preserve learned knowledge and learn new knowledge with constant memory from infinite data stream where task boundaries are blurry. Distilling the model’s response of reserved samples between the old and new models is an effective way to achieve promising performance on GCL. However, it accumulates the inherent old model’s response bias and is not robust to model changes. To this end, we propose a Mutual Mentor General Continual Learning (MMGCL) framework to tackle these problems, which explores a training process in which the student and teacher models mentor each other. Concretely, the student model consolidates the learned knowledge by respectively aligning the relation and adaptive responses with those of the teacher model while the teacher model updates its parameters by integrating the parameters of the student model to accumulate new knowledge. To further improve the effectiveness of the mutual mentor, we integrate the inter-instance knowledge to optimize the outputs of the teacher model, which can not only supervise the student model but also indirectly optimize the teacher model. Extensive experiments on six benchmark datasets demonstrate that our MMGCL significantly outperforms state-of-the-art approaches under diverse continual learning settings with various buffer sizes.}
}
@article{ZHAO2023119363,
title = {Multi-view computable online learner modeling by heterogeneous network: An AI-enabled perspective},
journal = {Information Sciences},
volume = {645},
pages = {119363},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119363},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523009489},
author = {Anping Zhao and Yu Yu},
keywords = {Computable modeling, Multi-view, Semantic embeddings, Heterogeneous network},
abstract = {To support accurate personalized online intelligent education applications in the big data environment, computable online learner modeling is very important. It can accurately capture the unique needs and characteristics of learners in a computable way and provides customized learning experiences. We propose a structured semantic embedding-based approach for online learner modeling, which considers both the multidimensional learners' characteristics and heterogeneous information on the learning context to develop a comprehensive learner model. Specifically, an AI-enabled heterogeneous information network embedding technique is employed to encode complications of different learners' characteristics and rich relationships between them, which integrate different explicit characteristic information and implicit association information about inherent multidimensional interactions for computable learners modeling. Extensive experimental results and evaluations demonstrate that the proposed model achieves the optimal performance on the Precision@N and Recall@N metrics in course recommendation tasks, and the Accuracy and F−score metrics in learner clustering tasks are also better than other baseline models, which prove the ability to boost the performance of downstream tasks by incorporating and capturing the underlying semantic and structural information among different types of nodes to fuse the embedding for learner modeling.}
}
@article{RAUDMAE2023e00436,
title = {ROBOTONT – Open-source and ROS-supported omnidirectional mobile robot for education and research},
journal = {HardwareX},
volume = {14},
pages = {e00436},
year = {2023},
issn = {2468-0672},
doi = {https://doi.org/10.1016/j.ohx.2023.e00436},
url = {https://www.sciencedirect.com/science/article/pii/S2468067223000433},
author = {Renno Raudmäe and Sandra Schumann and Veiko Vunder and Maarika Oidekivi and Madis Kaspar Nigol and Robert Valner and Houman Masnavi and Arun Kumar Singh and Alvo Aabloo and Karl Kruusamäe},
keywords = {Open-source, Educational robotics, ROS (Robot Operating System), Holonomic, Mobile robot, Software, Hardware, Electronics, Professional education, University teaching},
abstract = {In order to achieve visionary concepts such as Society 5.0 and Industry 5.0, there is a growing need for people who are able to create innovative robotic technologies. Training students to become such skilled professionals requires transitioning from often toy-like educational platforms with significant hardware limitations to costly research robots with full ROS (Robot Operating System) support. To aid in this transition, we propose Robotont – an open-source omnidirectional mobile robot platform with both physical hardware and a digital twin. Robotont enables robotics education with professional tools as well as provides researchers with a capable mobility platform for validating and demonstrating scientific results. Robotont has successfully been used for university teaching, professional education, and online courses about ROS and robotics.}
}
@article{ELMAZ2023108310,
title = {Reinforcement learning-based approach for optimizing solvent-switch processes},
journal = {Computers & Chemical Engineering},
volume = {176},
pages = {108310},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108310},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423001801},
author = {Furkan Elmaz and Ulderico {Di Caprio} and Min Wu and Yentl Wouters and Geert {Van Der Vorst} and Niels Vandervoort and Ali Anwar and M. Enis Leblebici and Peter Hellinckx and Siegfried Mercelis},
keywords = {Reinforcement learning, Process control, Solvent-switch optimization, Separation process},
abstract = {In chemical and pharmaceutical industries, process control optimization is a crucial step to improve economical efficiency and the environmental impact. The current state-of-practice heavily relies on expert knowledge and extensive lab experiments. This not only increases the development time but also limits the discovery of new strategies. In this study, we propose Reinforcement Learning-based optimization approach for solvent-switch processes. We utilize a digital twin as the environment for a process designed to switch the THF to 1-propanol. A reward function is created for minimizing the process time and constraints are implemented using logarithmic barrier functions. A PPO agent is trained on the environment. The agent proposed a novel strategy that combines two conventionally separate phases, evaporation and constant volume distillation. This strategy resulted in an overall cost decrease of 24.9% compared to the baseline strategy. Moreover, results were verified experimentally on a pilot plant of Johnson & Johnson (J&J).}
}
@article{KISHIMOTO2023233411,
title = {Conditional generative adversarial network for generation of three-dimensional porous structure of solid oxide fuel cell anodes with controlled volume fractions},
journal = {Journal of Power Sources},
volume = {580},
pages = {233411},
year = {2023},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2023.233411},
url = {https://www.sciencedirect.com/science/article/pii/S0378775323007875},
author = {Masashi Kishimoto and Yodai Matsui and Hiroshi Iwai},
keywords = {Solid oxide fuel cell, Synthetic structure, Machine learning, Generative adversarial network, Volume fraction, Digital twin},
abstract = {A structure generation model based on a generative adversarial network (GAN) is developed to synthesize artificial porous microstructures of solid oxide fuel cell (SOFC) anodes. Different from the conventional framework of GANs, additional training is performed for the generator to control statistical parameters, namely, volume fractions, of the generated structures. The developed model is validated by comparing the synthesized structures with the real electrode microstructures obtained by three-dimensional microscopy analysis. Microstructural parameters, such as volume fraction, specific surface area, and triple-phase boundary density, are used for the comparison in addition to the visual observation. The effect of the input vector size for the generator and the definition of the loss on the ability to generate realistic structures and control the volume fractions of the structures is investigated. The developed model successfully generates realistic anode microstructures with accurately controlled volume fractions, even for compositions not included in the training datasets. It is also found that the balance between the losses influences the accuracy of the volume fraction control and diversity of the generated structures. The GAN model developed is expected to be helpful in constructing a digital twin of electrode fabrication and evaluation processes.}
}
@article{ZHAO202330,
title = {Battery prognostics and health management for electric vehicles under industry 4.0},
journal = {Journal of Energy Chemistry},
volume = {84},
pages = {30-33},
year = {2023},
issn = {2095-4956},
doi = {https://doi.org/10.1016/j.jechem.2023.04.042},
url = {https://www.sciencedirect.com/science/article/pii/S2095495623002711},
author = {Jingyuan Zhao and Andrew F. Burke},
keywords = {Lithium-ion battery, Prognostics and health management, Machine learning, Cloud, Artificial intelligence, Digital twins, Lifelong learning}
}
@article{SOLIMAN2023106318,
title = {AI-based UAV navigation framework with digital twin technology for mobile target visitation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106318},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106318},
url = {https://www.sciencedirect.com/science/article/pii/S095219762300502X},
author = {Abdulrahman Soliman and Abdulla Al-Ali and Amr Mohamed and Hend Gedawy and Daniel Izham and Mohamad Bahri and Aiman Erbad and Mohsen Guizani},
keywords = {Deep Reinforcement Learning, UAVs, Target visitation, Energy minimization, Digital twin, Testbed development},
abstract = {Unmanned Air Vehicles (UAVs), i.e. drones, have become a key enabler technology of many reconnaissance applications in different fields, such as military, maritime, and transportation. UAVs offer several benefits, such as affordability and flexibility in deployment. However, their limited flight time due to energy consumption is one of the key limitations. Therefore, it is crucial to ensure that UAVs can complete the mission while consuming the least energy possible. In this paper, we propose a novel framework for UAV smart navigation to minimize the time and energy of planning mobile targets visitation. We develop a Deep Reinforcement Learning (DRL) approach to allow the drone to learn the targets’ mobility pattern and build its least energy scanning strategy accordingly. We conduct an initial evaluation of the system and our proposed DRL model policy using simulation. Then, to overcome the time-consuming exploration phase of DRL, we develop a Digital Twin (DT) environment of 3D physics-based simulator, which can be used to train the DRL agent efficiently. We also developed a testbed based on hardware integration with the parrot ANAFI drone to verify the feasibility of the proposed methodology. Our findings confirm that the DRL-based agent can achieve performance close to that of a benchmark policy. Moreover, the testbed experiment validates the practicality of utilizing the DT environment for DRL exploration.}
}
@article{ZHAO2023111863,
title = {A novel deep learning based forecasting model for carbon emissions trading: A comparative analysis of regional markets},
journal = {Solar Energy},
volume = {262},
pages = {111863},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.111863},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23004966},
author = {Yongchao Zhao and Lipeng Liu and Anqi Wang and Mengkai Liu},
keywords = {Deep learning, CNN-BFA, Carbon emission trading, Bacteria foraging algorithm, Forecasting},
abstract = {This paper proposes a deep learning-based forecasting system for carbon emissions trading in regional markets. The system utilizes a hybrid convolutional neural network (CNN) deep learning algorithm to predict future carbon emission levels and facilitate the trading of carbon emission allowances. The system is compared to traditional methods of forecasting in order to assess the accuracy and performance of the system. In order to enhance the CNN performance, a new optimization algorithm based on bacteria foraging algorithm (BFA) is proposed which uses a modification to make a global search. By leveraging digital twins in the markets, a comparison is conducted using data from three regional carbon markets: the European Union Emissions Trading System, the Regional Greenhouse Gas Initiative, and the China Carbon Market. Results show that the proposed BFA-CNN based deep learning-based system outperforms traditional forecasting methods in terms of accuracy and provides more reliable estimates of future carbon emissions. The proposed system is a novel approach to carbon emissions trading and has the potential to improve the efficiency of regional carbon markets.}
}
@article{LIU2023128992,
title = {Digital twin of the atmospheric turbulence channel based on self-supervised deep learning algorithm},
journal = {Physics Letters A},
volume = {481},
pages = {128992},
year = {2023},
issn = {0375-9601},
doi = {https://doi.org/10.1016/j.physleta.2023.128992},
url = {https://www.sciencedirect.com/science/article/pii/S0375960123003729},
author = {Ying Liu and HuiCun Yu and Jie Tang and YueXiang Cao and JiaHao Li and ZhiFeng Deng and Dan Wu and HuaZhi Lun and Lei Shi},
keywords = {Digital twin, Atmospheric turbulence, Deep learning, Self-supervised learning},
abstract = {High dimensional quantum entanglement based on orbital angular momentum (OAM) can provide infinite freedom theoretically, providing a significant improvement on the capacity of the quantum communication. However, the vortex beam that carries OAM signal can be easily distorted by atmospheric turbulence and can degrade the performance of the system. Consequently, for the operation, administration and maintenance of quantum system, an accurate digital twin model of the turbulent channel is necessary. Digital twin model is a mathematical model which can reflect the influence of atmospheric channel on quantum system by theoretical analysis. Nevertheless, it is challenging to achieve for the complex mechanism of atmospheric turbulence. To address this problem, deep learning (DL) techniques have been studied recently. Whereas, for the training of DL, a massive number of labeled samples are needed, i.e., the actual free-space channel, which are hard to be obtained in practical systems. The pool generalization also hinders the use of these DL-based algorithms in practice. To overcome the above challenges, we propose a self-supervised DL algorithm, which does not need any labeled samples in advance, meaning the training of the algorithm can be restarted any time once the environment changes. Compared with previous studies, the proposed algorithm can better suite as the digital twin of the turbulent channel. To verify the performance of the proposed algorithm, we perform extensive verification, whose results demonstrate the superior performance of the proposed method.}
}
@article{WANG2023168,
title = {The integration of digital twin and serious game framework for new normal virtual urban exploration and social interaction},
journal = {Journal of Urban Management},
volume = {12},
number = {2},
pages = {168-181},
year = {2023},
issn = {2226-5856},
doi = {https://doi.org/10.1016/j.jum.2023.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2226585623000225},
author = {Sheng-ming Wang and Lan Hong Vu},
keywords = {COVID-19, New normal, Serious game, Digital twin, Virtual urban exploration and social interaction, Urban planning},
abstract = {COVID-19 has disconnected humanity, reduced social interaction in physical urban areas, and led to the new normal, which describes the anticipated changes in human life and professionals due to the impact of the pandemic. In addition, as part of digital transformation, the post-pandemic New Normal includes accelerating digital solutions and new standards for virtual urban exploration and planning. This study applies the concept of digital twin and serious game design and uses Minecraft, a game-based platform that inspires creative, inclusive learning through play, for virtual urban exploration and the development of social interaction among participants. Dadaocheng, a historical area of Taipei city, is then studied as a case study with the Geoboxers application to develop the prototype for a co-creation experiment in Minecraft. The prototype development and the results of the experiment are then used in the Analytic Hierarchical Process (AHP) method to evaluate a set of influential criteria proposed in this study through pairwise comparisons by expert panelists. The results of the AHP analysis reveal users' simultaneous preferences for urban planning and social interaction with urban characteristics (22.14%), urban exploration (12.29%), and 3D models (11.97%). Subsequently, the research results showed a need to promote the integration of digital twins and serious game applications as digital tools for urban exploration and social interaction, increasing post-pandemic virtual urban planning and applying new urban design techniques. This study also contributes to the acceleration of digital transformation in urban planning and management.}
}
@article{TIAN2023109325,
title = {Digital twins of multiple energy networks based on real-time simulation using holomorphic embedding method, Part II: Data-driven simulation},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {153},
pages = {109325},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.109325},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523003824},
author = {Hang Tian and Haoran Zhao and Haoran Li and Xiaoli Huang and Xiaoyi Qian and Xu Huang},
keywords = {Digital twins, Holomorphic embedding, Multiple energy networks, Real-time simulation, Data-driven simulation},
abstract = {Digital twins can act as a transformative role in improving the operational performance of multiple energy networks (MEN) by examining the impact of implementing newer technologies, extra equipment, control strategies, etc. The objective of this series of papers is to present digital twins of MEN that can be simulated in real-time using the holomorphic embedding method. While Part I concentrated on mechanism-driven modeling of the holomorphic embedding-based model (HEM), this paper (Part II) focuses on data-driven simulation to ensure the twin is synchronized with actual physical objects. A parametric synchronization method (PSM) is proposed, which assists HEM in closely matching the actual dynamic behavior with time-varying characteristics. A machine learning surrogate model (MLSM) is proposed to accelerate the search of HEM’s convergence radius, which is critical to maintaining the twin’s real-time computational performance. Finally, the finalized digital twins are tested on the OPAL-RT simulation platform equipped with a real-time simulator. In a medium-sized MEN test case with a minor time step of 0.01s, the digital twins can be validated with a faster than real-time performance even without the assistance of parallel computing.}
}
@article{KAKLIS2023100178,
title = {Enabling digital twins in the maritime sector through the lens of AI and industry 4.0},
journal = {International Journal of Information Management Data Insights},
volume = {3},
number = {2},
pages = {100178},
year = {2023},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2023.100178},
url = {https://www.sciencedirect.com/science/article/pii/S2667096823000253},
author = {Dimitrios Kaklis and Iraklis Varlamis and George Giannakopoulos and Takis J. Varelas and Constantine D. Spyropoulos},
keywords = {Fuel oil consumption estimation, Digital twin, Splines, Quadratic estimators, Delaunay triangulation, Time-series forecasting, Unsupervised clustering, Ensemble learning, Deep learning, Least squares optimization},
abstract = {Sustainability and environmental compliance in ship operations is a prominent research topic as the waterborne sector is obliged to adopt ”green” mitigation strategies towards a low emissions operational blueprint. Fuel-Oil-Consumption (FOC) estimation, constitutes one of the key components in maritime transport information systems for efficiency and environmental compliance. This paper deals with FOC estimation in a more novel way than methods proposed in literature, by utilizing a reduced-sized feature set, which allows predicting vessel’s Main-Engine rotational speed (RPM). Furthermore, this work aims to place the deployment of such models in the broader context of a cutting-edge information system, to improve efficiency and regulatory adherence. Specifically, we integrate B-Splines in the context of two Deep Learning architectures and compare their performance against state-of-the-art regression techniques. Finally, we estimate FOC by combining velocity measurements and the predicted RPM with vessel-specific characteristics and illustrate the performance of our estimators against actual FOC data.}
}
@article{SUHAIL2023103961,
title = {ENIGMA: An explainable digital twin security solution for cyber–physical systems},
journal = {Computers in Industry},
volume = {151},
pages = {103961},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103961},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001112},
author = {Sabah Suhail and Mubashar Iqbal and Rasheed Hussain and Raja Jurdak},
keywords = {Cyber-physical system (CPS), Cybersecurity awareness, Digital twins (DTs), eXplainable AI (XAI), Gamification, Industry 5.0},
abstract = {Digital Twins (DTs), being the virtual replicas of their physical counterparts, share valuable knowledge of the underlying physical processes and act as data acquisition and dissemination sources to Cyber–Physical System (CPS). Moreover, without obstructing the ongoing operations, DTs also provide an assessment platform for evaluating the operational behavior and security of the CPS. Therefore, they become a potential source of data breaches and a broad attack surface for attackers to launch covert attacks. To detect and mitigate security loopholes in DTs, one of the potential solutions is to leverage a gamification approach that can assess the security level of DTs while providing security analysts with a controlled and supportive virtual training environment. Artificial Intelligence/Machine Learning (AI/ML)-based approaches can complement the idea of security orchestration and automation in the gamification approach. However, AI/ML-based DTs security solutions are generally constrained by the lack of transparency of AI operations, which results in less confidence in the decisions made by the AI models. To address the explainable security challenges of DTs, this article proposes a gamification approach called sEcuriNg dIgital twins through GaMification Approach (ENIGMA). While leveraging DTs as an offensive security platform, ENIGMA provides gaming scenarios to assess DTs’ security and train security analysts. The game players within ENIGMA are humans (the attacker team) and AI agents (the defender team). Furthermore, ENIGMA is supported by an eXplainable AI (XAI)-based DT security assessment model that explains the decisions made based on the SHAP values by the AI model on attack vectors for the defender team, i.e., the AI agent. The SHAP values illustrate the contribution of different features towards predicting the outcome of attack vectors. This explanation can help security analysts to take security measures based on reasoned and trustworthy decisions. Finally, experimental validation has been carried out to demonstrate the viability of ENIGMA.}
}
@article{NASERI2023113280,
title = {Digital twin of electric vehicle battery systems: Comprehensive review of the use cases, requirements, and platforms},
journal = {Renewable and Sustainable Energy Reviews},
volume = {179},
pages = {113280},
year = {2023},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2023.113280},
url = {https://www.sciencedirect.com/science/article/pii/S1364032123001363},
author = {F. Naseri and S. Gil and C. Barbu and E. Cetkin and G. Yarimca and A.C. Jensen and P.G. Larsen and C. Gomes},
keywords = {Artificial intelligence (AI), Battery management system (BMS), Battery passport, Battery recycling, Digital twin (DT), Electric vehicle (EV), Fault diagnosis, Internet-of-things (IoT), Machine learning (ML), Predictive maintenance, Remaining useful life (RUL), Second-life, Software architecture},
abstract = {Transportation electrification has been fueled by recent advancements in the technology and manufacturing of battery systems, but the industry yet is facing serious challenges that could be addressed using cutting-edge digital technologies. One such novel technology is based on the digital twining of battery systems. Digital twins (DTs) of batteries utilize advanced multi-layer models, artificial intelligence, advanced sensing units, Internet-of-Things technologies, and cloud computing techniques to provide a virtual live representation of the real battery system (the physical twin) to improve the performance, safety, and cost-effectiveness. Furthermore, they orchestrate the operation of the entire battery value chain offering great advantages, such as improving the economy of manufacturing, re-purposing, and recycling processes. In this context, various studies have been carried out discussing the DT applications and use cases from cloud-enabled battery management systems to the digitalization of battery testing. This work provides a comprehensive review of different possible use cases, key enabling technologies, and requirements for battery DTs. The review inclusively discusses the use cases, development/integration platforms, as well as hardware and software requirements for implementation of the battery DTs, including electrical topics related to the modeling and algorithmic approaches, software architectures, and digital platforms for DT development and integration. The existing challenges are identified and circumstances that will create enough value to justify these challenges, such as the added costs, are discussed.}
}
@article{ZAPAROLICUNHA2023110535,
title = {A review of machine learning methods applied to structural dynamics and vibroacoustic},
journal = {Mechanical Systems and Signal Processing},
volume = {200},
pages = {110535},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110535},
url = {https://www.sciencedirect.com/science/article/pii/S0888327023004430},
author = {Barbara {Zaparoli Cunha} and Christophe Droz and Abdel-Malek Zine and Stéphane Foulard and Mohamed Ichchou},
keywords = {Machine learning, Structural health monitoring, Surrogate model, Active vibration control, Active noise control, Digital twin, Physics-guided machine learning},
abstract = {The use of Machine Learning (ML) has rapidly spread across several fields of applied sciences, having encountered many applications in Structural Dynamics and Vibroacoustic (SD&V). An advantage of ML algorithms compared to traditional techniques is that physical phenomena can be modeled using only sampled data from either measurements or simulations. This is particularly important in SD&V when the model of the studied phenomenon is either unknown or computationally expensive to simulate. This paper presents a survey on the application of ML algorithms in three classical problems of SD&V: structural health monitoring, active control of noise and vibration, and vibroacoustic product design. In structural health monitoring, ML is employed to extract damage-sensitive features from sampled data and to detect, localize, assess, and forecast failures in the structure. In active control of noise and vibration, ML techniques are used in the identification of state-space models of the controlled system, dimensionality reduction of existing models, and design of controllers. In vibroacoustic product design, ML algorithms can create surrogates that are faster to evaluate than physics-based models. The methodologies considered in this work are analyzed in terms of their strength and limitations for each of the three considered SD&V problems. Moreover, the paper considers the role of digital twins and physics-guided ML to overcome current challenges and lay the foundations for future research in the field.}
}
@article{PHUA2023382,
title = {Smart recoating: A digital twin framework for optimisation and control of powder spreading in metal additive manufacturing},
journal = {Journal of Manufacturing Processes},
volume = {99},
pages = {382-391},
year = {2023},
issn = {1526-6125},
doi = {https://doi.org/10.1016/j.jmapro.2023.04.062},
url = {https://www.sciencedirect.com/science/article/pii/S1526612523004152},
author = {Arden Phua and Peter S. Cook and Chris H.J. Davies and Gary W. Delaney},
keywords = {Digital twin, Additive manufacturing, Powder spreading},
abstract = {We present a new framework for learning novel operational strategies and dynamically controlling the layering process in metal additive manufacturing. Metal additive manufacturing technologies such as powder bed fusion (PBF) are generally constrained by a fixed action powder spreading process. At every layer, the print platform is lowered by a fixed amount, and the same recoating action is performed. Ideally this would lead to consistent layering and identical properties each time, but frequently process variability disrupts this procedure, leading to inconsistent layers. This can be mitigated by intelligently controlling the powder spreading process, which we achieve via a shift to digital methodologies that can reveal new process strategies and dynamically update the printer commands. We employ Bayesian optimisation as a method to build and train surrogate models for real-time control. We then demonstrate the utility of this Smart Recoating approach within an integrated simulation framework driven by realistic Discrete Element Method powder spreading simulations. Our results inform new strategies for controlling the recoater and print stage displacements, and demonstrate the potential of a digital twin control system to mitigate process variation and achieve consistent print quality in each layer.}
}
@article{KABLAN2023105090,
title = {Evaluation of stacked ensemble model performance to predict clinical outcomes: A COVID-19 study},
journal = {International Journal of Medical Informatics},
volume = {175},
pages = {105090},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105090},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623001089},
author = {Rianne Kablan and Hunter A. Miller and Sally Suliman and Hermann B. Frieboes},
keywords = {Machine learning, Stacked ensemble, Stacked generalization, Meta learners, Clinical data analysis, COVID-19},
abstract = {Background
The application of machine learning (ML) to analyze clinical data with the goal to predict patient outcomes has garnered increasing attention. Ensemble learning has been used in conjunction with ML to improve predictive performance. Although stacked generalization (stacking), a type of heterogeneous ensemble of ML models, has emerged in clinical data analysis, it remains unclear how to define the best model combinations for strong predictive performance. This study develops a methodology to evaluate the performance of “base” learner models and their optimized combination using “meta” learner models in stacked ensembles to accurately assess performance in the context of clinical outcomes.
Methods
De-identified COVID-19 data was obtained from the University of Louisville Hospital, where a retrospective chart review was performed from March 2020 to November 2021. Three differently-sized subsets using features from the overall dataset were chosen to train and evaluate ensemble classification performance. The number of base learners chosen from several algorithm families coupled with a complementary meta learner was varied from a minimum of 2 to a maximum of 8. Predictive performance of these combinations was evaluated in terms of mortality and severe cardiac event outcomes using area-under-the-receiver-operating-characteristic (AUROC), F1, balanced accuracy, and kappa.
Results
The results highlight the potential to accurately predict clinical outcomes, such as severe cardiac events with COVID-19, from routinely acquired in-hospital patient data. Meta learners Generalized Linear Model (GLM), Multi-Layer Perceptron (MLP), and Partial Least Squares (PLS) had the highest AUROC for both outcomes, while K-Nearest Neighbors (KNN) had the lowest. Performance trended lower in the training set as the number of features increased, and exhibited less variance in both training and validation across all feature subsets as the number of base learners increased.
Conclusion
This study offers a methodology to robustly evaluate ensemble ML performance when analyzing clinical data.}
}
@article{MA2023110490,
title = {Digital twin-assisted enhanced meta-transfer learning for rolling bearing fault diagnosis},
journal = {Mechanical Systems and Signal Processing},
volume = {200},
pages = {110490},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110490},
url = {https://www.sciencedirect.com/science/article/pii/S0888327023003989},
author = {Leiming Ma and Bin Jiang and Lingfei Xiao and Ningyun Lu},
keywords = {Digital twin, Meta-transfer learning, Few-shot learning, Finite element model updating, Bearing fault diagnosis},
abstract = {Fault diagnosis of bearing under variable working conditions is widely required in practice, and the combination of working conditions and fault fluctuations increases the complexity of addressing its related problems. By developing a virtual simulation model, a digital twin (DT) can obtain the same or even more information than the physical object at a lower cost. Furthermore, it has great potential in the application of bearing fault diagnosis. In this paper, the DT model of the bearing test rig is robustly established, and the fault diagnosis bearing between the simulation and physical object is realized using the proposed enhanced meta-transfer learning (EMTL). First, the DT model is established through parameter identification and modal testing, and the modeling accuracy of DT model reaching 95.685%. The bearing simulation and experimental data are then collected under the same conditions using the DT model and bearing test rig, and the simulation data with little deviation from the experimental data is obtained. Finally, an attention mechanism and domain adaptation are introduced into the EMTL, with the average accuracy of fault diagnosis of bearing reaching 95.18% with few-label target domain data. The proposed strategy is both theoretically significant and practically useful. The experiment results demonstrate that our method outperforms a series of state-of-the-art methods on the bearing fault diagnosis across various limited data conditions. The proposed strategy effectively solves the few-shot problem, which is both theoretically significant and practically useful.}
}
@article{LI2023119202,
title = {Dynamic data-free knowledge distillation by easy-to-hard learning strategy},
journal = {Information Sciences},
volume = {642},
pages = {119202},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119202},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523007879},
author = {Jingru Li and Sheng Zhou and Liangcheng Li and Haishuai Wang and Jiajun Bu and Zhi Yu},
keywords = {Data-free knowledge distillation, Curriculum learning, Knowledge distillation, Self-paced learning},
abstract = {Data-free knowledge distillation (DFKD) is a widely-used strategy for Knowledge Distillation (KD) whose training data is not available. It trains a lightweight student model with the aid of a large pretrained teacher model without any access to training data. However, existing DFKD methods suffer from inadequate and unstable training process, as they do not adjust the generation target dynamically based on the status of the student model during learning. To address this limitation, we propose a novel DFKD method called CuDFKD. It teaches students by a dynamic strategy that gradually generates easy-to-hard pseudo samples, mirroring how humans learn. Besides, CuDFKD adapts the generation target dynamically according to the status of student model. Moreover, we provide a theoretical analysis of the majorization minimization (MM) algorithm and explain the convergence of CuDFKD. To measure the robustness and fidelity of DFKD methods, we propose two more metrics, and experiments shows CuDFKD has comparable performance to state-of-the-art (SOTA) DFKD methods on all datasets. Experiments also present that our CuDFKD has the fastest convergence and best robustness over other SOTA DFKD methods.}
}
@article{LI2023108283,
title = {Review on intelligent pipeline technologies: A life cycle perspective},
journal = {Computers & Chemical Engineering},
volume = {175},
pages = {108283},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108283},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423001539},
author = {Zhuochao Li and Yongtu Liang and Youyi Liang and Qi Liao and Bohong Wang and Liqiao Huang and Jianqin Zheng and Haoran Zhang},
keywords = {Oil and gas pipeline, Intelligent pipeline, Key technology, Life cycle},
abstract = {The pipeline is one of the important carriers in the petroleum industry, but facing with the development bottlenecks of low efficiency, high cost and high decision risk. With machine learning and advanced sensor, new technological breakthroughs are being opening up for intelligent pipeline systems to benefit. Nowadays, the intelligent transformation of pipeline is still in the initial stage. To accelerate this process, this paper elaborates the research on intelligent pipeline systems from the perspective of the full life cycle of pipeline. The life cycle is divided into six stages, including exploration, feasibility study, design, construction, operation and scrap. In each stage, the related theoretical research and key technologies have been systematically reviewed. It is observed that full digital construction and operation is an inevitable trend. The Internet of Things, big data, and multi-objective optimization are critical technologies. Finally, some future directions are put forward to prompt a pipeline into intelligence.}
}
@article{ZHANG2023102091,
title = {Efficient visual fault detection for freight train braking system via heterogeneous self distillation in the wild},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102091},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102091},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002197},
author = {Yang Zhang and Huilin Pan and Yang Zhou and Mingying Li and Guodong Sun},
keywords = {Fault detection, Freight train images, Knowledge distillation, Real-time, Light-weight},
abstract = {Efficient visual fault detection of freight trains is a critical part of ensuring the safe operation of railways under the restricted hardware environment. Although deep learning-based approaches have excelled in object detection, the efficiency of freight train fault detection is still insufficient to apply in real-world engineering. This paper proposes a heterogeneous self-distillation framework to ensure detection accuracy and speed while satisfying low resource requirements. The privileged information in the output feature knowledge can be transferred from the teacher to the student model through distillation to boost performance. We first adopt a lightweight backbone to extract features and generate a new heterogeneous knowledge neck. Such neck models positional information and long-range dependencies among channels through parallel encoding to optimize feature extraction capabilities. Then, we utilize the general distribution to obtain more credible and accurate bounding box estimates. Finally, we employ a novel loss function that makes the network easily concentrate on values near the label to improve learning efficiency. Experiments on four fault datasets reveal that our framework can achieve over 37 frames per second and maintain the highest accuracy of 98.88% in comparison with traditional distillation approaches. Moreover, compared to state-of-the-art methods, our framework demonstrates more competitive performance with lower memory usage and the smallest model size.}
}
@article{WANG2023119531,
title = {Digital twin-supported smart city: Status, challenges and future research directions},
journal = {Expert Systems with Applications},
volume = {217},
pages = {119531},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119531},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423000325},
author = {Hao Wang and Xiaowei Chen and Fu Jia and Xiaojuan Cheng},
keywords = {Digital twin, Smart city, Information management, Data management, Literature review},
abstract = {A city can be considered a carrier of multiple sources of data and information that are updated in real time and experiences continuous operation and development. Therefore, a system that can obtain and manage data/information gathered from different physical objects in a city in real time is needed. Digital twin (DT) technology is a virtual representation of an object or system that spans its lifecycle; it is updated from real-time data and uses simulation, machine learning and reasoning to help with decision-making. However, how to apply these features of the DT to better manage smart cities (SCs) has not yet been systematically summarized and analysed. In this study, 202 papers on DT-supported SCs are reviewed, based on which the drivers and challenges of applying DT-supported SCs and the solutions for the challenges were identified. In addition, this study explored the possible outcomes of applying DT-supported technologies in SCs. This study also contributes to the DT-supported SCs for city management research and practice.}
}
@article{RIZWAN2023100698,
title = {Intelligent digital twin for federated learning in AIoT networks},
journal = {Internet of Things},
volume = {22},
pages = {100698},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100698},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523000215},
author = {Atif Rizwan and Rashid Ahmad and Anam Nawaz Khan and Rongxu Xu and Do Hyeun Kim},
keywords = {Digital twin, Federated learning, AIoT networks, OCF IoTivity},
abstract = {Federated Learning (FL) promises to solve the data privacy problem by training the local model on each node and sharing the model parameters instead of the data itself. Next, the FL server applies model aggregation techniques to aggregate the received models and broadcast the resulting model to the connected clients. This study proposes a Digital Twin-based Federated Learning (DT-FL) framework to virtually monitor and controls the remotely deployed physical clients and their training process. The connection-oriented protocol, Open Connectivity Foundation (OCF) Iotivity, connects the FL clients with the FL server to ensure packet delivery. OCF Iotivity sends/receives the models’ weights to/from the server, and Hyper Text Transfer Protocol (HTTP) is used to monitor clients’ local training. After receiving partially trained models from clients, the server performs the optimal model selection using the normal distribution method by considering the performance of the model. Finally, the best-selected models are aggregated, and the final model is broadcasted to the clients. The framework utilizes Raspberrypi 4 devices as clients with limited computational capabilities, due to which the experiments are conducted with structured energy consumption data. The dataset comprises of 8 multistory residential buildings located in different geographical locations of the Republic of Korea. Each residential building is treated as an FL client and registered on DT using the IP address and port number. The DT-FL framework can be used with classification and regression datasets, and the model architecture for that data can be designed on the DT platform. The experiments are conducted with the partial and full participation of clients. The results show the minimum delay time in physical and virtual object synchronization and better performance and generalization of the global model for each client. The source code of the proposed DT-FL framework is available on GitHub.}
}
@article{FERNANDEZLEON2023105945,
title = {A deep encoder-decoder for surrogate modelling of liquid moulding of composites},
journal = {Engineering Applications of Artificial Intelligence},
volume = {120},
pages = {105945},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.105945},
url = {https://www.sciencedirect.com/science/article/pii/S095219762300129X},
author = {J. Fernández-León and K. Keramati and C. Miguel and C. González and L. Baumela},
keywords = {Composite materials, Liquid moulding, Surrogate models, Deep learning},
abstract = {The paper proposes a surrogate model for liquid moulding of structural composites. A methodology is presented to simulate the dual-phase Darcy’s flow in a heterogeneous porous medium. The approach is an encoder–decoder that receives as input a matrix of permeabilities and produces two scalar fields that represent the pressure and front flow. This model is trained with synthetic data generated with a computer fluid dynamics simulator. In this context, the lack of robustness of models trained with the popular L2 and L1 losses is highlighted and several enhancements to these baseline approaches are introduced. First, the study provides a piece-wise power-logarithmic loss that improves training in the presence of the bimodal distribution of error residuals produced by the dual-phase flow predictions. A non-uniform sampling strategy for the selection of time training snapshots is also included, which contributes to improve the prediction accuracy. The estimation of the front flow field is further refined with a multi-task training strategy. The introduction of these improvements in the baseline models reduce the relative error of the pressure and front flow fields by more than 50%, performing these simulations in a record time of 50 ms. The surrogate model is further evaluated as a digital twin to predict – in a real experiment – the location and spatial extent of race-tracking channels and regions with dissimilar degrees of permeability.}
}
@article{DESANTANA2023103029,
title = {Predicting the need for XAI from high-granularity interaction data},
journal = {International Journal of Human-Computer Studies},
volume = {175},
pages = {103029},
year = {2023},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2023.103029},
url = {https://www.sciencedirect.com/science/article/pii/S1071581923000356},
author = {Vagner Figueredo {de Santana} and Ana Fucs and Vinícius Segura and Daniel Brugnaro {de Moraes} and Renato Cerqueira},
keywords = {Explainability prediction, Fine-grained interaction, Micro behavior, User behavior analysis, Interaction log analysis, Interaction prediction, Node2vec},
abstract = {Recent advances in Artificial Intelligence (AI) and Machine Learning (ML) brought light on the need for explainability in multiple domains (e.g., healthcare, finance, justice, and recruiting). Explainability or Explainable AI (XAI) can be defined as everything that makes AI more understandable to human beings. However, XAI features may vary according to the AI algorithm used. Beyond XAI features, different AI algorithms vary in terms of speed, performance, and costs associated with training/running models. Knowing when to choose the right algorithm for the task at hand, therefore, is fundamental in multiple AI systems, for instance, AutoML and AutoAI. In this paper, we propose a method to analyze patterns of high-granularity user interface (UI) events (i.e., mouse, keyboard, and additional custom events triggered on the millisecond scale) to predict when users will interact with UI elements that provide explainability for the AI in place. In this context, this paper presents: (1) a user study involving 37 participants (7 in the pilot phase and 30 in the main experiment phase) in which people performed a task of reporting a bug using a text form associated with an AI data quality meter and its XAI UI element and (2) an approach to model micro behavior using node2vec to predict when the interaction with XAI UI element will occur. The proposed approach uses a rich dataset (approximately 129k events) and combines node2vec and a Logistic Regression classifier. Results obtained show we have obtained an event-by-event prediction of the interaction with XAI with an average F-score of 0.90 (σ=0.06). From the presented results, one expects to support researchers in the realm of UI personalization to consider high-granularity interaction data when predicting the need for XAI while users are interacting with AI model outputs.}
}
@article{MU2023174,
title = {Toward a smart wire arc additive manufacturing system: A review on current developments and a framework of digital twin},
journal = {Journal of Manufacturing Systems},
volume = {67},
pages = {174-189},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000237},
author = {Haochen Mu and Fengyang He and Lei Yuan and Philip Commins and Hongmin Wang and Zengxi Pan},
keywords = {WAAM, Additive manufacturing, Digital twin, Process planning, Monitoring, Control, Simulation},
abstract = {In recent years, Wire Arc Additive Manufacturing (WAAM) has attracted increasing scientific attention. With the rise of Industry 4.0 and smart manufacturing, Digital Twin (DT) has become an emerging technology that is finding increased acceptance in Additive Manufacturing (AM) processes. This paper aims to provide a systematic review of current developments of DT in AM processes and then derive a suitable DT for the WAAM system. Firstly, DT developments in AM processes are introduced from supervisory, control, and predictive aspects. This provides a reference and inspiration for designing process DTs by reviewing their structures, algorithms, and methodologies. Secondly, the current research on process planning, monitoring, modeling, online control, and simulation in WAAM is reviewed. Particular attention is given to intelligent algorithms, such as machine learning. Thirdly, the challenges to building a WAAM-DT are introduced step-by-step. Finally, the paper concludes by proposing a framework of WAAM-DT as a hybrid and intelligent solution for monitoring, modeling, control, and simulation.}
}
@article{DUMITRACHE2023955,
title = {Collaborative Decisions in Knowledge Management for Intelligent Cyber-Enterprises},
journal = {Procedia Computer Science},
volume = {221},
pages = {955-962},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.074},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923008323},
author = {Ioan Dumitrache and Larisa Stefania Predescu and Simona Iuliana Caramihai and Mihnea Alexandru Moisescu},
keywords = {multi-agent systems, knowledge management, cognitive systems, collaborative decision},
abstract = {The faster and faster IT integration of various categories of activities - from production to administration, passing through medicine, transport, education, government, etc. - led to important changes in terms of obtaining, storing and using data. On the other hand, the increasingly rapid accumulation of societal challenges has exerted immense pressure on the relatively nascent discipline of knowledge management, which is expected to help address increasingly complex issues, primarily related to decision-making aspects that necessitate the selection and transformation of data into information and knowledge. This article focuses on establishing a framework for developing a collaborative, dynamic, heterogeneous decision-making system that includes both human and cybernetic agents as well as digital twins. Such a system creates and sustains a knowledge management flow that is essential for making informed decisions, achieved by selectively collecting and appropriately transforming data into information.}
}
@article{HOSAMO2023112992,
title = {Improving building occupant comfort through a digital twin approach: A Bayesian network model and predictive maintenance method},
journal = {Energy and Buildings},
volume = {288},
pages = {112992},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2023.112992},
url = {https://www.sciencedirect.com/science/article/pii/S0378778823002220},
author = {Haidar Hosamo Hosamo and Henrik Kofoed Nielsen and Dimitrios Kraniotis and Paul Ragnar Svennevig and Kjeld Svidt},
keywords = {Digital Twin, Building information modeling (BIM), Occupants comfort, Predictive maintenance, Facility management, Decision-making},
abstract = {This study introduces a Bayesian network model to evaluate the comfort levels of occupants of two non-residential Norwegian buildings based on data collected from satisfaction surveys and building performance parameters. A Digital Twin approach is proposed to integrate building information modeling (BIM) with real-time sensor data, occupant feedback, and a probabilistic model of occupant comfort to detect and predict HVAC issues that may impact comfort. The study also uses 200000 points as historical data of various sensors to understand the previous building systems’ behavior. The study also presents new methods for using BIM as a visualization platform and for predictive maintenance to identify and address problems in the HVAC system. For predictive maintenance, nine machine learning algorithms were evaluated using metrics such as ROC, accuracy, F1-score, precision, and recall, where Extreme Gradient Boosting (XGB) was the best algorithm for prediction. XGB is on average 2.5% more accurate than Multi-Layer Perceptron (MLP), and up to 5% more accurate than the other models. Random Forest is around 96% faster than XGBoost while being relatively easier to implement. The paper introduces a novel method that utilizes several standards to determine the remaining useful life of HVAC, leading to a potential increase in its lifetime by at least 10% and resulting in significant cost savings. The result shows that the most important factors that affect occupant comfort are poor air quality, lack of natural light, and uncomfortable temperature. To address the challenge of applying these methods to a wide range of buildings, the study proposes a framework using ontology graphs to integrate data from different systems, including FM, CMMS, BMS, and BIM. This study’s results provide insight into the factors that influence occupant comfort, help to expedite identifying equipment malfunctions and point towards potential solutions, leading to more sustainable and energy-efficient buildings.}
}
@article{BRAGUEZ2023504,
title = {The possibilities of changes in learning experiences with Metaverse},
journal = {Procedia Computer Science},
volume = {219},
pages = {504-511},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.318},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923003277},
author = {Joana Braguez and Marta Braguez and Sílvia Moreira and Carla Filipe},
keywords = {Metaverse, virtual reality, augmented reality, virtual worlds, learning experiences},
abstract = {This review aims to define the Metaverse, present the roles of AR, MR and VR, and also the concepts of digital twins and lifelogging. The evolution of applications for Metaverse in various sectors, especially gaming, has created the possibility of using Metaverse for education. We present the vast field of these applications and educational projects. The challenges that educators face are discussed and the potential and limitations of its educational applications are explained. It's suggested to embrace the Metaverse in classes but not in a full-time learning environment, instead, it should be used as a complement, when justified. Some of its limitations may be weaker social connections there are concerns for privacy and security. The big potential offered by Metaverse technologies is the immersive experience of content and social interactions.}
}
@article{SHI2023108172,
title = {Real-time plume tracking using transfer learning approach},
journal = {Computers & Chemical Engineering},
volume = {172},
pages = {108172},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108172},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423000418},
author = {Jihao Shi and Weikang Xie and Junjie Li and Xinqi Zhang and Xinyan Huang and Asif Sohail Usmani and Faisal Khan and Guoming Chen},
keywords = {Natural gas release, Flammable area prediction, Variable-fidelity modeling, Transfer learning, Deep learning, Digital twin for emergency management},
abstract = {Deep learning has been used to track the real-time flammable plume of natural gas. However, a large volume of high-fidelity data is required to train the deep learning model for sufficient accuracy in congested industrial environments, which can be computationally prohibitive. This study proposes a transfer learning-based variable-fidelity approach for real-time plume tracking. A Gaussian dispersion model was applied to efficiently generate a large volume of low-fidelity data, which is then used to pre-train the deep learning model. A limited number of high-fidelity simulations were conducted by solving the Navier-Stokes equation to fine-tune the pre-trained model. A case study demonstrated our proposed approach could reduce high-fidelity computations by 72% while ensuring prediction accuracy with R2=0.96 for released plume area estimation in congested chemical facilities. Optimal number of frozen layers, learning rate and the number of high-fidelity simulations required were determined to ensure adequate efficiency for this approach. This study provides an efficient alternative to improve the generalization of deep learning for real-time plume area estimation for large-scale congested chemical plants.}
}
@article{YUAN2023398,
title = {Digital Twin-Based economic assessment of solar energy in smart microgrids using reinforcement learning technique},
journal = {Solar Energy},
volume = {250},
pages = {398-408},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2022.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X22009021},
author = {Guanghui Yuan and Fei Xie},
keywords = {Smart microgrid, Reinforcement learning, Load scheduling, Demand response, Renewable energy},
abstract = {Utility companies recognize the importance and necessity of demand response (DR) programs for reducing the increased production costs associated with rising energy demand. The advent of smart information and communication systems has made DR programs on the basis of cost a viable option to control load in smart microgrids. Small domestic consumers are rapidly using stochastic renewable energy resources such as photovoltaic (PV). The study examines an integrated layout for residential load scheduling or load commitment problems (LCP) with renewable energy resources no matter what kind of tariff is applied. Uncertainty-based decision-making problems are effectively solved using reinforcement learning (RL). The paper proposes an RL-enabled solution to the LCP in smart microgrids. An innovative aspect of the study is the development of an integrated layout containing an implementable solution that takes into account user satisfaction, stochastic renewable power, and tariffs. In simulation tests, the suggested layout is tested for its effectiveness and flexibility. An analysis of the algorithm's efficiency using a household user with schedule-able and non-schedulable devices, together with a PV resource, has been presented.}
}
@article{SHI2023114244,
title = {Real-time natural gas explosion modeling of offshore platforms by using deep learning probability approach},
journal = {Ocean Engineering},
volume = {276},
pages = {114244},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114244},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823006285},
author = {Jihao Shi and He Zhang and Junjie Li and Weikang Xie and Wenhua Zhao and Asif Sohail Usmani and Guoming Chen},
keywords = {Natural gas explosion, Offshore platform, Deep learning probability model, Accident reconstruction, Uncertainty estimation},
abstract = {Natural gas explosion of offshore platform is prone to cause accidental disaster such as platform collapse and casualties etc. Real-time natural gas explosion consequence reconstruction is essential to support a quick accidental emergency response planning to prevent the accidental escalation to disaster. The widely-used CFD is computationally intensive and thereby has a significant delay. Machine/deep learning-based models offer a potential real-time alternative, which however are not able to quantify the uncertainty of spatial overpressure prediction. This study aims to propose a hybrid deep learning probability model to real-time predict spatial explosion overpressure of offshore platform by using sparsely-observed overpressures. In this hybrid model, Variational Bayesian inference is incorporated into deep learning backbone. Both natural gas explosion experimental and numerical modeling of offshore platform are conducted to construct the benchmark dataset. By using this benchmark dataset, sensitivity analysis of Monte Carlo sampling number N, drop probability p on model's performance is also conducted. The results demonstrated our model exhibits high accuracy with R2 = 0.955 and real-time capability with inference time of 2.9s. Compared to the state-of-the-art model, the additional uncertainty estimation improves the accuracy and robustness of spatial overpressure prediction, which contributes to the reliable explosion accidental emergency decision-making. Overall, this study provides a reliable alternative for constructing digital twin emergency management system to effectively manage natural gas explosion risk of offshore platforms.}
}
@article{LIU2023104480,
title = {Towards Human-centric Digital Twins: Leveraging Computer Vision and Graph Models to Predict Outdoor Comfort},
journal = {Sustainable Cities and Society},
volume = {93},
pages = {104480},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104480},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723000914},
author = {Pengyuan Liu and Tianhong Zhao and Junjie Luo and Binyu Lei and Mario Frei and Clayton Miller and Filip Biljecki},
keywords = {Spatial analysis, Walkability, Built environment, Graph neural network, Urban study},
abstract = {Conventional sidewalk studies focused on quantitative analysis of sidewalk walkability at a large scale which cannot capture the dynamic interactions between the environment and individual factors. Embracing the idea of Tech for Social Good, Urban Digital Twins seek AI-empowered approaches to bridge humans with digitally-mediated technologies to enhance their prediction ability. We employ GraphSAGE-LSTM, a geo-spatial artificial intelligence (GeoAI) framework on crowdsourced data and computer vision to predict human comfort on the sidewalks. Conceptualising the pedestrians and their interactions with surrounding built and unbuilt environments as human-centric dynamic graphs, our model captures such spatio-temporal variations given by the sequential movements of human walking, enabling the GraphSAGE-LSTM to be spatio-temporal-explicit. Our experiments suggest that the proposed model provides higher accuracy by more than 20% than a traditional machine learning model and two state-of-art deep learning frameworks, thus, enhancing the prediction power of Urban Digital Twin. The source code for the model is shared openly on GitHub.}
}
@article{LI2023127289,
title = {Uncertainty quantification and aerodynamic robust optimization of turbomachinery based on graph learning methods},
journal = {Energy},
volume = {273},
pages = {127289},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.127289},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223006837},
author = {Jinxing Li and Tianyuan Liu and Guangya Zhu and Yunzhu Li and Yonghui Xie},
keywords = {Uncertainty quantification, Aerodynamic robust optimization, Turbomachinery, Field prediction, Graph neural network},
abstract = {The actual operation of turbomachinery is inevitably affected by multi-source uncertainties. Such uncertainties are detrimental to the performance and reliability of energy systems. Based on graph learning methods, this work aims to provide a convenient and effective approach for aerodynamic robust optimization of turbomachinery. A radial inflow turbine is taken as the research target and Dual Graph Neural Network (DGNN) regression model is constructed for flow field prediction and performance discrimination. By comparing the accuracy and time consumption, the advantages of DGNN over classical surrogate models and computational fluid dynamics (CFD) are clarified. The proposed model is integrated into uncertainty quantification and aerodynamic robust optimization. The effect of multi-source uncertainties on performance is quantified. The stochastic response of flow fields is also obtained conveniently through DGNN. Robust optimization is performed for power and efficiency, respectively. The power robust optimization improves the power by 1.52% and reduces the standard deviation of power by 15.45%. The efficiency robust optimization achieves an efficiency improvement of 1.76% (increment) and an efficiency standard deviation reduction of 36.82%. The proposed approach is an efficient and competitive choice for uncertainty quantification and robust optimization. The present work contributes to constructing the digital twin of turbomachinery systems.}
}
@article{SULTANA202318586,
title = {Biohydrogen from food waste: Modeling and estimation by machine learning based super learner approach},
journal = {International Journal of Hydrogen Energy},
volume = {48},
number = {49},
pages = {18586-18600},
year = {2023},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2023.01.339},
url = {https://www.sciencedirect.com/science/article/pii/S0360319923006249},
author = {Nahid Sultana and S. M. Zakir Hossain and Sumayh S. Aljameel and M.E. Omran and S.A. Razzak and B. Haq and M.M. Hossain},
keywords = {Biohydrogen, Biomethane, Food waste, Bayesian algorithm, Support vector regression, Machine learning},
abstract = {This study demonstrated the application of a hybrid Bayesian algorithm (BA) and support vector regression (SVR) as a potential super-learner tool (BA-SVR) to predict biohydrogen production from food waste-originated feedstocks. The novelty of the present approach, as compared to the existing response surface methodology (RSM), includes (i) hybridization of BA with SVR for modeling of biohydrogen production and minimization of biomethane formation, (ii) performance evaluation and comparison of the developed BA-SVR models with the existing RSM models based on the several indicators such as coefficient of determination (R2), relative error (RE), mean absolute error (MAE), mean absolute percentage error (MAPE), and root mean square error (RMSE), (iii) analysis of the robustness of the model and (iv) testing generalization ability. The calculated values of these indicators suggested that the proposed super leaner models demonstrated better performance predicting the biohydrogen and biomethane (products) responses than those using the existing RSM models - as reported in Rafieenia et al. 2019 [45]. The estimated low errors for biohydrogen: MAE = 0.5919, RMSE = 0.592, MAPE = 11.1387; for biomethane: MAE = 0.2681, RMSE = 0.2688, MAPE = 0.3708, signifie the reliable model predictions. The BA-SVR model also provided high adj R2 (>0.99 for both biohydrogen and biomethane), indicating an excellent fitting of the model. Concerning the MAPE, the proposed BA-SVR models for both the biohydrogen and biomethane responses showed superior performances (as compared to the RSM models) with a performance enhancement of 64.16% and 98.81%, respectively.}
}
@article{SALVADOR2023107402,
title = {Fast and robust parameter estimation with uncertainty quantification for the cardiac function},
journal = {Computer Methods and Programs in Biomedicine},
volume = {231},
pages = {107402},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107402},
url = {https://www.sciencedirect.com/science/article/pii/S016926072300069X},
author = {Matteo Salvador and Francesco Regazzoni and Luca Dede’ and Alfio Quarteroni},
keywords = {Cardiac electromechanics, Machine Learning, Surrogate modeling, Parameter estimation, Uncertainty quantification},
abstract = {Background and objectives
Parameter estimation and uncertainty quantification are crucial in computational cardiology, as they enable the construction of digital twins that faithfully replicate the behavior of physical patients. Many model parameters regarding cardiac electromechanics and cardiovascular hemodynamics need to be robustly fitted by starting from a few, possibly non-invasive, noisy observations. Moreover, short execution times and a small amount of computational resources are required for the effective clinical translation.
Methods
In the framework of Bayesian statistics, we combine Maximum a Posteriori estimation and Hamiltonian Monte Carlo to find an approximation of model parameters and their posterior distributions. Fast simulations and minimal memory requirements are achieved by using an accurate and geometry-specific Artificial Neural Network surrogate model for the cardiac function, matrix–free methods, automatic differentiation and automatic vectorization. Furthermore, we account for the surrogate modeling error and measurement error.
Results
We perform three different in silico test cases, ranging from the ventricular function to the entire cardiocirculatory system, involving whole-heart mechanics, arterial and venous hemodynamics. By employing a single central processing unit on a standard laptop, we attain highly accurate estimations for all model parameters in short computational times. Furthermore, we obtain posterior distributions that contain the true values inside the 90% credibility regions.
Conclusions
Many model parameters regarding the entire cardiovascular system can be fastly and robustly identified with minimal hardware requirements. This can be achieved when a small amount of non-invasive data is available and when high levels of signal-to-noise ratio are present in the quantities of interest. With these features, our approach meets the requirements for clinical exploitation, while being compliant with Green Computing practices.}
}
@article{WANG202323,
title = {Digital-Twin-Enhanced Quality Prediction for the Composite Materials},
journal = {Engineering},
volume = {22},
pages = {23-33},
year = {2023},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S2095809923000036},
author = {Yucheng Wang and Fei Tao and Ying Zuo and Meng Zhang and Qinglin Qi},
keywords = {Digital twin, Quality prediction, Composites, Coupling models},
abstract = {Composite materials are widely used in many fields due to their excellent properties. Quality defects in composite materials can lead to lower quality components, creating potential risk of accidents. Experimental and simulation methods are commonly used to predict the quality of composite materials. However, it is difficult to predict the quality of composite materials accurately due to the uncertain curing environment and incomplete feature space. To address this problem, a digital twin (DT) visual model of a composite material is first constructed. Then, a static autoclave DT virtual model is coupled with a variable composite material DT virtual model to construct a model of the curing process. Features are added to the proposed model by generating simulated data to enhance the quality prediction. An extreme learning machine (ELM) for quality prediction is trained with the generated data. Finally, the effectiveness of the proposed method is verified through result analysis.}
}
@article{YANG2023102564,
title = {Automation of SME production with a Cobot system powered by learning-based vision},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {83},
pages = {102564},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102564},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000406},
author = {Xingyu Yang and Zhengxue Zhou and Jonas H. Sørensen and Christoffer B. Christensen and Mikail Ünalan and Xuping Zhang},
keywords = {Collaborative robot, SME production, Learning-based vision, Multi-functional gripper, Digital twin},
abstract = {The features of collaborative robots (cobots), like lightweight, easy programming, and flexibility, meet the production automation requirements in SMEs. However, SME productions are usually in semi-structured or cluttered environments, which raises major challenges in implementing cobot systems in SME production, for instance, increasing the visual perception of cobots, handling diverse tasks, and fast deploying cobot systems, etc. Therefore, we propose an automation framework for SME production by addressing these challenges with cobots to facilitate their production. First, the learning-based vision system is developed and implemented with the You Only Look Once (YOLOv5) for object detection, and with the Convolutional Neural Network cascaded with a Support Vector Machine (CNN-SVM) for quality control of products. Then, the multi-functional gripper system is designed and fabricated to be capable of performing multiple operations and tasks without tool changing, and be able to tolerate a certain level of changes in the environment. After that, a digital twin of the robotic system is developed, which enables the system developer to save time in troubleshooting and debugging, and the customers to have a customized model with all the elements and functions required before system deployment. Finally, the onsite testing of the integrated system is conducted in collaboration with our SME industrial partner, and the test results show that the cobot system can perform the automated production process well and accurately. It is feasible to extend the application of such a cobot system to other SME productions.}
}
@article{OEDA20231191,
title = {A student modeling method combining Deep Learning and forgetting models with Knowledge Tracing},
journal = {Procedia Computer Science},
volume = {225},
pages = {1191-1200},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.107},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012656},
author = {Shinichi Oeda and Shunichi Hasegawa},
keywords = {Educational Data Mining, Deep Learning, Intelligent Tutoring System, Student Modeling, Knowledge Tracing, Item Response Theory},
abstract = {Educational Data Mining (EDM) aims to enhance education by analyzing learners’ skills and question difficulty levels using machine learning methods. Knowledge Tracing (KT), a subfield of EDM, utilizes Hidden Markov Models to estimate learners’ abilities and predict their performance on unseen questions. While deep learning methods, such as bi-directional RNNs, have improved KT's accuracy, they may lack interpretability from an educational psychology perspective. Item Response Theory (IRT), widely used in educational statistics, offers greater explanatory potential. This study proposes a model that integrates the concept of forgetting into IRT for improved accuracy and explainability in Knowledge Tracing using bi-directional RNNs. The forgetting concept is based on Ebbinghaus’ forgetting curve theory. Three experiments were conducted using synthetic data to compare a model from a previous study, a model based on the proposed method, and a model that combines the previous study's model with IRT but excludes the forgetting concept.}
}
@article{PESHKOVA2023100313,
title = {Digital twin concept: Healthcare, education, research},
journal = {Journal of Pathology Informatics},
volume = {14},
pages = {100313},
year = {2023},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2023.100313},
url = {https://www.sciencedirect.com/science/article/pii/S215335392300127X},
author = {Maria Peshkova and Valentina Yumasheva and Ekaterina Rudenko and Natalia Kretova and Peter Timashev and Tatiana Demura},
keywords = {Digital twins, Databases, Digital pathology, Biobanking},
abstract = {Introducing the concept of digital twins in healthcare, medical education, and research is a complex multistage challenge requiring participation of multidisciplinary teams. In pursuing this goal, we have created a validated database of scans of colorectal tumor slides associated with relevant clinical and histological information. This database is also linked to the blood bank, which opens a wide range of opportunities for further research. Herein, we present our experience within the scope of the digital twins initiative.}
}
@article{SHEN2023377,
title = {Digital twin application for attach detection and mitigation of PV-based smart systems using fast and accurate hybrid machine learning algorithm},
journal = {Solar Energy},
volume = {250},
pages = {377-387},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23000075},
author = {Zhongjie Shen and Wenqing Xu and Weikai Li and Yaoyao Shi and Fan Gao},
keywords = {Cyber security, Hybrid microgrids, Sequential hypothesis testing},
abstract = {Microgrids (MG) is originally designed to make Smart Grids more energy-efficient and reliable. The MG represents a complex cyber-physical system whose functioning is driven by the interaction of physical actions and computational elements, making it vulnerable to many forms of malicious cyber-attack. This study considers the effect of data integrity attack (DIA) on the performance of hybrid MGs, an important cyber threat to microgrids (MGs). Further, this paper develops a new process using sequential hypothesis testing (SHT) for detecting DIA on renewable energy resources and improving the security of information in hybrid MGs. By using a binary sample generated from the suggested approach, an analysis statistic is computed and afterward compared to 2 thresholds for deciding between the 3 options. On the measured energy production of renewable energy sources such as wind turbines, DIAs of various severity levels have been conducted for assessing the impact of DIAs on hybrid MG security. A standard IEEE test system has been used to evaluate the efficiency of the suggested process.}
}
@article{MISHRA2023100724,
title = {Advanced contribution of IoT in agricultural production for the development of smart livestock environments},
journal = {Internet of Things},
volume = {22},
pages = {100724},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100724},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523000471},
author = {Shailendra Mishra and Sunil Kumar Sharma},
keywords = {Digital twins in livestock farming (DTLF), Internet of Things (IoT), Livestock farming, Artificial intelligence (AI), Machine learning (ML), Global positioning satellite, Radio frequency identification},
abstract = {Many areas of contemporary life need the use of Artificial Intelligence (AI), Machine Learning (ML), and the Internet of Things (IoT) for analysis and comprehension. The application of AI and ML in livestock farming has led to an improved understanding of animal behavior and discomfort, the prevention and management of diseases, and the efficiency of the farmer's economic choices. Digital twin technology is a particularly promising field that builds on AI and is already being utilized to increase efficiency and decrease costs in cattle livestock agriculture. A Digital Twin (DT) is a constant-update numerical copy of smart livestock environments. The proposed research is IoT, and Digital Twins in Livestock Farming (DTLF) might expand extensive Precision Livestock Farming (PLF), technology and tools use, and farm animal health and well-being. Global Positioning Satellite (GPS), Radio Frequency IDentification (RFID), AI, and even ML are only some current technologies that may be used for automated tracking of individual animal whereabouts. There is promise in using these methods and innovations to monitor and evaluate animal welfare, but these techniques are difficult. Just like other revolutionary innovations, the utilization of the DT mechanism will improve disease analysis prediction for individual farms by a factor of 92%. The responsiveness of DTLF will be a 92% response rate in analyzing the Heartbeat, a 94% effectiveness rate in analyzing the Temperature Range, and a 94% overall effectiveness in evaluating the Humidity Range.}
}
@article{ARSIWALA2023112851,
title = {Digital twin with Machine learning for predictive monitoring of CO2 equivalent from existing buildings},
journal = {Energy and Buildings},
volume = {284},
pages = {112851},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2023.112851},
url = {https://www.sciencedirect.com/science/article/pii/S0378778823000816},
author = {Arva Arsiwala and Faris Elghaish and Mohammed Zoher},
keywords = {Digital Twin, BIM, Internet of things (IoT), Artificial Intelligence (AI), Big Data (BD), Net zero, Sustainability, Carbon emission},
abstract = {The revolution of the industry 4.0 presents a new era of digital transformation for the construction industry, advancing towards the concept of digital twins, while on the other hand it faces the global challenge of reducing carbon emissions from operational assets. The current research gap in the application of digital twins for achieving net zero were reviewed in this study, highlighting its potentials for enhancement in the built environment, and emphasizing the need for demonstration of a use-case analysis for its adoption by the industry. This research presents a digital twin solution to automate the monitoring and controlling of equivalent carbon dioxide (eCO2) emissions from existing assets through the integration of IoT, BIM, and artificial intelligence across a comprehensive solution, further validating its workability through a real-life use case analysis. The study revealed the significance of BIM and IoT, as essential components of a digital twin to visualise critical spatial information for enhanced facility management specifically for monitoring of indoor air quality of spaces, while also coalescing an AI-supported system to predict carbon emissions from the collected data through integration of machine learning features across the digital twin. The output of the entire solution is displayed as an interactive dashboard for observing trends and patterns, enabling stakeholders to implement effective data-driven retrofitting strategies. This research is a fundamental initiation for implementing digital twins to monitor emissions from existing assets, a step towards achieving the net zero targets.}
}
@incollection{SU2023309,
title = {Chapter 16 - Outlook of future landscape of artificial intelligence in health care of liver disease and challenges},
editor = {Tung-Hung Su and Jia-Horng Kao},
booktitle = {Artificial Intelligence, Machine Learning, and Deep Learning in Precision Medicine in Liver Diseases},
publisher = {Academic Press},
pages = {309-322},
year = {2023},
isbn = {978-0-323-99136-0},
doi = {https://doi.org/10.1016/B978-0-323-99136-0.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323991360000088},
author = {Tung-Hung Su and Jia-Horng Kao},
keywords = {Future medicine, Health care, Innovation, Precision medicine, Workflow},
abstract = {Artificial intelligence (AI) is growing in importance in hepatology. Various sources of data from electric health records, radiology, and pathology have been used to develop AI models for nonalcoholic fatty liver disease, viral hepatitis, cirrhosis, acute liver failure, liver transplantation, hepatocellular carcinoma, drug-induced liver injury, and precision medicine. AI will soon be integrated in the clinical workflow to manage liver disease, although several issues need to be considered. AI will facilitate interdisciplinary care and collaboration with physicians, but it will not replace them. Data are crucial in the development of AI algorithms, whereas the collection and standardization of medical data from various sources are important to develop AI models, so data disparity and security should be carefully managed. The deployment and efficacy of AI models should be evaluated in clinical scenarios. Several novel fields of AI research involve training models using multimodal data and multiomics data. Advances in natural language processing help to explore unstructured electric health records. Digital epidemiology, digital twins, and federated learning are other trends. Telemedicine and self-monitoring using multiple wearable devices facilitate a useful paradigm of health care. The era of AI will transform the health care of liver disease.}
}
@article{HUANG2023102545,
title = {Hybrid learning-based digital twin for manufacturing process: Modeling framework and implementation},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {82},
pages = {102545},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102545},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000212},
author = {Ziqi Huang and Marcel Fey and Chao Liu and Ege Beysel and Xun Xu and Christian Brecher},
keywords = {Digital twin, Digital shadow, Artificial intelligence, Machine tool, Smart manufacturing},
abstract = {Digital twin (DT) and artificial intelligence (AI) technologies are powerful enablers for Industry 4.0 toward sustainable resilient manufacturing. Digital twins of machine tools and machining processes combine advanced digital techniques and production domain knowledge, facilitate the enhancement of agility, traceability, and resilience of production systems, and help machine tool builders achieve a paradigm shift from one-time products provision to on-going service delivery. However, the adaptability and accuracy of digital twins at the shopfloor level are restricted by heterogeneous data sources, modeling precision as well as uncertainties from dynamical industrial environments. This article proposes a novel modeling framework to address these inadequacies by in-depth integrating AI techniques and machine tool expertise using aggregated data along the product development process. A data processing procedure is constructed to contextualize metadata sources from the design, planning, manufacturing, and quality stages and link them into a digital thread. On this consistent data basis, a modeling pipeline is presented to incorporate production and machine tool prior knowledge into AI development pipeline, while considering the multi-fidelity nature of data sources in dynamic industrial circumstances. In terms of implementation, we first introduce our existing work for building digital twins of machine tool and manufacturing process. Within this infrastructure, we developed a hybrid learning-based digital twin for manufacturing process following proposed modeling framework and tested it in an external industrial project exemplarily for real-time workpiece quality monitoring. The result indicates that the proposed hybrid learning-based digital twin enables learning uncertainties of the interaction of machine tools and machining processes in real industrial environments, thus allows estimating and enhancing the modeling reliability, depending on the data quality and accessibility. Prospectively, it also contributes to the reparametrization of model parameters and to the adaptive process control.}
}
@incollection{NALINI2023185,
title = {Chapter 10 - Impact of internet of things and digital twin on manufacturing era},
editor = {Rajesh Kumar Dhanaraj and Ali Kashif Bashir and Vani Rajasekar and Balamurugan Balusamy and Pooja Malik},
booktitle = {Digital Twin for Smart Manufacturing},
publisher = {Academic Press},
pages = {185-202},
year = {2023},
isbn = {978-0-323-99205-3},
doi = {https://doi.org/10.1016/B978-0-323-99205-3.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323992053000109},
author = {M. Nalini and M.R. Bharathkumar and R. Keerthivasan and N. Nithyashree and V. Dhanashree},
keywords = {Automation, interlink, less human interaction, simulation, monitoring, analytics},
abstract = {Every piece of information gathered must be shared and used effectively without compromising security. The digital twin and the Internet of Things (IoT) are two sides of the same technological coin. Both aspects have a similar sounding name, however, they are not the same. The backbone of IoT is referred to as a digital twin. Before real data can perform, it must be simulated and processed, which is what the digital twin and IoT combine to provide. In almost every other company, these qualities have been instilled in numerous fields. The main difference between digital twin and the IoT is that the digital twin is a real-time virtual representation of any plan or object using technologies such as reasoning, simulation, complex decision-making, and machine learning, whereas IoT is a network connected to physical devices that run, process, and act on data. Clearly, the proliferation of IoT devices contributes to the possibility of digital twins. Furthermore, as IoT devices improve, digital twin might be incorporated in smaller and less sophisticated products, providing significant benefits to businesses.}
}
@article{SEMERARO2023127086,
title = {Digital twin in battery energy storage systems: Trends and gaps detection through association rule mining},
journal = {Energy},
volume = {273},
pages = {127086},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.127086},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223004802},
author = {Concetta Semeraro and Haya Aljaghoub and Mohammad Ali Abdelkareem and Abdul Hai Alami and A.G. Olabi},
keywords = {Digital twin, Battery energy storage system, Formal concept analysis, Association rule mining, Unsupervised machine learning},
abstract = {Energy sector is being revolutionized with the introduction of digitalization technologies. Digitalization technologies converted conventional energy grids into smart grids. Therefore, the virtual representation of battery energy storage systems, known as a digital twin, has become a highly valuable tool in the energy industry. This technology seamlessly integrates battery energy storage systems into smart grids and facilitates fault detection and prognosis, real-time monitoring, temperature control, optimization, and parameter estimations. In general, the use of digital twin technology improves the efficiency of the battery system after a thorough assessment of the battery performance. Hence, this paper aims to review the advancements of digital twin technology in battery energy storage systems. In particular, this paper focuses on the different functions and architectures of the digital twin for battery energy storage systems. Then, this paper further analyzes the digital twin characteristics using the Formal Concept Analysis (FCA) algorithm. The FCA is run to find trends and gaps between the digital twin functions and architectures in the battery system. Exploring the trends and gaps from previous research associated with the integration of digital twin with battery energy systems is essential to pave the way for further enhancements in this field.}
}
@article{PIRES2023103884,
title = {Reinforcement learning based trustworthy recommendation model for digital twin-driven decision-support in manufacturing systems},
journal = {Computers in Industry},
volume = {148},
pages = {103884},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103884},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000349},
author = {Flávia Pires and Paulo Leitão and António Paulo Moreira and Bilal Ahmad},
keywords = {Digital twin, Decision-support, Recommendation systems, Similarity measures, Trust-based model},
abstract = {Digital twin is one promising and key technology that emerged with Industry 4.0 to assist the decision-making process in multiple industries, enabling potential benefits such as reducing costs, and risk, improving efficiency, and supporting decision-making. Despite these, the decision–making approach of carrying out a what-if simulation study using digital twin models of each and every possible scenario independently is time-consuming and requires significant computational resources. The integration of recommendation systems within the digital twin-driven decision-support framework can support the decision-making process by providing targeted scenario recommendations, reducing the decision-making time and imposing decision- making efficiency. However, recommendation systems have inherent challenges, such as cold-start, data sparsity, and prediction accuracy. The integration of trust and similarity measures with recommendation systems alleviates the challenges mentioned earlier, and the integration of machine learning techniques enables better recommendations through their ability to simulate human learning. Having this in mind, this paper proposes a trust-based recommendation approach using a reinforcement learning technique combined with similarity measures, which can be integrated within a digital twin-based what-if simulation decision-support system. This approach was experimentally validated by performing accurate recommendations in an industrial case study of a battery pack assembly line. The results show improvements in the proposed model regarding the accuracy of the prediction about the user rating of the recommended scenarios over the state-of-the-art recommendation approaches, particularly in cold-start and data sparsity scenarios.}
}
@article{FANG2023109108,
title = {Self-supervised intermittent fault detection for analog circuits guided by prior knowledge},
journal = {Reliability Engineering & System Safety},
volume = {233},
pages = {109108},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109108},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023000236},
author = {Xiaoyu Fang and Jianfeng Qu and Yi Chai},
keywords = {Self-supervised learning, Prior knowledge, Teacher–student model, Intermittent fault detection, Analog circuits},
abstract = {Intermittent faults (IFs) are common in electronic systems, which are short-term, repeatable and cumulative. IF samples are difficult to collect, so detection is usually performed using one-class learning approaches, which require only fault-free samples to participate in the training. Teacher–student model typically uses the cognitive biases of teacher and student on fault signals to detect faults. Introducing prior knowledge of IFs in the teacher model may help to produce greater fault cognitive bias and thus improve detection. Inspired by this, this paper proposes a prior knowledge-guided teacher–student (PKGTS) model based on self-supervised learning. In analog circuits, IFs cause transient changes in the circuit signal in terms of amplitude, frequency, and waveform. Therefore, based on this prior knowledge, corresponding signal transformations are designed to simulate possible fault variations and introduce prior knowledge to the teacher through a pretext task. Finally, only the knowledge of the teacher’s fault-free state is imparted to the student. During the testing phase, IF detection is achieved through the cognitive biases of faults, as the student model does not have prior knowledge of faults. In two typical analog filtering circuit experiments, the effectiveness of the proposed method under different noise levels and fault intensities is verified.}
}
@article{ABDELRAHMAN2023110511,
title = {Learning data teaching strategies via knowledge tracing},
journal = {Knowledge-Based Systems},
volume = {269},
pages = {110511},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110511},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123002617},
author = {Ghodai Abdelrahman and Qing Wang},
keywords = {Knowledge tracing, Machine teaching, Reinforcement learning, Key-value memory network, Attention},
abstract = {Teaching plays a fundamental role in human learning. Typically, a human teaching strategy involves assessing a student’s knowledge progress for tailoring the teaching materials to enhance the learning progress. A human teacher can achieve this by tracing a student’s knowledge over essential learning concepts in a task. Albeit, such a teaching strategy is not well exploited yet in machine learning as current machine teaching methods tend to directly assess the progress of individual training samples without paying attention to the underlying learning concepts in a learning task. In this paper, we propose a novel method, called Knowledge Augmented Data Teaching (KADT), which can optimize a data teaching strategy for a student model by tracing its knowledge progress over multiple learning concepts in a learning task. Specifically, the KADT method incorporates a knowledge tracing model to dynamically capture the knowledge progress of a student model in terms of latent learning concepts. We further develop an attention-pooling mechanism to distill knowledge representations of a student model with respect to class labels, which enables to develop a data teaching strategy on critical training samples. We have evaluated the performance of the KADT method on four different machine learning tasks, including knowledge tracing, sentiment analysis, movie recommendation, and image classification. The KADT method consistently outperforms the state-of-the-art methods on all these tasks.}
}
@article{ZHOU202329,
title = {Digital twin application for reinforcement learning based optimal scheduling and reliability management enhancement of systems},
journal = {Solar Energy},
volume = {252},
pages = {29-38},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.01.042},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23000506},
author = {Jun Zhou and Mei Yang and Yong Zhan and Li Xu},
keywords = {Demand response, Photovoltaic, Energy storages, Smart home scheduling, Reinforcement learning, digital twin simulation},
abstract = {Increasing populations and economic expansion have substantially increased the energy requirements of residential consumers. Energy storage system (ESS) and distributed generation (DGs) are key tools for tackling this problem in smart homes. This study investigates the cost of electricity for residential consumers as a result of the combination of distributed photovoltaics (PVs) and ESSs for IoT-based smart home. Moreover, this paper examines energy management advantages due to bidirectional energy flow (H2G). In order to formulate the home energy management issue, PV and ESS end-user satisfaction limitations are taken into account. This study exploits a Q value-enabled reinforcement learning (RL) method to optimize home appliance scheduling (HAS) according to end-user priority. According to simulation outcomes, the suggested scheduling for household appliances performs well, and demand response (DR) measures have been implemented. It can be seen that the cost of electricity consumption as well as the uncertainty of the system have decreased in digital twin real-based application.}
}
@incollection{GU2023773,
title = {Chapter 23 - Summary and outlook of future directions and perspectives for additive manufacturing research and development},
editor = {Dongdong Gu},
booktitle = {Laser Additive Manufacturing of Metallic Materials and Components},
publisher = {Elsevier},
pages = {773-782},
year = {2023},
series = {Additive Manufacturing Materials and Technologies},
isbn = {978-0-12-823783-0},
doi = {https://doi.org/10.1016/B978-0-12-823783-0.00012-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128237830000127},
author = {Dongdong Gu},
keywords = {Digital twins, Digitized material, Hybrid manufacturing, Intelligent manufacturing, Machine learning, Materials genome initiative},
abstract = {The material-structure-performance-integrated additive manufacturing (MSPI-AM), as a concept embodied throughout this book, continues to develop into a practical methodology that contributes to the high performance and multifunctionality goals of AM. Many opportunities exist to enhance MSPI-AM. MSPI-AM relies on a more digitized material and structure development and printing, which could be accomplished by considering different paradigms for AM materials discovery with the Materials Genome Initiative, standardization of formats for digitizing materials and structures to accelerate data aggregation, and a systematic printability database to enhance autonomous decision-making of printers. MSPI-oriented AM becomes more intelligent in processes and production, with the integration of intelligent detection, sensing, and monitoring; big-data statistics and analytics; machine learning; and digital twins. MSPI-AM further calls for more hybrid approaches to yield the final high-performance/multifunctional achievements, with more versatile materials selection and more comprehensive integration of virtual manufacturing and real production to navigate more complex printing. We hope that MSPI-AM can become a key strategy for the sustainable development of AM technologies.}
}
@article{YU2023104627,
title = {Conditional generative data-free knowledge distillation},
journal = {Image and Vision Computing},
volume = {131},
pages = {104627},
year = {2023},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2023.104627},
url = {https://www.sciencedirect.com/science/article/pii/S026288562300001X},
author = {Xinyi Yu and Ling Yan and Yang Yang and Libo Zhou and Linlin Ou},
keywords = {Data-free knowledge distillation, Generative adversarial networks, Model compression, Convolutional neural networks},
abstract = {Knowledge distillation has made remarkable achievements in model compression. However, most existing methods require the original training data, which is usually unavailable due to privacy and security issues. This paper proposes a conditional generative data-free knowledge distillation (CGDD) framework for training lightweight networks without real data. This framework realizes efficient knowledge distillation based on conditional image generation. Specifically, we treat the preset labels as ground truth to train a semi-supervised conditional generator. The trained generator can produce specified classes of training images. During training, we force the student model to extract the hidden knowledge in teacher feature maps, which provide crucial cues to the learning process. Meanwhile, we construct an adversarial training framework to promote distillation performance. The framework will help the student model to explore larger data space. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on different datasets. Compared with other data-free works, our method obtains state-of-the-art results on CIFAR100, Caltech101, and different versions of ImageNet datasets. The codes will be released.}
}
@incollection{SINGH2023221,
title = {Chapter 12 - Potential applications of digital twin technology in virtual factory},
editor = {Rajesh Kumar Dhanaraj and Ali Kashif Bashir and Vani Rajasekar and Balamurugan Balusamy and Pooja Malik},
booktitle = {Digital Twin for Smart Manufacturing},
publisher = {Academic Press},
pages = {221-241},
year = {2023},
isbn = {978-0-323-99205-3},
doi = {https://doi.org/10.1016/B978-0-323-99205-3.00011-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323992053000110},
author = {Anamika Singh and Md. Akkas Ali and Balamurugan Balusamy and Vandana Sharma},
keywords = {Digital twin, model of digital twin, simulation, product lifecycle, virtual reality},
abstract = {In the recent years, the digital twin technology has become an emerging approach, especially for the manufacturing sectors, and also is considered the center of attention for the industries and, currently to the academic sector as well. It can be understood as the virtual model drawn to show the replica of the physical model. The technology has expanded its importance to various sectors ranging from businesses to aircraft manufacturing, from academics to healthcare, and more. This approach integrates big data, artificial intelligence (AI), Internet of Things (IoT), and machine learning (ML), which are the core concepts of Industry 4.0. The IoT is a widespread concept in todays industries, which has rendered the twin technology more cost-effective and user friendly for the industrial as well as the business world. Digital twin technology represents the blueprint of the product or service being manufactured and works throughout the lifecycle of the system. Digital twin results as the emerging concept to the fourth revolution of the industries with immense opportunities. With further advancements in ML and AI, it is expected to lead the machines to the next level. It is a key concept for cyber-physical system. This chapter mainly explains: 1.Introduction and history of digital twin technology2.Types of digital twin technology and its conceptual model3.Working of digital twin technology4.Potential applications of digital twin in virtual factory.5.How it differs from simulation.}
}
@article{DUPUIS2023354,
title = {Forecasting Future Product Sequences To Be Processed In Tire Production Using Deep Learning Technique},
journal = {Procedia Computer Science},
volume = {219},
pages = {354-361},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.300},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923003095},
author = {Ambre Dupuis and Camélia Dadouchi and Bruno Agard and Robert Pellerin},
keywords = {Industry 4.0, Production sequencing, Sequence analysis, RNN, Seq2Seq},
abstract = {Production sequencing methodology using DeepLearning Seq2Seq-LSTM is applied to a tire production case study in Quebec, Canada. Production and demand data are used to predict the most likely product sequences to operate. The comparison of 4 forecasting models, differing in consideration of demand and a statistical component, leads to nearly 70% of good prediction when all machines are studied and 10 production scenarios are considered. This performance reaches 92% for a specific class of machines. The analysis of the forecasts by class of machine allows highlighting 2 factors influencing the performance of the models, namely the ratio of product/machine by class and the total number of available records. The forecasts of possible production scenarios can then be used in a digital twin to evaluate a reasonable number of options and develop a decision support system for production sequencing.}
}
@incollection{ICHIMURA2023129,
title = {Chapter 5 - A Teacher–Student-based adaptive structural deep learning model and its estimating uncertainty of image data},
editor = {Steven G. Krantz and Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {49},
pages = {129-149},
year = {2023},
booktitle = {Artificial Intelligence},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2023.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169716123000226},
author = {Takumi Ichimura and Shin Kamada and Toshihide Harada and Ken Inoue},
keywords = {Adaptive structural learning, Ensemble deep learning, Teacher–Student model, Deep belief network, Restricted Boltzmann machine, Alzheimer's disease},
abstract = {Deep learning has been successfully used as a model that can effectively represent multiple features of the input space and significantly improve image recognition performance on deep architectures. Adaptive structural learning methods of restricted Boltzmann machines (Adaptive RBM) and deep belief networks (Adaptive DBN) have been developed as self-organizing deep learning models. The model uses a neuron generation–annihilation algorithm to find the optimal number of hidden neurons in the RBM for a given input data and then layers a new RBM as a hidden layer on top of the trained RBM to obtain the appropriate DBN structure. The proposed learning model was applied to PET and MRI image data sets in ADNI digital archive for the early detection of MCI (Mild Cognitive Impairment) and AD (Alzheimer's Disease). Two deep learning models were constructed to classify the PET and MRI images, respectively. For the training set, our model showed 99.7% and 99.2% classification accuracy for PET and MRI images, and for the test set, the model showed 98.8% and 96.7% accuracy for them. The Adaptive DBN model achieved the highest classification accuracy among the other CNN models. Moreover, the Teacher–Student-based Adaptive DBN was developed as an ensemble learning to improve the classification power for the test data set of MRI images and the accuracy increased to 98.3%. The accuracy of AD vs CN, MCI vs CN, and MCI vs AD for the MRI images by Teacher–Student-based Adaptive DBN are 98.4%, 98.8%, and 97.8%, respectively. Moreover, the difference between the diagnostic results by the proposed deep learning model and Medical Questionnaire was discussed.}
}
@article{ZHONG2023104791,
title = {Blockchain-driven integration technology for the AEC industry},
journal = {Automation in Construction},
volume = {150},
pages = {104791},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104791},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523000511},
author = {Botao Zhong and Xing Pan and Lieyun Ding and Qiang Chen and Xiaowei Hu},
keywords = {Architecture, engineering, and construction (AEC), Blockchain-driven integration technology (BDIT), Critical review, Technological developments, Technological applications, Further evolutions},
abstract = {In the architecture, engineering, and construction (AEC) industry, blockchain-driven integration technology (BDIT) has witnessed rapid development. A critical literature review of BDIT can contribute toward innovation for the AEC industry. In this study, a quantitative mapping of 247 BDIT literatures from 2017 to 2022 was conducted. Following the clue of quantitative work, two critical levels of technological development and application for BDIT were analyzed. The findings suggest: (1) the technological developments of BDIT may involve technological integration (i.e., integration of blockchain and internet of thing/building information modeling/edge computing) and knowledge framework; and (2) the technological applications of BDIT may involve information automation management and building information management. To further explore the trends of BDIT, some future evolutions (i.e., integration of blockchain and federated learning/digital twin/cloud-edge-end), application scenarios and challenges of BDIT were discussed. This study provides valuable theoretical and practical references for future research on BDIT in the AEC industry.}
}
@article{WU2023100443,
title = {Internet of Everything and Digital Twin enabled Service Platform for Cold Chain Logistics},
journal = {Journal of Industrial Information Integration},
volume = {33},
pages = {100443},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100443},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X2300016X},
author = {Wei Wu and Leidi Shen and Zhiheng Zhao and Arjun Rachana Harish and Ray Y. Zhong and George Q. Huang},
keywords = {Service platform, Internet of everything, Digital twin, Cold chain logistics, Abnormal stationary detection, Indoor positioning},
abstract = {The proliferation of the e-commerce market has posed challenges to staff safety, product quality, and operational efficiency, especially for cold chain logistics (CCL). Recently, the logistics of vaccine supply under the worldwide COVID-19 pandemic rearouses public attention and calls for innovative solutions to tackle the challenges remaining in CCL. Accordingly, this study proposes a cyber-physical platform framework applying the Internet of Everything (IoE) and Digital Twin (DT) technologies to promote information integration and provide smart services for different stakeholders in the CCL. In the platform, reams of data are generated, gathered, and leveraged to interconnect and digitalize physical things, people, and processes in cyberspace, paving the way for digital servitization. Deep learning techniques are used for accident identification and indoor localization based on Bluetooth Low Energy (BLE) to actualize real-time staff safety supervision in the cold warehouse. Both algorithms are designed to take advantage of the IoE infrastructure to achieve online self-adapting in response to surrounding evolutions. Besides, with the help of mobile and desktop applications, paperless operation for shipment, remote temperature and humidity (T&H) monitoring, anomaly detection and warning, and customer interaction are enabled. Thus, information traceability and visibility are highly fortified in this way. Finally, a real-life case study is conducted in a pharmaceutical distribution center to demonstrate the feasibility and practicality of the proposed platform and methods. The dedicated hardware and software are developed and deployed on site. As a result, the effectiveness of staff safety management, operational informatization, product quality assurance, and stakeholder loyalty maintenance shows a noticeable improvement. The insights and lessons harvested in this study may spark new ideas for researchers and inspire practitioners to meet similar needs in the industry.}
}
@article{PAN2023355,
title = {Real-time digital twin machine learning-based cost minimization model for renewable-based microgrids considering uncertainty},
journal = {Solar Energy},
volume = {250},
pages = {355-367},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23000063},
author = {Mingyu Pan and Qijing Xing and Zhichao Chai and He Zhao and Qinfei Sun and Dapeng Duan},
keywords = {Reinforcement learning, Energy management, Markov chain scheme, Renewable-based microgrid, Digital twin},
abstract = {This research study aims to investigate the microgrid operation for distributing energy including of a local user, a wind turbine, 5 photovoltaics (PV), and a battery, which is linked by a transformer to the external network. This paper examines a reinforcement learning (RL) method that uses 2steps-ahead to schedule the batteries, which is essential for achieving the objective of the users. There is an essential architecture to make multi-criteria decisions via an individual user to increase the battery's usage at peak times and increase the wind turbine's usage for local consumption. RL algorithms select the optimum battery planning measures based on forecasts of wind power and photovoltaic availability. Through the suggested learning, the user can better understand the optimum battery planning measures for various time-varying environment factors. By using the proposed architecture, smart users are capable of learning the uncertain environment and selecting optimum energy management measures based on their experiences.}
}
@article{BUHALIS2023104724,
title = {Metaverse as a disruptive technology revolutionising tourism management and marketing},
journal = {Tourism Management},
volume = {97},
pages = {104724},
year = {2023},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2023.104724},
url = {https://www.sciencedirect.com/science/article/pii/S0261517723000067},
author = {Dimitrios Buhalis and Daniel Leung and Michael Lin},
keywords = {Metaverse, Immersive experience, Virtual experience, Information communication technologies},
abstract = {Metaverse is the next disruptive technology that will impact society in the coming decades, by enabling immersive experiences in both virtual and physical environments. Although still conceptual, Metaverse converges the physical and digital universe, allowing users to seamlessly traverse between them. Digital immersion offers opportunities for people to travel in time, supporting users to experience virtually ancient encounters, space explorations or dangerous natural phenomena, such as volcano eruptions. Users can explore immersive environments for working, learning, transacting, exploring interests and socialising with others. This is already evident in gaming ecosystems, where gamers effectively interact in the metaverse. Although still experimental, Metaverse is expected to revolutionize travel and tourism management and marketing. It empowers destination awareness, positioning and branding, as well as coordination and management, through digital twins. Metaverse provides opportunities to support trip planning, interaction and engagement, effectively transforming consumer behaviour. Visiting and engaging with destinations virtually is expected to motivate real travel, rather than replace it. This paper provides a vision of how Metaverse can revolutionize tourism experiences and transform tourism management and marketing. Drawing on a systematic review of scholarly works, articles from media and industry reports, this study defines and conceptualizes the Metaverse ecosystem for tourism and travel. It explores the foundations of the disruptions that Metaverse brings to tourism destinations and organisations and identifies the building blocks of Metaverse tourism. The study outlines research directions so that the tourism industry can take full advantage of the Metaverse capabilities and opportunities emerging as well as identify challenges for the future.}
}
@article{TRIPURA2023107008,
title = {Probabilistic machine learning based predictive and interpretable digital twin for dynamical systems},
journal = {Computers & Structures},
volume = {281},
pages = {107008},
year = {2023},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2023.107008},
url = {https://www.sciencedirect.com/science/article/pii/S004579492300038X},
author = {Tapas Tripura and Aarya Sheetal Desai and Sondipon Adhikari and Souvik Chakraborty},
keywords = {Predictive digital twin, Model update, Probabilistic machine learning, Stochastic differential equation},
abstract = {A framework for creating and updating digital twins for dynamical systems from a library of physics-based functions is proposed. The sparse Bayesian machine learning is used to update and derive an interpretable expression for the digital twin. Two approaches for updating the digital twin are proposed. The first approach makes use of both the input and output information from a dynamical system, whereas the second approach utilizes output-only observations to update the digital twin. Both methods use a library of candidate functions representing certain physics to infer new perturbation terms in the existing digital twin model. In both cases, the resulting expressions of updated digital twins are identical, and in addition, the epistemic uncertainties are quantified. In the first approach, the regression problem is derived from a state-space model, whereas in the latter case, the output-only information is treated as a stochastic process. The concepts of Itô calculus and Kramers-Moyal expansion are being utilized to derive the regression equation. The performance of the proposed approaches is demonstrated using highly nonlinear dynamical systems such as the crack-degradation problem. Numerical results demonstrated in this paper almost exactly identify the correct perturbation terms along with their associated parameters in the dynamical system. The probabilistic nature of the proposed approach also helps in quantifying the uncertainties associated with updated models. The proposed approaches provide an exact and explainable description of the perturbations in digital twin models, which can be directly used for better cyber-physical integration, long-term future predictions, degradation monitoring, and model-agnostic control.}
}
@article{SAITO20232892,
title = {Proposal of Cyber Range for Control System based on Virtual Commissioning Technology},
journal = {Procedia Computer Science},
volume = {225},
pages = {2892-2901},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.282},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923014400},
author = {Taishin Saito and Sou Takahashi and Jun Sato and Miki Matsunoki and Toshiyuki Kanmachi and Satoru Yamada and Kuniaki Yajima},
keywords = {Cyber Security, Security Education, Operational Technology, Industrial Control System, Cyber Range},
abstract = {Manufacturing companies are rapidly moving toward Digitalization. As a result, today's industrial systems integrate information systems and manufacturing systems. In this context, damage caused by cyber-attacks has become apparent, and cyber-attacks on factories are increasing both in Japan and overseas. Therefore, there is an increasing need to implement cyber security measures and education in manufacturing and factories from the BCP (Business Continuity Plan) perspective, which considers safety measures for individual processes and the supply chain. The industrial automation industry is beginning to develop designs through virtual commissioning based on VR product (digital twin) models, and all of this development work is increasingly taking place in virtual space. However, no security educational materials utilize CPS (Cyber-Physical Space), which is free from time and space, based on these technologies. This research aims to develop human resources who can respond to the paradigm shift and realize a suitable environment for cyber security education by constructing a remote training environment using VR product technology that can actually be used in student experiments and practical training.}
}
@article{ZHANG2023129094,
title = {Multi-step ahead probabilistic forecasting of multiple hydrological variables for multiple stations},
journal = {Journal of Hydrology},
volume = {617},
pages = {129094},
year = {2023},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2023.129094},
url = {https://www.sciencedirect.com/science/article/pii/S0022169423000367},
author = {Zhendong Zhang and Haihua Tang and Hui Qin and Bin Luo and Chao Zhou and Huayan Zhou},
keywords = {River Basin Digital Twin, Hydrological forecasting, Probabilistic forecasting, Deep learning},
abstract = {The demand for more accurate simulation of physical river basin puts forward higher requirements on the number of hydrological forecast stations, types of hydrological variables, forecast accuracy, forecast period and quantitative uncertainty. Therefore, how to simultaneously obtain multi-step ahead probabilistic forecasting of multiple hydrological variables for multiple stations is a key issue to be solved in this study. Firstly, the background of River Basin Digital Twin is introduced and the new problems faced by hydrological forecasting in this background are analyzed. Then, the input and output of hydrological forecasting are reconstructed into 4-D tensors. Next, a new hybrid deep learning model (B-CM-C3D) based on 3-D Convolutional Neural Network, Convolutional Minimum Gate Memory Neural Network and Variational Bayesian Neural Network is proposed to obtain multi-step ahead probabilistic forecasting of multiple hydrological variables for multiple stations. Finally, in order to verify the performance of this model, it was compared with four state-of-the-art models in the Yangtze River Basin. The experimental results show that: (1) the deterministic prediction accuracy and probabilistic forecasting comprehensive performance of B-CM-C3D are better than other comparison models in 80% of the results. (2) B-CM-C3D shortens the training time of B-CL-C3D by 43% and improves the prediction accuracy.}
}
@article{SOO2023529,
title = {MachIne learning for nutrient recovery in the smart city circular economy – A review},
journal = {Process Safety and Environmental Protection},
volume = {173},
pages = {529-557},
year = {2023},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2023.02.065},
url = {https://www.sciencedirect.com/science/article/pii/S0957582023001672},
author = {Allan Soo and Li Wang and Chen Wang and Ho Kyong Shon},
keywords = {Nutrient, Circular economy, Smart city, Machine learning, Internet of things, Sustainability},
abstract = {Urbanisation is leading to a concentration of growing city populations that contribute significantly to economic growth, while becoming epicentres of waste generation, greenhouse gas emissions, and food consumption. Nutrient smart city circular economy is currently an understudied intersection of growing city populations of food consumers, nutrient recovery technologies, Internet of Things (IoT), and agriculture. Meanwhile, machine learning has exploded with popularity over the years, with many circular economy literatures examining its usefulness in its predictive qualities to support management, optimisation, and recovery of useful resources from organic waste. This review paper examines advancements in machine learning for macronutrient recovery in city organic waste systems for a circular economy. The use of ML will greatly improve the scalability, transparency, productivity and accuracy of nutrient: recovery technologies, logistics, dissemination, and reuse. ML can also be combined with hardware to automate tedious waste separation, recovery and agricultural tasks using drones, hydroponics and satellites. Meanwhile, crop yields, nutrient demand-supply efficiencies, food security, environmental soil monitoring, and prosumer involvement could all increase. However, ML applications for urine, anaerobic digestion and prosumer economics are lacking.}
}
@article{YOU2023388,
title = {Digital Twin simulation for deep learning framework for predicting solar energy market load in Trade-By-Trade data},
journal = {Solar Energy},
volume = {250},
pages = {388-397},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X2300004X},
author = {Lili You and Mingli Zhu},
keywords = {Deep learning, Short-term market load forecast, Smart grid, Modified teaching–learning algorithm},
abstract = {Because users behave randomly and in a nonlinear way, predicting electrical loads proves to be a difficult procedure. Due to the development of the smart grid (SG) and advanced metering infrastructure (AMI), humans will be capable of recording, monitoring, and analyzing these non-linear behaviors. The use of electric load projection layouts is a necessity in order to make decisions, plan, and evaluate contracts in electrical systems. Consequently, there have been several load prediction methods in the research that demonstrate trade-offs among prediction precision and runtime (convergence rate). The current paper presents a method for short-term load prediction of market in trade-by-trade data that would be quick and precise. Modified mutual information (MMI) is used to extract abstractive characteristics from historic information. Learning empowers the factored conditional restricted Boltzmann machine (FCRBM) for predicting the electrical loads. Ultimately, the efficiency has been optimized using the suggested modified teaching–learning algorithm (MTLA). The suggested architecture has the advantage of improving prediction precision and convergence rate. The MMI method and FCRBM layout improve prediction precision. In addition, MTLA has been used to enhance the convergence rates. Based on simulation outcomes, the suggested quick and precise layout performs better than conventional layouts when it comes to forecasting precision and convergence rates, including Bi-level, MI-artificial neural network (MI-ANN), and accurate fast converging short-term load forecast (AFC-STLF).}
}
@article{NATGUNANATHAN2023100370,
title = {Deakin microgrid digital twin and analysis of AI models for power generation prediction},
journal = {Energy Conversion and Management: X},
volume = {18},
pages = {100370},
year = {2023},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2023.100370},
url = {https://www.sciencedirect.com/science/article/pii/S2590174523000260},
author = {Iynkaran Natgunanathan and Vicky Mak-Hau and Sutharshan Rajasegarar and Adnan Anwar},
keywords = {Solar energy, Microgrid, Power prediction, Machine learning, Deep learning},
abstract = {To achieve carbon neutral by 2025, Deakin University launched a AUD 23 million Renewable Energy Microgrid in 2020 with a 7-megawatt solar farm, the largest at an Australian University. A web-based digital twin (DT) is developed to provide operators with intelligence and insights through several AI-driven capabilities. Accurate and computationally efficient power generation prediction is one of the critical elements in this DT. To this end, we researched the literature and identified the commonly used Machine Learning-based prediction models and compared them computationally using power generation and weather sensor data obtained from the solar farm. From the computational experiments, we find that, overall, Artificial Neural Network (ANN) has achieved the highest R2-score (0.944) and the lowest RMSE (14.848). To obtain further insights, we compared the methods using our two novel metrics, the x-percentile Closeness scores and the x-percentile Absolute error scores. The new metrics provide us with a spectrum to measure the consistency and robustness of the prediction methods instead of just a single value. Further, power generation can fluctuate substantially, and a prediction model should be accurate regardless of the magnitude of the output, hence measuring the relative error has its merits. By our two new metrics, using the data from our Deakin Microgrid, Random Forrest (RF) outperformed the other methods tested, with the smallest absolute relative error across the whole spectrum (from 0.011 to 0.457). RF is also the fastest in model training time at 4.894 s and XGBoost came second at 5.115 s–a big contrast to ANN at 144.102 s. All prediction times are under 1 s. RF is therefore used as a power prediction algorithm in our Deakin Microgrid Digital Twin.}
}
@article{TENG2023110160,
title = {Structural damage detection based on transfer learning strategy using digital twins of bridges},
journal = {Mechanical Systems and Signal Processing},
volume = {191},
pages = {110160},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110160},
url = {https://www.sciencedirect.com/science/article/pii/S0888327023000675},
author = {Shuai Teng and Xuedi Chen and Gongfa Chen and Li Cheng},
keywords = {Structural damage detection, Convolutional neural network, Digital twin, Transfer learning, Real bridge case},
abstract = {In this paper, a novel structural damage detection (SDD) method based on the digital twin (DT) and transfer learning (TL) was proposed. The SDD methods based on the convolutional neural network (CNN) have proved their effectiveness in the many civil structures (models). However, their application to damage detection of real structures still faces some unprecedented challenges. It was widely known that a CNN needs a large number of training samples. It was difficult or/and impossible to obtain the sufficient samples covering various damage scenarios for in-service structures, which will limit the application of the CNN in real structures. Therefore, in this paper, a large number of damage samples of the numerical models were obtained by using the DT technology, and used to train a CNN as a pre-trained network. Then, the pre-trained CNN was transferred to the experimentally tested structure and real bridge structure by using the TL technology. The results confirm that the CNN trained by a large number of DT models has strong compatibility, and the detection accuracy of numerical models was more than 90%; the combination with TL technology significantly improves the performance of the CNN for experimental structures (the convergence speed was increased by 40–70%, and the detection accuracy was also improved by 5–17%). Meanwhile, the accuracy of damage detection for the real bridge structure reached 97.3% (76.6% higher than that of existing methods (non-digital twin)) by TL technology. It is demonstrated that the proposed method facilitates the application of the CNN in real structures.}
}
@article{LIEBENBERG2023102182,
title = {Information systems engineering with Digital Shadows: Concept and use cases in the Internet of Production},
journal = {Information Systems},
volume = {114},
pages = {102182},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102182},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923000182},
author = {Martin Liebenberg and Matthias Jarke},
keywords = {Digital Shadow, Internet of Production (IoP), Database views, World Wide Lab (WWL), Cooperative information systems, Digital twin, Informed machine learning, Knowledge pipeline},
abstract = {Entering the second decade of the Industrie 4.0 vision, the production sector is facing challenges in taking full advantage of global digitalization. Production research has focused on sophisticated mathematical models ranging from molecular materials modeling to production control to supply chain logistics. These models help simulate and control the related physical system but the variety of individual situations and behaviors is captured only as statistical uncertainty. The emergence of data-driven methods adds statistical or AI models learned from real-time production data to Digital Twins, and ideally allows for continuous synchronization (twinning) between physical and virtual system. However, the complexity of today’s production systems precludes Digital Twins covering more than just a few system perspectives, especially if realtime performance is required. To achieve better performance and more precise context adaptation, the interdisciplinary research cluster “Internet of Production” at RWTH Aachen University is exploring the concept of Digital Shadows. We conceptualize Digital Shadows as a generalization of compact views on dynamic processes, whose defining “query” combines condensed measurement data with efficient simplified mathematical models. Their small size makes Digital Shadows amenable to dynamic function allocation in hybrid cloud–edge settings. In addition to showing the similarities and differences to the traditional view concept, we also present a conceptual embedding of Digital Shadows in the context of large distributed system architectures, and sovereign data exchange in international Data Space communities. Two production use case experiences demonstrate that Digital Shadows can be valuable carriers of deep and reusable engineering knowledge for technical and ecological progress.}
}
@article{XU2023100455,
title = {Ensuring construction material provenance using Internet of Things and blockchain: Learning from the food industry},
journal = {Journal of Industrial Information Integration},
volume = {33},
pages = {100455},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100455},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X23000286},
author = {Jinying Xu and Jinfeng Lou and Weisheng Lu and Liupengfei Wu and Chen Chen},
keywords = {Material provenance, Construction logistics and supply chain, Food industry, Blockchain, Internet of Things, Framework},
abstract = {Ensuring material provenance is widely considered a promising solution to the persistent issues related to material fraudulence in the construction industry. However, current strategies of managing construction logistics and supply chain perplex provenance tracing and tracking by adding too many intermediaries and using low technologies. By learning from the food industry which shares similar complexity, prolonged supply chain, and numerous stakeholders, this research aims to develop a framework deployable for material provenance tracing and tracking in the construction industry. It does so by mixing the uses of (a) cross-sectoral learning; (b) design science research; and (c) internet of things (IoT) and blockchain technology. The developed framework has four interconnected layers, namely the business layer with different stakeholders and activities, the IoT layer to collect the provenance footprints, the blockchain layer with a mainchain to store open provenance data and sidechains to store organizational private data, and the application layer to facilitate the management of quality, safety, payment, logistic and supply chain, and sustainability. The underpinning philosophy of the framework is to capture the IoT-driven provenance footprints and put them in custody in blockchain. The framework is further illustrated and refined by using a pilot construction project in Hong Kong, which was endeavored to track steel provenance from its adjacent Pearl River Delta, the so-called “World's Factory”. The framework shows enormous prospects, e.g., adopting digital twins, lifecycle traceability, improved efficiency, and transparent operations, meanwhile facing challenges, e.g., under-developed regulations, scalability issues, and information leakage risks, which all call for future research.}
}
@article{CHO2023109541,
title = {Ambiguity-aware robust teacher (ART): Enhanced self-knowledge distillation framework with pruned teacher network},
journal = {Pattern Recognition},
volume = {140},
pages = {109541},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109541},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002418},
author = {Yucheol Cho and Gyeongdo Ham and Jae-Hyeok Lee and Daeshik Kim},
keywords = {Knowledge distillation, Self-knowledge distillation, Network pruning, Teacher-student model, Long-tail samples, Ambiguous samples, Sample ambiguity, Data augmentation},
abstract = {Self-knowledge distillation (self-KD) methods, which use a student model itself as the teacher model instead of a large and complex teacher model, are currently a subject of active study. Since most previous self-KD approaches relied on the knowledge of a single teacher model, if the teacher model incorrectly predicted confusing samples, poor-quality knowledge was transferred to the student model. Unfortunately, natural images are often ambiguous for teacher models due to multiple objects, mislabeling, or low quality. In this paper, we propose a novel knowledge distillation framework named ambiguity-aware robust teacher knowledge distillation (ART-KD) that provides refined knowledge, that reflects the ambiguity of the samples with network pruning. Since the pruned teacher model is simply obtained by copying and pruning the teacher model, re-training process is unnecessary in ART-KD. The key insight of ART-KD lies in the predictions of a teacher model and pruned teacher model for ambiguous samples providing different distributions with low similarity. From these two distributions, we obtain a joint distribution considering the ambiguity of the samples as teacher’s knowledge for distillation. We comprehensively evaluate our method on public classification benchmarks, as well as more challenging benchmarks for fine-grained visual recognition (FGVR), achieving much superior performance to state-of-the-art counterparts.}
}
@article{HONG20231963,
title = {Diagnosis of PV faults using digital twin and convolutional mixer with LoRa notification system},
journal = {Energy Reports},
volume = {9},
pages = {1963-1976},
year = {2023},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S2352484723000124},
author = {Ying-Yi Hong and Rolando A. Pula},
keywords = {Convolutional neural network, Diagnosis, Digital twin, Markov transition field, Photovoltaics},
abstract = {The amount of energy that is generated using photovoltaic (PV) arrays has been increasing rapidly over recent years. To avoid energy and financial losses due to PV faults, numerous methods for diagnosing PV faults have been proposed. Both digital twin (DT) and deep learning (DL) have been proven to be effective in solving detection/classification problems in various fields. This work develops a method of PV fault diagnosis that has the following three stages: (1) Detection of faults in a PV array using DT, (2) classification of the detected faults using ConvMixer, and (3) notification of detected and classified faults using a LoRa (long-range) system. DT is a virtual/digital model that is designed to reflect accurately the behavior and characteristics of a physical object. This work implements the model-based DC power generation of PV arrays using DT. In this study, a new DL method, called the convolutional mixer (ConvMixer) based on patch embedding and combining depthwise and pointwise convolutions to classify PV faults, is presented. The inputs of ConvMixer are 2D images generated from data on PV DC array power using a Markov transition field (MTF) transform. A LoRa notification system is very suitable for use in low-power wide-area networks (LPWANs), such as those needed in large PV farms. Simulation results demonstrate that the proposed ConvMixer outperforms other classical machine learning (ML) methods, such as decision tree, k-nearest neighbor, random forest, and support vector machine methods, as well as other classical CNN-based methods, such as AlexNet, ResNet50, VGG16, and VGG19. A real-time digital simulator (Opal-RT eMegasim) is used to verify the real-time applicability of the integration of DT, ConvMixer, and the LoRa notification system.}
}
@article{FAROOQ2023102173,
title = {Residual attention based uncertainty-guided mean teacher model for semi-supervised breast masses segmentation in 2D ultrasonography},
journal = {Computerized Medical Imaging and Graphics},
volume = {104},
pages = {102173},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2022.102173},
url = {https://www.sciencedirect.com/science/article/pii/S0895611122001434},
author = {Muhammad Umar Farooq and Zahid Ullah and Jeonghwan Gwak},
keywords = {Breast tumor segmentation, Mean teacher–student, Self-ensembling, semi-supervised learning, uncertainty estimation},
abstract = {Breast tumor is the second deadliest disease among women around the world. Earlier tumor diagnosis is extremely important for improving the survival rate. Recent deep-learning techniques proved helpful in the timely diagnosis of various tumors. However, in the case of breast tumors, the characteristics of the tumors, i.e., low visual contrast, unclear boundary, and diversity in shape and size of breast lesions, make it more challenging to design a highly efficient detection system. Additionally, the scarcity of publicly available labeled data is also a major hurdle in the development of highly accurate and robust deep-learning models for breast tumor detection. To overcome these issues, we propose residual-attention-based uncertainty-guided mean teacher framework which incorporates the residual and attention blocks. The residual for optimizing the deep network by enabling the flow of high-level features and attention modules improves the focus of the model by optimizing its weights during the learning process. We further explore the potential of utilizing unlabeled data during the training process by employing the semi-supervised learning (SSL) method. Particularly, the uncertainty-guided mean-teacher student architecture is exploited to demonstrate the potential of incorporating the unlabeled samples during the training of residual attention U-Net model. The proposed SSL framework has been rigorously evaluated on two publicly available labeled datasets, i.e., BUSI and UDIAT datasets. The quantitative as well as qualitative results demonstrate that the proposed framework achieved competitive performance with respect to the previous state-of-the-art techniques and outperform the existing breast ultrasound masses segmentation techniques. Most importantly, the study demonstrates the potential of incorporating the additional unlabeled data for improving the performance of breast tumor segmentation.}
}
@article{EDWARDS2023104774,
title = {Digital twin development through auto-linking to manage legacy assets in nuclear power plants},
journal = {Automation in Construction},
volume = {148},
pages = {104774},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104774},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523000341},
author = {Chloe Edwards and Daniel López Morales and Carl Haas and Sriram Narasimhan and Giovanni Cascante},
keywords = {Digital twins, Deep learning, Computer vision, 3D point cloud processing, Asset management, 3D scanning, Nuclear power plants, Semantic enrichment of 3D point clouds},
abstract = {Digitalization of Nuclear Power Plants (NPPs) is critical for their safe and effective operation and maintenance. Development of Digital Twins (DTs) of NPP legacy assets and subsystems is key to achieving this goal. Doing this effectively requires a framework for intelligent allocation of limited resources. This framework is developed here by synthesizing emerging best practices with NPP operators' needs for legacy assets management. Within the framework, a pipeline employs deep-learning object detection to read and locate equipment tags in images. It computes their locations in the corresponding 3D point clouds and then relates that data to an asset management system. The pipeline is premised on preservation and augmentation of existing NPP asset management processes that preclude options such as RFID tags or barcodes. It is a significant step toward more efficient development of DTs of legacy assets. The contributions are framed in the context of a typical Canadian legacy NPP.}
}
@incollection{WANG202397,
title = {Chapter 5 - Intelligent digital twin reference architecture models for medical and healthcare industry},
editor = {Abdulmotaleb {El Saddik}},
booktitle = {Digital Twin for Healthcare},
publisher = {Academic Press},
pages = {97-119},
year = {2023},
isbn = {978-0-323-99163-6},
doi = {https://doi.org/10.1016/B978-0-32-399163-6.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032399163600010X},
author = {Zhi Wang and Abdulmotaleb {El Saddik}},
keywords = {digital twin, healthcare, medical, AI, machine learning, architecture, multimedia, IoT, IoT edge, cloud, digital patient, federated machine learning, robot, remote surgery, PaaS},
abstract = {The recent redefinition of Digital Twin (DT) significantly has been expanding DT's potential and increasing momentum in both industries and academic research. DTs have not only widely adopted in manufacture industry but also become a new trend in medical and healthcare industry. However, traditional architectures do not provide satisfactory solutions for protecting privacy, breaking data silos, handling big data, and quickly integrating heterogeneous DTs. In this chapter, we introduced Intelligent DT architecture reference model for medical and health industry. It not only addresses the above challenges but also analytically defines the location of DT services deployed, the detail logic capabilities DT system provides and interfaces among them. We analytically molded DT's architecture models, analyzed their suitable application scenarios, and recommend using loosely coupled DT architecture model that combine data-centric, event-centric and application centric methodologies for complex cases in medical and healthcare industry. The architecture is helpful in identifying various levels of business and technical requirements and designing DT system. Additionally, we present the use case of digital patient and automatic remote surgery to give a detail explanation and demonstrate the efficacy of the architecture reference model: distributed DTs can analyze the current and historical medical data to plan, practice, and perform remote surgery. From feedback loop and machine learning module, DTs can gradually learn how to perform operations and make the operation automatic. This architecture reference model can works as a high level template to facilitate designing and integrating DTs across different systems, platforms, and domains.}
}
@article{AKINRINTOYO2023103243,
title = {(INVITED)Reconfigurable topology testbeds: A new approach to optical system experiments},
journal = {Optical Fiber Technology},
volume = {76},
pages = {103243},
year = {2023},
issn = {1068-5200},
doi = {https://doi.org/10.1016/j.yofte.2023.103243},
url = {https://www.sciencedirect.com/science/article/pii/S1068520023000226},
author = {Emmanuel Akinrintoyo and Zehao Wang and Bob Lantz and Tingjun Chen and Dan Kilper},
keywords = {Networking testbeds, Digital twin, Machine learning (ML), Stimulated Raman scattering (SRS), Software-defined networking (SDN), Optical physical layer control},
abstract = {Optical transmission systems provide high capacity, low latency and jitter, and high reliability for city-scale networks. Recirculating loop experiments have facilitated the study of signal propagation in long-haul optical transmission systems. However, they are unsuited for developing control and management software for city-scale optical networks with dozens or hundreds of reconfigurable optical add drop multiplexer (ROADM) units, diverse interconnection topologies, and dynamic traffic patterns. Large-scale testbeds can help, but may be inflexible and time- or cost-prohibitive. Reconfigurable testbeds such as COSMOS enable piece-wise emulation of a city-scale network by applying space and wavelength switching, dual-use software-defined networking (SDN) controllers, and comb sources, while digital twin models enable software emulation. Results from the development of a digital twin for COSMOS are presented for optical amplifiers and stimulated Raman scattering (SRS) including both analytical and machine learning (ML) models.}
}
@article{FU2023110,
title = {Forgery face detection via adaptive learning from multiple experts},
journal = {Neurocomputing},
volume = {527},
pages = {110-118},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223000279},
author = {Xinghe Fu and Shengming Li and Yike Yuan and Bin Li and Xi Li},
keywords = {Face forgery detection, Knowledge distillation, Adaptive learning, Multi-expert learning},
abstract = {As an important and challenging problem, Face Forgery Detection has gained considerable attention. Usually, it suffers from the diversity of forgery patterns in forgery images, which requires a detection model to have capability of capturing various patterns in the challenging scenarios. To address this problem, we present a divide-and-aggregate learning framework to build multi-expert models and integrate them into a unified model. Firstly, the built multi-expert models are pre-trained to capture and preserve the specific forgery pattern produced by each manipulation method separately. Secondly, to transfer diverse knowledge of experts, we propose an integrating approach based on knowledge distillation. However, the difference of manipulation-aware knowledge among these experts concerns the way of distillation when the knowledge is combined in the only student model. Thus, to determine the importance of each expert, we propose a sample-aware Adaptive Learning from Experts strategy (ALFE) to assign adaptive expert distillation weights for each fake sample based on the predictions of each expert. Experiments show that our method achieves SOTA performances on ACC/AUC in the benchmark of FaceForensics++, demonstrating the effectiveness of our proposed method.}
}
@article{HU2023937,
title = {Investigation on Smart Campus Management Platform Based on Digital Twin},
journal = {Procedia Computer Science},
volume = {228},
pages = {937-945},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.123},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923019543},
author = {Pan Hu},
keywords = {Digital Twin, Smart Campus, Big Data, Campus Management, Modernization of Education},
abstract = {With the improvement of digitalization and the development of information technology, intelligent campus management has become an important research direction in the current education field. Based on digital twin technology, this article proposed a design scheme for a smart campus management platform to address the current problems in campus management. This could manage and serve students, teachers, parents, and other aspects, and achieve the mining and analysis of school big data. Through the implementation and application of the platform, the efficiency and accuracy of campus management could be improved, and the modernization of education could be further promoted. The smart campus management platform of digital twin utilized digital twin technology to digitize the campus environment, equipment, students, teaching resources, and other information of the school, thus achieving three-dimensional visualization, intelligent analysis, and feedback. The experimental results showed that the security of the smart campus management platform designed in this article exceeded 90% and performed well in system testing.}
}
@article{WANG2023109142,
title = {Online bearing fault diagnosis using numerical simulation models and machine learning classifications},
journal = {Reliability Engineering & System Safety},
volume = {234},
pages = {109142},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109142},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023000571},
author = {Hui Wang and Junkang Zheng and Jiawei Xiang},
keywords = {Digital twin, Simulation numerical model, Machine learning model, Bearing fault diagnosis},
abstract = {Digital twin (DT) is the embodiment of the most advanced achievements of the current simulation technology theory development and the direction of intelligent development in the future. However, it is a great challenge to really integrate it into practical project application. Motivated by DT, an application method combining numerical simulation model and machine learning classification is proposed to show the advantages of digital twin. To ensure the reliability of the twin model, it is necessary to build a simulation model using a mature dynamic model, and modify it through the Pearson correlation coefficient (PCC) which is a kind of model online learning. Then, the required fault type is introduced by modifying the relevant fault influence factors, which is synchronously inserted into the normal operation model to obtain the normal, fault and other simulation numerical data. Finally, the machine learning model is used to predict the probability of each fault and feedback the impact value to the actual operation to guide the adjustment of actual parameters and the determination of maintenance plans. The experimental results show that this method can effectively predict the possibility of bearing failure synchronously and guide the adjustment and maintenance of actual bearing operating parameters.}
}
@article{ZHANG202336,
title = {Balanced knowledge distillation for long-tailed learning},
journal = {Neurocomputing},
volume = {527},
pages = {36-46},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.01.063},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223000711},
author = {Shaoyu Zhang and Chen Chen and Xiyuan Hu and Silong Peng},
keywords = {Long-tailed learning, Knowledge distillation, Vision and text classification},
abstract = {Deep models trained on long-tailed datasets exhibit unsatisfactory performance on tail classes. Existing methods usually modify the classification loss to increase the learning focus on tail classes, which unexpectedly sacrifice the performance on head classes. In fact, this scheme leads to a contradiction between the two goals of long-tailed learning, i.e., learning generalizable representations and facilitating learning for tail classes. In this work, we explore knowledge distillation in long-tailed scenarios and propose a novel distillation framework, named Balanced Knowledge Distillation (BKD), to disentangle the contradiction between the two goals and achieve both simultaneously. Specifically, given a teacher model, we train the student model by minimizing the combination of an instance-balanced classification loss and a class-balanced distillation loss. The former benefits from the sample diversity and learns generalizable representation, while the latter considers the class priors and facilitates learning for tail classes. We conduct extensive experiments on several long-tailed benchmark datasets and demonstrate that the proposed BKD is an effective knowledge distillation framework in long-tailed scenarios, as well as a competitive method for long-tailed learning. Our source code is available: https://github.com/EricZsy/BalancedKnowledgeDistillation.}
}
@incollection{HICKERSON2023267,
title = {Chapter 14 - Applying AI to advanced biomanufacturing},
editor = {Chandra P. Sharma and Thomas Chandy and Vinoy Thomas},
booktitle = {Artificial Intelligence in Tissue and Organ Regeneration},
publisher = {Academic Press},
pages = {267-288},
year = {2023},
isbn = {978-0-443-18498-7},
doi = {https://doi.org/10.1016/B978-0-443-18498-7.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184987000028},
author = {Darren H.M. Hickerson and Joshua Hunsberger},
keywords = {Advanced biomanufacturing, Artificial intelligence, Digital twin, Limited memory, Limited memory artificial intelligence, Machine learning, Organ regeneration, Predictive analytics, Quality assurance, Quality control, Reactive machine, Regulatory requirements, Somatic cells, Stem cells, Tissue engineered bladder, Tissue engineering},
abstract = {This chapter explores applying artificial intelligence and machine learning to advanced biomanufacturing. We start with current and potential applications of AI to different subcomponents of advanced biomanufacturing using the tissue-engineered bladder as a model use case. This includes logistics and materials planning and preparation; integration of clinical and manufacturing site systems for custom organ design and production; biopsy processing for key cell types; automation of cell expansion; final construct maturation and release testing; and workflow considerations and advantages of using AI systems. These topics address the application of digital twin technology and a discussion of regulatory constraints for GMP manufacturing using AI. Next we consider applying AI to small-scale operations and early-phase clinical trials using a novel shared-resource testbed platform.}
}
@article{ADEPU2023106571,
title = {Melanoma classification from dermatoscopy images using knowledge distillation for highly imbalanced data},
journal = {Computers in Biology and Medicine},
volume = {154},
pages = {106571},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106571},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523000367},
author = {Anil Kumar Adepu and Subin Sahayam and Umarani Jayaraman and Rashmika Arramraju},
keywords = {Cost-Sensitive Learning, Deep Learning, EfficientNet, Stratified K-fold Cross Validation, In-painting, ISIC-2020 dataset, Teacher Student Model},
abstract = {Melanoma is a deadly malignant skin cancer that generally grows and spreads rapidly. Early detection of melanoma can improve the prognosis of a patient. However, large-scale screening for melanoma is arduous due to human error and the unavailability of trained experts. Accurate automatic melanoma classification from dermoscopy images can help mitigate such issues. However, the classification task is challenging due to class-imbalance, high inter-class, and low intra-class similarity problems. It results in poor sensitivity scores when it comes to the disease classification task. The work proposes a novel knowledge-distilled lightweight Deep-CNN-based framework for melanoma classification to tackle the high inter-class and low intra-class similarity problems. To handle the high class-imbalance problem, the work proposes using Cost-Sensitive Learning with Focal Loss, to achieve better sensitivity scores. As a pre-processing step, an in-painting algorithm is used to remove artifacts from dermoscopy images. New CutOut variants, namely, Sprinkled and microscopic Cutout augmentations, have been employed as regularizers to avoid over-fitting. The robustness of the model has been studied through stratified K-fold cross-validation. Ablation studies with test time augmentation (TTA) and the addition of various noises like salt & pepper, pepper-only, and Gaussian noises have been studied. All the models trained in the work have been evaluated on the SIIM-ISIC Melanoma Classification Challenge - ISIC-2020 dataset. With our EfficientNet-B5 (FL) teacher model, the EfficientNet-B2 student model achieved an Area under the Curve (AUC) of 0.9295, and a sensitivity of 0.8087 on the ISIC-2020 test data. The sensitivity value of 0.8087 for melanoma classification is the current state-of-the-art result in the literature for the ISIC-2020 dataset which is a significant 49.48% increase from the best non-distilled standalone model, EfficientNet B5 (FL) teacher with 0.5410.}
}
@article{CHIURCO20231908,
title = {Data Modeling and ML Practice for Enabling Intelligent Digital Twins in Adaptive Production Planning and Control},
journal = {Procedia Computer Science},
volume = {217},
pages = {1908-1917},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.391},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922024760},
author = {Alessandro Chiurco and Mohaiad Elbasheer and Francesco Longo and Letizia Nicoletti and Vittorio Solina},
keywords = {Digital Twin, Machine Learning, Adaptive Production Planning, Framework, Industrial case study, HeuristicLab},
abstract = {Technological advancements in AI, IoT, and Simulation push the frontier of the industry 4.0 to realize intelligent Digital twins (DT) of the industrial systems. This growing interest in manufacturing DTs inspires solutions for the flexibility, reliability, and resilience of production plans. However, the effort in building a clear guideline for dealing with data and algorithms is still its infancy. This paper combines multidisciplinary knowledge in Machine Learning (ML), and Production Planning & Control (PPC) to facilitate the integration of the ML algorithms into Production systems’ DTs. The paper proposes an architecture-based workflow to introduce ML and data practitioners into the creation of intelligent DTs for adaptive PPC. The framework in this study is explained with a simplified industrial case study that uses Neural Networks, k-Nearest Neighbor, and the Symbolic regression algorithms to justify the utility of the proposed framework.}
}
@incollection{ALOQAILY202339,
title = {Chapter 3 - Digital twin for healthcare immersive services: fundamentals, architectures, and open issues},
editor = {Abdulmotaleb {El Saddik}},
booktitle = {Digital Twin for Healthcare},
publisher = {Academic Press},
pages = {39-71},
year = {2023},
isbn = {978-0-323-99163-6},
doi = {https://doi.org/10.1016/B978-0-32-399163-6.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323991636000081},
author = {Moayad Aloqaily and Ouns Bouachir and Fakhri Karray},
keywords = {digital twin, immersive technology, healthcare, AI, XR},
abstract = {Digital Twin (DT) and Immersive Services (XR) technologies are revolutionizing the medical sector through designing applications that support virtual representation and interactive reality. Both technologies leverage one another to advance healthcare services and provide professionals a virtual environment where they can interact with the digital information of their patients more conveniently. The integration of DT and XR technologies enables the creation of advanced 3D models of patients (e.g., organs or body) based on their accurate real data gathered and processed by the DT improving traditional healthcare treatments such as telemedicine, training, and consultation. This chapter introduces the DT technology in immersive healthcare services and presents its benefits to the medical sector. It discusses the various requirements and protocols to build immersive models of the DT using advanced Artificial Intelligence (AI) and Machine Learning (ML)-based mechanisms. The chapter also proposes various paradigms that can be used to enable rapid deployment of these models, meeting the strict demands of the medical sector in terms of efficiency, accuracy, and precision.}
}
@incollection{VEERAMAKALI202321,
title = {Chapter 2 - Knowledge-Driven Digital Twin Manufacturing},
editor = {Rajesh Kumar Dhanaraj and Ali Kashif Bashir and Vani Rajasekar and Balamurugan Balusamy and Pooja Malik},
booktitle = {Digital Twin for Smart Manufacturing},
publisher = {Academic Press},
pages = {21-34},
year = {2023},
isbn = {978-0-323-99205-3},
doi = {https://doi.org/10.1016/B978-0-323-99205-3.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323992053000055},
author = {T. Veeramakali and A. Shobanadevi and S. Prabu},
keywords = {Industry 4.0, human-centric industrial internet, smart factory, ubiquitous knowledge, service-oriented digital twin, knowledge-based intelligent skills},
abstract = {The current era is witnessing big data-driven product design, leading to the advent of new generation information technologies in industry and product design. Consequently, big data-driven product design focuses mostly on physical data analysis instead of virtual models; in other words, there is no frequent convergence between a product’s virtual and physical space. Recently, the interest in developing a digital twin, which interconnects the real and virtual world, has grown rapidly throughout the world. As Industry 4.0 hypes the power of artificial intelligence embedded into “things,” it neglects the role of the human component, which is still essential in many manufacturing activities, such as machine setup or maintenance. A digital twin of service-oriented manufacturing knowledge was developed, which leverages a flexible ontology-based knowledge structure combined with an augmented reality input system for intuitive, hands-on knowledge retrieval. The autonomous and self-optimizing characteristics of intelligent manufacturing suggest new demands, including learning and cognitive capabilities. Intelligent manufacturing offers enhanced quality, high productivity, low costs, and great flexibility for manufacturers. Intelligent manufacturing utilizes digital twins to monitor the status of their systems in real time and predict failures before they happen. In this chapter, the main focus is on reviewing and discussing the existing literature to convey the concept, operation, and application of digital twin manufacturing.}
}
@article{PANTOJAROSERO2023104842,
title = {Damage-augmented digital twins towards the automated inspection of buildings},
journal = {Automation in Construction},
volume = {150},
pages = {104842},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104842},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523001024},
author = {B.G. Pantoja-Rosero and R. Achanta and K. Beyer},
keywords = {Post-earthquake damage assessment, Digital twins, 3D building models, Structure from motion, Deep learning, Masonry buildings},
abstract = {Current procedures for the rapid inspection of buildings and infrastructure are subjective, time-consuming, and cumbersome to document, necessitating new technologies to automate the process and eliminate these shortcomings. Fortunately, recent developments in imaging devices and artificial intelligence, such as computer vision, provide the necessary tools for this, though they are not yet integrated into infrastructure applications. In this paper, we propose an end-to-end pipeline that generates damage-augmented digital twins for buildings at LOD3, including geometrical information as well as data pertaining to damage condition and its characterization. Our framework incorporates multiple-view images to (1) create a level of detail model, (2) segment damage information, and (3) characterize damage. The core of the method is the structure from motion, which is used to reconstruct the building scene, and machine-learning models that segment and characterize damage. In contrast to current practices, our method does not require manual intervention, generates lightweight models, and can be applied to a wide range of assets. The results generated with our pipeline represent a significant step towards an automated infrastructure damage assessment. We intend to expand our work in the future to include real-time applications and applications to other types of infrastructure. Codes and data sets are publicly available (https://github.com/eesd-epfl/DADT_buildings and https://doi.org/10.5281/zenodo.7767478).}
}
@article{CHENG2023241,
title = {Machine learning enabled learning based optimization algorithm in digital twin simulator for management of smart islanded solar-based microgrids},
journal = {Solar Energy},
volume = {250},
pages = {241-247},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2022.12.040},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X22009203},
author = {Tan Cheng and Xiangqian Zhu and Fan Yang and Wenfeng Wang},
keywords = {Support vector machine, Elephant herd optimizer, Energy storage, Photovoltaic production, Demand-side management},
abstract = {The key element of tomorrow's smart islanded microgrids (SMG) is Demand Side Management (DSM). Global energy awareness has led to an increase in SMGs' performance and peak balancing capabilities. DSMs are the control layout in these grids, and they aim at optimizing loads in various ways. The SMG includes batteries and distributed photovoltaics. This paper combines an Elephant Herding optimization algorithm (EHOA) and support sector machine (SVM) to the decision-making method in batteries for reducing the electric bill. EHOA could be an effective method for finding a nearly optimum solution to the load scheduling issue in order to reduce the consumers' expenses. Particularly, energy costs could be decreased if the consumer responds to prices that vary with the time of day. Therefore, The EHOA has been applied to assign the battery's optimum energy storage range. The SVM has been trained as a powerful machine learning method using optimal information from the EHOA. It has been employed to determine the amount of energy that has been transmitted into and out of batteries in order to take the lowest possible electric bills. In comparison to the current approach of 2.3 at the consumption of 8.2 kWh/day, the cost of the suggested method of mean generation oscillation (Gosc) has been equated to 2.27 dollars for the residential load. EHOA-SVM decreases 11.2% of energy costs, thereby assisting the decision-maker to balance the stability of demand-side measures.}
}
@article{GAO2023104835,
title = {AIoT-informed digital twin communication for bridge maintenance},
journal = {Automation in Construction},
volume = {150},
pages = {104835},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104835},
url = {https://www.sciencedirect.com/science/article/pii/S092658052300095X},
author = {Yan Gao and Haijiang Li and Guanyu Xiong and Honghong Song},
keywords = {Digital twin, Bridge maintenance, Communication complexity, Time delay, Resilience, Edge computing, LPWAN},
abstract = {Digital twin (DT) has been moving progressively from concept to practice for bridge operation and maintenance (O&M), but its issues of data synchronization and fault tolerance remain problematic. This paper investigates the time delay of bridge DT services according to communication and computation complexity, revealing the distinct impact of their sequence, and proposes an AIoT-informed DT communication framework to solve the above issues. The information hierarchy and two-way communication can be leveraged to minimize communication complexity in the framework. Meanwhile, the data flow and resilience of the proposed framework are demonstrated using a Petri net. Moreover, the framework is developed into a prototypical DT through cross-platform integration and validated with different cases. The results demonstrate that compared with other existing bridge DTs, the proposed framework has high efficiency, low-latency, and excellent fault tolerance, which can contribute to the efficiency and safety of bridge O&M, especially under communication-constraint circumstances. The framework is also promising for federated learning to protect the AI-model privacy of different stakeholders and has the potential to support agent-based intelligent bridge management in the future with little human intervention.}
}
@article{LI2023127067,
title = {Data-driven enabling technologies in soft sensors of modern internal combustion engines: Perspectives},
journal = {Energy},
volume = {272},
pages = {127067},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.127067},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223004619},
author = {Ji Li and Quan Zhou and Xu He and Wan Chen and Hongming Xu},
keywords = {Data driven, Enabling technology, Soft sensors, Internal combustion engines, Digital twin},
abstract = {Under the dual thrust of decarbonisation and digitalisation, data-driven enabling technologies become the most promising solutions to reducing the time, cost, and effort required in the development of modern internal combustion engines (ICEs) in which it is hard to handle high-data-cost, high-dimensional, complex nonlinear modelling problems. This paper proposes a view of data-driven enabling technologies used in ICE soft sensors with a focus on the reduction of experimental effort and model complexity to accelerate the development of ICE decarbonisation. The current progress in data-driven modelling of ICEs is briefly outlined from four aspects: data acquisition methods, data processing methods, machine learning methods and model validation methods. Moreover, the challenges of establishing ICE models with high accuracy, fast response, and strong robustness for real-time control are structured and analysed. Based on the challenges, perspectives on three aspects of versatility, practicality, and autonomy are presented. Finally, physics/data-enhanced machine learning and digital twin technology are suggested to empower soft sensors used for modern ICEs.}
}
@article{DANESHFAR2023108,
title = {An octonion-based nonlinear echo state network for speech emotion recognition in Metaverse},
journal = {Neural Networks},
volume = {163},
pages = {108-121},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001600},
author = {Fatemeh Daneshfar and Mohammad (Behdad) Jamshidi},
keywords = {Speech emotion recognition, Digital twins, Metaverse, Octonion algebra, Echo state network, Machine learning},
abstract = {While the Metaverse is becoming a popular trend and drawing much attention from academia, society, and businesses, processing cores used in its infrastructures need to be improved, particularly in terms of signal processing and pattern recognition. Accordingly, the speech emotion recognition (SER) method plays a crucial role in creating the Metaverse platforms more usable​ and enjoyable for its users. However, existing SER methods continue to be plagued by two significant problems in the online environment. The shortage of adequate engagement and customization between avatars and users is recognized as the first issue and the second problem is related to the complexity of SER problems in the Metaverse as we face people and their digital twins or avatars. This is why developing efficient machine learning (ML) techniques specified for hypercomplex signal processing is essential to enhance the impressiveness and tangibility of the Metaverse platforms. As a solution, echo state networks (ESNs), which are an ML powerful tool for SER, can be an appropriate technique to enhance the Metaverse’s foundations in this area. Nevertheless, ESNs have some technical issues restricting them from a precise and reliable analysis, especially in the aspect of high-dimensional data. The most significant limitation of these networks is the high memory consumption caused by their reservoir structure in face of high-dimensional signals. To solve all problems associated with ESNs and their application in the Metaverse, we have come up with a novel structure for ESNs empowered by octonion algebra called NO2GESNet. Octonion numbers have eight dimensions, compactly display high-dimensional data, and improve the network precision and performance in comparison to conventional ESNs. The proposed network also solves the weaknesses of the ESNs in the presentation of the higher-order statistics to the output layer by equipping it with a multidimensional bilinear filter. Three comprehensive scenarios to use the proposed network in the Metaverse have been designed and analyzed, not only do they show the accuracy and performance of the proposed approach, but also the ways how SER can be employed in the Metaverse platforms.}
}
@article{HARRIES2023306,
title = {Digital Twins for Predictive Maintenance},
journal = {Procedia CIRP},
volume = {118},
pages = {306-311},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.053},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123002779},
author = {Tobias Harries and Matthew Hartnoll and Mohammadmilad Hafezianrazavi and Harry Meek and Aydin Nassehi},
keywords = {Predictive maintenance, Digital twin},
abstract = {Modern maintenance strategies tend to be a mixture of different approaches, dependent on the type of machine and the component failure mode. For some machines, time-based maintenance is appropriate, however not all failure is fully age-related and other situations may require risk-based or condition-based maintenance. Predictive maintenance is a method originating from condition-based maintenance and is primarily used to predict the remaining useful life of a machine. Predictive models can be data driven, experimental, physics based or a hybrid thereof. In this paper, a virtual factory is used together with a data/experimental driven hybrid approach through the use of statistical models encompassed in a digital twin to calculate the remaining useful life of machines in the factory. An augmented statistical model is chosen for this purpose as it relies on data being fed from the factory during operation, therefore suiting the twin's functionality. Sensor data from previous machines or failure profiles is pre-processed and machine learning is used to extract condition indicators. These indicators are used to identify different types of fault and predict degradation paths. Such a method requires no mechanistic understanding of the process and is thus generalizable. Furthermore, with the possibility of using synthetic data, the method requires limited historic data compared to other methods, therefore suiting the scale of small to medium factories where predictive maintenance approaches are most challenging to implement.}
}
@article{XIA2023109256,
title = {A digital twin-enhanced semi-supervised framework for motor fault diagnosis based on phase-contrastive current dot pattern},
journal = {Reliability Engineering & System Safety},
volume = {235},
pages = {109256},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109256},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023001710},
author = {Pengcheng Xia and Yixiang Huang and Zhiyu Tao and Chengliang Liu and Jie Liu},
keywords = {Digital twin, Fault diagnosis, Motor, Semi-supervised learning, Transfer learning},
abstract = {Motor plays a core role in most industrial equipment. Accurate fault diagnosis of motor is a critical task and intelligent data-driven methods have gained significant advances. However, to obtain sufficient labeled data to train the models is expensive and laborious in industrial applications, and how to utilize three-phase current signals efficiently is a challenging task. To deal with these problems, a digital twin-enhanced semi-supervised framework is proposed for label-scarce motor fault diagnosis. First, a precise motor digital twin model is established based on multi-physics simulation and knowledge transfer is performed from the virtual space to the physical space. Second, a novel phase-contrastive current dot pattern (PCCDP) representation is proposed to transform three-phase motor stator current to a gray-scale image with an ordered arrangement and then characteristics of three phases can be contrasted in tight regions for efficient processing. Third, inter-space sample generation is proposed for continuous feature manifold learning to tackle discrepancy between spaces. Finally, intra-space sample generation and a clustering-based metric learning are also introduced to improve semi-supervised fault diagnosis performance. An induction motor fault experiment is conducted and a digital twin model is built correspondingly. Experiments verify the effectiveness and superiority of the proposed framework.}
}
@article{SALA2023640,
title = {On the development of the Digital Shadow of the Fischertechnik Training Factory Industry 4.0: an educational perspective},
journal = {Procedia Computer Science},
volume = {217},
pages = {640-649},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.260},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922023389},
author = {Roberto Sala and Fabiana Pirola and Giuditta Pezzotta},
keywords = {Digital Twin, Digital Shadow, Learning Factory, Industry 4.0, Bloom's Taxonomy},
abstract = {The fourth industrial revolution is characterized by the increasing availability of data, which can be collected from machines to create digital counterparts of them (e.g., Digital Shadow, Digital Twin), understand their status, and drive strategic and operational decisions. To effectively create and use such digital counterparts, it is necessary to hire skilled people or train them to achieve the necessary competencies, for instance using learning factories, which are nowadays becoming more and more common. From an educational standpoint, it is interesting to observe how such competencies could be developed starting from a personal background. The paper describes the development process of the Digital Shadow of the Fischertechnik Training Factory Industry 4.0 in the context of a university course. The aim was to understand if the competencies acquired during the Computer Science bachelor's degree were enough to allow for the development of a functioning Digital Shadow of the learning factory.}
}
@article{ZHENG20231,
title = {A collaborative intelligence-based approach for handling human-robot collaboration uncertainties},
journal = {CIRP Annals},
volume = {72},
number = {1},
pages = {1-4},
year = {2023},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2023.04.057},
url = {https://www.sciencedirect.com/science/article/pii/S0007850623000951},
author = {Pai Zheng and Shufei Li and Junming Fan and Chengxi Li and Lihui Wang},
keywords = {Human-robot collaboration, Manufacturing system, Collaborative intelligence},
abstract = {Human-Robot Collaboration (HRC) has played a pivotal role in today's human-centric smart manufacturing scenarios. Nevertheless, limited concerns have been given to HRC uncertainties. By integrating both human and artificial intelligence, this paper proposes a Collaborative Intelligence (CI)-based approach for handling three major types of HRC uncertainties (i.e., human, robot and task uncertainties). A fine-grained human digital twin modelling method is introduced to address human uncertainties with better robotic assistance. Meanwhile, a learning from demonstration approach is offered to handle robotic task uncertainties with human intelligence. Lastly, the feasibility of the proposed CI has been demonstrated in an illustrative HRC assembly task.}
}
@article{ASSADZADEH2023101875,
title = {Excavator 3D pose estimation using deep learning and hybrid datasets},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101875},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101875},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000034},
author = {Amin Assadzadeh and Mehrdad Arashpour and Heng Li and Reza Hosseini and Faris Elghaish and Shanaka Baduge},
keywords = {Computer vision, Construction machinery, Deep convilutional neural networks, Machine learning, Pose estimation, Safety and productivity analysis},
abstract = {Earthwork operations are crucial parts of most construction projects. Heavy construction equipment and workers are often required to work in limited workspaces simultaneously. Struck-by accidents resulting from poor worker and equipment interactions account for a large proportion of accidents and fatalities on construction sites. The emerging technologies based on computer vision and artificial intelligence offer an opportunity to enhance construction safety through advanced monitoring utilizing site cameras. A crucial pre-requisite to the development of safety monitoring applications is the ability to identify accurately and localize the position of the equipment and its critical components in 3D space. This study proposes a workflow for excavator 3D pose estimation based on deep learning using RGB images. In the proposed workflow, an articulated 3D digital twin of an excavator is used to generate the necessary data for training a 3D pose estimation model. In addition, a method for generating hybrid datasets (simulation and laboratory) for adapting the 3D pose estimation model for various scenarios with different camera parameters is proposed. Evaluations prove the capability of the workflow in estimating the 3D pose of excavators. The study concludes by discussing the limitations and future research opportunities.}
}
@article{CHABEAUTI202377,
title = {Digital twin of forged part to reduce distortion in machining},
journal = {CIRP Annals},
volume = {72},
number = {1},
pages = {77-80},
year = {2023},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2023.04.021},
url = {https://www.sciencedirect.com/science/article/pii/S0007850623000677},
author = {Hugo Chabeauti and Mathieu Ritou and Bruno Lavisse and Guenael Germain and Virginie Charbonnier},
keywords = {Digital twin, Residual stress, Deformation},
abstract = {When long parts are machined in forged blanks, the variability of bulk residual stress (RS) fields leads to uncontrolled deformation after machining, requiring manual reshaping. An original hybrid digital twin of forged part is thus proposed to manage the bulk RS variability and reduce part distortion in machining. The behavior model of parts relies both on reduced models of thermomechanical simulations of the forging process variability, on-line measurements and machine learning from the previous parts deformations. Adaptive machining solutions can then be simulated for a rapid decision-making. The approach was validated on a series of aeronautic forged parts.}
}
@article{JIA2023101915,
title = {From simple digital twin to complex digital twin part II: Multi-scenario applications of digital twin shop floor},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101915},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101915},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000435},
author = {Wenjie Jia and Wei Wang and Zhenzu Zhang},
keywords = {Digital twin, Shop floor, Intelligent manufacturing, Deep learning},
abstract = {The shop floor has always been an important application object for the digital twin. It is well known that production, process, and product are the core business of the shop floor. Therefore, the digital twin shop floor covers multi-dimensional information and multi-scale application scenarios. In this paper, the digital twin shop floor is constructed according to the modeling method of the complex digital twin proposed in Part I. The digital twin shop floor is firstly divided into several simple digital twins that focus on scenarios of different scales. Two simple application scenarios are constructed, including tool wear prediction and spindle temperature prediction. Main functions in different application scenarios, such as data acquisition, data processing, and data visualization, are implemented and encapsulated as components to construct simple digital twins. Secondly, ontology models, knowledge graphs, and message queues are used to assemble these simple digital twins into the complex digital twin shop floor. And two complex application scenarios are constructed, including machining geometry simulation considering spindle temperature and production scheduling considering tool wear. The implementation of the complex digital twin shop floor demonstrates the feasibility of the proposed modeling method.}
}
@article{CAO202377,
title = {Real-Time Machine Learning-based fault Detection, Classification, and locating in large scale solar Energy-Based Systems: Digital twin simulation},
journal = {Solar Energy},
volume = {251},
pages = {77-85},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2022.12.042},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X22009215},
author = {Hanhua Cao and Dongming Zhang and Shujuan Yi},
keywords = {Energy management, Machine learning, Bat optimization algorithm, Microgrid, Cyber security},
abstract = {The current study considers numerous renewable energy resources, distributed power generation units, energy storage, and plug-in hybrid electric vehicles (PHEV) in order to propose a reliable large-scale energy management framework that can be applied to islanded and grid-connected operations of renewable hybrid AC-DC microgrids (MGs). The framework uses a bat optimization algorithm (BOA) for minimizing the operating costs of the network and in addition introduces an intrusion detection system (IDS) according to the sequential hypothesis testing (SHT) method for detecting identity-enabled cyber-attacks (i.e classification of Sybil attacks, masquerading attacks) on the wireless-enabled advanced metering infrastructures (AMI). The suggested IDS uses the received signal strength (RSS) amount for distinguishing various signal resources and detecting cyberattacks. An IEEE 33-bus testing system has been used to construct a real-time hybrid MG in order to determine the reliability and efficiency of the suggested framework.}
}
@article{HERRMANN20231188,
title = {A Petri Net Architecture for Real-Time Human Activity Recognition in Work Systems},
journal = {Procedia Computer Science},
volume = {217},
pages = {1188-1199},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.317},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922024024},
author = {Jan-Phillip Herrmann and Alexander Atanasyan and Felix Casser and Sven Tackenberg},
keywords = {Colored Petri net, Human-centered Assistance, Experimentable Digital Twins},
abstract = {Real-time human-centered assistance in industrial processes depends on the individual history of the work person's activities in the work system and requires adequate methods for tracking the person's actions. Most research in human activity recognition is based on recognizing actions from video data using computer vision methods. Digital equipment, standardized machine data interfaces, and smart wearable devices extend the possibilities to describe the current state of the work system. Petri nets have already been applied to human activity recognition, however, without the requirement of detecting actions in real-time. This paper proposes a Petri net architecture that enables hierarchical description-based human activity recognition in industrial work processes. We present an extension, a Partitioned Colored Petri Net, based on the colored Petri net formalism that infers activities from state transitions of the work system in real-time. In a case study, we demonstrate the Petri net's application for an error-based learning system that visualizes error consequences in augmented reality using experimentable digital twins.}
}
@article{DIJKSTRA2023107754,
title = {Clustering children's learning behaviour to identify self-regulated learning support needs},
journal = {Computers in Human Behavior},
volume = {145},
pages = {107754},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107754},
url = {https://www.sciencedirect.com/science/article/pii/S074756322300105X},
author = {S.H.E. Dijkstra and M. Hinne and E. Segers and I. Molenaar},
keywords = {Self-regulated learning, Learning behaviour, Bayesian nonparametric clustering},
abstract = {When children are learning using adaptive learning technologies (ALTs), the technology builds a learner model, which creates temporal trajectories providing insight into how children's knowledge develops. Based on this learner model, ALTs adjust the difficulty of problems for each child, yet children still need to regulate their practice behaviour and uphold effort and accuracy. The temporal trajectories are consequently likely to, besides showing children's knowledge development, be indicative of children's regulation. Therefore, we explore clusters of these trajectories to further identify failure in children's self-regulated learning (SRL) and potential support needs. We propose a data-driven approach to cluster 354 trajectories of 134 5th graders learning three skills with different complexity. The resulting 9 clusters were interpreted using practice accuracy and effort as indicators of regulation of practice behaviour and prior and post-knowledge and learning gain as indicators of knowledge development. The differences between clusters regarding these indicators signal there are different levels of SRL failure and, consequently, different SRL support needs: high accuracy and knowledge development indicate minimal support needs, whereas clusters with low accuracy, showing no knowledge development, indicate extensive SRL support needs. In conclusion: clusters of temporal patterns in children's learning data can identify SRL support is needed.}
}
@article{XIE2023102428,
title = {Compressing convolutional neural networks with cheap convolutions and online distillation},
journal = {Displays},
volume = {78},
pages = {102428},
year = {2023},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2023.102428},
url = {https://www.sciencedirect.com/science/article/pii/S0141938223000616},
author = {Jiao Xie and Shaohui Lin and Yichen Zhang and Linkai Luo},
keywords = {Cheap convolution, Knowledge distillation, Online distillation, CNN compression and acceleration},
abstract = {Visual impairment assistance systems play a vital role in improving the standard of living for visually impaired people (VIP). With the development of deep learning technologies and assistive devices, many assistive technologies for VIP have achieved remarkable success in environmental perception and navigation. In particular, convolutional neural network (CNN)-based models have surpassed the level of human recognition and achieved a strong generalization ability. However, the large memory and computation consumption in CNNs have been one of the main barriers to deploying them into resource-limited systems for visual impairment assistance applications. To this end, most cheap convolutions (e.g., group convolution, depth-wise convolution, and shift convolution) have recently been used for memory and computation reduction but with a specific architecture design. Furthermore, it results in a low discriminability of the compressed networks by directly replacing the standard convolution with these cheap ones. In this paper, we propose to use knowledge distillation to improve the performance of compact student networks with cheap convolutions. In our case, the teacher is a network with the standard convolution, while the student is a simple transformation of the teacher architecture without complicated redesigning. In particular, we introduce a novel online distillation method, which online constructs the teacher network without pre-training and conducts mutual learning between the teacher and student network, to improve the performance of the student model. Extensive experiments demonstrate that the proposed approach achieves superior performance to simultaneously reduce memory and computation overhead of cutting-edge CNNs on different datasets, including CIFAR-10/100 and ImageNet ILSVRC 2012, compared to the previous CNN compression and acceleration methods. The codes are publicly available at https://github.com/EthanZhangYC/OD-cheap-convolution.}
}
@article{CHUNG2023138313,
title = {Ensemble machine learning approach for examining critical process parameters and scale-up opportunities of microbial electrochemical systems for hydrogen peroxide production},
journal = {Chemosphere},
volume = {324},
pages = {138313},
year = {2023},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2023.138313},
url = {https://www.sciencedirect.com/science/article/pii/S0045653523005805},
author = {Tae Hyun Chung and Manjila Shahidi and Symon Mezbahuddin and Bipro Ranjan Dhar},
keywords = {Machine learning, Meta-learning, Microbial electrochemical system, Microbial electrochemical technology, Hydrogen peroxide},
abstract = {Hydrogen peroxide (H2O2) production in microbial electrochemical systems (MESs) is an attractive option for enabling a circular economy in the water/wastewater sector. Here, a machine learning algorithm was developed, using a meta-learning approach, to predict the H2O2 production rates in MES based on the seven input variables, including various design and operating parameters. The developed models were trained and cross-validated using the experimental data collected from 25 published reports. The final ensemble meta-learner model (combining 60 models) demonstrated a high prediction accuracy with very high R2 (0.983) and low root-mean-square error (RMSE) (0.647 kg H2O2 m−3 d−1) values. The model identified the carbon felt anode, GDE cathode, and cathode-to-anode volume ratio as the top three most important input features. Further scale-up analysis for small-scale wastewater treatment plants indicated that proper design and operating conditions could increase the H2O2 production rate to as high as 9 kg m−3 d−1.}
}
@incollection{BROSINSKY202379,
title = {4 - Machine learning and digital twins: monitoring and control for dynamic security in power systems},
editor = {Emilio {Barocio Espejo} and Felix Rafael {Segundo Sevilla} and Petr Korba},
booktitle = {Monitoring and Control of Electrical Power Systems Using Machine Learning Techniques},
publisher = {Elsevier},
pages = {79-106},
year = {2023},
isbn = {978-0-323-99904-5},
doi = {https://doi.org/10.1016/B978-0-32-399904-5.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323999045000107},
author = {Christoph Brosinsky and Mert Karaçelebi and Jochen L. Cremer},
keywords = {surrogate models, digital twin, machine learning, dynamic security assessment, moving horizon estimation},
abstract = {The reader of the chapter will be able to connect techniques from machine learning (ML) and digital twins (DTs) to gain insights for monitoring and control of (dynamic) security for electrical power systems. DTs are validated and verified high-fidelity (hf) models providing high simulation accuracy. DTs can be used for simulation of the supervised process of system operation and are therefore able to provide synthetic studied data, where measurement data are scarce. However, for some real-time applications in monitoring and control, such high-fidelity simulation models are not appropriate due to the corresponding computational barrier. There, ML aims to create an application-specific, low-fidelity (lf) approximation of the digital twin. Such trained lf models are used in real-time applications where computational time is scarce and lf information is sufficient. The conceptual intersection of hf and lf models has been little explored and becomes increasingly complex. This chapter aims to provide a conceptual overview of how such hf and lf models can be combined. This chapter is split into two parts where the first part is to introduce ML, lf models, and digital twins, hf models, for power systems analysis, and the second chapter is to use these two types of models to form purpose-driven surrogate lf models, illustrated on the example of dynamic security assessment (DSA). In the first part, the concepts for using DTs as hf models for online power system studies and their corresponding tuning of model parameters are introduced. Subsequently, ML i.e., lf models, are introduced and their corresponding training frameworks.}
}
@article{LIPPI2023103892,
title = {Enabling causality learning in smart factories with hierarchical digital twins},
journal = {Computers in Industry},
volume = {148},
pages = {103892},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103892},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000428},
author = {Marco Lippi and Matteo Martinelli and Marco Picone and Franco Zambonelli},
keywords = {Smart factories, Causal models, Digital twins},
abstract = {Smart factories are complex systems where many different components need to interact and cooperate in order to achieve common goals. In particular, devices must be endowed with the skill of learning how to react in front of evolving situations and unexpected scenarios. In order to develop these capabilities, we argue that systems will need to build an internal, and possibly shared, representation of their operational world that represents causal relations between actions and observed variables. Within this context, digital twins will play a crucial role, by providing the ideal infrastructure for the standardisation and digitisation of the whole industrial process, laying the groundwork for the high-level learning and inference processes. In this paper, we introduce a novel hierarchical architecture enabled by digital twins, that can be exploited to build logical abstractions of the overall system, and to learn causal models of the environment directly from data. We implement our vision through a case study of a simulated production process. Our results in that scenario show that Bayesian networks and intervention via do-calculus can be effectively exploited within the proposed architecture to learn interpretable models of the environment. Moreover, we evaluate how the use of digital twins has a strong impact on the reduction of the physical complexity perceived by external applications.}
}
@article{KINACI2023114128,
title = {Ship digital twin architecture for optimizing sailing automation},
journal = {Ocean Engineering},
volume = {275},
pages = {114128},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114128},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823005127},
author = {Omer Kemal Kinaci},
keywords = {Maritime digital twin, Autonomous ship, Level 2 autonomy, Maritime autonomous surface ships},
abstract = {A fully autonomous ship should be monitored in a digital environment to track its real-time response to the changes in its surroundings. It is commanded by a controller algorithm whose optimum parameters is subject to change with respect to the changes in the environment. The ship's response with respect to these variations in the controller parameters need to be tracked to develop a full understanding of the ship's autonomy. As such, it is believed that the digital twin of a ship is an essential component of the path to fully autonomous ships. Despite maritime institutions not giving the credit to it yet, the ship digital twin concept is expected to be one of the leading topics in marine engineering soon. In this study, we try to establish the ship digital twin concept in terms of navigation autonomy. The paper starts by introducing the path to full autonomy in seas and the need for the digital twin by defining this concept for ships. A maneuvering mathematical model is used to represent the physical ship. The details of the model and the controller algorithms are given next. The propeller and rudder models are first validated by free-running self-propulsion and turning circle tests. The ship considered in this study has a twisted rudder and does not possess course-keeping ability; therefore, it requires autopilot to move straight ahead. Three types of simulation cases are identified with one being the autopilot mode. In the other two cases, the ship is forced to conduct two hard maneuvers while accelerating/decelerating. Investigation of the generated results reveals the effectiveness of the digital twin architecture used in this study. It has also allowed us to conduct a controller stability analysis, which has shown the behavior of the controller gains to detect the most optimum values for the considered model ship.}
}
@article{LUFTENSTEINER2023938,
title = {Improving Virtual Sensor Models by Censored Online Data},
journal = {Procedia Computer Science},
volume = {217},
pages = {938-947},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.291},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922023699},
author = {Sabrina Luftensteiner and Michael Zwick},
keywords = {Online Learning, Censored Data, Virtual Sensor},
abstract = {Digital twins are able to bridge the physical and the virtual world, which is especially useful in industrial environments. One small, but rather essential, kind of digital twin is the so called virtual sensor model. This type of model is used to enhance or replace a physical sensor in industrial settings to reduce costs or enable information retrieval from inaccessible locations within a machine or process. The virtual sensor model is usually trained once, whereat only a predefined amount of information without any adaptation possibilities on the clients side is provided. As manufacturers want to provide the possibility of custom adaptations for their clients, the virtual sensor models require to incorporate adaptive features, which also have to handle incomplete data recordings, e.g. uncensored data. Traditional offline machine learning approaches are often insufficient for such adaptive requirements, therefore the usage of online learning approaches is gaining increased attention, to avoid high computational, storage and temporal costs. This paper covers the continued training of such adaptive virtual sensor models, focusing on the handling and integration of censored online data. Different approaches to tackle the problems of catastrophic forgetting in online learning and correction of censored data are presented as well as the handling of censored data in online learning environments. The experiments sections compares various scenarios with and without censored data using an industrial dataset and demonstrates the positive influence of different online learning approaches.}
}
@article{WANG2023109152,
title = {Digital twin aided adversarial transfer learning method for domain adaptation fault diagnosis},
journal = {Reliability Engineering & System Safety},
volume = {234},
pages = {109152},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109152},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023000674},
author = {Jinrui Wang and Zongzhen Zhang and Zhiliang Liu and Baokun Han and Huaiqian Bao and Shanshan Ji},
keywords = {Fault diagnosis, Digital twin, Simscape, Transfer learning, Triplex pump},
abstract = {Machine health management has become the focus of equipment monitoring upgrading with the advance of digital twin (DT). The DT model is able to generate system performance data that is close to reality, which opens a new way for the cyber-physical integration of equipment monitoring. Furthermore, it also provides a significant opportunity for mechanical fault diagnosis when the collected fault signals are insufficient. In this paper, a DT aided intelligent fault diagnosis model is proposed for triplex pump. Specifically, the simulation model of the triplex pump is built by Simscape in MATLAB, and the measured simulation data is continuously updated to construct the DT model. Then a novel transfer learning model based on domain-adversarial strategy and Wasserstein distance is present and trained by the source domain data which generated from the DT model. Next, the opening pressure of the triplex pump is controlled to simulate different working conditions, so as to achieve feature transfer and fault diagnosis for the DT model. The experimental results show that the proposed method is effective and superior to other advanced transfer learning methods.}
}
@article{LIU2023112629,
title = {Automatic interpretation of strain distributions measured from distributed fiber optic sensors for crack monitoring},
journal = {Measurement},
volume = {211},
pages = {112629},
year = {2023},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.112629},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123001938},
author = {Yiming Liu and Yi Bao},
keywords = {Crack monitoring, Digital twin, Distributed fiber optic sensor, Machine learning, Structural health monitoring},
abstract = {Distributed fiber optic sensors have exhibited superior capabilities in monitoring cracks in engineering structures through measuring detailed strain distributions. However, manually interpreting the measurements from long distributed sensors deployed in large-scale structures is time-consuming, labor-intensive, and subject to human errors. This paper proposes to automate the identification, localization, quantification, and visualization of cracks through intelligent interpretation of strain distributions measured from distributed fiber optic sensors based on machine learning. Based on these intelligent capabilities, a live digital twin model based on building information modeling is developed to visualize cracks. The digital twin model is updatable with real-time measurements from strain distributions from distributed fiber optic sensors. The proposed approach is evaluated via laboratory testing of a concrete beam. The results show that the proposed approach achieves high accuracy in interpretation of sensor data for crack monitoring. This research advances the capabilities of structural health monitoring using distributed fiber optic sensors.}
}
@article{HE2023173,
title = {Management and real-time monitoring of interconnected energy hubs using digital twin: Machine learning based approach},
journal = {Solar Energy},
volume = {250},
pages = {173-181},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2022.12.041},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X22009227},
author = {Qingsu He and Muqing Wu and Chun Liu and Dan Jin and Min Zhao},
keywords = {Smart Grids, Smart Energy Hub, Energy Management System, Reinforcement Learning, Financial analysis},
abstract = {Getting equipped by highly new smart technologies, Energy Hubs (EHs) and Smart Grids (SGs) are gaining interest these days. Energy management will advance over time as a result of the interaction impact among power and natural gas grids, and the use of smart technology for communications. The present study proposes a novel approach entitled Smart EH (SEH) for modeling multi-carrier energy systems in SG environments. Furthermore, this paper determines the optimum management and sizing of combined heat and power, auxiliary boiler, absorption chiller, as well as transformer unit as the essential components of an SEH. It is difficult to address the requirements of SGs with most conventional load scheduling algorithms because they lack robustness and performance in complex environments. An evaluation of the benefits and costs of optimizing such parameters was carried out in this paper and the Reinforcement Learning (RL) algorithm is applied to solve the optimization problem. An individual user in a dynamic electrical market was examined as an SEH in support of the suggested approach. According to simulation outcomes, the suggested method is effective regarding time efficiencies and load variations.}
}
@article{WANG2023134,
title = {Multi-objective optimal scheduling of laminar cooling water supply system for hot rolling mills driven by digital twin for energy-saving},
journal = {Journal of Process Control},
volume = {122},
pages = {134-146},
year = {2023},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2023.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959152423000045},
author = {Fenjia Wang and Yong Song and Chao Liu and Anrui He and Yi Qiang},
keywords = {Laminar cooling, Digital twin, Optimal scheduling, Online Sequential Extreme Learning Machine, Improved Sparrow Search Algorithm, Energy saving},
abstract = {As a result of the complex structure of laminar cooling water supply systems, both facets of strict process requirements and various intermittent running conditions could cause challenges of energy-saving and equipment upkeep. Given the energy wastage issue of the laminar cooling system of hot rolling, this paper develops an optimal scheduling system according to digital twin using Online Sequential Extreme Learning Machine (OS-ELM) and multi-objective evolutionary optimization using Improved Sparrow Search Algorithm (ISSA). The optimal scheduling system according to digital twins can accurately predict the water consumption trend of the water supply system and optimize the scheduling instructions and operation scheme through dynamic information interaction and mapping of process constraints, intermittent operating conditions, rolling rhythm, measured data, etc. between physical space and virtual space. Experimental results reveal that the proposed method can lessen power usage by 13.60% and water consumption by 10.54% regarding the premise of ensuring what is needed of cooling procedures. In addition, the water pump can maintain high effectiveness during operation to guarantee the security and stability of laminar cooling water supply systems.}
}
@article{WU2023104446,
title = {High Fidelity Digital Twin-Based Anomaly Detection and Localization for Smart Water Grid Operation Management},
journal = {Sustainable Cities and Society},
volume = {91},
pages = {104446},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104446},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723000574},
author = {Zheng Yi Wu and Alvin Chew and Xue Meng and Jianping Cai and Jocelyn Pok and Rony Kalfarisi and Kah Cheong Lai and Sock Fang Hew and Jia Jie Wong},
keywords = {Digital twin, Machine learning, Data analytics, Smart water grid, Anomaly detection, Anomaly localization},
abstract = {Smart Water Grid (SWG) plays a critical role in sustaining cities economic and social development, but challenges remain in fully realizing the benefits of SWG. While Digital twin (DT) has been discussed in some literature for possible SWG applications, there has been limited, or no technical framework developed to facilitate SWG operation and management. In this paper, a generic framework is developed for constructing SWG high fidelity Digital Twin (DT) by integrating digital thread with various digital models for visualization, data-driven analysis, physics-based simulations, and decision-making support. Both the physics-based models and data-driven models are trained/retrained and calibrated/re-calibrated respectively by using the data collected with the sensors installed throughout a SWG. The information derived from the SWG DT can be diagnostic, predictive, and prescriptive to significantly augment users’ intelligence for improving SWG operation and management. One important application of digital twin augmented intelligence is illustrated to timely detect and localize anomaly events, which may include, but not be limited to, pipe bursts and unauthorized water usages. The solution is tested on the selected areas in Singapore to construct the ever-green DT by calibrating and recalibrating the models for near real-time SWG operation management. The case study was conducted for three supply zones with the total pipeline length of more than 1000 km and 40 weeks of monitoring data, collected by 89 pressure monitoring stations and 8 flow meters at inlets and boundary. More than 3300 data-driven models are trained for optimizing the model performance to achieve accuracy of greater than 80% F1 score for detecting anomaly events, which are subsequently localized within 400 m with 2–3 days lead time.}
}
@article{LI2023106688,
title = {Machine learning assisted advanced battery thermal management system: A state-of-the-art review},
journal = {Journal of Energy Storage},
volume = {60},
pages = {106688},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2023.106688},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X23000853},
author = {Ao Li and Jingwen Weng and Anthony Chun Yin Yuen and Wei Wang and Hengrui Liu and Eric Wai Ming Lee and Jian Wang and Sanghoon Kook and Guan Heng Yeoh},
keywords = {Battery thermal management, Thermal runaway, Mitigation, Artificial neural networks, Machine learning},
abstract = {With an increasingly wider application of the lithium-ion battery (LIB), specifically the drastic increase of electric vehicles in cosmopolitan cities, improving the thermal and fire resilience of LIB systems is inevitable. Thus, in-depth analysis and performance-based study on battery thermal management system (BTMs) design have arisen as a popular research topic in energy storage systems. Among the LIB system parameters, such as battery temperature distribution, battery heat generation rate, cooling medium properties, electrical properties, physical dimension design, etc., multi-factor design optimisation is one of the most difficult experimental tasks. Computational simulations deliver a holistic solution to the BTMs design, yet it demands an immense amount of computational power and time, which is often not practical for the design optimisation process. Therefore, machine learning (ML) models play a non-substitute role in the safety management of battery systems. ML models aid in temperature prediction and safety diagnosis, thereby assisting in the early warning of battery fire and its mitigation. In this review article, we summarise extensive lists of literature on BTMs employing ML models and identify the current state-of-the-art research, which is expected to serve as a much-needed guideline and reference for future design optimisation. Following that, the application of various ML models in battery fire diagnosis and early warning is illustrated. Finally, the authors propose improved approaches to advanced battery safety management with ML. This review paper aims to bring new insights into the application of ML in the LIB thermal safety issue and BTMs design and anticipate boosting further advanced battery system design not limited to the thermal management system, as well as proposing potential digital twin modelling for BTMs.}
}
@article{TIAN2023105328,
title = {Data-driven and physics-informed Bayesian learning of spatiotemporally varying consolidation settlement from sparse site investigation and settlement monitoring data},
journal = {Computers and Geotechnics},
volume = {157},
pages = {105328},
year = {2023},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2023.105328},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X2300085X},
author = {Huaming Tian and Yu Wang},
keywords = {Machine learning, Digital twin, Bayesian method, Dictionary learning, Compressive sampling/sensing},
abstract = {A digital twin of a geotechnical project (e.g., a reclamation or ground improvement project) is a virtual model that aims to continuously learn from actual observations (e.g., site investigation and monitoring data) and improve model prediction (e.g., spatiotemporally varying consolidation settlement). However, real geotechnical observation data obtained from a site are often spatially sparse (e.g., site investigation data) and spatiotemporally varying (e.g., settlement monitoring data). The sparse and spatiotemporally varying data pose great challenges for continuous learning of data and improvement in model prediction. To address these challenges, this study proposes a novel data-driven and physics-informed Bayesian learning framework that automatically develops ground models from spatially sparse site investigation data, performs geotechnical analysis, and integrates geotechnical analysis results with limited, but spatiotemporally varying, settlement monitoring data to improve model prediction in a systematic and quantitative manner. The proposed method contains three key components, (1) data-driven ground modeling by Bayesian compressive sampling (BCS) using sparse site investigation data as input, (2) finite element modeling (FEM) of consolidation settlement that incorporates domain knowledge, and (3) Bayesian sparse dictionary learning of settlement monitoring data together with FEM results. The proposed method is illustrated using a real ground improvement project, and the results show that the proposed approach performs well.}
}
@article{XU2023101950,
title = {A mini imitation game: How individuals model social robots via behavioral outcomes and social roles},
journal = {Telematics and Informatics},
volume = {78},
pages = {101950},
year = {2023},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2023.101950},
url = {https://www.sciencedirect.com/science/article/pii/S073658532300014X},
author = {Kun Xu},
keywords = {Human-robot interaction, Media Equation, The Computers are Social Actors paradigm, Social presence, Social cognitive theory, Modeling effects},
abstract = {In past works, social robots have been designed to mimic human appearances and behavior. However, little is known about how human beings may imitate social robots. Drawing on social cognitive theory and the Media Equation, this study focuses on the modeling effects of social robots in an environment protection context. A lab experiment (N = 128) with a between-subjects factorial design was conducted to examine how social robots’ behavioral outcomes and social roles affected individuals’ modeling behavior. This study suggested that social robots’ positive behavioral outcomes were effective in evoking users’ modeling tendencies serially through social presence and identification or only through identification. Robots’ mere presentation of behavior with no outcomes exerted effects serially through social presence and identification. Additionally, assigning social robots an instructor role led to users’ modeling behavior serially through users’ perception of robots’ expertise and credibility. The study analyzed the psychological mechanisms behind users’ modeling behavior.}
}
@article{MACHADO202336,
title = {Digital twin of an absorption chiller for solar cooling},
journal = {Renewable Energy},
volume = {208},
pages = {36-51},
year = {2023},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2023.03.048},
url = {https://www.sciencedirect.com/science/article/pii/S0960148123003403},
author = {Diogo Ortiz Machado and William D. Chicaiza and Juan M. Escaño and Antonio J. Gallego and Gustavo A. {de Andrade} and Julio E. Normey-Rico and Carlos Bordons and Eduardo F. Camacho},
keywords = {Dynamic modeling, Fuzzy, Principal Component Analysis, Heat Ventilation and Air Conditioning, Fresnel Solar Collector},
abstract = {The aim of this study is to create a digital twin of a commercial absorption chiller for control and optimization purposes. The chiller is a complex system that is affected by solar intermittency and non-linearities. The authors use Adaptive Neuro-fuzzy Inference System (ANFIS) to model the chiller’s behavior during transients and part-load events. The chiller is divided into four sub-models, each modeled by ANFIS, and trained and validated using data from 15 days of operation. The ANFIS models are precise, accurate, and fast, with a worst-case Mean Absolute Percentage Error (MAPE) of 3.30% and reduced error dispersion (σE=0.88) and Standard Error (SE=0.01). The models outperformed literature models in terms of MAPE, with MAPEs of 1.12%, 2.21%, and 3.24% for the High Temperature Generator (HTG), absorber + condenser, and evaporator outlet temperatures, respectively. The computational execution time of the model is also a valuable asset, with an average simulation step taking less than 0.20 ms and a total simulation time of 8.9 s for three days of operation. The resulting digital twin is suitable for Model Predictive Control applications and fast what-if analysis and optimization due to its gray-box representation and computational speed.}
}
@incollection{JIANG2023297,
title = {Polymer Grade Transition Control via Reinforcement Learning Trained with a Physically Consistent Memory Sequence-to-Sequence Digital Twin},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {297-303},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50048-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740500482},
author = {Zhen-Feng Jiang and David Shan-Hill Wong and Jia-Lin Kang and Yuan Yao and Yao-Chen Chuang},
keywords = {Reinforcement learning, Sequence-to-Sequence with Memory Layer, Model Predictive Control, Grade transition},
abstract = {In this work, a memory layer sequence-to-sequence digital twin (ML-StSDT) of a high-density polyethylene (HDPE) reactor simulated by ASPEN DynamicsTM was constructed using simulated grade transition and steady-state operating data. A reinforcement learning control (RLC) algorithm was developed by training with the ML-StSDT. The RLC was able to control both grade transition and steady-state operation of the simulated plant. The RLC performs better or equally well when compared with the direct application of ML-StSDT in nonlinear model predictive control (NLMPC) but substantially reduces the computation load. Our results demonstrate the feasibility of deep learning models serving as a digital twin for RLC training in nonlinear process control applications.}
}
@article{HUANG20238,
title = {Machine learning-based demand response in PV-based smart home considering energy management in digital twin},
journal = {Solar Energy},
volume = {252},
pages = {8-19},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.01.044},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X2300052X},
author = {Jueru Huang and Dmitry D. Koroteev and Marina Rynkovskaya},
keywords = {Energy management, Demand response, Smart home, Machine learning, Electricity cost},
abstract = {Energy management (EM) systems need to have the flexibility to take the optimum real-time decision in the face of constantly varying market factors. Demand response (DR) has become the newest method of improving the performance and reliability of the electrical system. Here, an hour-ahead DR algorithm is proposed for EM at home. This paper presents an artificial neural network approach that uses stable cost predictions as a method for dealing with upcoming price uncertainties. For making optimum and decentralized decisions for various household devices, multi-agent reinforcement learning has been used along with predicted upcoming costs. This paper conducts the simulation with shiftable, non-shiftable, and controllable loads to determine the effectiveness of this suggested EM strategy. Based on the outcomes of the experiments, this suggested DR algorithm has capable of handling EM for a number of devices, minimizing consumer electricity expenses and discomfort prices, and helping the consumer considerably reduce its energy expenses in comparison to a benchmark using no DR.}
}
@article{KAKIZAKI20232808,
title = {Student modeling considering learning behavior history with deep factorization machines},
journal = {Procedia Computer Science},
volume = {225},
pages = {2808-2815},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.273},
url = {https://www.sciencedirect.com/science/article/pii/S187705092301431X},
author = {Toma Kakizaki and Shinichi Oeda},
keywords = {Intelligent tutoring system, Educational data mining, Student modeling, Knowledge tracing, Factorization machines, DeepLearning},
abstract = {To use an intelligent tutoring system (ITS) in an educational setting effectively, it is necessary to understand the skill status of students and recommend appropriate questions. Existing studies focused on improving the performance of ITS using student modeling to estimate the skill status of students. Knowledge tracing (KT) is the mainstream student modeling method, and deep learning approaches such as deep knowledge tracing and self-attentive knowledge tracing have been studied extensively in recent years. These models take only the questions solved by the student and the correct or incorrect answers to those questions as input; they do not assume the use of other features. In this study, we perform student modeling using DeepFM and FiBiNET that combines factorization machines and deep learning. Our results indicate that these models are more accurate than KT because of their ability to cope with sparse data and consider pairwise feature interactions, and more suited for real-world applications than KT.}
}
@incollection{SALINI202335,
title = {Chapter 3 - Digital twin and artificial intelligence in industries},
editor = {Rajesh Kumar Dhanaraj and Ali Kashif Bashir and Vani Rajasekar and Balamurugan Balusamy and Pooja Malik},
booktitle = {Digital Twin for Smart Manufacturing},
publisher = {Academic Press},
pages = {35-58},
year = {2023},
isbn = {978-0-323-99205-3},
doi = {https://doi.org/10.1016/B978-0-323-99205-3.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323992053000146},
author = {S. Salini and B. Persis Urbana Ivy},
keywords = {Digital twins, cyber twins, DTS, artificial intelligence, taxonomy, meta-dimension},
abstract = {Applications of digital twin (DT) contribute to smart manufacturing via the integration of the cyber domain and the real world. Artificial Intelligence (AI) that is based on machine learning (ML) is generally recognized as being among the most promising technological developments in the manufacturing industry. Despite this, machine learning techniques call for enormous quantities of high-quality training datasets, and in the case of supervised ML, human labeling of such datasets is often necessary. Collecting and analyzing historical performance data to uncover operational insights has been a long-standing driver of efficiency and innovation for a wide variety of businesses, across all industrial sectors and even farther afield. However, the possible advantages of this method have not yet been leveraged to their maximum. There is a tendency for vast volumes of data, spanning anything from the current state of subsurface assets to the attitude that customers express on social media postings, to go mostly unrecorded and unanalyzed. This may be a problem for businesses. However, due to the limitations of existing IoT platforms in representing complex industrial machines, supporting production line-based application testing, and the lack of cost models for application cost/benefit analysis, the development of such Industry 4.0 applications is currently quite costly. Cyber Twins (CTs) are an extension of DTs that we propose using to facilitate the rapid and low-cost creation of Industry 4.0 applications. CTs may incorporate machine simulators and provide semantic descriptions of the machines they model, which makes it possible to test applications without putting the production line at risk or incurring additional costs. The applications for Industry 4.0 are based on CT and the accompanying cost models are the primary topics of this research. Second, the paper provides empirical results that connect existing academic literature to the interdependencies between edge components and internal and external services and systems in the Industry 4.0 paradigm. The novel aspect of this research is a new method for creating a DT, or a virtual representation that acts as the real-time digital counterpart of a physical object or process as defined by a conceptual model. This is the chapter’s primary contribution. The approach that was used to conduct the research for this article was conceptually similar to an investigation of complex, linked, and coupled systems using the grounded theory. This research provides an outline of how to increase AI development in IoT networks via integrating human-computer interactions across different knowledge management systems. These interactions take place in different information management systems. In addition, the previously used real-time controllable method has been improved, as well as optimized. Additionally, it performs simulation testing on the method in both a serial and a parallel fashion. The relevant mining time of the improved algorithm is shown to be substantially less than the time required by the conventional data mining method when applied to the same dataset, as shown by the test results when the ideal scenario of the parallel algorithm has been achieved. The amount of time required for conventional mining is around three and a half times as long as the amount of time required for data mining in this study, and the amount of power that the optimization method requires to operate has been decreased to 20W. With the help of AI, the manufacturing model (DT) and the decision support system are able to include all of the information generated by these subsystems. DT data technology facilitates the identification of pertinent information that can be processed and used to inform managerial decision-making. Applications of AI may open a wide variety of doors in the manufacturing industry, leading to the creation of new business models and the improvement of existing procedures. A formal model was used to define the landscape to guarantee the feasibility of conducting an in-depth analysis of the status and progression of the landscape, while taking into account DT and other technologies. The adoption of DT and Industrial Internet of Things for modeling of a real enterprise’s production process was taken into consideration. However, Digital Twin Store (DTS) has stricter standards, particularly in regard to the real-time engagement that is expected. A new technique that satisfies the aforementioned characteristics may be found in the use of AI, which is an efficient way of boosting the intelligence of the physical shop floor. In this piece of research, a conceptual framework for AI-enhanced DTS in interaction is presented. The real-time engagement is significantly increased by AI-enhanced DTS, thanks to its predictive control. The AI-enhanced interaction implementation technique in DTS is also discussed in length in this presentation. Finally, the DTS has implemented the necessary technology to support interaction.}
}
@article{IRINO2023345,
title = {Digital twin based accuracy compensation},
journal = {CIRP Annals},
volume = {72},
number = {1},
pages = {345-348},
year = {2023},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2023.04.088},
url = {https://www.sciencedirect.com/science/article/pii/S0007850623001269},
author = {Naruhiro Irino and Akihito Kobayashi and Yuta Shinba and Kengo Kawai and Daniel Spescha and Konrad Wegener},
keywords = {Digital twin, Dynamics, Thermal error compensation},
abstract = {A new method for predicting thermal displacements and dynamic characteristics of a machine tool using a digital twin to compensate for accuracy has been developed. The compensation method is based on an accuracy prediction method using model order reduction to reproduce the behavior of machine tools digitally according to their physical characteristics. Additionally, by coupling with the digital twin of dynamics, a comprehensive digital machine tool is generated. Using this model, the compensation of machining accuracy for ambient temperature conditions was validated on the actual machine. As a result, the error in the machining space was successfully compensated. Additionally, the dynamic cutting force was accurately estimated. A new method of compensation for thermal displacement and volumetric accuracy was established, which can visualize the actual machine phenomena in more detail than the conventional compensation of mathematical models by regression, machine learning, and neural networks.}
}
@article{ZHANG2023101485,
title = {Hybrid deep learning model for accurate classification of solid waste in the society},
journal = {Urban Climate},
volume = {49},
pages = {101485},
year = {2023},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2023.101485},
url = {https://www.sciencedirect.com/science/article/pii/S2212095523000792},
author = {Huanping Zhang and Hanhua Cao and Yuhuai Zhou and Changle Gu and Danyu Li},
keywords = {Solid waste management, Classification, Deep learning, Convolution neural network (CNN), Deep belief network (DBN), Optuna, Alexnet, Urban city},
abstract = {Due to the increasing initiatives for urbanization and the development of smart cities, waste generation, segregation, and its management have become fundamental tasks. To provide efficient planning for waste management and its processes, such as collection, sorting, recycling, and disposal, recently, machine learning (ML) approaches have been utilized to assist the authorities. However, the identification of the best ML approach for the prediction of waste is a challenging effort. Finding adequate waste litter measurement is necessary for the ecological characteristics to improve over time. The waste from the trash may divide into organic and recycling types. In this paper, the optimized hybrid deep learning model has been developed for waste classification. This proposed work takes advantage of (i) data collection and preprocessing (ii) feature extraction using CNN (AlexNet) (iii) waste prediction from the urban cities' wastes using DBN, and (iv) hyperparameter optimization using Optuna. This model obtained an R2 score of 0.94, MPE 0.02 than other state-of-the-art approaches. Compared to the individual learners model, this proposed optimized hybrid deep learning model boosts the performance to predict waste generation and classify it with increased accuracy.}
}
@article{GU2023286,
title = {Accurate and fast machine learning algorithm for systems outage prediction},
journal = {Solar Energy},
volume = {251},
pages = {286-294},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23000130},
author = {Chan Gu and Chen Chen and Wei Tang},
keywords = {Digital Twin, Solar-based System Outages, Cyber-vulnerabilities, Cyber Threats, Machine Learning, Resiliency},
abstract = {Cyber-attacks (CAs) on electrical networks in presents of renewable energies, particularly solar energy, have become more complex and sophisticated over the past few years by making severe system outages. In light of increased automation in solar-based energy industries, a holistic cyber-physical infrastructure must be considered to predict the effect of CAs on electrical networks and the ways to enhance its resilience. The present study examines the resilience characteristics at the equipment area of the diverse control methods and their effect on the outage severity of the smart grid using digital twins (DT) simulation technology. The paper presents a machine learning based metric for measuring the resilience of cyber-physical features that considers device-level characteristics, vulnerabilities, and system models. Resiliency refers to a system's capability of providing energy even during severe contingencies and relates to the ability to resist, forecast, and recover. An example based on the newest CA against Ukraine has been provided and simulated on DT environment. A case study is proposed to illustrate how cyber-physical resilience metrics can be applied to improve operators' situational awareness and provide more proactive or corrective control measures for improving resilience.}
}
@article{SOLOMON2022109446,
title = {SDNSandbox — Enabling learning-based innovation in provider networks},
journal = {Computer Networks},
volume = {219},
pages = {109446},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109446},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622004807},
author = {Yossi Solomon and Osnat Mokryn and Tsvi Kuflik},
keywords = {SDN in-a-box, Load prediction, Deep learning, Provider networks, SDN-DCRNN},
abstract = {Provider networks are looking to follow the footsteps of cloud-based networks/data centers and incorporate Software-Defined Networking (SDN) technology. This move is problematic for various reasons, such as the networks’ size and the providers’ inability to control users’ activity. Additionally, research into these networks is handicapped by the lack of information stemming from the confidentiality of these complex networks. To that end, we have created SDNSandbox — an SDN-based provider network simulator prototype. SDNSandbox is an open-source, easy-to-use, provider-network in-a-laptop simulator. It aims to facilitate the creation of reproducible experiments and large-scale synthetic datasets. In its current prototype form, it uses a basic traffic generator module alongside real-world provider topologies. SDNSandbox allows users to simulate provider networks, enabling them to conduct research in the field and examine practical applications. To demonstrate SDNSandbox, we use the prototype to simulate basic traffic conditions over several topologies. We then feed the generated datasets to DCRNN, a Convolutional Neural Network (CNN) traffic patterns prediction module. We adapt DCRNN to accept SDNSandbox output and show that it can predict traffic conditions at various points within the network tens of seconds into the future. We further compare its performance with other baseline algorithms. Our results demonstrate that SDNSandbox can also be used as a testbed for a digital twin, creating datasets that are hard to replicate in production networks. It also serves as a demonstration of the framework’s power and versatility as a modular research tool.}
}
@article{MORANDE2022100124,
title = {Enhancing psychosomatic health using artificial intelligence-based treatment protocol: A data science-driven approach},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {2},
pages = {100124},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000672},
author = {Swapnil Morande},
keywords = {Healthcare services, Data science, Machine learning, Psychosomatic health, Well-being},
abstract = {The present study opens an avenue for an improved holistic state of health. Several parameters related to an individual's cognitive interactions to manage stress have been explored. It induces multiple therapeutic interventions in an experimental setting to fulfill the same. This is a quantitative study in which relevant data are captured using a questionnaire and Brain-Computer Interaction (BCI) simultaneously. The research findings establish the role of significant factors that help to determine psychosomatic health. Data-driven models built using unique cognitive instances can augment psychosomatic health in terms of time, cost, and access. Since this study is based on a technological foundation defined by an electroencephalograph (EEG), it can further be scaled for other healthcare applications and incorporated into ‘Digital Twin.’ Using Artificial Intelligence (AI) based treatment protocol, the findings could be extended to the healthcare ecosystem.}
}
@article{JI202329,
title = {Challenges and Opportunities in Product Life Cycle Management in the Context of Industry 4.0},
journal = {Procedia CIRP},
volume = {119},
pages = {29-34},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004225},
author = {Xinxiang Ji and Shiva Abdoli},
keywords = {Product-system, Lifecycle Management, Industry 4.0, Digital Twin},
abstract = {Global competition forces assembly, production, and logistic systems to be highly efficient and agile in delivering their product-systems to customers. To achieve this, Product Lifecycle Management (PLM) and Product Data Management approaches have been developed for managing product-related information across its lifecycle. However, the full potential of PLM has yet to be realized. Industry 4.0 refers to the fourth industrial revolution which promotes the usage of information and digital production with key enabling technologies including digital twins, internet of things, cloud technologies and machine learning. This paper aims to investigate opportunities that industry 4.0 technologies can offer to realize PLM by reviewing existing scholarly papers in this context. The characteristics of industry 4.0 technologies and their usage on PLM is also discussed. As a result of such critical analysis and cross-investigation, this paper discusses the challenges with PLM and proposes future research directions and a roadmap for realization of an integrated PLM by application of key industry 4.0 technologies.}
}
@article{SANTILLANCOOPER2022103104,
title = {Predicting future sedentary behaviour using wearable and mobile devices},
journal = {Information Processing & Management},
volume = {59},
number = {6},
pages = {103104},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103104},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322002059},
author = {Martín {Santillán Cooper} and Marcelo G. Armentano},
keywords = {Sedentary behaviour prediction, Machine learning, User modelling, Wearable and mobile devices},
abstract = {Sedentarism is a common problem that can affect human health and wellbeing. Predicting sedentary behaviour is an emerging area that can benefit from data collected from sensors available in ubiquitous devices, such as wearables and smartphones. In this paper, we present an approach aiming at predicting the sedentary behaviour of a user from data collected from sensors installed in wearable/mobile devices. We compare personal and impersonal models using a real-life dataset consisting of sensing data of 48 users during 10 weeks. We found that impersonal models using Deep Neural Networks were able to accurately predict the subject’s future sedentary behaviour.}
}
@article{SOTIRIADIS20232038,
title = {A Digital Twin Assisted and Embedded Strain Gauge Monitoring System},
journal = {Transportation Research Procedia},
volume = {72},
pages = {2038-2045},
year = {2023},
note = {TRA Lisbon 2022 Conference Proceedings Transport Research Arena (TRA Lisbon 2022),14th-17th November 2022, Lisboa, Portugal},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.11.686},
url = {https://www.sciencedirect.com/science/article/pii/S2352146523009845},
author = {George Sotiriadis and Thanasis Kotzakolios and Vassilis Kostopoulos and Maria Gemou},
keywords = {Strain monitoring, Digital Twin, Machine Learning, Finite Element},
abstract = {In this work, a strain monitoring system (strip) for assessing the road pavement distress under vehicle loads was developed. The system consists of the sensing element, the data processing and storage unit, and a graphical user interface with post- processing features. The sensing elements were designed to be adhesively bonded on the pavement and are protected by an encapsulating plastic strip. Strain data are sent to a digital reconstruction (the Digital Twin) of a real-life asset (the pavement model) that is frequently and automatically updated through data sampling. This tool provides functionalities to monitor and optimize assets and make informed and data-based decisions, in the context of day-to-day operative conditions and after extreme events. These data not only include sensor data, but also regularly revalidated structural reliability indices formulated on the grounds of the frequently updated Digital Twin model.}
}
@article{CAO2022112347,
title = {Digital twin real time monitoring method of turbine blade performance based on numerical simulation},
journal = {Ocean Engineering},
volume = {263},
pages = {112347},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.112347},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822016419},
author = {Yu Cao and Xiaobo Tang and Oleg Gaidai and Fang Wang},
keywords = {Real time, Digital twin, HATT, Kriging interpolation, Machine learning},
abstract = {Due to the limited number of sensor arrangements, the hydrodynamic performance test and detection of marine equipment cannot achieve real time monitoring and complete coverage of the flow field. The digital twin (DT) technology can solve this problem and help achieve real time monitoring and performance evaluation at sea. This paper takes the horizontal axis tidal turbine (HATT) as the research object, studies the applicability of the simulation-based DT real time monitoring method. Firstly, the feasibility of computational fluid dynamics (CFD) simulation of HATT is verified by experiments. Secondly, the simulation database is established under various working conditions. Then the DT method is used to reduce the three-dimensional numerical model to a first-order digital model and the simulation result data can be quickly loaded into the digital model for real time data monitoring. If the monitoring data is not calculated in the database, the Kriging interpolation method is used to reconstruct the database for flow field display quickly. For the data with large deviation of comparison result curves, the optimization algorithm is used along with machine learning. A real time monitoring engineering reference is provided for flow fields distribution and hydrodynamic performance assessment of blades.}
}
@article{FERRIOLGALMES2022109329,
title = {Building a Digital Twin for network optimization using Graph Neural Networks},
journal = {Computer Networks},
volume = {217},
pages = {109329},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109329},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622003681},
author = {Miquel Ferriol-Galmés and José Suárez-Varela and Jordi Paillissé and Xiang Shi and Shihan Xiao and Xiangle Cheng and Pere Barlet-Ros and Albert Cabellos-Aparicio},
keywords = {Digital Twin, Graph Neural Networks, Network optimization, Deep Learning, Network modeling},
abstract = {Network modeling is a critical component of Quality of Service (QoS) optimization. Current networks implement Service Level Agreements (SLA) by careful configuration of both routing and queue scheduling policies. However, existing modeling techniques are not able to produce accurate estimates of relevant SLA metrics, such as delay or jitter, in networks with complex QoS-aware queueing policies (e.g., strict priority, Weighted Fair Queueing, Deficit Round Robin). Recently, Graph Neural Networks (GNNs) have become a powerful tool to model networks since they are specifically designed to work with graph-structured data. In this paper, we propose a GNN-based network model able to understand the complex relationship between (i) the queueing policy (scheduling algorithm and queue sizes), (ii) the network topology, (iii) the routing configuration, and (iv) the input traffic matrix. We call our model TwinNet, a Digital Twin that can accurately estimate relevant SLA metrics for network optimization. TwinNet can generalize to its input parameters, operating successfully in topologies, routing, and queueing configurations never seen during training. We evaluate TwinNet over a wide variety of scenarios with synthetic traffic and validate it with real traffic traces. Our results show that TwinNet can provide accurate estimates of end-to-end path delays in 106 unseen real-world topologies, under different queuing configurations with a Mean Absolute Percentage Error (MAPE) of 3.8%, as well as a MAPE of 6.3% error when evaluated with a real testbed. We also showcase the potential of the proposed model for SLA-driven network optimization and what-if analysis.}
}
@article{LIU2023106466,
title = {An explainable knowledge distillation method with XGBoost for ICU mortality prediction},
journal = {Computers in Biology and Medicine},
volume = {152},
pages = {106466},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.106466},
url = {https://www.sciencedirect.com/science/article/pii/S001048252201174X},
author = {Mucan Liu and Chonghui Guo and Sijia Guo},
keywords = {Intensive care units, Mortality prediction, Knowledge distillation, Explainable machine learning},
abstract = {Background and Objective:
Mortality prediction is an important task in intensive care unit (ICU) for quantifying the severity of patients’ physiological condition. Currently, scoring systems are widely applied for mortality prediction, while the performance is unsatisfactory in many clinical conditions due to the non-specificity and linearity characteristics of the used model. As the availability of the large volume of data recorded in electronic health records (EHRs), deep learning models have achieved state-of-art predictive performance. However, deep learning models are hard to meet the requirement of explainability in clinical conditions. Hence, an explainable Knowledge Distillation method with XGBoost (XGB-KD) is proposed to improve the predictive performance of XGBoost while supporting better explainability.
Methods:
In this method, we first use outperformed deep learning teacher models to learn the complex patterns hidden in high-dimensional multivariate time series data. Then, we distill knowledge from soft labels generated by the ensemble of teacher models to guide the training of XGBoost student model, whose inputs are meaningful features obtained from feature engineering. Finally, we conduct model calibration to obtain predicted probabilities reflecting the true posterior probabilities and use SHapley Additive exPlanations (SHAP) to obtain insights about the trained model.
Results:
We conduct comprehensive experiments on MIMIC-III dataset to evaluate our method. The results demonstrate that our method achieves better predictive performance than vanilla XGBoost, deep learning models and several state-of-art baselines from related works. Our method can also provide intuitive explanations.
Conclusions:
Our method is useful for improving the predictive performance of XGBoost by distilling knowledge from deep learning models and can provide meaningful explanations for predictions.}
}
@article{GHENAI2022102837,
title = {Recent trends of digital twin technologies in the energy sector: A comprehensive review},
journal = {Sustainable Energy Technologies and Assessments},
volume = {54},
pages = {102837},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102837},
url = {https://www.sciencedirect.com/science/article/pii/S2213138822008852},
author = {Chaouki Ghenai and Lama Alhaj Husein and Marwa {Al Nahlawi} and Abdul Kadir Hamid and Maamar Bettayeb},
keywords = {Digital twin, Energy, Renewable energy, Energy supply, Energy demand, Energy storage, Digitalization, Energy forecasting, Energy optimization, Energy management, IoT},
abstract = {The purpose of a digital twin (DT) is to gain insight into and predict the performance of a physical product, process, or piece of infrastructure. Numerous advantages accrue from the energy industry's adoption of DT technology, such as improved asset performance, higher profits and efficiencies, and less harmful effects on the environment. This paper's goal is to present a literature evaluation that classifies DT principles, usage patterns, and benefits in the energy sector. A thorough literature review covering the past decade of studies on DT in the energy sector was conducted. The originality of this study is in-depth examination of DT's use across the whole energy value chain from power generation and storage to energy usage in buildings, transportation, and industrial applications. From this analysis, it was clear that there is a growing interest in using DT in the energy industry and minimizing energy use is the primary focus of the literature on digital twins. Growth of DT technologies will be aided by recent developments in machine learning and artificial intelligence, as well as the development of more sophisticated control systems, allowing for the enhancement of energy system efficiency and effectiveness, thereby fostering the clean energy transition, and reshaping the future of energy.}
}
@article{SEPAHVAND2023105560,
title = {An adaptive teacher–student learning algorithm with decomposed knowledge distillation for on-edge intelligence},
journal = {Engineering Applications of Artificial Intelligence},
volume = {117},
pages = {105560},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105560},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622005504},
author = {Majid Sepahvand and Fardin Abdali-Mohammadi and Amir Taherkordi},
keywords = {Deep learning, Knowledge distillation, On-edge intelligence, Feature representation, Tensor decomposition},
abstract = {In case the spatial shape of the feature maps of the teacher in feature-based knowledge distillation (KD) is significantly greater than the student model, first, they cannot be compared directly. Second, the knowledge of these complex feature maps cannot be quite apprehensible for the student. This paper proposed a new KD, in which Tucker decomposition was used to decompose the large-dimension feature maps of a teacher to obtain core tensors from the feature maps of the teacher. The knowledge of these tensors can be easily understood by students due to their low complexity. Furthermore, in the proposed KD, an adaptor function is suggested, which balances the spatial shape of the core tensors of the teacher and student and helps compare them using a convolution regressor. Finally, a hybrid loss based on adaptor function is suggested to distill the knowledge of the core tensors of the teacher to the student. Both teacher and student models were implemented on smartphones used as edge devices, and the experiments were evaluated in terms of recognition rate and complexity. According to the results, the student model designed by ResNet-18 architecture has ∼65.44 million fewer parameters, ∼6.45 GFLOPs less computational complexity, ∼1.12 G less GPU memory use, and ∼265.67 times greater compression rate than its teacher model designed by ResNet-50 architecture. While the recognition rate of the student model merely dropped down to 1.5% in the benchmark dataset.}
}
@article{SUN2023100120,
title = {Effects of integrating an open learner model with AI-enabled visualization on students' self-regulation strategies usage and behavioral patterns in an online research ethics course},
journal = {Computers and Education: Artificial Intelligence},
volume = {4},
pages = {100120},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100120},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000753},
author = {Jerry Chih-Yuan Sun and Hsueh-Er Tsai and Wai Ki Rebecca Cheng},
keywords = {Personalized learning, Self-regulation, Data visualization, Sequential analysis, Open learner model},
abstract = {This study seeks to understand the effects of a personalized learning platform, which applied an open learner model with self-regulation and AI-enabled data visualization features, on students' self-regulated learning strategies used and behavioral patterns during their self-directed online learning process. Results were based on a self-regulated learning scale and a prior knowledge test from 182 university students and supplemented with students' system logs to understand their behavioral patterns while interacting with the personalized learning platform. It showed that the combined self-regulation and data visualization features (dual features) improve student performance via self-regulated activities of goal setting and help-seeking. In addition, the dual features improve students' self-regulation behavior of self-assessment and motivate their self-directed learning, reflected by more frequent review of learning content after checking their performance progress charts. Therefore, this study showed that combining self-regulation strategies with data visualization can effectively enhance self-regulated learning behaviors, in which students were able to govern their learning from the feedback provided through data visualization. The results also inferred that the graphical representation using a radar chart provides an intuitive interface with low cognitive affordance to interpret learning analytics data.}
}
@article{VIVIANO2023691,
title = {Synthetic Learner: Model-free inference on treatments over time},
journal = {Journal of Econometrics},
volume = {234},
number = {2},
pages = {691-713},
year = {2023},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2022.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S030440762200152X},
author = {Davide Viviano and Jelena Bradic},
keywords = {Synthetic control, Difference in differences, Causal inference, Random Forests},
abstract = {Understanding the effect of a particular treatment or a policy pertains to many areas of interest, ranging from political economics, marketing to healthcare. In this paper, we develop a non-parametric algorithm for detecting the effects of treatment over time in the context of Synthetic Controls. The method builds on counterfactual predictions from many algorithms without necessarily assuming that the algorithms correctly capture the model. We introduce an inferential procedure to detect treatment effects and show that the testing procedure controls size asymptotically for stationary, beta mixing processes without imposing any restriction on the set of base algorithms under consideration. We discuss consistency guarantees for average treatment effect estimates and derive regret bounds for the proposed methodology. The class of algorithms may include Random Forest, Lasso, or any other machine-learning estimator. Numerical studies and an application illustrate the advantages of the method.}
}
@incollection{ZOU202361,
title = {Chapter 4 - Optimization-based meta-learning approaches},
editor = {Lan Zou},
booktitle = {Meta-Learning},
publisher = {Academic Press},
pages = {61-87},
year = {2023},
isbn = {978-0-323-89931-4},
doi = {https://doi.org/10.1016/B978-0-323-89931-4.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323899314000018},
author = {Lan Zou},
keywords = {Optimization-based meta-learning, LSTM meta-learner, Meta-LSTM, Reptile, Model-agnostic meta-learning, MAML, First-order model-agnostic meta-learning, FOMAML, Transfer learning, Fine-tuning},
abstract = {This chapter examines the benefits of the state-of-the-art approaches to optimization-based meta-learning: classical long short-term memory (LSTM) meta-learner; model-agnostic meta-learning (MAML) and its variations in few-shot learning, reinforcement learning, and intimation learning; first-order model-agnostic meta-learning; and Reptile. It recaps the motivation behind LSTM meta-learner: covariate shift, batch normalization, LSTM cell, and classical gradient-based optimization. Then, it discusses the intuition of transfer learning and fine-tuning for MAML and MAML's key concepts: adaptation and MAML. Finally, this chapter focuses on several scenarios of Reptile: serial version, parallel, or batch version. It also explains several creative techniques in Reptile: the optimization assumption, minimization of the expected loss function, maximization of generalization within a task, and three modifications of Reptile algorithm. This chapter ends with a precise comparison of these three main methods of optimization-based meta-learning.}
}
@article{HE2023108874,
title = {Structural performance prediction based on the digital twin model: A battery bracket example},
journal = {Reliability Engineering & System Safety},
volume = {229},
pages = {108874},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108874},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022004914},
author = {Wenbin He and Jianxu Mao and Kai Song and Zhe Li and Yulong Su and Yaonan Wang and Xiangcheng Pan},
keywords = {Predictive monitoring, Structural reliability of battery bracket, Digital twin, Machine learning, Response surface, Finite element simulation},
abstract = {Battery bracket for new energy commercial vehicles is subjected to variable loads and battery temperature changes both during the design road test phase and in-service operation. Therefore, their structural performance must be evaluated in real-time for reliability design and health monitoring. With the rapid development of industrial digitization, the digital twin has become an indispensable technology. This paper proposes a digital twin approach for predictive monitoring of the performance of mechanical structures. Taking the structural performance for the battery bracket of new energy commercial vehicles as an example, this paper builds a unit-level digital twin model—DTMAR. It comprises the numerical model, NN-RSR model, and hybrid machine learning model. The results reveal that the DTMAR model can efficiently and accurately calculate and predict the structural performance. This can not only provide constructive guidance for optimal design of the next generation product structure, but also aid in evaluating the structural reliability of the battery bracket of new energy commercial vehicles and improve their driving safety.}
}
@article{SEIDEL2023103831,
title = {Development and validation of a digital twin framework for SMT manufacturing},
journal = {Computers in Industry},
volume = {145},
pages = {103831},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103831},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522002275},
author = {Reinhardt Seidel and Ben Rachinger and Nils Thielen and Konstantin Schmidt and Sven Meier and Jörg Franke},
keywords = {Electronics production, Surface mount technology, Machine learning, Digital twin, Data analysis, Design for manufacturing},
abstract = {Electronics manufacturing is a global industry and key to innovation in the virtual world because it is the physical backbone. The cost-effective development and manufacture of such electronic modules are critical to maintaining the competitiveness of high-wage manufacturing countries. Therefore, two approaches suggest themselves: Design for Manufacturing (DfM) and manufacturing optimization. To apply both approaches in industry, it is necessary to introduce models that describe the relationship between process input and process output. Applied to electronics manufacturing processes, this means that design, process parameters, and material properties must be mapped to process quality criteria in end-of-line testing. Recently, machine learning (ML) algorithms have been emerging and dominating other modeling methods such as numerical simulation. To develop such complex ML models, a unified data structure for each input and output must be defined. This paper proposes an extensible ML-enabled framework that provides direct and structured access to the printed circuit board (PCB) design and process parameters. This framework is used to perform structured data acquisition using a custom data mining board. During the manufacturing of these PCBs on a full surface mount technology (SMT) process line, all available process machine-level data is collected, archived, and parsed into a uniform, standardized, and flat data structure. This enables fast analysis of the correlation between inputs and quality criteria, direct access by ML algorithms, and training of models. Through these measures, the quality of the framework is validated and correlations are revealed and compared to previous literature. This shows the great importance of a data framework for the integration of data analysis technologies into industrial processes.}
}
@article{BARAN202259,
title = {Semantics-driven attentive few-shot learning over clean and noisy samples},
journal = {Neurocomputing},
volume = {513},
pages = {59-69},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.121},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222012140},
author = {Orhun Bugra Baran and Ramazan Gokberk Cinbis},
keywords = {Few-shot learning, Vision and language integration},
abstract = {Over the last couple of years, few-shot learning (FSL) has attracted significant attention towards minimizing the dependency on labeled training examples. An inherent difficulty in FSL is handling ambiguities resulting from having too few training samples per class. To tackle this fundamental challenge in FSL, we aim to train meta-learner models that can leverage prior semantic knowledge about novel classes to guide the classifier synthesis process. In particular, we propose semantically-conditioned feature attention and sample attention mechanisms that estimate the importance of representation dimensions and training instances. We also study the problem of sample noise in FSL, towards utilizing meta-learners in more realistic and imperfect settings. Our experimental results demonstrate the effectiveness of the proposed semantic FSL model with and without sample noise.}
}
@article{GAO2023135782,
title = {A digital twin-based approach for optimizing operation energy consumption at automated container terminals},
journal = {Journal of Cleaner Production},
volume = {385},
pages = {135782},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.135782},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622053562},
author = {Yinping Gao and Daofang Chang and Chun-Hsien Chen},
keywords = {Sustainable development, Energy consumption, Digital twin, Automatic stacking crane scheduling, Container yard},
abstract = {The sustainable development of port operation management is strongly related to the energy consumption of production at automated container terminals (ACTs). This paper focuses on the production activities at a container yard, which is the primary facility of ACTs. A digital twin-based approach is proposed to optimize the operation of an automatic stacking crane (ASC) handling containers in terms of energy consumption. A virtual container yard that syncs with a physical container yard in the ACT digital twin system for observation and validation is developed. A mathematical model is established to minimize the total energy consumption of completing all tasks. Then, the Q-learning algorithm is adapted to optimize a solution based on the operating data from the ACT digital twin system. Numerical experiments are conducted to demonstrate the effectiveness of the proposed approach by comparing it with two other solution algorithms, viz., genetic algorithm (GA) and particle swarm optimization (PSO). The total energy consumption of two operation strategies (i.e., centralized and decentralized) are also compared using the proposed digital twin-based approach. With digital twin, the operational environment and energy consumption are visualized to support optimization and management of ASCs. Managers and operators can choose an appropriate strategy according to the designated sustainable goal.}
}
@article{MAES2022110282,
title = {Features and defects characterisation for virtual verification and certification of composites: A review},
journal = {Composites Part B: Engineering},
volume = {246},
pages = {110282},
year = {2022},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2022.110282},
url = {https://www.sciencedirect.com/science/article/pii/S1359836822006552},
author = {Vincent K. Maes and Kevin Potter and James Kratz},
keywords = {Defects, Characterisation, Virtual testing, Certification},
abstract = {Composite manufacturing is driven by a balance between costs (i.e. material and time) and quality. Due to the brittle nature of composite materials, even small deviations in the parts (i.e. defects) can result in significant reductions in load carrying ability of a part. The occurrence of defects is a complex problem, with many sources and factors which affect them. To assist in better understanding and predicting part quality, statistical tools and advanced machine learning can be used to help fill the gaps. A solid understanding of part quality can then in turn be used in combination with a digital twin to achieve virtual testing and certifcation of a part while requiring less physical tests. However, as this review shows, the available data in the literature does not sufficiently characterise key defects, nor their dependence on part design and process parameters to achieve this goal. As such it is argued here that enhanced characterisation and manufacturing trials of more complex parts are needed to generate the required database.}
}
@article{ZHOU2023345,
title = {A Machine-Learning-based Surrogate Modeling Methodology for Submodel Integration in the Holistic Railway Digital Twin Platform},
journal = {Procedia CIRP},
volume = {119},
pages = {345-350},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.02.141},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004997},
author = {Shiyang Zhou and Alexander Meierhofer and Ozan Kugu and Yuxi Xia and Manfred Grafinger},
keywords = {Surrogate Model, Multibody Dynamics Simulation, Machine Learning, Railway Vehicle-Track System},
abstract = {A holistic railway infrastructure digital twin (DT) platform is sophisticated and consists of a series of submodels (e.g., turnouts, tracks, vehicles, etc.) that are built through various methodologies and software. However, integrating these submodels into the DT platform is tremendously challenging due to considerable computational complexity, software and interface restrictions. To this end, we designed a machine learning (ML) based surrogate modeling methodology for the submodel integration in the holistic railway infrastructure DT platform and illustrated the methodology through a case study. In this case study, an ML-based surrogate model for multibody simulation of railway vehicle-track dynamics is created, which can replace the railway vehicle-track simulation executed with the Multibody Dynamics (MBD) Simulation commercial software SimPACK. The well-built ML model can accurately and quickly predict the vehicle-track system's dynamic responses to different track irregularities. Besides, the integration process of the ML-based surrogate model into the DT platform through a standardized open-source Functional Mock-up Interface (FMI) is also proposed. The developed surrogate modeling methodology shows great promise owing to its high fidelity, which is verified by the measurement data collected from the Austrian national railway track system. The main contribution of our work lies in the well-built ML-based surrogate modeling methodology for reducing the computation complexity and time of different submodels, which facilitates the unification and integration of different submodels. Furthermore, this approach can also be applied to other submodels and help to build the holistic railway DT platform collaboratively.}
}
@article{LI2023100441,
title = {Graph-powered learning methods in the Internet of Things: A survey},
journal = {Machine Learning with Applications},
volume = {11},
pages = {100441},
year = {2023},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100441},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022001165},
author = {Yuxi Li and Shuxuan Xie and Zhibo Wan and Haibin Lv and Houbing Song and Zhihan Lv},
keywords = {Internet of Things, Graph-powered learning methods, Graph embedding, Graph neural network, Graph convolution network, IoT security},
abstract = {The trend of the era of the Internet of Everything has promoted the integration of various industries and the Internet of Things (IoT) technology, and the scope of influence of the IoT is developing in a wider and deeper level. With the extension of the fields involved, the in-depth progress of the IoT is facing a bottleneck. For example, the security of IoT network and software have problems that are difficult to reconcile. Graph-powered learning methods such as graph embedding and graph neural network (GNN) are expected. How to use the graph learning method in IoT is a question that has to be discussed in relation to the future of the Internet of Things. This paper comprehensively discusses related research and summarizes the progress of using graph-powered learning to promote the network anomaly detection, malware detection, IoT device management, service recommendation and other aspects of IoT. And discuss the results of using graph theory and graph-powered learning methods according to the IoT fields such as smart transportation, Industrial Internet of Things (IIoT), Social Internet of Things (SIoT), smart medical care, smart home, smart grid, and smart city. Finally, in view of the existing issues and trends, this paper proposes future research directions including city various predictions, dynamics and heterogeneity, semantic analysis, resource consumption, point cloud, digital twins, and remote sensing.}
}
@article{SONG2022119995,
title = {Online autonomous calibration of digital twins using machine learning with application to nuclear power plants},
journal = {Applied Energy},
volume = {326},
pages = {119995},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119995},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922012521},
author = {Houde Song and Meiqi Song and Xiaojing Liu},
keywords = {Nuclear power plant, Digital twin, Online calibration, K-means cluster, Artificial neural networks},
abstract = {As a near-zero carbon emission energy source, nuclear energy plays an important role in the current world energy decarbonization scenario. Digital twin is a key technology for the continued development of nuclear energy applications. The digital twin requires real-time, high-precision simulations that are beyond the capabilities of current nuclear energy system simulation programs. Therefore, this study proposes an autonomous calibration method for the digital twin of nuclear power plants to compensate for the error in the results of the low accuracy digital twin that can run quickly to obtain higher accuracy results to meet both high accuracy and real-time requirements. The proposed method consists of offline and online stages. In the offline stage, digital twin simulations are first performed. The simulated data and corresponding measurements data (or real data) are used to build an error database, which will be used for the next step of data-driven model training. To reduce the complexity of calibration model, the error database samples are then grouped by clustering. Data-driven calibration models are built on each group based on the simulated data and errors. In the online stage, the digital twin runs in parallel with the nuclear power plant and receives real-time data. The calibration model is continuously updated using dynamic error database. The feasibility of the new proposed method has been demonstrated on measured data from the PKLIII B3.1 steam generator pipe rupture (SGTR) experiment. The results showed that the physical quantities such as pressure, temperature and mass flow rate were well calibrated during the 1000 s of parallel running. The R2 of all physical quantities including temperature, flow rate, and pressure are above 0.99.}
}
@article{CURRIE2023457,
title = {Radiation Dosimetry, Artificial Intelligence and Digital Twins: Old Dog, New Tricks},
journal = {Seminars in Nuclear Medicine},
volume = {53},
number = {3},
pages = {457-466},
year = {2023},
note = {Hematology},
issn = {0001-2998},
doi = {https://doi.org/10.1053/j.semnuclmed.2022.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0001299822000940},
author = {Geoffrey M. Currie and Eric M. Rohren},
abstract = {Developments in artificial intelligence, particularly convolutional neural networks and deep learning, have the potential for problem solving that has previously confounded human intelligence. Accurate prediction of radiation dosimetry pre-treatment with scope to adjust dosing for optimal target and non-target tissue doses is consistent with striving for improved the outcomes of precision medicine. The combination of artificial intelligence and production of digital twins could provide an avenue for an individualised therapy doses and enhanced outcomes in theranostics. While there are barriers to overcome, the maturity of individual technologies (i.e. radiation dosimetry, artificial intelligence, theranostics and digital twins) places these approaches within reach.}
}
@article{TAKAKURA20236174,
title = {A Log-Likelihood-Based Evaluation Metric for the Reproducibility and Simplicity of Logistics Graphs},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {6174-6180},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.731},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323011084},
author = {Yuriko Takakura and Junichi Mori and Hirokazu Kobayashi},
keywords = {Machine learning and data analytics in process control, Digital twins for power and process systems, Data visualization, Logistics in manufacturing, Production & logistics over manufacturing networking},
abstract = {Logistics are sometimes complex and usually entail the interrelation of different processes. It is beneficial to visualize actual process-flow logs to better understand the underlying processes. However, it is difficult to analyze logistics using graph visualization as the graphs are typically quite large and complex. In the field of process mining, several metrics have been proposed in previous studies to examine the quality of process models created using process mining algorithms. However, these metrics evaluate the efficacy of the graph in terms of reproducing the actual process-flow logs. Thus, there is often process-flow information loss in the resulting graph structure when nodes or edges are aggregated to simplify the graph. To address these limitations, in this paper, we propose a maximum log-likelihood-based metric for measuring the reproducibility of graphs and define the concept in terms of how much of the actual process-flow information the graph retains. To obtain the metrics, we developed a graph model that can generate process-flow logs according to its probability parameters. In addition, we also developed an evaluation metric, which is a weighted sum of both the log-likelihood and model dimensions. An empirical evaluation was conducted using the actual process-flow patterns of steel-making process data. The results revealed that the maximum log-likelihood-based metric effectively evaluated the process by which the graphs with node and edge aggregations reproduced the actual process-flow patterns.}
}
@article{PEDROSOUZADEOLIVEIRA2022104930,
title = {Coupling a neural network technique with CFD simulations for predicting 2-D atmospheric dispersion analyzing wind and composition effects},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {80},
pages = {104930},
year = {2022},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2022.104930},
url = {https://www.sciencedirect.com/science/article/pii/S0950423022002066},
author = {João {Pedro Souza de Oliveira} and Joao {Victor Barbosa Alves} and João {Neuenschwander Escosteguy Carneiro} and Ricardo {de Andrade Medronho} and Luiz {Fernando Lopes Rodrigues Silva}},
abstract = {The Computational Fluid Dynamics (CFD) tool has a remarkable applicability for the prediction of gas dispersion flows by numerically solving the proper governing equations in realistic scenarios. Depending on the problem complexity, undesirably high computational costs can be incurred, which has encouraged the combined use of Machine Learning (ML) seeking to attenuate the CFD simulations requirement for multiple scenario studies. The present work aims at demonstrating the employment the coupling between CFD and the Artificial Neural Network (ANN) algorithm for representative problems in atmospheric dispersion in a preliminary assessment. A limited set of CFD simulation results was used for training neural networks, whose output is given by flow field interpolators, the potential uses of which include digital twin designing and optimization procedures. One possible strategy is the local approach, which treats the network as a transition rule in the scope of Cellular Automata (CA) modeling, allowing it to learn the dynamic behavior of the addressed physics locally. This method gives rise to simpler neural network architectures with closer computing relatively to the CFD calculation. Assessments have been done by predicting, initially, a scalar field time evolution governed by a 1-D advection-diffusion transport equation to verify the method implementation. Subsequently, species concentration distributions were sought in atmospheric dispersion cases from CFD simulations datasets, comprising four case studies followed in the performed analysis, all considering a bidimensional flow domain and a scenario involving methane leaks. The first one indicated an accurate reproduction of subsequent time steps concentration field referring to the displacement of a methane cloud. The second and third cases concerned a plume formation, in transient and steady-state regimes, respectively; their main outcome was the evidence of the CA-ANN methodology's flexibility to address time-dependent and permanent flow simulations interpolation. The last CFD-based case study comprised an additional complexity feature of gas dispersion problems: the wind influence. By redesigning the investigated data-driven approach in terms of ANN's features and labels choice, promising results followed from the analysis with respect to the simultaneous capturing of two global simulation parameters (wind and leakage speeds boundary conditions) in the species concentration field interpolation.}
}
@article{PATIL2023162,
title = {Fairness-driven link scheduling approach for heterogeneous gateways for digital twin enabled industry 4.0},
journal = {International Journal of Intelligent Networks},
volume = {4},
pages = {162-170},
year = {2023},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2023.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666603023000155},
author = {Suvarna Patil and Mandeep Kaur and Katarina Rogulj},
abstract = {The advent of Industry 4.0 has brought with it the integration of digital twin technology, which has enabled businesses to develop a virtual replica of their physical assets. This technology allows businesses to optimize their operations and improve their overall efficiency. However, the successful implementation of digital twin technology in Industry 4.0 heavily relies on the effective utilization of gateways. A significant challenge in gateway utilization is the fair allocation of resources, particularly in heterogeneous environments where gateways have different capabilities. Digital Twin is helping Industry 4.0 vision by connecting the authorized people to the exact data and processes to protect the data/assets from unauthorized access. It is accomplished by connecting sensing devices using a unique addressing system and transmitting their combined data to the Internet of Things (IoT) cloud. Massive volumes of heterogeneous data have resulted from the rapid growth of IoT applications and services. As a result, evaluation of data which affects the Digital Twin enabled industry is studied in this article which focuses on data traffic generated from different Industry 4.0 applications and protection of data along with the industry assets is looked by Digital Twin technology. IoT gateways are currently used to connect the devices from various technologies to the Digital Twins. In such networks, sudden increase in demand of IoT gateways will increase with the increase in IoT devices and the operational cost will also be increased. In the proposed system, low-cost specific gateways are proposed to minimize cost and maximize network performance for protecting assets of smart city through Digital Twin technology. In order to accomplish effective resource allocation in a Digital Twin based infrastructure, data transmission fairness at every gateway is accomplished in an IIoT network by considering link scheduling issues. To address these issues and provide fairness in heterogeneous networks with enhanced data transfer, two steps solution is implemented. The Long Short-Term Memory (LSTM) technique is used in the initial step of traffic prediction to assess the minimal time of prior traffic conditions before being applied to estimate dynamic traffic. In the second step, effective link scheduling and selection are made for each wireless technology, taking into account predicted load, gateway distance, link capacity, and estimated time. More data is transmitted at maximum capacity as a result of improved data transfer fairness for all gateways and then the data is protected by Digital Twin technology. Simulated results show that our suggested strategy performs better than other approaches by obtaining maximum network throughput in Industry 4.0 to provide protective solutions using Digital Twin technology. Index Terms – Internet of Things (IoT), Link Scheduling, Traffic Prediction, Machine Learning (ML), Industry 4.0.}
}
@article{DING2023119060,
title = {Distilling and transferring knowledge via cGAN-generated samples for image classification and regression},
journal = {Expert Systems with Applications},
volume = {213},
pages = {119060},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119060},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422020784},
author = {Xin Ding and Yongwei Wang and Zuheng Xu and Z. Jane Wang and William J. Welch},
keywords = {Knowledge distillation, Unified framework, Conditional generative adversarial networks},
abstract = {Knowledge distillation (KD) has been actively studied for image classification tasks in deep learning, aiming to improve the performance of a student model based on the knowledge from a teacher model. However, applying KD in image regression with a scalar response variable is also important (e.g., age estimation) yet has been rarely studied. Besides, existing KD methods often require a practitioner to carefully select or adjust the teacher and student architectures, making these methods less flexible in practice. To address the above problems in a unified way, we propose a comprehensive KD framework based on conditional generative adversarial networks (cGANs), termed cGAN-KD. Fundamentally different from existing KD methods, cGAN-KD distills and transfers knowledge from a teacher model to a student model via specifically processed cGAN-generated samples. This novel mechanism makes cGAN-KD suitable for both classification and regression tasks, compatible with other KD methods, and insensitive to the teacher and student architectures. An error bound for a student model trained in the cGAN-KD framework is derived in this work, providing a theory for why cGAN-KD is effective as well as guiding the practical implementation of cGAN-KD. Extensive experiments on CIFAR-100 and ImageNet-100 (a subset of ImageNet with only 100 classes) datasets show that the cGAN-KD framework can leverage state-of-the-art KD methods to yield a new state of the art. Moreover, experiments on Steering Angle and UTKFace datasets demonstrate the effectiveness of cGAN-KD in image regression tasks. Notably, in classification, incorporating cGAN-KD into training improves the state-of-the-art SSKD by an average of 1.32% in test accuracy on ImageNet-100 across five different teacher–student pairs. In regression, cGAN-KD decreases the test mean absolute error of a WRN16 × 1 student model from 5.74 to 1.79 degrees (i.e., 68.82% drop) on Steering Angle.}
}
@article{WANG2023109568,
title = {Data coverage assessment on neural network based digital twins for autonomous control system},
journal = {Annals of Nuclear Energy},
volume = {182},
pages = {109568},
year = {2023},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109568},
url = {https://www.sciencedirect.com/science/article/pii/S0306454922005989},
author = {Longcong Wang and Linyu Lin and Nam Dinh},
keywords = {Machine learning, Neural network, Digital twin, Data coverage assessment},
abstract = {In a recently developed Nearly Autonomous Management and Control (NAMAC) system, neural networks (NNs) are used to develop digital twins for diagnosis (DT-Ds). However, NNs are not usually considered extrapolation models and may result in large errors if they are applied to unseen data outside the training data (uncovered). In this study, we propose a data coverage assessment (DCA) to determine if the NN-based DT-Ds are extrapolated based on their epistemic uncertainty. The uncertainty quantification algorithms and uncertainty thresholds are selected based on the confusion matrix of classifying evaluation data into covered or uncovered data. To demonstrate the adaptability of the proposed framework, we applied it to a basic feedforward neural network and a more advanced recurrent neural network based on a more nonlinear database. Case studies show that the proposed framework can distinguish unseen data for both basic and advanced applications with proper uncertainty quantification algorithms and thresholds.}
}
@article{ROSSI202264,
title = {Neural networks and NARXs to replicate extrusion simulation in digital twins for fused filament fabrication},
journal = {Journal of Manufacturing Processes},
volume = {84},
pages = {64-76},
year = {2022},
issn = {1526-6125},
doi = {https://doi.org/10.1016/j.jmapro.2022.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S1526612522006685},
author = {A. Rossi and M. Moretti and N. Senin},
keywords = {Additive manufacturing, Fused filament fabrication, Machine learning, Digital twin, Simulation of the extrusion process, In-process monitoring},
abstract = {In this work we propose the use of nonlinear autoregressive models with exogenous variables (NARXs), powered by dynamic recurrent neural networks, to replicate the results of a previously developed, complex simulation of the extrusion process in fused filament fabrication. The NARXs predict extrusion rate of the extrudate and compression force acting on the filament with an average discrepancy of 0.12 % with respect to the original simulation, but at a fraction of the computational time (0.1 s of the NARX vs. 600 s needed by the original simulation to process the same one-minute time interval). In addition to illustrating how NARXs can be created to mimic an existing simulation, in this work we show how the NARXs can be physically connected to the sensors of a real FFF machine, thus creating an effective digital twin of the extrusion process, useful to support real-time decision making by an AI machine controller. Finally, we show how the implemented digital twin can be used for in-process monitoring, bringing as example the automated detection of an extrusion clogging event.}
}
@article{ZHANG2022105363,
title = {Building Artificial-Intelligence Digital Fire (AID-Fire) system: A real-scale demonstration},
journal = {Journal of Building Engineering},
volume = {62},
pages = {105363},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.105363},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222013699},
author = {Tianhang Zhang and Zilong Wang and Yanfu Zeng and Xiqiang Wu and Xinyan Huang and Fu Xiao},
keywords = {Digital twin, Cyber-physics, IoT, Building fire, Deep learning, Smart firefighting},
abstract = {The identification of building fire evolution in real-time is of great significance for firefighting, evacuation, and rescue. This work proposed a novel framework of Artificial-Intelligence Digital Fire (AID-Fire) that can identify complex building fire information in real-time. The smart system consists of four main parts, Internet of Things sensor network (data collection and transfer), cloud server (data storage and management), AI Engine (data processing), and User Interface (fire information display). A large numerical database, containing 533 fire scenarios with varying fire sizes, positions, and number of fire sources, is established to train a Convolutional Long-Short Term Memory (Conv-LSTM) neural network. The proposed fire digital twin is demonstrated and validated in a full-scale fire test room (26 m2). Results show that the AI engine successfully identify the fire information by learning the spatial-temporal features of the temperature data with a relative error of less than 15% and a delay time of less than 1 s. Moreover, detailed fire development and spread can be accurately displayed in the digital-twin interface. This proposed AID-Fire system can provide valuable support for smart firefighting practices, thus paving the way for a fire-resilient smart city.}
}
@article{RENZI20231228,
title = {Knowledge and Digitalization: a way to improve safety of Road and Highway Infrastructures},
journal = {Procedia Structural Integrity},
volume = {44},
pages = {1228-1235},
year = {2023},
note = {XIX ANIDIS Conference, Seismic Engineering in Italy},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2023.01.158},
url = {https://www.sciencedirect.com/science/article/pii/S2452321623001658},
author = {Emanuele Renzi and Carla Assunta Trifarò},
keywords = {Safety Assessment, Risk Management, Sesmic Risk, Digitalization, Multi-BIM, Collaboration Platforms, Decision making},
abstract = {This paper, with reference to the activity of the National Agency for the Safety of Rails and Road and Highway Infrastructures (ANSFISA), describes the application of digital methods and tools (BIM modeling; collaboration platforms, Machine Learning / Artificial Intelligence, Digital Twin, etc.) for the management of existing road infrastructure networks, on the basis of the principles contained in the current Italian Guidelines for risk classification and management, safety assessment and monitoring of existing bridges.The safety of infrastructures passes through the digitization of processes and tools aimed primarily at the knowledge of the assets, the evaluation of the risks (structural-foundational, seismic, hydrogeological) and the conscious optimal decisions. The optimal decision making have to be based on structured, quantitative and reliable information. The Agency's objective is to improve the digital information management of road infrastructure assets, in order to enhance the information, making it more accessible and with interoperable languages and consequently also provide a monitoring platform, as well as structuring the bases for subsequent modeling also, possibly, to support the training activities of inspectors using both information models and VR simulations in order to create an educational environment in which to train inspectors. The first step to be taken concerns the identification and insertion of data, by the managers, in the IT Archive of Public Works (AINOP). In this work, the Guidelines of the existing bridges have been imagined in an overall framework of digital information management of infrastructures, where BIM, understood as "multi-BIM" according to a multi-level approach, is one of the main tools, with a view to conscious risk mitigation.}
}
@article{MOKHTARI2023109909,
title = {A digital twin-based framework for multi-element seismic hybrid simulation of structures},
journal = {Mechanical Systems and Signal Processing},
volume = {186},
pages = {109909},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109909},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022009773},
author = {Fardad Mokhtari and Ali Imanpour},
keywords = {Seismic hybrid simulation, Digital twin, Model updating, Machine learning, Structural response evaluation},
abstract = {This paper proposes a digital twin-based multi-element hybrid simulation (DMHS) framework to predict the nonlinear cyclic response of structural components (digital twin), e.g., seismic fuses, that are not physically tested due to laboratory limitations by leveraging the experimental test data collected from the physical test specimen (physical twin) during hybrid simulation. This data-based simulation approach can address biased results of hybrid simulation of structures that contain multiple critical components while improving the efficiency of the seismic hybrid simulation. The digital twin is trained in two phases: (1) passive (initial) training phase using past experimental test data before the hybrid simulation starts, and (2) recursive model updating phase using the active (real-time) data produced by the physical specimen during hybrid simulation. The passive training is achieved using the Prandtl–Ishlinskii (PI) hysteresis model combined with the sparse identification technique, while the recursive least-squares algorithm is used in the second phase as the model updating scheme. In particular, the sparse identification technique facilitates the selection of the optimal number of hysteretic model parameters in the passive training phase, which are then tuned in the model updating phase. The architecture of the proposed DMHS framework is first presented, followed by digital twin training steps. The application of the proposed DMHS is then demonstrated, and its simulation accuracy is assessed through virtual hybrid simulation of a two-storey steel buckling-restrained braced frame, which consists of a digital twin (second-storey brace) and a virtual experimental specimen (first-storey brace) integrated into the numerical model of the structure that is subjected to a set of earthquake ground motion accelerations. The results obtained from the verification study serve to validate the proposed architecture of the DMHS framework and evaluate the accuracy and efficiency of this technique in simulating the nonlinear seismic response of structural systems.}
}
@article{HE2022123424,
title = {A deep-learning reduced-order model for thermal hydraulic characteristics rapid estimation of steam generators},
journal = {International Journal of Heat and Mass Transfer},
volume = {198},
pages = {123424},
year = {2022},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2022.123424},
url = {https://www.sciencedirect.com/science/article/pii/S0017931022008936},
author = {Shaopeng He and Mingjun Wang and Jing Zhang and Wenxi Tian and Suizheng Qiu and G.H. Su},
keywords = {Reduced-order models, Deep-learning, POD, CFD, Steam generators},
abstract = {Model reduction is a method that maps full-order conservation equations into lower-order subspaces or establish a data-driven surrogate model to reduce the complexity of the entire physical system, which has been widely applied in various fields in recent years. Compared with computational fluid dynamics (CFD) simulations, reduced-order model (ROM) can quickly and instantly obtain simulation results at low cost, which provides an economical alternative approach for the research and design process which need large number of repetitive simulations. In this paper, a deep-learning ROM was developed based on the proper orthogonal decomposition (POD) and machine learning (ML) method. The rapid estimation of two significant thermal hydraulic parameters in steam generator (SG), including the void fraction and temperature, was carried out by ROM. By POD mode analysis, the order for void fraction and temperature field was reduced by 88.3% and 96.7%, respectively. An artificial neural network was trained to reflect the implicit nonlinear mapping relationship between the CFD inputs and feature coefficients. The ROM was validated by comparing the predicted results with refined CFD results. The maximum absolute errors of void fraction and temperature are 0.1 and 0.03 K with speedup on the order of 104, indicating that the developed ROM can quickly and accurately estimate the thermal hydraulic characteristics of SG under different operating conditions. This work may provide a novel approach for the parameter sensitivity analysis and optimization design of SG and give valuable reference for the digital twin and the real-time online monitoring of the SG.}
}
@article{WANG2023115172,
title = {Combined digital twin and hierarchical deep learning approach for intelligent damage identification in cable dome structure},
journal = {Engineering Structures},
volume = {274},
pages = {115172},
year = {2023},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2022.115172},
url = {https://www.sciencedirect.com/science/article/pii/S0141029622012482},
author = {Longxuan Wang and Hongbo Liu and Zhihua Chen and Fan Zhang and Liulu Guo},
keywords = {Cable dome, Deep learning, Digital twin, Damage identification},
abstract = {Accurate identification of structural damage is the most critical step in structural health monitoring. Traditional damage identification strategies are easily interfered by environmental and human factors, resulting in low-accuracy of identification. In this light, a combined digital twin (DT) and hierarchical deep learning (DL) approach for intelligent damage identification in cable dome structures is proposed in this paper. Based on actual engineering cases, a DT model that accurately maps the physical structure of the cable dome is constructed using APDL based on data. A cable dome structure damage sample database is then automatically established through the large-scale finite element analysis of DT. Finally, the damage features of the data samples are extracted using the hierarchical DL framework proposed in this study. Accuracy verification based on cable force confirms that the established DT model can accurately reflect the mechanical state of the physical structure. The identification results of the trained network on a test set demonstrate that the proposed framework can intelligently identify the damage type, damage location, and damage degree in the cable dome structure with a high accuracy and strong robustness. The proposed intelligent damage identification approach is feasible and reliable and can provide a new basis for structural damage identification with broad application prospects.}
}
@article{LI2023128,
title = {Renewable-based microgrids’ energy management using smart deep learning techniques: Realistic digital twin case},
journal = {Solar Energy},
volume = {250},
pages = {128-138},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2022.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X2200901X},
author = {Qinghui Li and Zhigao Cui and Yanping Cai and Yanzhao Su and Bo Wang},
keywords = {Renewable microgrid, Demand response management, Energy management, Deep learning modelling, Optimization and prediction algorithms},
abstract = {In this research study, a novel demand response program (DRP) has been proposed for renewable-based microgrids (MGs) which takes into account the high penetration of tidal units and solar energy as important, prevalent renewable resources in the power systems. To this end, multi-objective problem (MOP) structure is a promising solution to reduce the whole operating costs of the scheduling problem as well as decrease the high risk of failure in electrical power transmission because of component increasing failure rates and extended repair time. The complexities and nonlinearity of the problem necessitate an innovative heuristic solution which is derived from the Grey Wolf optimization algorithm to help for resolving the problem without making any assumptions or compromising precision. This paper also proposes the dynamic 3-phase correction (DPC) formulation to boost the layout convergence ability by increasing the global search features. Through such a modification, the diversity of the members in the algorithm population increases which would result in low computational burden and low possibility of trapping in local optima. Moreover, it is necessary to have a clear and accurate estimation of the output power of renewable sources in the system. Considering that solar irradiance is hard to anticipate, this paper develops a deep learning layout that uses generative adversarial networks (GAN) to forecast the hourly power generation of the tidal and solar agents. GAN model consists of two competing networks which help to enhance the training process by increasing the accuracy of distinguishing real data from fake data. At the end, IEEE standard test system is used to evaluate the efficiency and effectiveness of the suggested multi-layer problem. The simulation results display that the suggested deep model outperforms other well-known algorithms in smart microgrids.}
}
@article{LONG202312,
title = {Diversified branch fusion for self-knowledge distillation},
journal = {Information Fusion},
volume = {90},
pages = {12-22},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001397},
author = {Zuxiang Long and Fuyan Ma and Bin Sun and Mingkui Tan and Shutao Li},
keywords = {Deep learning, Self-knowledge distillation, Diversity loss, Knowledge fusion, Multiple branches},
abstract = {Knowledge distillation improves the performance of a compact student network by adding supervision from a pre-trained cumbersome teacher network during training. To avoid the resource consumption of acquiring an extra teacher network, the self-knowledge distillation designs a multi-branch network architecture with shared layers for teacher and student models, which are trained collaboratively in a one-stage manner. However, this method ignores the knowledge of shallow branches and rarely provides diverse knowledge for effective collaboration of different branches. To solve these two shortcomings, this paper proposes a novel Diversified Branch Fusion approach for Self-Knowledge Distillation (DBFSKD). Firstly, we design lightweight networks for adding to the middle layers of the backbone. They capture discriminative information by global–local attention. Then we introduce a diversity loss between different branches to explore diverse knowledge. Moreover, the diverse knowledge is further integrated to form two knowledge sources by a Selective Feature Fusion (SFF) and a Dynamic Logits Fusion (DLF). Thus, the significant knowledge of shallow branches is efficiently utilized and all branches learn from each other through the fused knowledge sources. Extensive experiments with various backbone structures on four public datasets (CIFAR100, Tiny-ImageNet200, ImageNet, and RAF-DB) show superior performance of the proposed method over other methods. More importantly, the DBFSKD achieves even better performance with fewer resource consumption than the baseline.}
}
@article{SAMAK2023277,
title = {Towards Sim2Real Transfer of Autonomy Algorithms using AutoDRIVE Ecosystem},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {3},
pages = {277-282},
year = {2023},
note = {3rd Modeling, Estimation and Control Conference MECC 2023},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.12.037},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323023704},
author = {Chinmay Samak and Tanmay Samak and Venkat Krovi},
keywords = {Autonomous Vehicles, Mobile Robots, Digital Twins, Sim2Real, Real2Sim},
abstract = {The engineering community currently encounters significant challenges in the development of intelligent transportation algorithms that can be transferred from simulation to reality with minimal effort. This can be achieved by robustifying the algorithms using domain adaptation methods and/or by adopting cutting-edge tools that help support this objective seamlessly. This work presents AutoDRIVE, an openly accessible digital twin ecosystem designed to facilitate synergistic development, simulation and deployment of cyber-physical solutions pertaining to autonomous driving technology; and focuses on bridging the autonomy-oriented simulation-to-reality (sim2real) gap using the proposed ecosystem. In this paper, we extensively explore the modeling and simulation aspects of the ecosystem and substantiate its efficacy by demonstrating the successful transition of two candidate autonomy algorithms from simulation to reality to help support our claims: (i) autonomous parking using probabilistic robotics approach; (ii) behavioral cloning using deep imitation learning. The outcomes of these case studies further strengthen the credibility of AutoDRIVE as an invaluable tool for advancing the state-of-the-art in autonomous driving technology.}
}
@article{PATRA2023108069,
title = {Recent advances in machine learning applications in metabolic engineering},
journal = {Biotechnology Advances},
volume = {62},
pages = {108069},
year = {2023},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2022.108069},
url = {https://www.sciencedirect.com/science/article/pii/S0734975022001653},
author = {Pradipta Patra and Disha B.R. and Pritam Kundu and Manali Das and Amit Ghosh},
keywords = {Neural networks, Knowledge engineering, Supervised learning, Omics datasets, Gene circuits, CRISPR/Cas, Protein engineering, Digital Twin},
abstract = {Metabolic engineering encompasses several widely-used strategies, which currently hold a high seat in the field of biotechnology when its potential is manifesting through a plethora of research and commercial products with a strong societal impact. The genomic revolution that occurred almost three decades ago has initiated the generation of large omics-datasets which has helped in gaining a better understanding of cellular behavior. The itinerary of metabolic engineering that has occurred based on these large datasets has allowed researchers to gain detailed insights and a reasonable understanding of the intricacies of biosystems. However, the existing trail-and-error approaches for metabolic engineering are laborious and time-intensive when it comes to the production of target compounds with high yields through genetic manipulations in host organisms. Machine learning (ML) coupled with the available metabolic engineering test instances and omics data brings a comprehensive and multidisciplinary approach that enables scientists to evaluate various parameters for effective strain design. This vast amount of biological data should be standardized through knowledge engineering to train different ML models for providing accurate predictions in gene circuits designing, modification of proteins, optimization of bioprocess parameters for scaling up, and screening of hyper-producing robust cell factories. This review briefs on the premise of ML, followed by mentioning various ML methods and algorithms alongside the numerous omics datasets available to train ML models for predicting metabolic outcomes with high-accuracy. The combinative interplay between the ML algorithms and biological datasets through knowledge engineering have guided the recent advancements in applications such as CRISPR/Cas systems, gene circuits, protein engineering, metabolic pathway reconstruction, and bioprocess engineering. Finally, this review addresses the probable challenges of applying ML in metabolic engineering which will guide the researchers toward novel techniques to overcome the limitations.}
}
@article{SUN2022e12375,
title = {Instance segmentation using semi-supervised learning for fire recognition},
journal = {Heliyon},
volume = {8},
number = {12},
pages = {e12375},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e12375},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022036635},
author = {Guangmin Sun and Yuxuan Wen and Yu Li},
keywords = {Fire image recognition, Instance segmentation, Deep learning, Semi-supervised learning, Self-training},
abstract = {Fire disaster brings enormous danger to the safety of human life and property, and it is important to identify the fire situation in time through image processing technology. The current instance segmentation algorithms suffer from problems such as inadequate fire images and annotations, low recognition accuracy, and slow inference speed for fire recognition tasks. In this paper, we propose a semi-supervised learning-based fire instance segmentation method based on deep learning image processing technology. We used a lightweight version of the SOLOv2 network and optimized the network structure to improve accuracy. We propose a semi-supervised learning method based on fire features. To reduce the negative impact of error pseudo-labels on the model training, the pseudo-labels are matched by the color and morphological features of flames and smoke at the pseudo-label generation stage, and some images are screened for strong image enhancement before entering the next round of training for the student model. We further exploit the potential of the model with a limited dataset and improve the model accuracy without affecting the inference efficiency of the model. Experiments show that our proposed algorithm can successfully improve the accuracy of fire instance segmentation with good inference speed.}
}
@article{RUB2023102705,
title = {Hydrocephalus classification in brain computed tomography medical images using deep learning},
journal = {Simulation Modelling Practice and Theory},
volume = {123},
pages = {102705},
year = {2023},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2022.102705},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X22001745},
author = {Salsabeel Abu Al Rub and Ahmad Alaiad and Ismail Hmeidi and Muhannad Quwaider and Omar Alzoubi},
keywords = {Healthcare, Hydrocephalus, Big data analytics, Deep learning, Classification, Segmentation},
abstract = {Recent technological advancements, like big data analytics, is driving the growing adoption of cyber-physical systems and digital twins in the area of healthcare. Congenital hydrocephalus is one important example of recent healthcare data analytics. Congenital hydrocephalus is a buildup of excess cerebrospinal fluid (CSF) in the brain at birth. Congenital hydrocephalus can be lethal without treatment and represents an urgent issue in present-day clinical practice. Congenital hydrocephalus has a significant effect on a human entire life since it causes damage to the brain. It is important to accurately diagnose hydrocephalus early, which will help in the early treatment of the infant by a surgical procedure called ventriculoperitoneal (VP) shunt which will reduce the damage caused by hydrocephalus on the brain. Deep Learning is an evolving technology that is currently actively researched in the field of radiology. Compared to the traditional hydrocephalus diagnosing techniques, automatic diagnosing algorithms in deep learning can save diagnosis time, improve diagnosing accuracy, reduce cost, and reduce the radiologist's workload. In this paper, we have used a novel dataset collected from king Hussein medical center hospital in Jordan that consists of CT scans for hydrocephalus and non-hydrocephalus infants, the dataset has gone through multiple stages in preprocessing which are; cropping and filtering, normalization, segmentation (three segmentation techniques have been applied), and augmentation. These data have been used to build deep learning and machine learning models that will help physicians in the early and accurate diagnosing of congenital hydrocephalus which will lead to a decrease in the death rate and brain damage. The results of our models were impressive with a 98.5% accuracy for congenital hydrocephalus classification in infants' brain CT images.}
}
@article{ZHOU2022106790,
title = {Aero-engine gas path system health assessment based on depth digital twin},
journal = {Engineering Failure Analysis},
volume = {142},
pages = {106790},
year = {2022},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2022.106790},
url = {https://www.sciencedirect.com/science/article/pii/S1350630722007579},
author = {Liang Zhou and Huawei Wang and Shanshan Xu},
keywords = {Aero-engine, Depth digital twin, Multi-scale simulation, Data-driven, Health assessment},
abstract = {Aero-engine health assessment is of great significance for accurately understanding the health status of aircraft, supporting maintenance decision-making and ensuring flight safety. However, aero-engine has the characteristics of complex structure, fault coupling and state nonlinearity, coupled with the constraints of many factors such as acquisition means, analysis methods and the limitation of abnormal data. It is difficult to obtain a mapping relationship that fully characterizes its operating status through monitoring information. Therefore, this paper proposes a health assessment method based on depth digital twin, which can be used for real-time monitoring of aero-engine operation state. Firstly, the mechanism model is constructed for the multi-scale simulation of aero-engine gas path system. Combined with the advantages of dynamic learning and self-optimization of deep learning method, the data-driven model for data prediction is constructed, and the two are fused to realize the depth digital twin of aero-engine. Then, the digital twin model is used to simulate the high-dimensional monitoring data generated during the operation of aero-engine. Finally, a multi-scale one-dimensional convolution neural network model (MultiScale1DCNN) is proposed to analyze the simulated data, so as to assess the real-time health status of aero-engine. Through the simulation test of aero-engine sensor data, it is verified that the digital twin model has high reliability. Compared with the traditional simulation model, it has higher accuracy. In the aero-engine health assessment tests, the MultiScale1DCNN model can accurately identify the failure mode and assess the failure level, and has high assessment accuracy. In several assessment tests, the assessment accuracy rate is above 96%. The test results show that the health assessment method can accurately reflect the health status of aero-engine, and has certain real-time performance, which shows that it has high engineering application value.}
}
@article{LOVERDOS2023115256,
title = {Geometrical digital twins of masonry structures for documentation and structural assessment using machine learning},
journal = {Engineering Structures},
volume = {275},
pages = {115256},
year = {2023},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2022.115256},
url = {https://www.sciencedirect.com/science/article/pii/S0141029622013323},
author = {Dimitrios Loverdos and Vasilis Sarhosis},
keywords = {Masonry, Image processing, Watershed transform segmentation, Feature extraction, Structural analysis, Documentation},
abstract = {The generation of numerical models for masonry structures is a timely and costly procedure since it requires the discretization of a large quantity of smaller particles. Similarly, traditional visual inspection involves the cautious consideration of each element on a masonry construction. In both cases, each brick element needs to be considered individually. The work presented in this document aims to alleviate the issues arising from documenting individual masonry units and cracks on a structure using computer vision and convolutional neural networks (CNN). In particular, for the first time a dynamic workflow has been developed in which masonry units and cracks in masonry structures are automatically detected and used for the development of a complete geometric digital twin. The outcome is a collection of space coordinates and geometrical objects that represent the masonry fabric entity and allow the comprehension of the object for documentation and structural assessment. This interoperability between architectural, structural, and structural analysis models paves the way to use engineering to create a smarter, safer, and more sustainable future for our existing infrastructures.}
}
@article{TRIPURA2023115783,
title = {Wavelet Neural Operator for solving parametric partial differential equations in computational mechanics problems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {404},
pages = {115783},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.115783},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522007393},
author = {Tapas Tripura and Souvik Chakraborty},
keywords = {Nonlinear mappings, Operator learning, Wavelet, Wavelet neural operator, Scientific machine learning},
abstract = {With massive advancements in sensor technologies and Internet-of-things (IoT), we now have access to terabytes of historical data; however, there is a lack of clarity on how to best exploit the data to predict future events. One possible alternative in this context is to utilize an operator learning algorithm that directly learns the nonlinear mapping between two functional spaces; this facilitates real-time prediction of naturally arising complex evolutionary dynamics. In this work, we introduce a novel operator learning algorithm referred to as the Wavelet Neural Operator (WNO) that blends integral kernel with wavelet transformation. WNO harnesses the superiority of the wavelets in time–frequency localization of the functions and enables accurate tracking of patterns in the spatial domain and effective learning of the functional mappings. Since the wavelets are localized in both time/space and frequency, WNO can provide high spatial and frequency resolution. This offers learning of the finer details of the parametric dependencies in the solution for complex problems. The efficacy and robustness of the proposed WNO are illustrated on a wide array of problems involving Burger’s equation, Darcy flow, Navier–Stokes equation, Allen–Cahn equation, and Wave advection equation. A comparative study with respect to existing operator learning frameworks is presented. Finally, the proposed approach is used to build a digital twin capable of predicting Earth’s air temperature based on available historical data.}
}
@article{CHAKRABARTY20235500,
title = {Moving Horizon Estimation for Digital Twins using Deep Autoencoders},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {5500-5505},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.207},
url = {https://www.sciencedirect.com/science/article/pii/S240589632300558X},
author = {Ankush Chakrabarty and Abraham P. Vinod and Hassan Mansour and Scott A. Bortoff and Christopher R. Laughman},
keywords = {Learning, state observers, nonlinear systems, Koopman operator, system identification, black-box models},
abstract = {Digital twins have emerged in recent years as black-box, software-based simulation tools that can mirror the behavior of complex dynamical systems. Digital twin simulations generate the same outputs as the target system using internal states; however, these states are not readily available online from the real system. In this paper, we develop a data-driven moving horizon estimation framework capable of using online noisy measurements of the real system in order to estimate digital twin states. Our framework combines the high expressiveness of deep autoencoders with a moving horizon state estimator that accurately predicts the internal state of the black-box digital twin without access to an analytical model of the system dynamics. We demonstrate that our approach outperforms extended and Koopman Kalman filter solutions on a benchmark reverse van der Pol oscillator example.}
}
@article{WANG2023103804,
title = {Simplexity testbed: A model-based digital twin testbed},
journal = {Computers in Industry},
volume = {145},
pages = {103804},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103804},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522002007},
author = {Tiexin Wang and Chao Tan and Lei Huang and Yize Shi and Tao Yue and Zhiqiu Huang},
keywords = {Digital twin, Model-based systems engineering, Testbed, Uncertainty},
abstract = {In the last few years, due to the advances of computation, network, and wide-spread applications of artificial intelligence and machine learning techniques, developing and benefiting from the digital twin technology to connect the virtual to the physical becomes promising and practically feasible; therefore, a lot of attention has been attracted across industries of various domains. However, there lacks of platforms in the academic to facilitate research activities in digital twin technologies. This shortage has non-eligible and negative impact not only on advancing digital twin technologies, but also leading to scarcely publicly-available datasets for conducting research. To this end, in this paper, we present Simplexity Testbed. Simplexity Testbed is equipped with a physical model — an indoor ”driving ground” featuring various driving scenarios and surface conditions, and four land rovers (named SiLaRs) of two different types. Most importantly, the digital twin model of Simplexity Testbed currently is an integration of multiple models developed with different modeling paradigms (i.e., SysML enhanced with uncertainty information, Modelica, 3D simulators for autonomous driving), which enables model executions, simulations, and cross-model interactions. The architecture of Simplexity Testbed enables future integration of other modeling paradigms. In the paper, we also share our process of developing Simplexity Testbed and lessons learnt. In addition, we put lights on how various research activities can be enabled with Simplexity Testbed. We consider that with Simplexity Testbed, context-aware, autonomous, and adaptive capabilities of digital twins can be studied, which lead to full-fledged applications of digital twins in industry.}
}
@article{BALU202271,
title = {Physics-aware machine learning surrogates for real-time manufacturing digital twin},
journal = {Manufacturing Letters},
volume = {34},
pages = {71-74},
year = {2022},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2022.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S2213846322001845},
author = {Aditya Balu and Soumik Sarkar and Baskar Ganapathysubramanian and Adarsh Krishnamurthy},
keywords = {Real-time digital twin, Physics-aware machine learning, Additive manufacturing, Real-time control},
abstract = {In this Manufacturing Blue Sky idea, we envision a Cyber Adaptive Manufacturing Intelligent System (CyAMIS, pronounced Siamese) that integrates concepts from the emerging area of physics-aware machine learning (ML) to formulate, develop, and deploy a near-real-time Siamese (digital) twin to reliably and efficiently achieve exceptional part quality and desired material properties in additive manufacturing processes. We believe such a real-time digital twin framework to be the future of modern additive manufacturing systems, ultimately leading to Manufacturing 5.0 systems.}
}
@article{BERNARD2022109779,
title = {Improving online education through automatic learning style identification using a multi-step architecture with ant colony system and artificial neural networks},
journal = {Applied Soft Computing},
volume = {131},
pages = {109779},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109779},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622008286},
author = {Jason Bernard and Elvira Popescu and Sabine Graf},
keywords = {Hybrid intelligent systems, Learning styles, Learner model, Learning management systems, Ant colony systems, Artificial neural networks},
abstract = {Learning style is one of the individual differences which play an important role in learning. Being aware of it helps the student to understand their strengths and weaknesses, and the teacher to provide more valuable personalized interventions. Furthermore, learning style-based adaptive educational systems can be designed, which have been shown to increase student satisfaction or learning gain, while reducing the time needed to learn. It is therefore important to have an accurate method for identifying students’ learning styles. Since the traditional approach of filling in dedicated psychological questionnaires has several disadvantages, automatic methods have been proposed, based on investigating student observable behavior in a learning environment. Research done so far generally takes a mono-algorithmic approach to identify learning styles, and the precision rates leave room for improvement. Hence, in this paper we propose a novel hybrid multi-step architecture based on ant colony system and artificial neural networks to increase the precision of learning styles identification. Two different variants are proposed and evaluated with data from 75 students; results show high precision values, outperforming existing automatic approaches for learning style identification. The proposed architecture can be integrated into widely used educational systems (e.g., learning management systems) to provide learners and/or teachers with information about students’ learning styles. In addition, it can be integrated into adaptive educational systems and plugins of learning management systems to automatically identify learning styles and personalize instruction respectively.}
}
@article{GOPAL2023100661,
title = {Digital twin and IOT technology for secure manufacturing systems},
journal = {Measurement: Sensors},
volume = {25},
pages = {100661},
year = {2023},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100661},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422002951},
author = {Lisa Gopal and Harbaksh Singh and Panguluri Mounica and N. Mohankumar and Nagendra Panini Challa and P. Jayaraman},
keywords = {Industry 4.0, IoT, Digital twin, Fault diagnosis},
abstract = {The Digital Twin (DT) method offers new concepts for choosing the context of information technologies and smart production systems. This research focuses on low-price, extremely effective defect diagnosis methods, low-efficiency, high-cost devices to obtain timely feedback and accurate fault detection results, and secure manufacturing systems. The data structure, control plane, and output units are the three components of the manufacturing system that creates a data link between the virtual model using Micro-Electro-Mechanical (MEM) devices and the Zigbee wireless transmission system in the database layer. This study acquired DT information from the control plane using the Internet of Things (IoT) through sensors for secure manufacturing information. It separates and calls the pertinent data by the attribute processor and transfers it to the outcome units. To produce the classification and outcomes of work build features information, the evaluation method analysis the output nodes that split the test set and learning group using a dynamic database. The hybrid IoT with DT technology to examine the consequences of defects detected efficiently predicted and secures the manufacturing system.}
}
@article{WANG2023103171,
title = {Data information processing of traffic digital twins in smart cities using edge intelligent federation learning},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103171},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103171},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322002722},
author = {Weixi Wang and Fan He and Yulei Li and Shengjun Tang and Xiaoming Li and Jizhe Xia and Zhihan Lv},
keywords = {Digital twins cities, Deep learning, Traffic safety, Sign recognization, Edge computing},
abstract = {The present work analyzes the application of deep learning in the context of digital twins (DTs) to promote the development of smart cities. According to the theoretical basis of DTs and the smart city construction, the five-dimensional DTs model is discussed to propose the conceptual framework of the DTs city. Then, edge computing technology is introduced to build an intelligent traffic perception system based on edge computing combined with DTs. Moreover, to improve the traffic scene recognition accuracy, the Single Shot MultiBox Detector (SSD) algorithm is optimized by the residual network, form the SSD-ResNet50 algorithm, and the DarkNet-53 is also improved. Finally, experiments are conducted to verify the effects of the improved algorithms and the data enhancement method. The experimental results indicate that the SSD-ResNet50 and the improved DarkNet-53 algorithm show fast training speed, high recognition accuracy, and favorable training effect. Compared with the original algorithms, the recognition time of the SSD-ResNet50 algorithm and the improved DarkNet-53 algorithm is reduced by 6.37ms and 4.25ms, respectively. The data enhancement method used in the present work is not only suitable for the algorithms reported here, but also has a good influence on other deep learning algorithms. Moreover, SSD-ResNet50 and improved DarkNet-53 algorithms have significant applicable advantages in the research of traffic sign target recognition. The rigorous research with appropriate methods and comprehensive results can offer effective reference for subsequent research on DTs cities.}
}
@article{LU2022953,
title = {A novel stochastic configuration network with iterative learning using privileged information and its application},
journal = {Information Sciences},
volume = {613},
pages = {953-965},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.08.088},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522009987},
author = {Jun Lu and Jinliang Ding},
keywords = {Stochastic configuration network, Iterative learning using privileged information, Sparsity, Alternating optimization},
abstract = {In the real-world applications, some auxiliary information is commonly available in addition to the standard data, which is often ignored by the traditional learning algorithms. To effectively utilize the auxiliary information for assisting in building learner models with improved performance, this paper presents a novel stochastic configuration network (SCN) incorporating the iterative learning using privileged information (LUPI) paradigm, termed ISCN+. Meanwhile, to make the comments generated by the privileged information more closely match the ISCN+, these comments should be iteratively updated as the number of hidden nodes of the SCN increases. To this end, the training of SCN and the learning of comments generated by the privileged information is integrated into a new objective function, and the alternating optimization strategy is adopted to optimize the parameters of ISCN + and update the comments generated by the privileged information. Moreover, the L1/2-norm-regularization-based sparse ISCN+ (S-ISCN+) is proposed to further improve the generalization capacity and reduce the complexity of ISCN+. Furthermore, the convergence analysis of the optimization process is provided. The experimental results on two benchmark data sets and a real-world data set demonstrate the effectiveness of the proposed method.}
}
@article{JIANG2022469,
title = {A multi-dimensional cognitive framework for cognitive manufacturing based on OAR model},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {469-485},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001686},
author = {Tengyuan Jiang and Jingtao Zhou and Jianhua Zhao and Mingwei Wang and Shusheng Zhang},
keywords = {Intelligent manufacturing, Cognitive manufacturing, Cognitive framework, Computable digital twin, Multi-dimensional, OAR model},
abstract = {With the production system shifting to a multi-variety and small-batch production mode, the production process faces more user requirements, changes, and uncertainties. To solve the above problems, it is necessary to obtain the status and trend changes information and provide information support for the optimization of decision-making and dynamic adjustment of the production system. However, the production system cognition faces the problems of state coupling, state dynamic transfer and transition, and multi-system interweaving, which makes the production system cognition face huge challenges. Combining technologies such as the Internet of Things, industrial big data, and artificial intelligence, cognitive manufacturing can realize dynamic cognition of the production process, support dynamic adjustment, and become a promising way to solve the dynamic changes and uncertainties of production systems. In addition, as a formal expression of information processing and knowledge learning process in cognitive informatics, the Object-Attribute-Relation (OAR) model can effectively guide the construction of the production process cognitive mechanism. Therefore, this paper proposes a multi-dimensional cognitive framework based on OAR model of the human cognitive world for the dynamic cognitive needs of production system. The framework carries out dynamic cognition from the three dimensions of the manufacturing unit, production situation, and production system, and builds the continuous cognitive abilities from the three dimensions of analysis, decision-making, and learning. By integrating intelligent algorithms in the fields of artificial intelligence, a computable digital twin model is constructed as a carrier to provide the cognitive enabling technologies and capabilities for the production system. Finally, the feasibility of the proposed framework is illustrated by the developed computational digital twin platform. The computable digital twin platform provides the production system with important cognitive capabilities such as states perception, trend prediction, optimization decision-making, and knowledge learning, to support the dynamic cognition and optimization decision-making of the production system, and lay a technical foundation for adaptive production and cognitive manufacturing.}
}
@article{QI2023119309,
title = {ICD: A new interpretable cognitive diagnosis model for intelligent tutor systems},
journal = {Expert Systems with Applications},
volume = {215},
pages = {119309},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119309},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422023272},
author = {Tianlong Qi and Meirui Ren and Longjiang Guo and Xiaokun Li and Jin Li and Lichen Zhang},
keywords = {Cognitive diagnosis, Learner modeling, Interpretability, Knowledge concept interaction, The quantitative relationship, Potential unknown ability},
abstract = {Numerous models have been proposed for cognitive diagnosis in intelligent tutoring systems. However, the existing models still have room for improvement: (1) they ignore the interaction among knowledge concepts and (2) they ignore the quantitative relation between exercises and concepts. Here, we propose a cognitive diagnostic model comprising three layers of novel neural networks called ICD to solve the above two problems. Specifically, the first layer fits the influence of exercises on concepts, the second layer fits the interaction between concepts, and the third layer fits the influence of concepts on exercises. The three layers allow ICD to effectively distinguish learners with different cognitive levels, that is, ICD has good interpretability. The experimental results show that both the performance and interpretability of ICD are better than those of the latest state-of-the-art CDMs such as RCD, NCDM, and CDGK, and classical CDMs such as DINA and MIRT.}
}
@article{MCLAUGHLIN2023100103,
title = {Utilizing machine learning models to estimate energy savings from an industrial energy system},
journal = {Resources, Environment and Sustainability},
volume = {12},
pages = {100103},
year = {2023},
issn = {2666-9161},
doi = {https://doi.org/10.1016/j.resenv.2022.100103},
url = {https://www.sciencedirect.com/science/article/pii/S2666916122000470},
author = {Eva McLaughlin and Jun-Ki Choi},
keywords = {Compressed air system, Energy efficiency, Supervised machine learning, Compressed air leaks, Industrial energy audits},
abstract = {Energy audits are an important part of reducing energy usage, costs, and carbon emissions, but there have been discrepancies in the quality of audits depending upon the auditor, which can negatively affect the impacts and credibility of the energy assessment. In this paper, historical energy auditing data from a U.S. Department of Energy sponsored research program was gathered and analyzed with a machine-learning algorithm to predict demand savings from a compressed air system assessment recommendation in industrial manufacturing facilities. Different energy auditors calculate savings for repairing leaks in compressed air systems in various ways, so the energy demand savings have been calculated differently throughout the historical assessment recommendations. Machine learning models are utilized in order to enhance the accuracy of the existing practice and reduce variations resulting from the abovementioned discrepancies. A large set of historical assessment recommendation data was used to train five unique machine learning models. Four base learner models and one metalearner model were devised and compared. Results showed that the distributed random forest model best predicted compressed air energy demand savings against the new scenarios within an error of 17%. This indicates that the distributed random forest model can more accurately quantify savings from repairing leaks in compressed air systems. In addition, the results from this study provide insight into the important factors contributing to leaks in the compressed air systems and why it is crucial to repair those leaks regularly to save money and energy while decreasing emissions.}
}
@article{SEPAHVAND2023106476,
title = {Joint learning method with teacher–student knowledge distillation for on-device breast cancer image classification},
journal = {Computers in Biology and Medicine},
volume = {155},
pages = {106476},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.106476},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522011842},
author = {Majid Sepahvand and Fardin Abdali-Mohammadi},
keywords = {Breast cancer images, On-device classification, Knowledge distillation, Teacher-student learning, Lightweight classification},
abstract = {The deep learning models such as AlexNet, VGG, and ResNet achieved a good performance in classifying the breast cancer histopathological images in BreakHis dataset. However, these models are not practically appropriate due to their computational complexity and too many parameters; as a result, they are rarely utilized on devices with limited computational resources. This paper develops a lightweight learning model based on knowledge distillation to classify the histopathological images of breast cancer in BreakHis. This method employs two teacher models based on VGG and ResNext to train two student models, which are similar to the teacher models in development but have fewer deep layers. In the proposed method, the adaptive joint learning approach is adopted to transfer the knowledge in the final-layer output of a teacher model along with the feature maps of its middle layers as the dark knowledge to a student model. According to the experimental results, the student model designed by ResNeXt architecture obtained the recognition rate 97.09% for all histopathological images. In addition, this model has ∼69.40 million fewer parameters, ∼0.93 G less GPU memory use, and 268.17 times greater compression rate than its teacher model. While in the student model the recognition rate merely dropped down to 1.75%. The comparisons indicated that the student model had a rather acceptable outputs compared with state-of-the-art methods in classifying the images of breast cancer in BreakHis.}
}
@article{SU2022100346,
title = {Adaptation of a robotic dialog system for medication reminder in elderly care},
journal = {Smart Health},
volume = {26},
pages = {100346},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2022.100346},
url = {https://www.sciencedirect.com/science/article/pii/S2352648322000800},
author = {Zhidong Su and Weihua Sheng and Guanci Yang and Alex Bishop and Barbara Carlson},
keywords = {Human–robot interaction, Reinforcement learning, Social robot, Elderly care, Dialog adaptation},
abstract = {Social robots can assist older adults in their daily life. Verbal conversation is a natural and convenient way for older adults to interact with social robots. However, most of the existing conversation-based robot services, such as medication reminders, are rule-based systems. These systems require many hand-crafted rules and a significant amount of expert knowledge, therefore they cannot adapt to older adults’ characteristics and dialog history. There are many reinforcement learning (RL) based methods for task-oriented dialogues, but they mainly focus on completing the tasks through text-based conversations. Those methods cannot be directly used for elderly care applications involving human–robot interactions (HRI). Considering the above shortcomings, we proposed a dialog system adaptation method (DSAM) for social robots. The DSAM is based on reinforcement learning which considers the characteristics of older adults, the dialog history and user preference to adapt the dialog policy and improve the dialog module. We implemented DSAM in our custom-made ASCCBot social robot. To evaluate DSAM, we firstly tested the dialog agent which was trained by a user simulator with different settings. The results show that the obtained agent achieves a good result with the desired dialog flow compared to the baseline agent. Based on the obtained dialog policy, the adaptation process is evaluated. The results show that with a good success rate, the number of dialog turns is decreased and the NLU module performance is improved by the adaptation process, which proves the effectiveness of DSAM. We also tested DSAM with human subjects. The results show that the average adaptation success rate is 94.7% and the preference distance reaches 0 after 6 rounds of adaptation while creating reminders successfully with a limited amount of user feedback.}
}
@article{BUCCHIARONE2022471,
title = {Gamification and virtual reality for digital twin learning and training: architecture and challenges},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {6},
pages = {471-486},
year = {2022},
note = {Virtual-reality and intelligent hardware in digital twins B)},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000675},
author = {Antonio Bucchiarone},
keywords = {Digital twins, Virtual reality, Gamification, Learning, Training},
abstract = {Background
Digital Twins are becoming increasingly popular in a variety of industries to manage complex systems. As digital twins become more sophisticated, there is an increased need for effective training and learning systems. Teachers, project leaders, and tool vendors encounter challenges while teaching and training their students, co-workers, and users.
Methods
In this study, we propose a new method for training users in using digital twins by proposing a gamified and virtual environment. We present an overall architecture and discuss its practical realization.
Results
We propose a set of future challenges that we consider critical to enabling a more effective learning/training approach.}
}
@article{YEUNG2022103957,
title = {Keyhole pores reduction in laser powder bed fusion additive manufacturing of nickel alloy 625},
journal = {International Journal of Machine Tools and Manufacture},
volume = {183},
pages = {103957},
year = {2022},
issn = {0890-6955},
doi = {https://doi.org/10.1016/j.ijmachtools.2022.103957},
url = {https://www.sciencedirect.com/science/article/pii/S0890695522001080},
author = {H. Yeung and F.H. Kim and M.A. Donmez and J. Neira},
keywords = {Keyhole pores, Scan strategies, X-ray computed tomography, Digital twins},
abstract = {Keyhole pores are common in additively manufactured parts and can badly deteriorate the part's performance. In this study, we demonstrated that the keyhole pores formation in the laser powder bed fusion additive manufacturing process can be significantly reduced by the constant laser power density scan strategy. The constant laser power density is implemented on a custom-built testbed by continuously varying the laser power with the laser scan speed through the time-stepped digital commands developed. Two cubic nickel alloy 625 parts of identical geometry were built, one with the constant laser power density scan strategy, and another with the conventional constant laser power scan strategy. The X-ray computed tomography (XCT) measurement shows a 67% porosity reduction in the part built with constant laser power density. However, the mechanisms for defect formation are not easily distinguishable in XCT, which gives a ‘total’ count of pores. To further investigate the effect of scan strategies on pore formation, two digital twins of process monitoring (DTPM), meltpool intensity volume (MPIV) and melt pool area volume (MAV), were created. The DTPM not only helps to distinguish the keyhole pores from the lack of fusion defects but also provides a foundation for the future development of machine learning models.}
}
@article{HU202289,
title = {Underwater gas leak detection using an autonomous underwater vehicle (robotic fish)},
journal = {Process Safety and Environmental Protection},
volume = {167},
pages = {89-96},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0957582022007662},
author = {Shuyu Hu and Ao Feng and Jihao Shi and Junjie Li and Faisal Khan and Hongwei Zhu and Jian Chen and Guoming Chen},
keywords = {Autonomous underwater leak detection, Deep learning, Robotic fish, Jetson Nano, Artificial Intelligence, Physical dataset},
abstract = {Gas leaks from subsea oil and gas facilities could cause significant ocean environment damage. Such leaks can cause fire and explosion, for example, a fire on the ocean surface west of Mexico's Yucatan peninsula. Detecting a gas leak is critical in managing fire and explosion risks. This study proposes using autonomous underwater vehicles -robotic fish- for gas leak plume detection. The robotic fish is equipped with advance two well-known deep learning models, Faster RCNN and YOLOV4. A physical experiment system of various sizes of underwater gas leaks is used to generate the benchmark dataset. The results demonstrated the YOLOV4 model has a stronger online real-time capability. It is 43 times faster than the Faster RCNN model with the same level of accuracy. This study verifies the feasibility of integrating deep learning models with the mobile vehicle for real-time autonomous gas leak detection. This contribution will enable the development of a safe and reliable digital twin of subsea emergency management.}
}
@article{JAUHARI2023471,
title = {Modeling of Deep Learning Applications for Chatter Detection in the Milling Process},
journal = {Procedia CIRP},
volume = {118},
pages = {471-476},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.081},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123003050},
author = {Khairul Jauhari and Achmad Zaki Rahman and Mahfudz Al Huda and Muizuddin Azka and Achmad Widodo and Toni Prahasto and Keiji Yamada},
keywords = {Chatter, Digital Twin, Data-driven, Deep Learning, Milling},
abstract = {This study introduced a preliminary investigation of the development of a Digital Twin (DT) model for the milling machining process for the chatter detection phenomenon. Subsequently, chatter has a dynamic interaction in which there is an unstable condition in the material removal process between the cutting tool and work-piece, leading to a decline in surface roughness and tool life, ultimately reducing the quality of machining output. Therefore, this study aimed to develop a chatter detection model using a deep learning application that can identify stable or unstable chatter. The model was built based on the data-driven method where vibration signal data from the milling process is used to train and test various supervised deep learning methods. The result showed that a model with a good level of accuracy was built, and with the help of a chatter detection application, regular operator staff can monitor the machining conditions when no specialist is available.}
}
@article{SIFAT2023100213,
title = {Towards electric digital twin grid: Technology and framework review},
journal = {Energy and AI},
volume = {11},
pages = {100213},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000593},
author = {Md. Mhamud Hussen Sifat and Safwat Mukarrama Choudhury and Sajal K. Das and Md. Hafiz Ahamed and S.M. Muyeen and Md. Mehedi Hasan and Md. Firoj Ali and Zinat Tasneem and Md. Manirul Islam and Md. Robiul Islam and Md. Faisal R. Badal and Sarafat H. Abhi and Subrata K. Sarker and Prangon Das},
keywords = {Electric digital twin grid, Online analysis of grid, Cloud platform of grid, Real-time grid analysis, Self-healing, Cybersecurity},
abstract = {The major hindrances in the energy system are ecological consciousness, lack of clean and sustainable energy management, insufficient energy distribution–transmission–optimization, expensive power transfer costs, and increased customer knowledge of energy charges. Thus why, universal access to the grid with high cybersecurity, and reliability is needed to solve all these challenges. The digital twin concept turns a new dimension of technology into the world. Electric Digital Twin grid can perform online analysis of the grid in real-time and integrates all the past and present data and express the current grid status to the producers and consumers and also predicts the future grid status. Thus, the power grid transmission loss and location of the overheated line and power connection missing can be detected in addition decision-making and self-healing can possible. The future prediction saves the power grid from small to long accidents such as power outages and even blackout problems. The whole consumers and nation feel relief from these types of accidents and saves from large economic and business loss. The blockchain-enabled digital twin grid provides high security for the grid from cyberattacks. The paper conveys the framework of the electric digital twin grid and the concept of the DT grid processing and the way of serving the producer, prosumers, consumers even the whole nation in infrastructure, education, research, economic, business, and political development.}
}
@article{DELCARO202311154,
title = {Dealing with the curse of dimensionality in Twin-in-the-Loop observer design},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {11154-11159},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.834},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323012120},
author = {Giacomo Delcaro and Federico Dettù and Simone Formentin and Sergio M. Savaresi},
keywords = {vehicle dynamics, observer design, bayesian optimization, twin-in-the-loop},
abstract = {Many vehicle dynamics controllers require the knowledge of unmeasured signals, for instance, the sideslip angle in electronic stability control. For this reason, vehicles are usually equipped with several observers running in parallel in different electronic control units. The Twin-in-the-loop approach represents an effective alternative paradigm, in which a single complex Digital Twin is run on-board and a data-driven correction matrix is employed to adjust the estimate of the whole vehicle state in real-time. However, such a complex observer might require the tuning of (too) many parameters if no prior knowledge is available. In this work, we propose an unsupervised learning approach to reduce the dimensionality of the problem, so as to deal also with numerically intractable problems. The strategy is experimentally tested on speed/yaw rate estimation for a real-world vehicle setup.}
}
@article{QIAN2023102456,
title = {Digital twin driven production progress prediction for discrete manufacturing workshop},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {80},
pages = {102456},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102456},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001387},
author = {Weiwei Qian and Yu Guo and Hao Zhang and Shaohua Huang and Litong Zhang and Hailang Zhou and Weiguang Fang and Shanshan Zha},
keywords = {Digital twin model (DTM), Synchronous evolution, Discrete manufacturing workshop (DMW), Production progress (PP), Adaboost, Deep neural networks (DNN), Long-short term memory (LSTM), Competition mechanism},
abstract = {In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. Digital twin technology offers an enabling tool for PP analysis. Although the production process can be observed, analyzed, and controlled in real-time by digital twin model (DTM), there exist some uncertain events, degradation of manufacturing elements, and abnormal disturbance in physical workshop (PW), which would cause the deviation between DTM and PW performance and affect the prediction accuracy of PP. Synchronous evolution of DTM for precision holding to ensure the consistency between DTM and the performance of PW, and guarantee the accuracy of DTM is still a challenging issue, especially when dealing with new dynamic samples for complex production environment of discrete manufacturing workshop (DMW). This article focuses on how to effectively construct DTM synchronous update methods based on dynamic sample data for DMW. This study proposes a representation model of performance degradation and an Adaboost-DNN-LSTM based synchronous update model with competitive election mechanism to enhance the accuracy of PP prediction with time in industrial environment. The experiment is conducted in the realistic production dataset, which demonstrates that the proposed synchronous evolution model has good performance for realizing the synchronization of the performance of physical workshop in industrial environment, and can greatly improve the prediction ability for PP.}
}
@article{ZHOU202356,
title = {AdaDS: Adaptive data selection for accelerating pre-trained language model knowledge distillation},
journal = {AI Open},
volume = {4},
pages = {56-63},
year = {2023},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2023.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S2666651023000074},
author = {Qinhong Zhou and Peng Li and Yang Liu and Yuyang Guan and Qizhou Xing and Ming Chen and Maosong Sun and Yang Liu},
keywords = {Knowledge distillation, Pre-trained language model, Active learning},
abstract = {Knowledge distillation (KD) is a widely used method for transferring knowledge from large teacher models to computationally efficient student models. Unfortunately, the computational cost of KD becomes unaffordable as pre-trained language models (PLMs) grow larger. Computing KD loss on only part of the training set is a promising way to accelerate KD. However, existing works heuristically leverage only one static data selection strategy during the KD process, demonstrating inconsistent improvements across different distillation scenarios. In this work, we conduct a thorough study on various typical data selection strategies for KD, and show that this problem is due to the fact that the best data selection strategy is specific to various factors, including task, selected data size, and training stage. To automatically adapt to these factors, we propose a framework named AdaDS to learn to choose the data selection strategy adaptively during the KD process. Experimental results show that our proposed method is effective for various tasks and selected data sizes under both fine-tuning and pre-training stages, achieving comparable performance to DistilBERT with only 10% amount of queries to the teacher model.}
}
@article{SZPYTKO20236612,
title = {Maintenance Management Practices Using Digital Twins Framework: Oil Extraction Pumping System Case Study},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {6612-6617},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.360},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323007279},
author = {Janusz Szpytko and Yorlandys Salgado-Duarte},
keywords = {Simulation of stochastic systems, Model predictive control for distributed parameter systems, Decision making and cognitive processes},
abstract = {Conceptually, the digital twins’ framework is a virtual representation of an object, system, or process. However, one of its significant contributions has been to merge certain features into a single framework: updating the system modeled from real-time data, using simulation, machine learning, and reasoning to aid decision-making. Specifically, in the maintenance management field and its associated decision-making, the contributions have been prominent, and in this paper, we intend to contribute in the same direction. Here, we describe an adaptive digital twin model designed to coordinate maintenance activities in an oil extraction pumping system. The model is a digital representation of the maintenance decision-making process and cognitive management. The adaptability comes from a self-calibration of the distributed operating parameters of the modeled system under study using a machine learning approach with smart layers in the data management filtering and synthesis. Given the nature of a maintenance modeling process, where randomness and planning are merged, the Monte Carlo method emerges as an easy way to convolute the stochastic degradation due to the system operation and the planning of maintenance activities to keep it working with the required standards. This paper discusses the conceptual model implementation for an oil extraction pumping system using the digital twins’ framework and proposes the first exposure of the modeling results in practice as a study case. The modeling results justify the improvements introduced by implementing the digital twins’ framework.}
}
@article{TANG2023118151,
title = {Particle classification of iron ore sinter green bed mixtures by 3D X-ray microcomputed tomography and machine learning},
journal = {Powder Technology},
volume = {415},
pages = {118151},
year = {2023},
issn = {0032-5910},
doi = {https://doi.org/10.1016/j.powtec.2022.118151},
url = {https://www.sciencedirect.com/science/article/pii/S0032591022010324},
author = {Kunning Tang and Ying Da Wang and Yufu Niu and Tom A. Honeyands and Damien O’ Dea and Peyman Mostaghimi and Ryan T. Armstrong and Mark Knackstedt},
keywords = {Iron ore, Sinter green bed, Micro-CT image, Machine learning, Particle classification, Domain inconsistency},
abstract = {The iron ore sintering process needs to be optimised to decrease its energy intensity and emissions of carbon and atmospheric pollutants, while continuing to produce sinter of sufficient quality for current and future low carbon blast furnace operations. Ideally, the sinter structure and mineralogy should be related back to the particle-level structure of the iron ore types mixed from different mine sources. This particle-level detail can be visually obtained by 3D X-ray micro-Computed Tomography (micro-CT), but requires subsequent algorithms to individually identify and classify particles and identify the relationship between ore sources and sinter quality. In this study, individual particles in sinter green — beds comprising a mixture of coking coal, fluxes, return fines and 5 iron ore samples from different mine sources are identified and classified in high resolution micro-CT images using a machine learning algorithm and associated data processing workflow. Coking coal, fluxes, and return fines are first segmented from iron ores based on their X-ray attenuation and texture. By imaging individual samples from each iron ore source, reliable training data is readily obtained from particle isolation with Convolutional Neural Networks (CNNs) guided by Trainable Weka Segmentation (TWS). Supervised machine learning is then applied to the datasets of isolated particles to produce a per-particle segmented digital sinter green bed image. A collection of geometric, texture, and greyscale features are computed for the particles and used to train a gradient boosting classifier. Tests are then performed on unseen subsets of the single ore source data, on a stratified mixture, and on a random mixture. An accuracy over 90% is achieved for iron ores that are morphologically domain-distinct in their feature space, while lower accuracy in the order of 40%–80% is achieved between iron ore particles that derive from different mine sources, but are domain-similar, suggesting similar mineralogy. The effect of limited training domain, the visual/morphological/feature space similarities and the resulting domain shift in data between training and testing are carefully analysed to identify major sources of similarity. This per-particle multilabel classification of sinter green bed mixtures distinguishes both similar and distinct ores from different mines, and provides a high resolution, accurately characterised digital twin analogue of mixed iron ore sinter green beds. This allows for future detailed analysis of sinter quality, energy intensity, and carbon emissions during the metallurgical process, all of which could be optimised to produce cleaner, higher quality iron.}
}
@article{FICSOR2023109769,
title = {Machine learning model ensemble based on multi-scale predictors confirms ecological segregation and accurately predicts the occurrence of net-spinning caddisfly larvae species groups (Trichoptera: Hydropsychidae) at catchment-scale},
journal = {Ecological Indicators},
volume = {146},
pages = {109769},
year = {2023},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2022.109769},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X22012420},
author = {Márk Ficsór and Zoltán Csabai},
keywords = {Hydropsychidae, Longitudinal distribution, Environmental factors, Machine learning ensemble model, Caddisfly, Distribution modelling},
abstract = {In riverine ecosystems the species distribution, determined primarily by their environment often shows zonation patterns that are also typical in the case of net-spinning caddisfly larvae (Trichoptera: Hydropsychidae). In the present research, we aimed to build an ensemble of base learner machine learning (ML) models based on the most important environmental parameters shaping the sequential distribution of ten Central European species of the genus Hydropsyche in the North Hungarian catchment area of Tisza, one of the major rivers of Central and Eastern Europe. The model could explain and effectively predict the occurrence of species and/or groups of them with similar niche preferences. Variable selection revealed the importance of predictors, measured at various spatial scales and with gradient-like characteristics, such as elevation, annual means of discharge, water temperature or the composition of habitat substrates as well as those related to the ecological quality of water or anthropogenic impacts, like annual means of dissolved oxygen and orthophosphate-phosphorous content. Trained on the predictions of different base learner models a final ensemble model predicted the presence and absence of three individual species and three species-groups with significantly improved overall accuracy. High group-wise balanced accuracies of the final model shows that longitudinal, catchment-scale distribution models in stream ecosystems are best built on predictors with variable spatial scales, several of which are routinely measured or recorded in environmental monitoring programmes. Accurate species distribution models (SDMs), capable of adequately predicting presence and absence of bio-indicator taxa, such as Hydropsyche species, can be applied to support environmental management or conservation measures regarding streams and rivers, that are among the most vulnerable of anthropogenic pollution, hydrologic alteration, climate change and biodiversity loss.}
}
@article{TARIQ2023106538,
title = {Deep learning artificial intelligence framework for sustainable desiccant air conditioning system: Optimization towards reduction in water footprints},
journal = {International Communications in Heat and Mass Transfer},
volume = {140},
pages = {106538},
year = {2023},
issn = {0735-1933},
doi = {https://doi.org/10.1016/j.icheatmasstransfer.2022.106538},
url = {https://www.sciencedirect.com/science/article/pii/S0735193322006601},
author = {Rasikh Tariq and Muzaffar Ali and Nadeem Ahmed Sheikh and Muhammad Wakil Shahzad and Ben Bin Xu},
keywords = {Artificial neural network, Desiccant evaporative cooling, Water footprint, White-box modeling, Multicriteria decision-analysis, Sustainable buildings},
abstract = {Desiccant evaporative cooling systems pave the path towards energy and environmental sustainability in buildings especially; however, the direct evaporative coolers in such configurations result in high water consumption. The application of modern computational intelligence tools, including artificial intelligence and meta-heuristic optimization algorithms, can improve the operational comprehension of desiccant cooling systems while addressing the minimization of total water footprints with the maximization of the cooling capacity. The contribution/objective of this research is to address the gaps in understanding through the application of deep learning, genetic algorithm, and multicriteria decision analysis applied to a desiccant cooling system working under real transient experimental conditions of a building located in Austria. Within the methodology, calibrated, experimental, and validated data monitoring system displaying the real desiccant-enhanced cooling system is adapted to generate a set of input-output data sets. The set of data includes ambient temperature, ambient humidity, regeneration temperature, supply airflow rate, and return airflow rate yielding the cooling capacity and total water footprints of the system. The results of deep learning algorithm using an artificial neural network have suggested that the architectures 5-[6]-[6]-1 and 5-[12]-[12]-1 are the best to accurately predict the cooling capacity and total water footprints with a coefficient of determination of 0.98856 and 0.99246, respectively. Secondly, the “white-box model” of the deep learning algorithm is used to develop a digital twin model which helps in the replication of the earlier experimental conditions. The optimization results have suggested that the optimized total water footprints are 45.17 kg/h with a system of 3.32 tons of refrigeration. These optimal values are found in the best combination of design variables in which the ambient temperature is 28 °C, ambient relative humidity is 52.0%, supply airflow rate is 2.13 kg/s, and regeneration flow rate is 2.35 kg/s, and the regeneration temperature is 70.0 °C. It is concluded that the application of data-driven models can extend the interpretation of desiccant cooling systems and can participate in its performance enhancement.}
}
@article{FENG2023109896,
title = {Digital twin-driven intelligent assessment of gear surface degradation},
journal = {Mechanical Systems and Signal Processing},
volume = {186},
pages = {109896},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109896},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022009645},
author = {Ke Feng and J.C. Ji and Yongchao Zhang and Qing Ni and Zheng Liu and Michael Beer},
keywords = {Gearbox, Digital twin, Surface degradation, Health management, Wear assessment},
abstract = {Gearbox has a compact structure, a stable transmission capability, and a high transmission efficiency. Thus, it is widely applied as a power transmission system in various applications, such as wind turbines, industrial machinery, aircraft, space vehicles, and land vehicles. The gearbox usually operates in harsh and non-stationary working environments, expediting the degradation process of the gear surface. The degradation process may lead to severe gear failures, such as tooth breakage and root crack, which could damage the gear transmission system. Therefore, it is essential to assess the progression of gear surface degradation in order to ensure a reliable operation. The digital twin is an emerging technology for machine health management. A high-fidelity digital twin model can help reflect the operation status of the gearbox and reveal the corresponding degradation mechanism, which could benefit the remaining useful life (RUL) prediction and the predictive maintenance-based decision-making framework. This paper develops a digital twin-driven intelligent health management method to monitor and assess the gear surface degradation progression. The developed method can effectively reveal the gear wear propagation characteristics and predict the RUL accurately. Furthermore, the knowledge learned from digital twin models can be well transferred to the surface wear assessment of the physical gearbox in wide industrial applications, which is of great practical significance. Two endurance tests with different dominant degradation mechanisms were conducted to validate the effectiveness of the proposed methodology for gear wear assessment.}
}
@article{ARABLOUEI2023100159,
title = {In-situ animal behavior classification using knowledge distillation and fixed-point quantization},
journal = {Smart Agricultural Technology},
volume = {4},
pages = {100159},
year = {2023},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2022.100159},
url = {https://www.sciencedirect.com/science/article/pii/S277237552200123X},
author = {Reza Arablouei and Liang Wang and Caitlin Phillips and Lachlan Currie and Jordan Yates and Greg Bishop-Hurley},
keywords = {Animal behavior classification, Deep learning, Dynamic quantization, Embedded systems, Fix-point arithmetic, Knowledge distillation},
abstract = {We explore the use of knowledge distillation (KD) for learning compact and accurate models that enable classification of animal behavior from accelerometry data on wearable devices. To this end, we take a deep and complex convolutional neural network, known as residual neural network (ResNet), as the teacher model. ResNet is specifically designed for multivariate time-series classification. We use ResNet to distill the knowledge of animal behavior classification datasets into soft labels, which consist of the predicted pseudo-probabilities of every class for each datapoint. We then use the soft labels to train our significantly less complex student models, which are based on the gated recurrent unit (GRU) and multilayer perceptron (MLP). The evaluation results using two real-world animal behavior classification datasets show that the classification accuracy of the student GRU-MLP models improves appreciably through KD, approaching that of the teacher ResNet model. To further reduce the computational and memory requirements of performing inference using the student models trained via KD, we utilize dynamic fixed-point quantization (DQ) through an appropriate modification of the computational graph of the considered models. We implement both unquantized and quantized versions of the developed KD-based models on the embedded systems of our purpose-built collar and ear tag devices to classify animal behavior in situ and in real time. Our evaluations corroborate the effectiveness of KD and DQ in improving the accuracy and efficiency of in-situ animal behavior classification.}
}
@article{LV2023102489,
title = {A bio-inspired LIDA cognitive-based Digital Twin architecture for unmanned maintenance of machine tools},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {80},
pages = {102489},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102489},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001715},
author = {Jianhao Lv and Xinyu Li and Yicheng Sun and Yu Zheng and Jinsong Bao},
keywords = {Unmanned maintenance, Digital Twin, Machine tools, Learning intelligent distribution agent (LIDA), Cognitive manufacturing},
abstract = {Affected by COVID-19, the maintenance process of machine tools is significantly hindered, while unmanned maintenance becomes an emerging trend in such background. So far, three challenges, namely, the dependence on maintenance experts, the dynamic maintenance environments, and unsynchronized interactions between physical and information sides, exist as the main obstacles in its widespread applications. In order to fill this gap, a bio-inspired LIDA cognitive-based Digital Twin architecture is proposed, so as to achieve unmanned maintenance of machine tools through a self-constructed, self-evaluated, and self-optimized manner. A three phases process in the architecture, including the physical phase, virtual phase, and service phase, is further introduced to support the cognitive cycle for unmanned maintenance of machine tools. An illustrative example is depicted in the unmanned fault diagnosis on the rolling bearing of a drilling platform, which validates the feasibility and advantages of the proposed architecture. As an explorative study, it is wished that this work provides useful insights for unmanned maintenance of machine tools in a dynamic production environment.}
}
@article{SEHRAWAT202390,
title = {Solar irradiance forecasting models using machine learning techniques and digital twin: A case study with comparison},
journal = {International Journal of Intelligent Networks},
volume = {4},
pages = {90-102},
year = {2023},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2023.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666603023000064},
author = {Neha Sehrawat and Sahil Vashisht and Amritpal Singh},
abstract = {The ever-increasing demand for energy and power consumption due to population growth, economic expansion, and evolving consumer choices has led to the need for renewable energy sources. Traditional energy sources such as coal, oil, and gas have contributed to global pollution and have adverse effects on human health. As a result, the use of renewable energy for power generation has increased tremendously. One such area of research is solar irradiation prediction, which utilizes Artificial Intelligence and Machine Learning techniques. With the use of real-time predicted data, the digital twins are intended to add value to the organization by identifying and preventing problems, predicting performance, and improving operations. This paper provides an overview of various learning methods used for predicting irradiance and presents a new ensemble solar irradiance forecasting model that combines eight machine learning models to ensure model diversity. The model's most critical factors for predicting irradiance include temperature, cloudiness index, relative humidity, and day of the week. To conduct a comprehensive analysis, the proposed 8-Stacking Regression Cross Validation (8 STR-CV) model was tested using data from three different climatic zones in India. The model's high accuracy scores of 98.8% for Visakhapatnam, 98% for Nagpur, and 97.8% for the mountainous region make it a valuable tool for future prediction in various sectors, including power generation and utilization planning.}
}
@article{ZHAO2022109832,
title = {Multi-instance semantic similarity transferring for knowledge distillation},
journal = {Knowledge-Based Systems},
volume = {256},
pages = {109832},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109832},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122009327},
author = {Haoran Zhao and Xin Sun and Junyu Dong and Hui Yu and Gaige Wang},
keywords = {Deep neural networks, Image classification, Model compression, Knowledge distillation},
abstract = {Knowledge distillation is a popular paradigm for learning portable neural networks by transferring the knowledge from a large model into a smaller one. Most existing approaches enhance the student model by utilizing the similarity information between the categories of instance level provided by the teacher model. However, these works ignore the similarity correlation between different instances that plays an important role in confidence prediction. To tackle this issue, we propose a novel method in this paper, called multi-instance semantic similarity transferring for knowledge distillation (STKD), which aims to fully utilize the similarities between categories of multiple samples. Furthermore, we propose to better capture the similarity correlation between different instances by the mixup technique, which creates virtual samples by a weighted linear interpolation. Note that, our distillation loss can fully utilize the incorrect classes similarities by the mixed labels. The proposed approach promotes the performance of student model as the virtual sample created by multiple images produces a similar probability distribution in the teacher and student networks. Experiments and ablation studies on several public classification datasets including CIFAR-10, CIFAR-100, CINIC-10 and Tiny-ImageNet verify that this light-weight method can effectively boost the performance of the compact student model. It shows that STKD has substantially outperformed the vanilla knowledge distillation and achieved superior accuracy over the state-of-the-art knowledge distillation methods.}
}
@article{CHAUDHARI20231128,
title = {A Generic Digital Twin Application Framework for Emerging Trends in Industrial Process Heaters},
journal = {Procedia CIRP},
volume = {119},
pages = {1128-1133},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123006297},
author = {Sanket Sharad Chaudhari and Kiran Suresh Bhole and Santosh Rane and Suhas Deshmukh},
keywords = {Digital twin, Design Thinking, Process Heater, IoT Framework},
abstract = {Industrial process heaters have challenging future considering recent emission norms and efficient performance for cost optimization. It is therefore necessary to have real-time technical data and vigilant system to improve operations and elimination of abrupt failures. Though digital twin is highly suitable to address this requirement however its application knowledge for Process Heater is obsolete. Therefore, this paper objects to illustrate a generic digital twin methodology for industrial process heaters including key operational aspects which contain potential of optimization of performance and cost. The framework proposed in this study will integrate it to show its practical implications. This study is performed for typical vertical cylindrical process heaters; however, it is also applicable to other types of heaters. The ready to implement framework of digital twin will impact on saving of infrastructure development and limited dependability on commercial agencies while installation of digital twin. It will also address industrial challenges including reduction in emissions, fuel consumption, equipment downtime and unexpected failures. The framework will be path forward for future necessities like Artificial Intelligence, Machine Learning etc.}
}
@article{ALEXOPOULOS20232963,
title = {Machine Learning Agents Augmented by Digital Twinning for Smart Production Scheduling},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {2963-2968},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.1420},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323018281},
author = {Kosmas Alexopoulos and Nikolaos Nikolakis and Emmanouil Bakopoulos and Vasilis Siatras and Panagiotis Mavrothalassitis},
keywords = {Machine Learning, Digital Twin, Production Scheduling, Deep Learning, Asset Administration Shell, Smart Agents, Industry 4.0},
abstract = {Digital manufacturing tools aim to provide intelligent solutions that can support manufacturing industry to adapt to the volatile operational environment. The successful implementation of such tools highly depends on the capabilities of the digital frameworks or platforms they are deployed upon as well as the quality of their intelligence. The objective of this work is to develop and discuss a framework for training and deploying Machine Learning (ML) agents for production scheduling with the augmentation of Digital Twin (DT) technologies. Two types of ML production scheduling agents have been developed and integrated with the DT framework: a Deep Learning agent and a Deep Reinforcement Learning agent. In order to increase interoperability, Asset Administration Shell Industry4.0 standard has been utilized for the integration and deployment of the proposed DT framework into industrial practice. The proposed framework is tested and validated upon an industrial case study from the bicycles’ production industry.}
}
@article{LU2023104203,
title = {Uncertainty-aware pseudo-label and consistency for semi-supervised medical image segmentation},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104203},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104203},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422006577},
author = {Liyun Lu and Mengxiao Yin and Liyao Fu and Feng Yang},
keywords = {Semi-supervised learning, Medical image segmentation, Pseudo-labeling, Consistency regularization, Uncertainty estimation},
abstract = {In medical image segmentation tasks, fully-supervised learning has been a huge success by using abundant labeled data. However, it is time-consuming and expensive for technicians to label medical images. In this paper, we propose a novel framework for semi-supervised medical image segmentation, named Uncertainty-aware Pseudo-label and Consistency. Our framework is made up of the student–teacher models. The supervised loss on labeled data and the consistency loss on both labeled and unlabeled data are weighted and combined to optimize the models. Our method combines the recent state-of-the-art semi-supervised methods, which are consistency regularization and pseudo-labeling. More importantly, we calculate the Kullback–Leibler variance between the student model’s prediction and the teacher model’s prediction as uncertainty estimation, and directly use the uncertainty to rectify the learning of noisy pseudo-labels, instead of setting a fixed threshold to filter the pseudo-labels. Experiments on the Left Atrium dataset show that our method can efficiently utilize unlabeled data to achieve high performance and outperform other state-of-the-art semi-supervised methods. In addition, we have also analyzed its difference from conventional methods of consistency regularization and pseudo-labeling in semi-supervised medical image segmentation. Code is available in https://github.com/GXU-GMU-MICCAI/UPC-Pytorch.}
}
@article{HUYNHTHE2023105581,
title = {Artificial intelligence for the metaverse: A survey},
journal = {Engineering Applications of Artificial Intelligence},
volume = {117},
pages = {105581},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105581},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622005711},
author = {Thien Huynh-The and Quoc-Viet Pham and Xuan-Qui Pham and Thanh Thi Nguyen and Zhu Han and Dong-Seong Kim},
keywords = {Artificial intelligence, Blockchain, Deep learning, Immersive experience, Machine learning, Machine vision, Metaverse, Metaverse applications, Networking, Virtual worlds},
abstract = {Along with the massive growth of the Internet from the 1990s until now, various innovative technologies have been created to bring users breathtaking experiences with more virtual interactions in cyberspace. Many virtual environments have been developed with immersive experience and digital transformation, but most are incoherent instead of being integrated into a platform. In this context, metaverse has been introduced as a shared virtual world that is fueled by many emerging technologies. Among such technologies, artificial intelligence (AI) has shown the great importance of enhancing immersive experience and enabling human-like intelligence of virtual agents. In this survey, we make a beneficial effort to explore the role of AI, including machine learning algorithms and deep learning architectures, in the foundation and development of the metaverse. As the main contributions, we convey a comprehensive investigation of AI-based methods concerning several technical aspects (e.g., natural language processing, machine vision, blockchain, networking, digital twin, and neural interface) that have potentials to build virtual worlds in the metaverse. Furthermore, several primary AI-aided applications, including healthcare, manufacturing, smart cities, and gaming, are studied to be promisingly deployed in the virtual worlds. Finally, we conclude the key contribution and open some future research directions of AI for the metaverse. Serving as a foundational survey, this work will help researchers, including experts and non-experts in related fields, in applying, developing, and optimizing AI techniques to polish the appearance of virtual worlds and improve the quality of applications built in the metaverse.}
}
@article{LIN2022109439,
title = {Development and assessment of prognosis digital twin in a NAMAC system},
journal = {Annals of Nuclear Energy},
volume = {179},
pages = {109439},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109439},
url = {https://www.sciencedirect.com/science/article/pii/S0306454922004698},
author = {Linyu Lin and Anil Gurgen and Nam Dinh},
keywords = {Digital twin, Prognosis, Machine learning, Autonomous control},
abstract = {The nearly autonomous management and control (NAMAC) system is a comprehensive control system to assist plant operations by furnishing control recommendations to operators. Prognosis digital twin (DT-P) is a critical component in NAMAC for predicting action effects and supporting NAMAC decision-making during normal and accident scenarios. To quantifying and reducing uncertainty of machine-learning-based DT-Ps in multi-step predictions, this work investigates and derives insights from the application of three techniques for optimizing the performance of DT-P by long short-term memory recurrent neural networks, including manual search, sequential model-based optimization, and physics-guided machine learning. Sequential model-based optimization and physics-guide machine learning result in smallest errors when the predicting transients are similar to the training data.}
}
@article{JIMENEZ20239588,
title = {A simple framework for working with MATLAB and Home I/O},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {9588-9593},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.262},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323006134},
author = {Javier Jiménez and Elena M. Mosquera and José M. Maestre},
keywords = {Control education, virtual and remote labs, centralized internet repository, predictive control, system Identification},
abstract = {A software framework that communicates MATLAB and Home I/O software, which provides a pedagogic digital twin of a smart home, is presented. This software is packaged as a MATLAB class code and allows reading and writing to Home I/O from MATLAB. It also includes useful tools to simplify the implementation of learning and research algorithms using its built-in methods. A basic thermal Identification and a simple MPC (Model Predictive Controller) with all rooms’ heaters are built using this framework to prove its functionality.}
}
@article{GONG2022109431,
title = {An efficient digital twin based on machine learning SVD autoencoder and generalised latent assimilation for nuclear reactor physics},
journal = {Annals of Nuclear Energy},
volume = {179},
pages = {109431},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109431},
url = {https://www.sciencedirect.com/science/article/pii/S0306454922004613},
author = {Helin Gong and Sibo Cheng and Zhang Chen and Qing Li and César Quilodrán-Casas and Dunhui Xiao and Rossella Arcucci},
keywords = {Operational digital twins, Machine learning, Latent assimilation, SVD-autoencoder, Nuclear reactor physics},
abstract = {This paper proposes an approach that combines reduced-order models with machine learning in order to create an digital twin to predict the power distribution over the core during the operation stage. The operational digital twin is designed to solve forward problems given input operation parameters, as well as to solve inverse problems given some observations of the power field. The forward model is non-intrusive and realised using SVD autoencoder reduced order model with the combination of machine learning methods, namely, k-nearest-neighbours and decision trees to build the input–output map. For model parameter estimation, the inverse model is based on a generalised latent assimilation method. The proposed approach is able to make use of the non intrusive reduced order model and the online measurements of the power field. The effectiveness in the sense of accuracy and real-time solver of the digital twin is illustrated through a real engineering problem in nuclear reactor physics — reactor core simulation in the life cycle of HPR1000 affected by input parameters, i.e., control rod inserting step, burnup, power level and inlet temperature of the coolant, which shows potential applications for on-line monitoring purpose.}
}
@article{ZHOU2022,
title = {Multimodal fusion recognition for digital twin},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822002176},
author = {Tianzhe Zhou and Xuguang Zhang and Bing Kang and Mingkai Chen},
keywords = {Digital twin, Multimodal fusion, Object recognition, Deep learning, Transfer learning},
abstract = {The digital twin is the concept of transcending reality, which is the reverse feedback from the real physical space to the virtual digital space. People hold great prospects for this emerging technology. In order to realize the upgrading of the digital twin industrial chain, it is urgent to introduce more modalities, such as vision, haptics, hearing and smell, into the virtual digital space, which assists physical entities and virtual objects in creating a closer connection. Therefore, perceptual understanding and object recognition have become an urgent hot topic in the digital twin. Existing surface material classification schemes often achieve recognition through machine learning or deep learning in a single modality, ignoring the complementarity between multiple modalities. In order to overcome this dilemma, we propose a multimodal fusion network in our article that combines two modalities, visual and haptic, for surface material recognition. On the one hand, the network makes full use of the potential correlations between multiple modalities to deeply mine the modal semantics and complete the data mapping. On the other hand, the network is extensible and can be used as a universal architecture to include more modalities. Experiments show that the constructed multimodal fusion network can achieve 99.42% classification accuracy while reducing complexity.}
}
@article{BANAFAA2023245,
title = {6G Mobile Communication Technology: Requirements, Targets, Applications, Challenges, Advantages, and Opportunities},
journal = {Alexandria Engineering Journal},
volume = {64},
pages = {245-274},
year = {2023},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2022.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S111001682200549X},
author = {Mohammed Banafaa and Ibraheem Shayea and Jafri Din and Marwan {Hadri Azmi} and Abdulaziz Alashbi and Yousef {Ibrahim Daradkeh} and Abdulraqeb Alhammadi},
keywords = {6G, Autonomous vehicle, Visible light communication, Intelligence systems, Machine learning, Massive multi-input multi-output (MIMO)},
abstract = {The sixth-generation (6G) technology of mobile networks will establish new standards to fulfill unreachable performance requirements by fifth-generation (5G) mobile networks. This is due to the high requirements for more intelligent network, ultra-lower latency, extreme network communication speed, and supporting massive number of various connected applications. In the long term, the convergence of various business developments with communication platforms, as initiated by 5G, will exaggerate and highlight areas where 5G's capabilities will fall short of performance requirements. Motivated by the development of applications in massive connections, future networks, developments, and technological advancements for mobile communications that go beyond fifth-generation (B5G) networks are being developed. In this context, highly immersive applications are demanded, such as three-dimensional (3D) communications, digital twins, or massive extended reality (XR)/virtual reality (VR) applications, which will need 6G capabilities to be realized at scale to be commercially feasible. Mainly, we anticipate that only the upcoming 6G networks will be capable of running extremely high-performance connectivity with massive numbers of connected devices, even under laborious scenarios such as extreme density, diverse mobility, and energetic environments. In this article, we look at the most recent trends and future emerging trends that are possible to operate 6G network. Paper aims to provide more inclusive and brief review about 6G mobile communication technology in one survey paper. Initially, a comprehensive overview of the 6G system is introduced in terms of visions, drivers, requirements, architecture, and usage scenarios required to enable 6G applications. After that, the opportunities and advantages of 6G mobile technology has been discussed. Further, the promising new techniques that enable 6G technology has been highlighted. This is followed by a potential discussion of challenges and research directions. This article is envisioned to serve as an informative guideline to stimulate interest and further studies for subsequent research and development of 6G networks. Paper will enable the readers to briefly figure out the key requirements, targets, that will be need and the applications, advantages, and opportunities that can be offered as well as the challenges that need to be addressed before the implementation of this new technology.}
}
@article{JEON2023466,
title = {Leveraging angular distributions for improved knowledge distillation},
journal = {Neurocomputing},
volume = {518},
pages = {466-481},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222014096},
author = {Eun Som Jeon and Hongjun Choi and Ankita Shukla and Pavan Turaga},
keywords = {Knowledge distillation, Angular distribution, Angular margin, Image classification},
abstract = {Knowledge distillation as a broad class of methods has led to the development of lightweight and memory efficient models, using a pre-trained model with a large capacity (teacher network) to train a smaller model (student network). Recently, additional variations for knowledge distillation, utilizing activation maps of intermediate layers as the source of knowledge, have been studied. Generally, in computer vision applications, it is seen that the feature activation learned by a higher-capacity model contains richer knowledge, highlighting complete objects while focusing less on the background. Based on this observation, we leverage the teacher’s dual ability to accurately distinguish between positive (relevant to the target object) and negative (irrelevant) areas. We propose a new loss function for distillation, called angular margin-based distillation (AMD) loss. AMD loss uses the angular distance between positive and negative features by projecting them onto a hypersphere, motivated by the near angular distributions seen in many feature extractors. Then, we create a more attentive feature that is angularly distributed on the hypersphere by introducing an angular margin to the positive feature. Transferring such knowledge from the teacher network enables the student model to harness the teacher’s higher discrimination of positive and negative features, thus distilling superior student models. The proposed method is evaluated for various student–teacher network pairs on four public datasets. Furthermore, we show that the proposed method has advantages in compatibility with other learning techniques, such as using fine-grained features, augmentation, and other distillation methods.}
}
@article{ZHAO2023104645,
title = {A blockchain 3.0 paradigm for digital twins in construction project management},
journal = {Automation in Construction},
volume = {145},
pages = {104645},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104645},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522005155},
author = {Rui Zhao and Zhe Chen and Fan Xue},
keywords = {Blockchain 3.0, Construction project management, Digital twin, Smart contract, Modular construction},
abstract = {Construction project management (CPM) is inherently complex and distributed, while digital twin and blockchain are recognized as promising solutions for information-reliant CPM. By learning from the lessons of Blockchain 1.0 and 2.0 paradigms in the literature, such as slow synchronization and failed offline functions, this paper proposes ChainPM as a Blockchain 3.0 paradigm. ChainPM extends Blockchain 2.0 with innovative indexing, query, and analysis function sets for key CPM data. Experimental results from a pilot study of a modular construction project showed that the information synchronization latency was reduced by 99.2% to 99.8%, and query and analytical functions worked equally well without network connections. ChainPM contributes to a novel trend of Blockchain 3.0 paradigms for CPM digital twins, emphasizing indexing key CPM data, combinatorial query, digital authorship, and fast response without downgrading the ‘single source of truth.’ For practitioners, ChainPM addresses key barriers of Internet reliance and information delay to CPM digital twins.}
}
@article{TANG2023693,
title = {Towards better utilization of pseudo labels for weakly supervised temporal action localization},
journal = {Information Sciences},
volume = {623},
pages = {693-708},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522015390},
author = {Yiping Tang and Junyao Ge and Kaitai Guo and Yang Zheng and Haihong Hu and Jimin Liang},
keywords = {Untrimmed video analysis, Temporal action detection, Weakly supervised learning, Pseudo label},
abstract = {Weakly supervised temporal action localization (WS-TAL) aims to simultaneously recognize and localize action instances of interest in untrimmed videos with the use of the video-level label only. Some works have demonstrated that pseudo labels play an important role for performance improvement in WS-TAL. Since pseudo labels are inevitably inaccurate, direct adoption of noisy labels can lead to inappropriate knowledge transfer. Although some previous studies have shown the benefits of using only “reliable” pseudo labels, performance improvement is still limited. In this work, we experimentally analyze how the noise in pseudo labels affects model performance within the self-distillation framework. Motivated by the finding that incorrect pseudo labels with large confidence scores have a significant impact on performance, we propose the overconfidence suppression (OCS) strategy to mitigate the effect of the overconfident pseudo labels, and thus prevent over-fitting of the student model. In addition, a simplified contrast learning method is utilized to fine-tune the feature representation by increasing the separation of the foreground and background snippets. Equipped with the proposed methods, the benefits of pseudo labels can be better exploited and allow the model to achieve state-of-the-art performance on THUMOS’14 and ActivityNet-1.2 benchmarks.}
}
@article{ZHANG202356,
title = {A multi-access edge computing enabled framework for the construction of a knowledge-sharing intelligent machine tool swarm in Industry 4.0},
journal = {Journal of Manufacturing Systems},
volume = {66},
pages = {56-70},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522002060},
author = {Chao Zhang and Guanghui Zhou and Jingjing Li and Fengtian Chang and Kai Ding and Dongxu Ma},
keywords = {Multi-access edge computing, Digital twin, Intelligent machine tool, Industry 4.0, Knowledge sharing},
abstract = {Developing intelligent machine tools has been front and center for manufacturing enterprises to take a step towards intelligent manufacturing in Industry 4.0, which has attracted increasing attention from both academics and industry. Nevertheless, most current approaches focus on the construction of a single digital twin machine tool with limited intelligence due to the lack of data and knowledge accumulated by that machine tool for decision-making support. Consequently, this paper integrates digital twin with multi-access edge computing (MEC) and proposes a novel framework for the construction of a knowledge-sharing intelligent machine tool swarm that supports the secure knowledge sharing across the authorized machine tools in the swarm with ultra-low latency performance. Then, three key enabling methodologies of the framework are introduced from the perspective of digital twin machine tool swarm construction, knowledge-based cloud brain learning, and MEC-enhanced system deployment. Finally, a prototype system is implemented, where its application examples and evaluation experiments demonstrate the feasibility and effectiveness of the proposed approach.}
}
@article{LI2023148,
title = {An improved stochastic configuration network for concentration prediction in wastewater treatment process},
journal = {Information Sciences},
volume = {622},
pages = {148-160},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.134},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522014451},
author = {Kang Li and Cuili Yang and Wei Wang and Junfei Qiao},
keywords = {Stochastic configuration networks, Incremental learning, Randomized neural networks, Wastewater treatment process},
abstract = {A learner model with fast learning and compact architecture is expected for industrial data modeling. To achieve these goals during stochastic configuration networks (SCNs) construction, we propose an improved version of SCNs in this paper. Unlike the original SCNs, the improved one employs a new inequality constraint in the construction process. In addition, to speed up the construction efficiency of SCNs, a node selection method is proposed to adaptively select nodes from a candidate pool. Moreover, to reduce the redundant nodes of the built SCNs model, we further compress the model based on the singular value decomposition algorithm. The improved SCNs are compared with other methods over four datasets and then applied to the ammonia–nitrogen concentration prediction task in the wastewater treatment process. Experimental results indicate that the proposed method has good potential for industrial data analytics.}
}
@article{WANG20236921,
title = {Neural network and Sparse identification of Nonlinear Dynamics Integrated Algorithm for Digital Twin identification},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {6921-6926},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.503},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323008704},
author = {Jingyi Wang and Jesús Moreira and Yankai Cao and R. Bhushan Gopaluni},
keywords = {Digital twin, Feature engineering, Hybrid modelling, Industry 4.0, Nonlinear model reduction, Sparse process modelling and identification},
abstract = {Digital twins play a critical role in simulating industrial manufacturing systems to increase productivity and reduce time spent on troubleshooting. Owing to the complexity of real-world industrial systems, automatic sparse identification has emerged as an attractive approach to perform digital twin modelling. The sparse identification of nonlinear dynamics (SINDy) is a machine learning algorithm that performs feature engineering by generating a model term library and then solves a sparse regression problem between the objective outputs and the generated features. By solving a linear-in-parameter sparse regression problem, SINDy provides automatic discovery of system governing equations. However, the performance of SINDy-based algorithms may decline dramatically when applied to identify complex nonlinear relationships, such as implicit relationships. The substantial number of input variables for a real industrial process may further complicate the modelling procedure. We therefore propose the neural network and SINDy integrated algorithm to automatically select the critical features from a model term library and utilize the neural network to capture the process nonlinearity that cannot be captured by a linear-in-parameter model. SINDy performs feature generation considering both numerical methods and first-principles knowledge, making the proposed algorithm a hybrid system identification approach. A diesel hydrotreating unit case study with 37 input variables is analyzed in this paper to demonstrate the advantages of the proposed algorithm for nonlinear digital twin identification. By combining the advantages from both SINDy and neural networks, the proposed algorithm is able to improve the output prediction accuracy for all the three objectives.}
}
@article{WANG2023102693,
title = {SSD-KD: A self-supervised diverse knowledge distillation method for lightweight skin lesion classification using dermoscopic images},
journal = {Medical Image Analysis},
volume = {84},
pages = {102693},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102693},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522003218},
author = {Yongwei Wang and Yuheng Wang and Jiayue Cai and Tim K. Lee and Chunyan Miao and Z. Jane Wang},
keywords = {Skin cancer detection, Knowledge distillation, Deep learning, Dermoscopy},
abstract = {Skin cancer is one of the most common types of malignancy, affecting a large population and causing a heavy economic burden worldwide. Over the last few years, computer-aided diagnosis has been rapidly developed and make great progress in healthcare and medical practices due to the advances in artificial intelligence, particularly with the adoption of convolutional neural networks. However, most studies in skin cancer detection keep pursuing high prediction accuracies without considering the limitation of computing resources on portable devices. In this case, the knowledge distillation (KD) method has been proven as an efficient tool to help improve the adaptability of lightweight models under limited resources, meanwhile keeping a high-level representation capability. To bridge the gap, this study specifically proposes a novel method, termed SSD-KD, that unifies diverse knowledge into a generic KD framework for skin disease classification. Our method models an intra-instance relational feature representation and integrates it with existing KD research. A dual relational knowledge distillation architecture is self-supervised trained while the weighted softened outputs are also exploited to enable the student model to capture richer knowledge from the teacher model. To demonstrate the effectiveness of our method, we conduct experiments on ISIC 2019, a large-scale open-accessed benchmark of skin diseases dermoscopic images. Experiments show that our distilled MobileNetV2 can achieve an accuracy as high as 85% for the classification tasks of 8 different skin diseases with minimal parameters and computing requirements. Ablation studies confirm the effectiveness of our intra- and inter-instance relational knowledge integration strategy. Compared with state-of-the-art knowledge distillation techniques, the proposed method demonstrates improved performance. To the best of our knowledge, this is the first deep knowledge distillation application for multi-disease classification on the large-scale dermoscopy database. Our codes and models are available at https://github.com/enkiwang/Portable-Skin-Lesion-Diagnosis.}
}
@article{VOOGD20231510,
title = {Reinforcement Learning from Simulation to Real World Autonomous Driving using Digital Twin},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {1510-1515},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.1846},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323022553},
author = {Kevin L. Voogd and Jean Pierre Allamaa and Javier Alonso-Mora and Tong Duy Son},
keywords = {Learning and adaptation, autonomous vehicles, Sim2Real, reinforcement learning},
abstract = {Reinforcement learning (RL) is a promising solution for autonomous vehicles to deal with complex and uncertain traffic environments. The RL training process is however expensive, unsafe, and time-consuming. Algorithms are often developed first in simulation and then transferred to the real-world, leading to a common sim2real challenge where performance decreases when the domain changes. In this paper, we propose a transfer learning process to minimize the gap by exploiting digital twin technology, relying on a systematic and simultaneous combination of virtual and real world data coming from vehicle dynamics and traffic scenarios. The model and testing environment is evolved from model, hardware to vehicle in the loop and proving ground testing stages, similar to standard development cycle in the automotive industry. In particular, we also integrate other transfer learning techniques such as domain randomization and adaptation in each stage. The simulation and real data are gradually incorporated to accelerate and make the transfer learning process more robust. The proposed RL methodology is applied to develop a path-following steering controller for an autonomous electric vehicle. After learning and deploying the real-time RL control policy on the vehicle, we obtained satisfactory and safe control performance already from the first deployment, demonstrating the advantages of the proposed digital twin based learning process.}
}
@article{YI2022,
title = {Digital twin driven and intelligence enabled content delivery in end-edge-cloud collaborative 5G networks},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001894},
author = {Bo Yi and Jianhui Lv and Xingwei Wang and Lianbo Ma and Min Huang},
keywords = {Digital twin, IoE, Content delivery, Caching, Routing},
abstract = {The rapid development of 5G/6G and AI enables an environment of Internet of Everything (IoE) which can support millions of connected mobile devices and applications to operate smoothly at high speed and low delay. However, these massive devices will lead to explosive traffic growth, which in turn cause great burden for the data transmission and content delivery. This challenge can be eased by sinking some critical content from cloud to edge. In this case, how to determine the critical content, where to sink and how to access the content correctly and efficiently become new challenges. This work focuses on establishing a highly efficient content delivery framework in the IoE environment. In particular, the IoE environment is re-constructed as an end-edge-cloud collaborative system, in which the concept of digital twin is applied to promote the collaboration. Based on the digital asset obtained by digital twin from end users, a content popularity prediction scheme is firstly proposed to decide the critical content by using the Temporal Pattern Attention (TPA) enabled Long Short-Term Memory (LSTM) model. Then, the prediction results are input for the proposed caching scheme to decide where to sink the critical content by using the Reinforce Learning (RL) technology. Finally, a collaborative routing scheme is proposed to determine the way to access the content with the objective of minimizing overhead. The experimental results indicate that the proposed schemes outperform the state-of-the-art benchmarks in terms of the caching hit rate, the average throughput, the successful content delivery rate and the average routing overhead.}
}
@article{GONG2023109497,
title = {Parameter identification and state estimation for nuclear reactor operation digital twin},
journal = {Annals of Nuclear Energy},
volume = {180},
pages = {109497},
year = {2023},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109497},
url = {https://www.sciencedirect.com/science/article/pii/S0306454922005278},
author = {Helin Gong and Tao Zhu and Zhang Chen and Yaping Wan and Qing Li},
keywords = {Digital twin, Machine learning, Differential evolution, Nuclear reactor physics},
abstract = {Reactor Operation Digital Twin (RODT) is now receiving increasing attention and investment in nuclear engineering domain. A prototype of a RODT was first brought out by Gong et al. at Nuclear Power Institute of China. The RODT contains a forward solver for online real-time simulation and an inverse problem solver for parameter identification and state estimation. To further improve the efficiency and accuracy of RODT and promote the practical deployment of RODT (i) we first propose an advanced differential evolution algorithm to upgrade the inverse solver; (ii) then we bring out a systematical uncertainty quantification of RODT considering assimilation noisy observations. The accuracy and validity of the proposed RODT are tested along one cycle operating stage of HPR1000, at various operating temperatures, control rod steps, general power level and the inlet temperature of a practical nuclear reactor core. Numerous numerical results confirm its potential for practical engineering applications for on-line parameter identification and state estimation.}
}
@article{ABHIRAMAN2023274,
title = {Fault detection for vaccine refrigeration via convolutional neural networks trained on simulated datasets},
journal = {International Journal of Refrigeration},
volume = {149},
pages = {274-285},
year = {2023},
issn = {0140-7007},
doi = {https://doi.org/10.1016/j.ijrefrig.2022.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0140700722004790},
author = {Bhaskar Abhiraman and Riley Fotis and Leo Eskin and Harvey Rubin},
keywords = {Digital twin, Modeling, Machine learning, Fault detection, Cold chain, Synthetic data, Jumeau numérique, Modélisation, Apprentissage automatique, Détection des défaillances, Chaîne du froid, Données synthétiques},
abstract = {In low-and middle-income countries, the cold chain that supports vaccine storage and distribution is vulnerable due to insufficient infrastructure and interoperable data. To bolster these networks, we developed a convolutional neural network-based fault detection method for vaccine refrigerators using datasets synthetically generated by thermodynamic modeling. We demonstrate that these thermodynamic models can be calibrated to real cooling systems in order to identify system-specific faults under a diverse range of operating conditions. If implemented on a large scale, this portable, flexible approach has the potential to increase the fidelity and lower the cost of vaccine distribution in remote communities.}
}
@article{MANOCHA2023110138,
title = {Digital Twin-assisted Blockchain-inspired irregular event analysis for eldercare},
journal = {Knowledge-Based Systems},
volume = {260},
pages = {110138},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110138},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122012345},
author = {Ankush Manocha and Yasir Afaq and Munish Bhatia},
keywords = {Digital Twin, Irregular event determination, Smart healthcare, Internet of Things, Eldercare},
abstract = {Since the development of smart healthcare services, different solutions have been developed in the field of healthcare to increase the life expectancy of the patient by reducing the cost of healthcare. Digital Twin (DT) is considered one of the most promising technologies and a game changer in the field of healthcare. DT is generating a virtual imitation of a physical object that mimics the status of an event by changing the information in real time. In this article, a smart context-aware physical activity monitoring framework is developed by combining different advanced techniques such as IoT, DT, FoT, CoT, and Blockchain to maintain the sensitiveness of the healthcare domain. In the proposed study, the physical movements of an elder are analyzed by utilizing the sequential data processing capability of deep learning to detect irregular physical events. In addition, the proposed framework can keep the data of an individual secured by applying progressed security highlights of blockchain. The proposed solution effectively analyzed an irregular event of an individual with considerable accuracy in real time. The calculated outcomes have shown the effectiveness of DT with smart healthcare solutions that would help to develop effective medical services by bringing patients and medical care experts together. Furthermore, the performance of the proposed solution is measured with respect to irregular event recognition, model training and testing, rate of latency, and data processing cost. In this manner, a case study defines the effectiveness of the proposed methodology in the smart healthcare industry.}
}
@article{JASSIM2023109908,
title = {Forecasting domestic waste generation during successive COVID-19 lockdowns by Bidirectional LSTM super learner neural network},
journal = {Applied Soft Computing},
volume = {133},
pages = {109908},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109908},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622009577},
author = {Majeed S. Jassim and Gulnur Coskuner and Nahid Sultana and S.M. Zakir Hossain},
keywords = {Solid waste, ARIMA, BiLSTM, Machine learning, Predictive modeling},
abstract = {Accurate prediction of domestic waste generation is a challenging task for municipalities to implement sustainable waste management strategies. In the present study, domestic waste generation in the Kingdom of Bahrain, representing a Small Island Developing State (SIDS) case study, has been investigated during successive COVID-19 lockdowns due to the pandemic in 2020. Temporal trends of daily domestic waste generation between 2019 and 2020 and their statistical analyses exhibited remarkable variations highlighting the impact of consecutive COVID-19 lockdowns on domestic waste generation. Machine learning has great potential for predicting solid waste generation rates, but only a few studies utilized deep learning approaches. The state-of-the-art Bidirectional Long Short-Term Memory (BiLSTM) network model as a deep learning method is applied to forecast daily domestic waste data in 2020. Bayesian optimization algorithm (BOA) was hybridized with BiLSTM to generate a super learner approach. The performance of the BOA-BiLSTM super learner model was further compared with the statistical ARIMA model. Performance indicators of the developed models using ARIMA and BiLSTM showed that the latter yielded superior performance for short-term forecasts of domestic waste generation. The MAE, RMSE, MAPE, and R2 were 47.38, 60.73, 256.43, and 0.46, respectively, for the ARIMA model, compared to 3.67, 12.57, 0.24, and 0.96, respectively, for the BiLSTM model. Additionally, the relative errors for the BiLSTM model were lower than those of the ARIMA model. This study highlights that the BiLSTM can be a reliable forecasting tool for solid waste management policymakers during public health emergencies.}
}
@article{SHEN2023142,
title = {UniSKGRep: A unified representation learning framework of social network and knowledge graph},
journal = {Neural Networks},
volume = {158},
pages = {142-153},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022004518},
author = {Yinghan Shen and Xuhui Jiang and Zijian Li and Yuanzhuo Wang and Chengjin Xu and Huawei Shen and Xueqi Cheng},
keywords = {Social knowledge graph, Graph representation learning, Knowledge graph, Social network},
abstract = {The human-oriented applications aim to exploit behaviors of people, which impose challenges on user modeling of integrating social network (SN) with knowledge graph (KG), and jointly analyzing two types of graph data. However, existing graph representation learning methods merely represent one of two graphs alone, and hence are unable to comprehensively consider features of both SN and KG with profiling the correlation between them, resulting in unsatisfied performance in downstream tasks. Considering the diverse gap of features and the difficulty of associating of the two graph data, we introduce a Unified Social Knowledge Graph Representation learning framework (UniSKGRep), with the goal to leverage the multi-view information inherent in the SN and KG for improving the downstream tasks of user modeling. To the best of our knowledge, we are the first to present a unified representation learning framework for SN and KG. Concretely, the SN and KG are organized as the Social Knowledge Graph (SKG), a unified representation of SN and KG. For the representation learning of SKG, first, two separate encoders in the Intra-graph model capture both the social-view and knowledge-view in two embedding spaces, respectively. Then the Inter-graph model is learned to associate the two separate spaces via bridging the semantics of overlapping node pairs. In addition, the overlapping node enhancement module is designed to effectively align two spaces with the consideration of a relatively small number of overlapping nodes. The two spaces are gradually unified by continuously iterating the joint training procedure. Extensive experiments on two real-world SKG datasets have proved the effectiveness of UniSKGRep in yielding general and substantial performance improvement compared with the strong baselines in various downstream tasks.}
}
@article{QIAO202345,
title = {A blockchain-based decentralized collaborative learning model for reliable energy digital twins},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {3},
pages = {45-51},
year = {2023},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2023.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S2667345223000147},
author = {Liang Qiao and Zhihan Lv},
keywords = {Blockchain, Federated learning, Collaborative learning, Digital twins},
abstract = {This paper proposes a blockchain-based decentralized collaborative learning method for the Industrial Internet environment to solve the trust and security issues in Federated Learning. Deploy a decentralized network for collaborative learning based on the alliance chain, design a block data structure suitable for asynchronous learning, and model three stages of computing event triggering, computing task distribution, and computing result integration for cross-domain device collaborative learning. List the critical steps for network deployment, including inspection, tearing down old networks, creating organizational encryption material, creating channels, and deploying chaincode. It also introduces the development of crucial chaincode such as initialization, creation, query, and modification. Finally, the correlation between the number of data pieces of the network, the number of communications, and the time of communications are analyzed through experiments. This paper also proposes a decentralized asynchronous collaborative learning algorithm, develops chaincode middleware between the blockchain network and Artificial Intelligence training, and conducts experimental analysis on the industrial steam volume prediction data set in thermal power generation. The performance on the data set, and the experimental results prove that the asynchronous collaborative learning algorithm proposed in this paper can achieve a good convergence effect. It is also compared with the single-machine single-card regression prediction algorithm, proving that the proposed model has better generalization.}
}
@article{CHABANET202310384,
title = {An object-oriented architecture to couple simulators and their machine learning surrogates models in the context of digital shadows},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {10384-10389},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.1051},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323014532},
author = {Sylvain Chabanet and Emmanuel Zimmermann and Philippe Thomas and Hind Bril El-Haouzi},
keywords = {active learning, surrogate models, digital twins, artificial intelligence, sawmill simulation},
abstract = {This article studies a method to couple two digital models in the context of digital twins. The first model is a simulation model which is supposed to be very accurate but computationally intensive. The second is a fast but approximate machine-learning model of the simulation. Both models serve, therefore, the same prediction task in an online environment but have different advantages and drawbacks. An object-oriented architecture is introduced to implement the proposed coupling strategy. Numerical experiment results on four datasets are also provided to evaluate the performances of the proposed strategy and compare it with a baseline. Three of these datasets originate from the University of California, Irvine machine learning repository. The last one originates from the Canadian forest product industry and contains the outputs of sawing simulation for real wood logs. These experiments demonstrate that the proposed method allows to consistently reduce the average error of the couple predictions.}
}
@article{LI2023102471,
title = {An AR-assisted Deep Reinforcement Learning-based approach towards mutual-cognitive safe human-robot interaction},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {80},
pages = {102471},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102471},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001533},
author = {Chengxi Li and Pai Zheng and Yue Yin and Yat Ming Pang and Shengzeng Huo},
keywords = {Smart manufacturing, Human robot interaction, Augmented reality, Deep reinforcement learning, Manufacturing safety},
abstract = {With the emergence of Industry 5.0, the human-centric manufacturing paradigm requires manufacturing equipment (robots, etc.) interactively assist human workers to deal with dynamic and complex production tasks. To achieve symbiotic human–robot interaction (HRI), the safety issue serves as a prerequisite foundation. Regarding the growing individualized demand of manufacturing tasks, the conventional rule-based safe HRI measures could not well address the safety requirements due to inflexibility and lacking synergy. To fill the gap, this work proposes a mutual-cognitive safe HRI approach including worker visual augmentation, robot velocity control, Digital Twin-enabled motion preview and collision detection, and Deep Reinforcement Learning-based robot collision avoidance motion planning in the Augmented Reality-assisted manner. Finally, the feasibility of the system design and the performance of the proposed approach are validated by establishing and executing the prototype HRI system in a practical scene.}
}
@article{YE2022112886,
title = {Research on acoustic reconstruction methods of the hull vibration based on the limited vibration monitor data},
journal = {Ocean Engineering},
volume = {266},
pages = {112886},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.112886},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822021692},
author = {Linchang Ye and Jianping Shen and Zongpeng Tong and Yun Liu},
keywords = {Hull vibration reconstruction, Limited monitor points, P-GRU Neutral network, Acoustic experiment, Multi-parameter comprehensive method},
abstract = {The prediction of hull vibration response is the basis for the quantitative control of cabin noise and underwater radiated noise, which has an important engineering value to obtain vibration characteristics accurately. In this paper, a new method is proposed to reconstruct hull vibration by some monitoring data in low frequencies. The hull vibration response is reconstructed by the monitoring data and the transfer function, and the ill-posed problem of inversion for the transfer function matrix is solved by the total least-squares regularization. A P-GRU (Peak-based Gated Recurrent Unit) neural network correction model is proposed to improve the prediction accuracy of vibration response. Taking a certain cabin structure as an example, some research such as the hull vibration data monitoring, the vibration transfer function calculation, vibration response reconstruction, and correction based on experimental data were carried out, and the simulation calculations and verification of experiment models were obtained. The multi-parameter comprehensive method is adopted to quantitatively evaluate the results between the simulation and the experiment. The results show that the acoustic reconstruction results are in good agreement with the experimental data. The overall level error is within 3 dB, the main peak value correlation coefficient is above 0.9, and the main peak difference is within 4 dB. The mechanism of the rapid and accurate prediction of the low-frequency vibration for the hull based on limited monitoring data is discussed by using a semi-physical simulation method combining the simulation calculation, vibration monitoring, and machine learning. The problem of phase asynchrony was solved, and strong support was provided for the construction of the ship's acoustic digital twin.}
}
@article{MILLO2023100156,
title = {Development of a neural network-based energy management system for a plug-in hybrid electric vehicle},
journal = {Transportation Engineering},
volume = {11},
pages = {100156},
year = {2023},
issn = {2666-691X},
doi = {https://doi.org/10.1016/j.treng.2022.100156},
url = {https://www.sciencedirect.com/science/article/pii/S2666691X22000549},
author = {Federico Millo and Luciano Rolando and Luigi Tresca and Luca Pulvirenti},
keywords = {Hybrid electric vehicle, Energy management system, Artificial intelligence, LSTM deep learning},
abstract = {The high potential of Artificial Intelligence (AI) techniques for effectively solving complex parameterization tasks also makes them extremely attractive for the design of the Energy Management Systems (EMS) of Hybrid Electric Vehicles (HEVs). In this framework, this paper aims to design an EMS through the exploitation of deep learning techniques, which allow high non-linear relationships among the data characterizing the problem to be described. In particular, the deep learning model was designed employing two different Recurrent Neural Networks (RNNs). First, a previously developed digital twin of a state-of-the-art plug-in HEV was used to generate a wide portfolio of Real Driving Emissions (RDE) compliant vehicle missions and traffic scenarios. Then, the AI models were trained off-line to achieve CO2 emissions minimization providing the optimal solutions given by a global optimization control algorithm, namely Dynamic Programming (DP). The proposed methodology has been tested on a virtual test rig and it has been proven capable of achieving significant improvements in terms of fuel economy for both charge-sustaining and charge-depleting strategies, with reductions of about 4% and 5% respectively if compared to the baseline Rule-Based (RB) strategy.}
}
@article{POLLOCK2023102761,
title = {Quality assurance of uncured polymer matrix prepregs through the application of non-destructive methods},
journal = {NDT & E International},
volume = {133},
pages = {102761},
year = {2023},
issn = {0963-8695},
doi = {https://doi.org/10.1016/j.ndteint.2022.102761},
url = {https://www.sciencedirect.com/science/article/pii/S0963869522001608},
author = {Luke Pollock and Sean O'Byrne and Graham Wild},
keywords = {Composites, Non-destructive testing, Non-destructive evaluation, Prepreg, Uncured},
abstract = {The Airbus A350 is 53% composite, primarily Hexcel prepreg, making uncured polymer matrix prepregs a key material in aerospace applications. Non-Destructive Testing and Evaluation (NDT&E) of prepregs is essential for Quality Assurance (QA) purposes during manufacturing. Following an introduction to fibre reinforced polymers, this study reports current and evolving technologies such as automated tape laying, automated fibre placement, and out-of-autoclave prepregs. Sources of variations in quality of prepregs are identified to highlight the need to ensure their quality as structural materials. The technologies that may be employed to ensure the quality of prepreg materials is broken into the three main life cycle stages prior to curing: manufacturing, storage, and lay-up. These stages are further sub-classified dependent upon the methodologies and technologies applied for QA. A final discussion is given regarding the future trends of NDT&E QA for prepreg materials including the potential use of digital twin and machine learning technologies for future aerospace systems.}
}
@article{LIU2022,
title = {A survey on blockchain-enabled federated learning and its prospects with digital twin},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001626},
author = {Kangde Liu and Zheng Yan and Xueqin Liang and Raimo Kantola and Chuangyue Hu},
keywords = {Digital twin, Artificial intelligence, Federated learning, Blockchain},
abstract = {Digital Twin (DT) supports real time analysis and provides a reliable simulation platform in the Internet of Things (IoT). The creation and application of DT hinges on amounts of data, which poses pressure on the application of Artificial Intelligence (AI) for DT descriptions and intelligent decision-making. Federated Learning (FL) is a cutting-edge technology that enables geographically dispersed devices to collaboratively train a shared global model locally rather than relying on a data center to perform model training. Therefore, DT can benefit by combining with FL, successfully solving the ”data island” problem in traditional AI. However, FL still faces serious challenges, such as enduring single-point failures, suffering from poison attacks, lacking effective incentive mechanisms. Before the successful deployment of DT, we should tackle the issues caused by FL. Researchers from industry and academia have recognized the potential of introducing Blockchain Technology (BT) into FL to overcome the challenges faced by FL, where BT acting as a distributed and immutable ledger, can store data in a secure, traceable, and trusted manner. However, to the best of our knowledge, a comprehensive literature review on this topic is still missing. In this paper, we review existing works about blockchain-enabled FL and visualize their prospects with DT. To this end, we first propose evaluation requirements with respect to security, fault-tolerance, fairness, efficiency, cost-saving, profitability, and support for heterogeneity. Then, we classify existing literature according to the functionalities of BT in FL and analyze their advantages and disadvantages based on the proposed evaluation requirements. Finally, we discuss open problems in the existing literature and the future of DT supported by blockchain-enabled FL, based on which we further propose some directions for future research.}
}
@article{TANG2022,
title = {Analyzing topics in social media for improving digital twinning based product development},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000657},
author = {Wenyi Tang and Ling Tian and Xu Zheng and Ke Yan},
keywords = {Digital twinning, Product development, Topic analysis, Social media},
abstract = {Digital twinning enables manufacturers to create digital representations of physical entities, thus implementing virtual simulations for product development. Previous efforts of digital twinning neglect the decisive consumer feedback in product development stages, failing to cover the gap between physical and digital spaces. This work mines real-world consumer feedbacks through social media topics, which is significant to product development. We specifically analyze the prevalent time of a product topic, giving an insight into both consumer attention and the widely-discussed time of a product. The primary body of current studies regards the prevalent time prediction as an accompanying task or assumes the existence of a preset distribution. Therefore, these proposed solutions are either biased in focused objectives and underlying patterns or weak in the capability of generalization towards diverse topics. To this end, this work combines deep learning and survival analysis to predict the prevalent time of topics. We propose a specialized deep survival model which consists of two modules. The first module enriches input covariates by incorporating latent features of the time-varying text, and the second module fully captures the temporal pattern of a rumor by a recurrent network structure. Moreover, a specific loss function different from regular survival models is proposed to achieve a more reasonable prediction. Extensive experiments on real-world datasets demonstrate that our model significantly outperforms the state-of-the-art methods.}
}
@article{GAO2022102515,
title = {Segmentation only uses sparse annotations: Unified weakly and semi-supervised learning in medical images},
journal = {Medical Image Analysis},
volume = {80},
pages = {102515},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102515},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522001621},
author = {Feng Gao and Minhao Hu and Min-Er Zhong and Shixiang Feng and Xuwei Tian and Xiaochun Meng and Ma-yi-di-li Ni-jia-ti and Zeping Huang and Minyi Lv and Tao Song and Xiaofan Zhang and Xiaoguang Zou and Xiaojian Wu},
keywords = {Weakly supervised learning, Semi-supervised learning, Medical image, Semantic segmentation},
abstract = {Since segmentation labeling is usually time-consuming and annotating medical images requires professional expertise, it is laborious to obtain a large-scale, high-quality annotated segmentation dataset. We propose a novel weakly- and semi-supervised framework named SOUSA (Segmentation Only Uses Sparse Annotations), aiming at learning from a small set of sparse annotated data and a large amount of unlabeled data. The proposed framework contains a teacher model and a student model. The student model is weakly supervised by scribbles and a Geodesic distance map derived from scribbles. Meanwhile, a large amount of unlabeled data with various perturbations are fed to student and teacher models. The consistency of their output predictions is imposed by Mean Square Error (MSE) loss and a carefully designed Multi-angle Projection Reconstruction (MPR) loss. Extensive experiments are conducted to demonstrate the robustness and generalization ability of our proposed method. Results show that our method outperforms weakly- and semi-supervised state-of-the-art methods on multiple datasets. Furthermore, our method achieves a competitive performance with some fully supervised methods with dense annotation when the size of the dataset is limited.}
}
@article{LI2022119030,
title = {Towards unified machine learning characterization of lithium-ion battery degradation across multiple levels: A critical review},
journal = {Applied Energy},
volume = {316},
pages = {119030},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119030},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922004354},
author = {Alan G. Li and Alan C. West and Matthias Preindl},
keywords = {Battery management systems, Machine learning, Lithium batteries},
abstract = {Lithium-ion battery (LIB) degradation is often characterized at three distinct levels: mechanisms, modes, and metrics. Recent trends in diagnostics and prognostics have been heavily influenced by machine learning (ML). This review not only provides a unique multi-level perspective on characterizing LIB degradation, but also highlights the role of ML in achieving higher accuracies with accelerated computation times. We survey the state-of-the-art in degradation research and show that existing techniques lay the foundations for a unified ML method – a single tool for characterizing degradation at multiple levels. This could inform optimal management of lithium-ion systems, thus extending lifetimes and reducing costs. We propose a framework for the hypothesized technique using pulse injection, digital-twinning, and neural networks, and identify the challenges and future trends in degradation research.}
}
@article{LEE2022102833,
title = {Control framework for collaborative robot using imitation learning-based teleoperation from human digital twin to robot digital twin},
journal = {Mechatronics},
volume = {85},
pages = {102833},
year = {2022},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2022.102833},
url = {https://www.sciencedirect.com/science/article/pii/S0957415822000691},
author = {Hyunsoo Lee and Seong Dae Kim and Mohammad Aman Ullah Al Amin},
keywords = {Collaborative robot, Teleoperation framework, Imitation learning, Digital twin, Bezier curve-based smooth pose mapping, Convolutional encoder-decoder},
abstract = {Despite the deployment of collaborative robots for various industrial processes, their teaching and control remain comparatively difficult tasks compared with general industrial robots. Various imitation learning methods involving the transfer of human poses to a collaborative robot have been proposed. However, most of these methods depend heavily on deep learning-based human recognition algorithms that fail to recognize complicated human poses. To address this issue, we propose an automated/semi-automated vision-based teleoperation framework using human digital twin and a collaborative robot digital twin models. First, a human pose is recognized and reasoned to a human skeleton model using a convolution encoder-decoder architecture. Next, the developed human digital twin model is taught using the skeletons. As human and collaborative robots have different joints and rotation architectures, pose mapping is achieved using the proposed Bezier curve-based smooth approximation. Then, a real collaborative robot is controlled using the developed robot digital twin. Furthermore, the proposed framework works successfully using a human digital twin in the case of recognition failures of human poses. To verify the effectiveness of the proposed framework, transfers of several human poses to a real collaborative robot are tested and analyzed.}
}
@article{LI2022677,
title = {Predicting hot-strip finish rolling thickness using stochastic configuration networks},
journal = {Information Sciences},
volume = {611},
pages = {677-689},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.07.173},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522008684},
author = {Xu Li and Yaodong He and Jingguo Ding and Feng Luan and Dianhua Zhang},
keywords = {Stochastic configuration networks, Industrial data modeling, Thickness prediction, Metal forming process},
abstract = {In the hot-rolling metal forming process, the consistency and accuracy of the thickness of the metal strip are the most important factors for the product quality control. The current method of utilizing a mechanism prediction model with pre-defined parameters does not perform well due to some limits on the model assumptions and environmental interference. Manually tuning these parameters of the mechanism model may even result in worse performance. To resolve this problem, an advanced randomized learner model, termed stochastic configuration network (SCN), is employed to build a data-driven prediction model which can be trained by using a dataset collected from a real-world hot-rolling production site. Based on the rolling theory and gray relational analysis (GRA), 36 features are selected as the inputs of the prediction model. Experimental results with comparisons show that our proposed method is feasible and outperforms other machine learning methods, such as deep learning models and the random vector functional link (RVFL) model.}
}
@article{VANDINTER2022107008,
title = {Predictive maintenance using digital twins: A systematic literature review},
journal = {Information and Software Technology},
volume = {151},
pages = {107008},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001331},
author = {Raymon {van Dinter} and Bedir Tekinerdogan and Cagatay Catal},
keywords = {Systematic literature review, Active learning, Digital twin, Predictive maintenance},
abstract = {Context
Predictive maintenance is a technique for creating a more sustainable, safe, and profitable industry. One of the key challenges for creating predictive maintenance systems is the lack of failure data, as the machine is frequently repaired before failure. Digital Twins provide a real-time representation of the physical machine and generate data, such as asset degradation, which the predictive maintenance algorithm can use. Since 2018, scientific literature on the utilization of Digital Twins for predictive maintenance has accelerated, indicating the need for a thorough review.
Objective
This research aims to gather and synthesize the studies that focus on predictive maintenance using Digital Twins to pave the way for further research.
Method
A systematic literature review (SLR) using an active learning tool is conducted on published primary studies on predictive maintenance using Digital Twins, in which 42 primary studies have been analyzed.
Results
This SLR identifies several aspects of predictive maintenance using Digital Twins, including the objectives, application domains, Digital Twin platforms, Digital Twin representation types, approaches, abstraction levels, design patterns, communication protocols, twinning parameters, and challenges and solution directions. These results contribute to a Software Engineering approach for developing predictive maintenance using Digital Twins in academics and the industry.
Conclusion
This study is the first SLR in predictive maintenance using Digital Twins. We answer key questions for designing a successful predictive maintenance model leveraging Digital Twins. We found that to this day, computational burden, data variety, and complexity of models, assets, or components are the key challenges in designing these models.}
}
@article{LIU2022102365,
title = {A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {78},
pages = {102365},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102365},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000539},
author = {Yongkui Liu and He Xu and Ding Liu and Lihui Wang},
keywords = {Deep reinforcement learning, Sim-to-real transfer, Digital twin, Robot grasping},
abstract = {Deep reinforcement learning (DRL) has proven to be an effective framework for solving various complex control problems. In manufacturing, industrial robots can be trained to learn dexterous manipulation skills from raw pixels with DRL. However, training robots in the real world is a time-consuming, high-cost and of safety concerns process. A frequently adopted approach for easing this is to train robots through simulations first and then deploy algorithms (or policies) on physical robots. How to transfer policies of robot learning from simulation to the real world is a challenging issue. Digital twin that is able to create a dynamic, up-to-date representation of a physical robotic grasping system provides an effective approach for addressing this issue. In this paper, we focus on the scenario of DRL-based assembly-oriented industrial grasping and propose a digital twin-enabled approach for achieving effective transfer of DRL algorithms to a physical robot. Two parallel training systems, i.e., the physical robotic system and corresponding digital twin system, respectively, are established, which take virtual and real images as inputs. The output of the digital twin system is used to correct the real grasping point so that accurate grasping can be achieved. Experimental results verify the effectiveness of the intelligent grasping algorithm and the digital twin-enabled sim-to-real transfer approach and mechanism.}
}
@article{CHIACHIO2022104333,
title = {Structural digital twin framework: Formulation and technology integration},
journal = {Automation in Construction},
volume = {140},
pages = {104333},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104333},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002060},
author = {Manuel Chiachío and María Megía and Juan Chiachío and Juan Fernandez and María L. Jalón},
keywords = {Digital twin, Petri nets, Bayesian learning, Internet of things, Structural health monitoring},
abstract = {This work presents a digital twin framework for structural engineering. The digital twin is conceptualised and mathematically idealised within the context of structural integrity, and includes the main attributes to behave as a functional digital twin, namely simulation, learning, and management. The manuscript makes special emphasis on the autonomous interactions between the physical and digital counterparts along with on the workflow modelling of the digital twin, which are both missing aspects in the majority of use cases found in the literature, specially within the civil and structural engineering domain. The proposed framework is demonstrated in a proof of concept using a laboratory scale test structure monitored using internet-of-things-based sensors and actuators. The results reveal that the virtual counterpart can respond in real-time with self-adaptability in liaison to the performance of the physical counterpart. Moreover, the tests show that the digital twin is able to provide automated decision making for structural integrity.}
}
@article{CLEMENTE2022118171,
title = {A proposal for an adaptive Recommender System based on competences and ontologies},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118171},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118171},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422013392},
author = {Julia Clemente and Héctor Yago and Javier {de Pedro-Carracedo} and Javier Bueno},
keywords = {Recommender system, , Ontology network, Methodological development, Student modeling},
abstract = {Context:
Competences represent an interesting pedagogical support in many processes like diagnosis or recommendation. From these, it is possible to infer information about the progress of the student to provide help targeted both, trainers who must make adaptive tutoring decisions for each learner, and students to detect and correct their learning weaknesses. For the correct development of any of these tasks, it is important to have a suitable student model that allows the representation of the most significant information possible about the student. Additionally, it would be very advantageous for this modeling to incorporate mechanisms from which it would be possible to infer more information about the student’s state of knowledge.
Objective:
To facilitate this goal, in this paper a new approach to develop an adaptive competence-based recommender system is proposed.
Method:
We present a methodological development guide as well as a set of ontological and non-ontological resources to develop and adapt the prototype of the proposed recommender system.
Results:
A modular flexible ontology network previously built for this purpose has been extended, which is responsible for recording the instructional design and student information. Furthermore, we describe a case study based on a first aid learning experience to assess the prototype with the proposed methodology.
Conclusions:
We highlight the relevance of flexibility and adaptability in learning modeling and recommendation processes. In order to promote improvement in the personalized learning of students, we present a Recommender System prototype taking advantages of ontologies, with a methodological guide, a broad taxonomy of recommendation criteria and the nature of competences. Future lines of research lines, including a more comprehensive evaluation of the system, will allow us to demonstrate in depth its adaptability according to the characteristics of the student, flexibility and extensibility for its integration in various environments and domains.}
}
@article{XU2022230,
title = {Knowledge distillation guided by multiple homogeneous teachers},
journal = {Information Sciences},
volume = {607},
pages = {230-243},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.05.117},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522005576},
author = {Quanzheng Xu and Liyu Liu and Bing Ji},
keywords = {Deep learning, Knowledge distillation (KD), Teacher–student model, Network compression},
abstract = {Knowledge distillation (KD) transfers knowledge from a heavy teacher network to a lightweight student network while maintaining the student’s performance closely to that of the teacher. However, the large gap between the teacher and the student in terms of capacity is not conducive to KD. Consequently, a large teacher network is not necessarily the most suitable teacher to guide the student. Therefore, this study proposes a multiple homogeneous teacher-guided KD method. First, multiple networks with the same structure as that of the student are pretrained to act as a teacher group, which is different from utilizing a large teacher network in traditional KD, to alleviate the capacity gap between the teacher and student. Second, a confidence-adaptive initialization strategy is developed to initialize the student network, which learns knowledge from the pretrained teacher group. Experiments are performed on CIFAR10, CIFAR100, and Tiny-ImageNet using three networks. The experimental results demonstrate that the proposed KD method outperforms existing advanced KD methods. Furthermore, a similarity loss function is introduced to optimize the parameters of the classifier in the student network. The experimental results indicate that this loss function improves the performance for basic classification tasks without KD and efficiently works in the proposed KD method.}
}
@article{PHUA2022103667,
title = {A digital twin hierarchy for metal additive manufacturing},
journal = {Computers in Industry},
volume = {140},
pages = {103667},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103667},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000641},
author = {A. Phua and C.H.J. Davies and G.W. Delaney},
keywords = {Digital twin, Additive manufacturing, Part qualification, Artificial intelligence, Control policy, Machine learning, Industry 4.0, Smart manufacturing},
abstract = {Digital twins present a conceptual framework for product life-cycle monitoring and control using a simulated replica of the physical system. Since their emergence, they have garnered particular attention as a shift away from costly physical testing and towards the use of high fidelity simulations, sensor data and intelligent control. Metal additive manufacturing (AM), a 3D printing technology prone to defects, requires a digital twin capable of tackling issues of printed part qualification, certification and optimisation. In this paper, we evaluate the key features specific to metal AM and review the current literature of modelling, sensing, control and machine intelligence. We find that the body of research toward the development of an metal additive manufacturing (AM) digital twin can be organised logically into a hierarchy of four levels of increasing complexity. The elements composing each level require deep integration and we highlight the key enabling technologies: surrogate modelling, in-situ sensing, hardware control systems and intelligent control policies. Our proposed digital twin hierarchy for AM provides a developer framework for engineering digital twins, both for AM and other intelligent manufacturing systems.}
}
@article{PHOON2022967,
title = {Unpacking data-centric geotechnics},
journal = {Underground Space},
volume = {7},
number = {6},
pages = {967-989},
year = {2022},
issn = {2467-9674},
doi = {https://doi.org/10.1016/j.undsp.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2467967422000514},
author = {Kok-Kwang Phoon and Jianye Ching and Zijun Cao},
keywords = {Data-centric geotechnics, Bayesian machine learning, Data-driven site characterization (DDSC), Project DeepGeo, Data-informed decision support index},
abstract = {The purpose of this paper (presented online as a keynote lecture at the 25th Annual Indonesian Geotechnical Conference on 10 Nov 2021) is to broadly conceptualize the agenda for data-centric geotechnics, an emerging field that attempts to prepare geotechnical engineering for digital transformation. The agenda must include (1) development of methods that make sense of all real-world data (not selective input data for a physical model), (2) offering insights of significant value to critical real-world decisions for current or future practice (not decisions for an ideal world or decisions of minor concern to geotechnical engineers), and (3) sensitivity to the physical context of geotechnics (not abstract data-driven analysis connected to geotechnics in a peripheral way, i.e., engagement with the knowledge and experience base should be substantial). These three elements are termed “data centricity”, “fit for (and transform) practice”, and “geotechnical context” in the agenda. Given that a knowledge of the site is central to any geotechnical engineering project, data-driven site characterization (DDSC) must constitute one key application domain in data-centric geotechnics, although other infrastructure lifecycle phases such as project conceptualization, design, construction, operation, and decommission/reuse would benefit from data-informed decision support as well. One part of DDSC that addresses numerical soil data in a site investigation report and soil property databases is pursued under Project DeepGeo. In principle, the source of data can also go beyond site investigation, and the type of data can go beyond numbers, such as categorical data, text, audios, images, videos, and expert opinion. The purpose of Project DeepGeo is to produce a 3D stratigraphic map of the subsurface volume below a full-scale project site and to estimate relevant engineering properties at each spatial point based on actual site investigation data and other relevant Big Indirect Data (BID). Uncertainty quantification is necessary, as current real-world data is insufficient, incomplete, and/or not directly relevant to construct a deterministic map. The value of a deterministic map for decision support is debatable. The computational cost to do this for a 3D true scale subsurface volume must be reasonable. Ultimately, geotechnical structures need to be a part of a completely smart infrastructure that fits the circular economy and need to focus on delivering service to end-users and the community from project conceptualization to decommission/reuse with full integration to smart city and smart society. Although current geotechnical practice has been very successful in taking “calculated risk” informed by limited data, imperfect theories, prototype testing, observations, among others and exercising judicious caution and engineering judgment, there is no clear pathway forward to leverage on big data and digital technologies such as machine learning, BIM, and digital twin to meet more challenging needs such as sustainability and resilience engineering.}
}
@article{LIU2022102390,
title = {Adaptive reconstruction of digital twins for machining systems: A transfer learning approach},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {78},
pages = {102390},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102390},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000771},
author = {Shimin Liu and Yuqian Lu and Pai Zheng and Hui Shen and Jinsong Bao},
keywords = {Digital twin, Machining system, Intelligent machining, Adaptability, Transfer learning},
abstract = {Digital twin technology has been gradually explored and applied in the machining process. A digital twin machining system creates high-fidelity virtual entities of physical entities to observe, analyze, and control the machining process in real-time. However, the current digital twin machining systems lack sufficient adaptability because they are usually customized for specific scenes. Usually, if a decision model is directly reused in a different working condition, the accuracy of the decision model is often poor and difficult to work effectively. Meanwhile, the decision model remodeled from scratch will cause a waste of resources and low modeling efficiency. This paper proposes an adaptive reconstruction method to adjust the decision model in the digital twin machining system to enhance adaptability. The proposed method can ensure the rapid development of the digital twin decision model under new working conditions. Finally, taking the drilling process as an example, this paper establishes the experimental drilling platform and verifies the feasibility of this method in the burr prediction task.}
}
@article{NIELSEN2022111579,
title = {Machine learning enhancement of manoeuvring prediction for ship Digital Twin using full-scale recordings},
journal = {Ocean Engineering},
volume = {257},
pages = {111579},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.111579},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822009453},
author = {Rasmus E. Nielsen and Dimitrios Papageorgiou and Lazaros Nalpantidis and Bugge T. Jensen and Mogens Blanke},
keywords = {Digital Twin, Machine learning, Ship motion prediction, Ship performance prediction, System identification, Manoeuvring simulation, Manoeuvring model},
abstract = {Digital Twins have much attention in the shipping industry, attempting to support all phases of a vessel’s life cycle. With several tools appearing in Digital Twin software suites, high-quality manoeuvring and performance prediction remain cornerstones. Propulsion efficiency is in focus while in service. Simulator-based training is in focus to ensure safety of manoeuvring in confined waters and harbours. Prediction of ships’ velocity and turn rate are essential for correct look and feel during training, but phenomena like dynamic inflow to propellers, bank and shallow water effects limit simulators’ accuracy, and master mariners often comment that simulations could be in better agreement with actual behaviours of their vessel. This paper focuses on digital twin enhancements to better match reality. Using data logged during in-service operation, we first consider a system identification perspective, employing a first-principles model structure. Showing that a complete first-principles model is not identifiable under the excitation met in service, we employ a Recurrent Neural Network to predict deviations between measured velocities and the model output. The outcome is a hybrid of a first-principles model with a machine learning generic approximator add-on. The paper demonstrates significant improvements in prediction accuracy of both in-harbour manoeuvring and shallow water passage conditions.}
}
@article{WANG2022102447,
title = {Semi-supervised medical image segmentation via a tripled-uncertainty guided mean teacher model with contrastive learning},
journal = {Medical Image Analysis},
volume = {79},
pages = {102447},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102447},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522000925},
author = {Kaiping Wang and Bo Zhan and Chen Zu and Xi Wu and Jiliu Zhou and Luping Zhou and Yan Wang},
keywords = {Semi-supervised segmentation, Mean teacher, Multi-task learning, Tripled-uncertainty, Contrastive learning},
abstract = {Due to the difficulty in accessing a large amount of labeled data, semi-supervised learning is becoming an attractive solution in medical image segmentation. To make use of unlabeled data, current popular semi-supervised methods (e.g., temporal ensembling, mean teacher) mainly impose data-level and model-level consistency on unlabeled data. In this paper, we argue that in addition to these strategies, we could further utilize auxiliary tasks and consider task-level consistency to better excavate effective representations from unlabeled data for segmentation. Specifically, we introduce two auxiliary tasks, i.e., a foreground and background reconstruction task for capturing semantic information and a signed distance field (SDF) prediction task for imposing shape constraint, and explore the mutual promotion effect between the two auxiliary and the segmentation tasks based on mean teacher architecture. Moreover, to handle the potential bias of the teacher model caused by annotation scarcity, we develop a tripled-uncertainty guided framework to encourage the three tasks in the student model to learn more reliable knowledge from the teacher. When calculating uncertainty, we propose an uncertainty weighted integration (UWI) strategy for yielding the segmentation predictions of the teacher. In addition, following the advance of unsupervised learning in leveraging the unlabeled data, we also incorporate a contrastive learning based constraint to help the encoders extract more distinct representations to promote the medical image segmentation performance. Extensive experiments on the public 2017 ACDC dataset and the PROMISE12 dataset have demonstrated the effectiveness of our method.}
}
@article{LIU2022,
title = {Design of modified model of intelligent assembly digital twins based on optical fiber sensor network},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001353},
author = {Zhichao Liu and Jinhua Yang and Juan Wang and Lin Yue},
keywords = {Digital twins, Intelligent manufacturing, Intelligent assembly, Optical fiber sensor network, Assembly condition monitoring algorithm},
abstract = {Intelligent assembly of large-scale, complex structures using an intelligent manufacturing platform represents the future development direction for industrial manufacturing. During large-scale structural assembly processes, several bottleneck problems occur in the existing auxiliary assembly technology. First, the traditional LiDAR-based assembly technology is often limited by the openness of the manufacturing environment, in which there are blind spots, and continuous online assembly adjustment thus cannot be realized. Second, for assembly of large structures, a single-station LiDAR system cannot achieve complete coverage, which means that a multi-station combination method must be used to acquire the complete three-dimensional data; many more data errors are caused by the transfer between stations than by the measurement accuracy of a single station, which means that the overall system's measurement and adjustment errors are increased greatly. Third, because of the large numbers of structural components contained in a large assembly, the accumulated errors may lead to assembly interference, but the LiDAR-assisted assembly process does not have a feedback perception capability, and thus assembly component loss can easily be caused when assembly interference occurs. Therefore, this paper proposes to combine an optical fiber sensor network with digital twin technology, which will allow the test data from the assembly entity state in the real world to be applied to the “twin” model in the virtual world and thus solve the problems with test openness and data transfer. The problem of station and perception feedback is also addressed and represents the main innovation of this work. The system uses an optical fiber sensor network as a flexible sensing medium to monitor the strain field distribution within a complex area in real time, and then completes real-time parameter adjustment of the virtual assembly based on the distributed data. Complex areas include areas that are laser-unreachable, areas with complex contact surfaces, and areas with large-scale bending deformations. An assembly condition monitoring system is designed based on the optical fiber sensor network, and an assembly condition monitoring algorithm based on multiple physical quantities is proposed. The feasibility of use of the optical fiber sensor network as the real-state parameter acquisition module for the digital twin intelligent assembly system is discussed. The offset of any position in the test area is calculated using the convolutional neural network of a residual module to provide the compensation parameters required for the virtual model of the assembly structure. In the model optimization parameter module, a correction data table is obtained through iterative learning of the algorithm to realize state prediction from the test data. The experiment simulates a large-scale structure assembly process, and performs virtual and real mapping for a variety of situations with different assembly errors to enable correction of the digital twin data stream for the assembly process through the optical fiber sensor network. In the plane strain field calibration experiment, the maximum error among the test values for this system is 0.032 mm, and the average error is 0.014 mm. The results show that use of visual calibration can correct the test error to within a very small range. This result is equally applicable to gradient curvature surfaces and freeform surfaces. Statistics show that the average measurement accuracy error for regular surfaces is better than 11.2%, and the average measurement accuracy error for irregular surfaces is better than 14.8%. During simulation of large-scale structure assembly experiments, the average position deviation accuracy is 0.043 mm, which is in line with the designed accuracy.}
}
@article{BELLAVISTA2022101646,
title = {Digital twin oriented architecture for secure and QoS aware intelligent communications in industrial environments},
journal = {Pervasive and Mobile Computing},
volume = {85},
pages = {101646},
year = {2022},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2022.101646},
url = {https://www.sciencedirect.com/science/article/pii/S1574119222000736},
author = {Paolo Bellavista and Carlo Giannelli and Marco Mamei and Matteo Mendula and Marco Picone},
keywords = {Internet of things, Industry 4.0, Zones and conduits, Digital twin, Intelligent reconfiguration},
abstract = {In modern networking industrial environments, characterized by the integration of Operation Technology and Information Technology, there is a strong need to ensure both safety and security of operations and communications. In this regard, IEC 62443 zones and conduits represent powerful high-level abstractions stressing the importance of clearly separating machines in relation to safety requirements and of clearly defining inter-machine communication security requirements. However, their actual implementation is still demanded to human-centric error-prone procedures performed by technicians directly on network elements, without any integrated plant-wide point of view. To overcome these issues, first of all we originally state the need of applying the Digital Twin approach to zones and conduits, making easier the definition and management of inter-machine security requirements. For instance, industrial technicians can specify that communication among two zones should always flows through a ciphered conduit with a given algorithm and key length, at the cost of increased latency. Secondly, we state the need of exploiting an intelligent reasoner to monitor the current state of the environment (represented by asset and network Digital Twins), actively reconfiguring them in case desired requirements are not satisfied. Then, the reasoner allows to enforce requirements while also considering the fulfillment of a proper trade-off between security and performance, e.g., by reducing the ciphering complexity to ensure prompt packet dispatching whenever required. Performance results based on our working prototype demonstrate the feasibility and efficiency of the proposed solution under stringent requirements typical of industrial environments. In particular, in terms of better flexibility we proved that our orchestrator is able to create a new Digital Twin in less than 2.5 s in a typical edge node with a medium load. In addition, proposed routing policies based on our machine learning reasoner led to the satisfaction of well-defined low latency requirements (250 ms) while avoiding packet dropping.}
}
@article{OSHO2022370,
title = {Four Rs Framework for the development of a digital twin: The implementation of Representation with a FDM manufacturing machine},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {370-380},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000656},
author = {John Osho and Anna Hyre and Minas Pantelidakis and Allison Ledford and Gregory Harris and Jia Liu and Konstantinos Mykoniatis},
keywords = {Digital twin, Cyber-physical systems, Industry 4.0, Additive manufacturing, Fused deposition modeling},
abstract = {This work considers the conceptualization and design of a 4 Rs framework for creating a general purpose, modular Digital Twin. The 4 Rs, correspond to the 4 different phases of a Digital Twin implementation, namely Representation, Replication, Reality, and Relational. Representation is about understanding the physical system, its behavior, actions, components, relationships and describing the significant features for the identified use case as data and algorithms. Replication duplicates the chosen components/variables in a virtual environment from a set of inputs identified in Representation. Reality employs machine learning to produce a virtual device that runs independent of the physical device with the ability to make predictions, enhance models, provide alternative scenarios and optimizations. Reality enhances the virtual system to become autonomous and self-aware with the ability to make decisions and take corrective actions. We introduce these phases and outline their core elements and principles. We showcase the implementation of phase 1, Representation, using a Fused Deposition Modeling (FDM) additive manufacturing machine via temperature and position sensors. We evaluate their precision in representing the actual FDM machine and lay the foundation work for the implementation of the 4R framework in our next work.}
}
@article{ZHANG2022238,
title = {A digital twin dosing system for iron reverse flotation},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {238-249},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000413},
author = {Dingsen Zhang and Xianwen Gao},
keywords = {Digital twin, Dosage of reagent, Iron reverse flotation, Soft sensor},
abstract = {Flotation is an essential process in beneficiation production. The amount of flotation reagent has a significant influence on the quality of the product. An inappropriate dosing system will lead to metal loss and reagent waste, especially when the nature of the raw ore changes frequently. If the dosing system is not updated in time, it will cause economic losses. Based on digital twin technology and machine learning algorithms, this research designed a digital twin system for iron reverse flotation reagents. Based on the flotation froth image and transformer algorithm, a soft sensor model of tailings grade is established to monitor the product quality in real-time. The flotation dosing model established based on the ELM algorithm automatically updates the reagent system and intelligently assigns the controller. On the basis of stabilizing product quality, this research avoids the waste of reagents and improves the economic benefits of production efficiency. The system was applied in an iron flotation plant, and industrial operation effect verified the method.}
}
@article{PURCELL2023100094,
title = {Digital Twins in Agriculture: A State-of-the-art review},
journal = {Smart Agricultural Technology},
volume = {3},
pages = {100094},
year = {2023},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2022.100094},
url = {https://www.sciencedirect.com/science/article/pii/S2772375522000594},
author = {Warren Purcell and Thomas Neubauer},
keywords = {Digital Twin, Agriculture},
abstract = {The Digital Twin enables the distinctions between state sensing, entity understanding and physical automation to be eliminated, through high-fidelity modelling and bi-directional data streams. The concept of real-time virtual representation places the Digital Twin in a unique position to enable digitization in agriculture. The union of data, modelling and what-if simulation can provide an approach to overcome current limitations in decision-making support and automation, across a diverse range of agricultural enterprises. This paper conducts a Systematic Literature Review of Digital Twins in agriculture, identifying current trends and open questions with the goal of increasing awareness and understanding of the Digital Twin and its possibilities.}
}
@article{PANTOJAROSERO2022104430,
title = {Generating LOD3 building models from structure-from-motion and semantic segmentation},
journal = {Automation in Construction},
volume = {141},
pages = {104430},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104430},
url = {https://www.sciencedirect.com/science/article/pii/S092658052200303X},
author = {B.G. Pantoja-Rosero and R. Achanta and M. Kozinski and P. Fua and F. Perez-Cruz and K. Beyer},
keywords = {Digital twin, LOD models, Deep learning, Structure from motion, 3D building models, Masonry buildings},
abstract = {This paper describes a pipeline for automatically generating level of detail (LOD) models (digital twins), specifically LOD2 and LOD3, from free-standing buildings. Our approach combines structure from motion (SfM) with deep-learning-based segmentation techniques. Given multiple-view images of a building, we compute a three-dimensional (3D) planar abstraction (LOD2 model) of its point cloud using SfM techniques. To obtain LOD3 models, we use deep learning to perform semantic segmentation of the openings in the two-dimensional (2D) images. Unlike existing approaches, we do not rely on complex input, pre-defined 3D shapes or manual intervention. To demonstrate the robustness of our method, we show that it can generate 3D building shapes from a collection of building images with no further input. For evaluating reconstructions, we also propose two novel metrics. The first is a Euclidean–distance-based correlation of the 3D building model with the point cloud. The second involves re-projecting 3D model facades onto source photos to determine dice scores with respect to the ground-truth masks. Finally, we make the code, the image datasets, SfM outputs, and digital twins reported in this work publicly available in github.com/eesd-epfl/LOD3_buildings and doi.org/10.5281/zenodo.6651663. With this work we aim to contribute research in applications such as construction management, city planning, and mechanical analysis, among others.}
}
@article{WU2022104252,
title = {Real-time mixed reality-based visual warning for construction workforce safety},
journal = {Automation in Construction},
volume = {139},
pages = {104252},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104252},
url = {https://www.sciencedirect.com/science/article/pii/S092658052200125X},
author = {Shaoze Wu and Lei Hou and Guomin (Kevin) Zhang and Haosen Chen},
keywords = {Construction safety, Mixed reality, Digital twin, Deep learning, Wearable device, Visualisation},
abstract = {Spatial locations of personnel, equipment, and materials are constantly changing as construction projects progress. The dynamic nature of the construction industry affects workers' performance of identifying hazards. Even though a great deal of effort has been made to improve construction safety, the construction industry still witnesses a high accident rate. In order to complement the existing body of knowledge relating to construction safety, this paper integrates Digital Twin (DT), Deep Learning (DL), and Mixed Reality (MR) technologies into a newly developed real-time visual warning system, which enables construction workers to proactively determine their safety status and avoid accidents. Next, system tests were conducted under three quasi-on-site scenarios, and the feasibility was proven in terms of synchronising construction activities over a large area and visually representing hazard information to its users. These evidenced merits of the development testing scenarios can improve workers' risk assessment accuracy, reinforce workers' safety behaviour, and provide a new perspective for construction safety managers to analyse construction safety status.}
}
@article{ALRIGE20221124,
title = {Utilizing geospatial intelligence and user modeling to allow for a customized health awareness campaign during the pandemic: The case of COVID-19 in Saudi Arabia},
journal = {Journal of Infection and Public Health},
volume = {15},
number = {10},
pages = {1124-1133},
year = {2022},
issn = {1876-0341},
doi = {https://doi.org/10.1016/j.jiph.2022.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S1876034122002337},
author = {Mayda Alrige and Hind Bitar and Maram Meccawy and Balakrishnan Mullachery},
keywords = {COVID-19, Geospatial intelligence, Space-time cube (STC), User modeling, Customization, Health awareness campaign},
abstract = {Background
As of 2022, people are getting better at learning how to coexist with the Covid-19 global pandemic. In Saudi Arabia, many attempts have been made to raise public health awareness. However, most health awareness campaigns are generic and might not influence the desired behavior among individuals.
Objectives
This study aims to apply geospatial intelligence and user modeling to profile the districts of the city of Jeddah. This customized map can provide a baseline for a customized health awareness campaign that targets the locals of each district individually based on the virus spread level.
Methodology
It is ongoing research, which has resulted in the creation of a health messages library in the first phase [1]. This paper focuses on a second phase of the research study, which aims to provide a customized baseline for this campaign by applying the geospatial artificial intelligence technique known as space-time cube (STC). STC was applied to create a local map of the Saudi city of Jeddah, representing three different profiles for the city’s districts. The model is built using valid COVID-19 clinical data obtained from one of Jeddah’s general hospitals.
Results and implications
When applied, STC displays three profiles for the districts of Jeddah city: high infection, moderate infection, and low infection. To assess the geo-intelligent map, a new instrument was created and validated. The usability and practicality of this map were quantitatively evaluated in a cross-sectional survey using the goal-question-metric measurement framework, and a total of 43 participants filled out the questionnaire. The results indicate that the geo-intelligent map is suitable for everyday use, as evidenced by the participants’ responses. We argue that the developed instrument can also be used to assess any geo-intelligence map. This research provides a legitimate approach to customizing health awareness messages during pandemics.}
}
@article{PANG2022102859,
title = {3D building reconstruction from single street view images using deep learning},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102859},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102859},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222000619},
author = {Hui En Pang and Filip Biljecki},
keywords = {3D geoinformation, GeoAI, Urban morphology, Digital twin, Google Street View, 3D GIS},
abstract = {3D building models are an established instance of geospatial information in the built environment, but their acquisition remains complex and topical. Approaches to reconstruct 3D building models often require existing building information (e.g.their footprints) and data such as point clouds, which are scarce and laborious to acquire, limiting their expansion. In parallel, street view imagery (SVI) has been gaining currency, driven by the rapid expansion in coverage and advances in computer vision (CV), but it has not been used much for generating 3D city models. Traditional approaches that can use SVI for reconstruction require multiple images, while in practice, often only few street-level images provide an unobstructed view of a building. We develop the reconstruction of 3D building models from a single street view image using image-to-mesh reconstruction techniques modified from the CV domain. We regard three scenarios: (1) standalone single-view reconstruction; (2) reconstruction aided by a top view delineating the footprint; and (3) refinement of existing 3D models, i.e.we examine the use of SVI to enhance the level of detail of block (LoD1) models, which are common. The results suggest that trained models supporting (2) and (3) are able to reconstruct the overall geometry of a building, while the first scenario may derive the approximate mass of the building, useful to infer the urban form of cities. We evaluate the results by demonstrating their usefulness for volume estimation, with mean errors of less than 10% for the last two scenarios. As SVI is now available in most countries worldwide, including many regions that do not have existing footprint and/or 3D building data, our method can derive rapidly and cost-effectively the 3D urban form from SVI without requiring any existing building information. Obtaining 3D building models in regions that hitherto did not have any, may enable a number of 3D geospatial analyses locally for the first time.}
}
@article{TARIQ2022133517,
title = {Computational intelligence for empirical modeling and optimization of methylene blue adsorption phenomena using available local zeolites and clay of Morocco},
journal = {Journal of Cleaner Production},
volume = {370},
pages = {133517},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133517},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622030979},
author = {Rasikh Tariq and Mohamed Abatal and A. Bassam},
keywords = {Artificial neural network, Particle swarm optimization, Genetic algorithm, Simulated annealing, Support vector machine, Sensitivity analysis, Methylene blue},
abstract = {The present study investigated the performance of natural Moroccan zeolites and clay in the removal of methylene blue (MB) from aqueous solutions. The deposits of the samples are extracted from the Teteoune (sample 1), Kenitra (sample 2), Khroubga (sample 3), and Benghrir (sample 4) regions of Morocco. The samples were characterized by the X-ray diffraction technique. Sorption experiments were carried out by batch experimental to examine the effects of contact time, solution pH, initial concentration of MB, and biosorbent dosage of samples. In the next step, an integrated methodology is adapted by applying the computational intelligence packages to this problem to develop empirical models and to conduct an optimization to maximize the removal percentage. The empirical or digital twin models are generated using machine learning techniques including regression tree (RT), support vector machine, ensemble, and gaussian process regression (GPR), a statistical technique including multivariate regression (MVR), and neural network techniques including artificial neural network (ANN) and group method of data handling (GMDH). The results have shown that the ANN technique having 10 hidden layers trained using the Bayesian regularization algorithm can accurately predict the removal percentage with an RMSE = 0.88, R2 = 0.9961, MSE = 0.67, and MAE = 0.43. The second-best technique is GPR followed by GMDH. The worst techniques are MVR and RT. The single-objective optimization was applied using simulated annealing (SA), particle swarm optimization (PSO), and genetic algorithm (GA) to maximize the removal percentage for each material to find the optimal mass, initial concentration, contact time, and pH. The results have revealed that the SA technique is not fully capable to find the true optima, whereas PSO and GA have shown conformity on the optimal removal percentage. The optimized indicator came out to be 85.32%, 90.23%, 93.25%, and 97.21% for material 1, 2, 3, and 4 which was 49%, 14.69%, 20.16%, and 45.11% for material 1, 2, 3, and 4, respectively. It is concluded that the sample from the region of Benghrir has the best removal performance of methylene blue from aqueous solutions marking up to a high digit of 97.21%.}
}
@article{FELICETTI2022819,
title = {Deep stochastic configuration networks with different random sampling strategies},
journal = {Information Sciences},
volume = {607},
pages = {819-830},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522006120},
author = {Matthew J. Felicetti and Dianhui Wang},
keywords = {Stochastic configuration networks, Weight distributions, Random sampling, Generalization},
abstract = {Stochastic configuration networks (SCNs) are a class of randomized learner models that ensure the universal approximation property, whereby random weights and biases are drawn from the uniform distribution and selected by a supervisory mechanism. This paper looks into the impact of the distribution of random weights on the performance of SCNs. In the light of a fundamental principle in machine learning, that is, a model with smaller parameters holds improved generalization, we recommend using symmetric zero-centered distributions in constructing SCNs to improve the generalization performance. Further, we introduce a scalar in the distributions to make the SCN model adaptively feasible to different datasets. Simulation results are reported for both regression and classification tasks over twenty-one benchmark datasets using SCN. Results are also presented on ten regression datasets using a deep implementation of SCN, known as deep stochastic configuration networks (DeepSCN).}
}
@article{RITTO2022109485,
title = {Reinforcement learning and approximate Bayesian computation for model selection and parameter calibration applied to a nonlinear dynamical system},
journal = {Mechanical Systems and Signal Processing},
volume = {181},
pages = {109485},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109485},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022005970},
author = {T.G. Ritto and S. Beregi and D.A.W. Barton},
keywords = {Nonlinear dynamics, Parameter identification, Model selection, Reinforcement learning, ABC, Decision under uncertainty},
abstract = {In the context of digital twins and integration of physics-based models with machine learning tools, this paper proposes a new methodology for model selection and parameter identification. It combines (i) reinforcement learning (RL) for model selection through a Thompson-like sampling with (ii) approximate Bayesian computation (ABC) for parameter identification and uncertainty quantification. These two methods are applied together to a nonlinear mechanical oscillator with periodic forcing. Experimental data are used in the analysis and two different nonlinear models are tested. The initial Beta distribution that represents the likelihood of the model is updated depending on how successful the model is at reproducing the reference data (reinforcement learning strategy). At the same time, the prior distribution of the model parameters is updated using a likelihood-free strategy (ABC). In the end, the rewards and the posterior distribution of the parameters of each model are obtained. The results show that the combined methodology (RL-ABC) is promising for model selection from bifurcation diagrams. Prior parameter distribution was successfully updated, correlations between parameters were found, probabilistic envelopes of the posterior model are consistent with the available data, the most rewarded model was selected, and the reinforcing strategy allows to speed up the selection process.}
}
@article{FU2022118973,
title = {The role of deep learning in urban water management: A critical review},
journal = {Water Research},
volume = {223},
pages = {118973},
year = {2022},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2022.118973},
url = {https://www.sciencedirect.com/science/article/pii/S0043135422009204},
author = {Guangtao Fu and Yiwen Jin and Siao Sun and Zhiguo Yuan and David Butler},
keywords = {Artificial intelligence, Data analytics, Deep learning, Digital twin, Water management},
abstract = {Deep learning techniques and algorithms are emerging as a disruptive technology with the potential to transform global economies, environments and societies. They have been applied to planning and management problems of urban water systems in general, however, there is lack of a systematic review of the current state of deep learning applications and an examination of potential directions where deep learning can contribute to solving urban water challenges. Here we provide such a review, covering water demand forecasting, leakage and contamination detection, sewer defect assessment, wastewater system state prediction, asset monitoring and urban flooding. We find that the application of deep learning techniques is still at an early stage as most studies used benchmark networks, synthetic data, laboratory or pilot systems to test the performance of deep learning methods with no practical adoption reported. Leakage detection is perhaps at the forefront of receiving practical implementation into day-to-day operation and management of urban water systems, compared with other problems reviewed. Five research challenges, i.e., data privacy, algorithmic development, explainability and trustworthiness, multi-agent systems and digital twins, are identified as key areas to advance the application and implementation of deep learning in urban water management. Future research and application of deep learning systems are expected to drive urban water systems towards high intelligence and autonomy. We hope this review will inspire research and development that can harness the power of deep learning to help achieve sustainable water management and digitalise the water sector across the world.}
}
@article{ZHAO2022107247,
title = {Digital twin for rapid damage detection of a fixed net panel in the sea},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107247},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107247},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005610},
author = {Yun-Peng Zhao and Likai Lian and Chun-Wei Bi and Zhijing Xu},
keywords = {Aquaculture net, Artificial neural network, Digital twin, Damage detection},
abstract = {The fishing net is the most important component of aquaculture net cages. Once damaged, it can cause substantial economic losses and ecological problems to the aquaculture industry. To avoid these subsequent issues caused by the damage to the fishing net, seeking a computerized, labor-saving approach and detecting damage in real time becomes the primary task of this paper. Inspired by recent development of the artificial neural networks and machine learning, this study proposes a fast and accurate approach for fishing net damage detection based on digital twin. Firstly, time-domain numerical simulations of the fishing net are conducted in a series of wave and current conditions to develop the artificial neural network-based digital twin. Then, the significant wave height, the spectral peak period, and tensions of vertical and horizontal ropes are used as input variables during the artificial neural network training; the intact and damaged states of the fishing net are considered outputs. Alongside this, the back-propagation learning algorithm is used for training to maximize damage detection performance. The results highlight that the digital twin model can effectively identify the fishing net damage using the sensor data, and the accuracy of the damage detection reaches above 93%. When the damage occurs at different net positions, the prediction accuracies of the artificial neural network model for training and testing sets are 93.97% and 93.00%, respectively. Regarding different wave-current directions, the prediction accuracies of the artificial neural network model are 99.87 % and 99.34 % for training and testing sets. Moreover, the developed digital twin can accurately detect the damage of the net even sea conditions and sensor data are not included in the training. The digital twin model proposed in the present study can potentially be used for damage detection in other aquaculture components and structures.}
}
@article{ZHAN2022112278,
title = {Calibrating building simulation models using multi-source datasets and meta-learned Bayesian optimization},
journal = {Energy and Buildings},
volume = {270},
pages = {112278},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112278},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822004492},
author = {Sicheng Zhan and Gordon Wichern and Christopher Laughman and Adrian Chong and Ankush Chakrabarty},
keywords = {Meta learning, Deep learning, Parameter estimation, Probabilistic machine learning, Bayesian methods, Digital twin},
abstract = {Reliable building simulation models are key to optimizing building performance and reducing greenhouse gas emissions. Informed decision making requires simulation models to be accurate, extrapolatable, and interpretable, all of which require calibrating model simulations to ground truth. Complicated building dynamics and highly uncertain exogenous disturbances make the model calibration process challenging and expensive; hence, a scalable and efficient calibration approach is needed to enable actual application. Current automatic calibration algorithms do not leverage data collected from multiple sources: for example, data obtained from previous calibration tasks on other buildings. In this paper, we employ probabilistic deep learning to meta-learn a distribution using multi-source data acquired during previous calibration. Subsequently, the meta-learned Bayesian optimizer accelerates calibration of new, unseen tasks. The few-shot (that is, requiring few model simulations) nature of the proposed algorithm is demonstrated on a Modelica library of residential buildings validated by the United States Department of Energy (USDoE). The proposed algorithm is compared against classical Bayesian optimization-based calibration, and it is shown that ANP significantly sped up the calibration procedure: the optimal model parameters are identified with 40–60% less simulations compared to the baseline.}
}
@article{PAN2022104375,
title = {Enriching geometric digital twins of buildings with small objects by fusing laser scanning and AI-based image recognition},
journal = {Automation in Construction},
volume = {140},
pages = {104375},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104375},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002485},
author = {Yuandong Pan and Alexander Braun and Ioannis Brilakis and André Borrmann},
keywords = {Digital twin, Deep learning, Object detection, Text recognition, 3D reconstruction},
abstract = {This paper addresses the challenge of enriching geometric digital twins of buildings, with a particular emphasis on capturing small but important entities from the electrical and the fire-safety domain, such as signs, sockets, switches, smoke alarms, etc. Unlike most previous research that focussed on structural elements and processed laser point clouds and images separately, we propose a novel method that fuses laser scanning and photogrammetry methods to capture the relevant objects, recognise them in 2D images and then map these to a 3D space. The considered object classes include electrical elements (light switch, light, speaker, socket, elevator button), safety elements (emergency switch, smoke alarm, fire extinguisher, escape sign), plumbing system elements (pipes), and other objects with useful information (door sign, board). Semantic information like class labels is extracted by applying AI-based image segmentation and then mapped to the 3D point cloud, segmenting the point cloud into point clusters. We subsequently fit geometric primitives to the point clusters and extract text information by AI-based text detection and recognition. The final output of our proposed method is an information-rich digital twin of buildings that contains geometric information, semantic information such as object categories and useful text information which is valuable in many aspects, like condition monitoring, facility maintenance and management. In summary, the paper presents a nearly fully-automated pipeline to enrich a geometric digital twin of buildings with details and provides a comprehensive case study.}
}
@article{KANDASAMY2022108061,
title = {An electric power digital twin for cyber security testing, research and education},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {108061},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108061},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622003196},
author = {Nandha Kumar Kandasamy and Sarad Venugopalan and Tin Kit Wong and Nicholas Junming Leu},
keywords = {Cyber Physical System, Smartgrid, Security, Digital twin, Test-bed},
abstract = {Cyber–Physical Systems (CPS) rely on communication and control technologies to efficiently manage devices in the system. However, a wide variety of potential security challenges has emerged due to the evolution of critical infrastructures (CI) from siloed sub-systems into integrated networks, including smart grid (SG). SG security studies are carried out on physical test-beds to train and test cyber attacks in a safe/controlled environment. However, it has limitations w.r.t modifying physical configuration and scalability. To overcome these shortcomings, we built a digital power twin for a physical test-bed that is used for cyber security studies on SGs. The twin enable users to deploy real world attacks and countermeasures, and study its effectiveness. In the twin users may easily modify the components and configurations unlike in a physical test-bed. Further, reproducing the twin for advancing the research is significantly cheaper. We illustrate the typical use case with an example case study.}
}
@article{XIE2022105972,
title = {Semi-supervised region-connectivity-based cerebrovascular segmentation for time-of-flight magnetic resonance angiography image},
journal = {Computers in Biology and Medicine},
volume = {149},
pages = {105972},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105972},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522007004},
author = {Lei Xie and Zan Chen and Xuanshuo Sheng and Qingrun Zeng and Jiahao Huang and Caiyun Wen and Liang Wen and Guoqiang Xie and Yuanjing Feng},
keywords = {TOF-MRA, Cerebrovascular segmentation, Deep learning, Semi-supervised learning, Region-connectivity-based},
abstract = {Deep-learning-based methods have achieved state-of-the-art results in cerebrovascular segmentation. However, it is costly and time-consuming to acquire labeled data because of the complex structure of cerebral vessels. In this paper, we propose a novel semi-supervised cerebrovascular segmentation with a region-connectivity-based mean teacher model (RC-MT) from time-of-flight magnetic resonance angiography (TOF-MRA), where unlabeled data is introduced into the training. Concretely, the RC-MT framework consists of a mean teachers (MT) model and a region-connectivity-based model. The region-connectivity-based model dynamically controls the balance between the supervised loss and unsupervised consistency loss by taking into account that the predicted vessel voxels should be continuous in the underlying anatomy of the brain. Meanwhile, we design a novel multi-scale channel attention fusion Unet (MSCAF-Unet) as a backbone for the student model and the teacher model. The MSCAF-Unet is a multi-scale channel attention fusion layer used to construct an image pyramid input and achieve multi-level receptive field fusion. The proposed method is evaluated on diverse TOF-MRA datasets (three clinical datasets and a public dataset). Experimental results show that the proposed method achieves high-performance gains by incorporating the unlabeled data and outperforms competing semi-supervised-based methods. The code will be openly available at https://github.com/IPIS-XieLei/RC-MT.}
}
@article{YANG2022109156,
title = {A novel quantitative relationship neural network for explainable cognitive diagnosis model},
journal = {Knowledge-Based Systems},
volume = {250},
pages = {109156},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109156},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122005755},
author = {Haowen Yang and Tianlong Qi and Jin Li and Longjiang Guo and Meirui Ren and Lichen Zhang and Xiaoming Wang},
keywords = {Cognitive diagnosis, Cognitive status, Learner modeling, Quantitative relationships, Implicit concepts},
abstract = {Cognitive diagnosis is a fundamental task to assist personalized learning in education, and aims to discover learners’ proficiency in knowledge concepts. Because cognitive diagnosis models play a very important role in predicting learner performance and recommending personalized learning resources such as exercises, course videos, and course audio, they have received great attention from researchers. However, existing cognitive diagnosis models mostly start from the interactive perspective of learners’ answers, ignoring the internal quantitative relationship between exercises and knowledge concepts. This study proposes a novel quantitative relationship-based explainable cognitive diagnosis model called QRCDM. First, learners’ concept proficiency was defined based on their answers to objective and subjective questions. Correlation hypotheses are then proposed, which include the explicit correlation between exercises and their corresponding knowledge concepts, as well as the implicit correlation between exercises and the non-inclusive concept. Finally, two contribution matrices of exercises and knowledge concepts through a neural network designed in this study are calculated based on the above hypotheses, which can predict the learner’s concept proficiency and answer score. To reduce the noisy data, the learners’ faults and guessing factors were also considered. In the experiments, the proposed QRCDM was compared with two classical models, DINA, FuzzyCDF and three latest state-of-the-art models, DeepCDM, NeuralCDM and RCD on five real datasets, and the most experimental results on the majority metrics show the effectiveness and interpretability of this work.}
}
@article{NASER2022104640,
title = {Digital twin for next gen concretes: On-demand tuning of vulnerable mixtures through Explainable and Anomalous Machine Learning},
journal = {Cement and Concrete Composites},
volume = {132},
pages = {104640},
year = {2022},
issn = {0958-9465},
doi = {https://doi.org/10.1016/j.cemconcomp.2022.104640},
url = {https://www.sciencedirect.com/science/article/pii/S0958946522002335},
author = {M.Z. Naser},
keywords = {Machine learning, Digital twin, Concrete, Explainability, Clustering},
abstract = {This paper presents a framework for integrating Explainable and Anomalous Machine Learning (EAML) into a digital twin to enable finetuning of mixtures as a mean to realize next-gen concretes with favorable performance. In this framework, both anomalous unsupervised and explainable supervised ML algorithms are joined to create a virtual assistant capable of exploring the influence of mixture materials and proportions on the required performance of concrete. This virtual assistant is not only trained to detect inherent vulnerabilities within mixtures but can also finetune such mixtures to overcome potential weaknesses – especially when concrete is expected to serve under extreme loading conditions. The proposed framework has been rigorously examined on three case studies to identify vulnerable mixtures to: 1) fire-induced spalling, 2) chloride penetration, and 3) failing to attain full design strength in job sites, using small and large datasets comprised from actual measurements. Results from our analysis show how the proposed framework was capable of identifying vulnerable concrete mixtures and of satisfying various performance metrics. While the proposed framework is designed to be algorithm-independent and hence can be scalable across multiple platforms, this work showcases the application of anomaly detecting and clustering algorithms, together with an ensemble of classifiers encompassing extreme and light gradient boosted trees (GBT), generalized additive models (GAM), and keras deep residual neural network (KDP).}
}
@article{YANG2022109367,
title = {Assessment of reactor flow field prediction based on deep learning and model reduction},
journal = {Annals of Nuclear Energy},
volume = {179},
pages = {109367},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109367},
url = {https://www.sciencedirect.com/science/article/pii/S0306454922004029},
author = {Jun Yang and Xi Sui and Yanping Huang and Ling Zhao and Minyun Liu},
keywords = {Digital twin, Computational fluid dynamics, Model reduction, Singular value decomposition, Deep learning},
abstract = {All-around and full-cycle digital simulation technology can improve the safety and economy of the reactor in research, development, operation, and maintenance processes. However, for the local three-dimensional fluid dynamic problems in complex reactor system, traditional computational fluid dynamic (CFD) methods are severely limited by the solution efficiency and accuracy. This study proposed a rapid flow field prediction method based on singular value decomposition (SVD) and deep learning. The raw flow field snapshot data was compressed using SVD to extract low-dimensional features, and the deep neural network was used to construct a flow field reduced-order model to achieve rapid flow field prediction. A benchmark problem of flow around a cylinder was selected to assess the efficacy of this method. Furtherly, this method was applied in the three-dimensional flow field simulations of the fuel assembly. The results demonstrated that the reduced-order model (ROM) error was less than 6% compared with that of the CFD model, and the time consumption was less than 1% of that of the CFD model. This exploration illustrates that high-fidelity ROMs based on order reduction and deep learning are a viable route to developing engineering-ready digital twins of nuclear reactors.}
}
@article{ARUMUGARAJA2022111603,
title = {Design and development of foot worn piezoresistive sensor for knee pain analysis with supervised machine learning algorithms based on gait pattern},
journal = {Measurement},
volume = {200},
pages = {111603},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111603},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122008144},
author = {M. Arumugaraja and B. Padmapriya and S. Poornachandra},
keywords = {Gait monitoring, Wearable sensorized insole, Piezoresistive sensor, Gaussian noise filter, Supervised machine learning, Knee pain detection},
abstract = {Gait monitoring and analysis have garnered more attention in gait analysis to verify the potential improvements of lower limb disorders like arthritis, trauma, and degenerative disorders. The irregularities in gait often manifest as knee pain or physical discomfort experienced by the patient. Existing vision and floor sensor-based systems have the limitations of operational complexity and high cost that make them uncomfortable for individual use. These limitations led to research interest in the design and development of insole embedded with 102 sensors to detect foot pressure distribution image for detecting lower limb disorder-based problems. The quality of these heat images is enhanced by a hybrid filter (RMSE = 2.748 and PSNR = 39.35) and feature extraction technique is utilized on the enhanced foot pressure images for classification. The KNN learner model yields accuracy of 99.4% in the detection of knee pain.}
}
@article{GAO2022102857,
title = {SEDML: Securely and efficiently harnessing distributed knowledge in machine learning},
journal = {Computers & Security},
volume = {121},
pages = {102857},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102857},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822002516},
author = {Yansong Gao and Qun Li and Yifeng Zheng and Guohong Wang and Jiannan Wei and Mang Su},
keywords = {Distributed learning, Knowledge transfer, Privacy protection, Secure computation, Differential privacy},
abstract = {Training high-performing machine learning models require a rich amount of data which is usually distributed among multiple data sources in practice. Simply centralizing these multi-sourced data for training would raise critical security and privacy concerns, and might be prohibited given the increasingly strict data regulations. To resolve the tension between privacy and data utilization in distributed learning, a machine learning framework called private aggregation of teacher ensembles (PATE) has been recently proposed. PATE harnesses the knowledge (label predictions for an unlabeled dataset) from distributed teacher models to train a student model, obviating access to distributed datasets. Despite being enticing, PATE does not offer protection for the individual label predictions from teacher models, which still entails privacy risks. In this paper, we propose SEDML, a new protocol which allows to securely and efficiently harness the distributed knowledge in machine learning. SEDML builds on lightweight cryptography and provides strong protection for the individual label predictions, as well as differential privacy guarantees on the aggregation results. Extensive evaluations show that while providing privacy protection, SEDML preserves the accuracy as in the plaintext baseline. Meanwhile, SEDML outperforms the state-of-the-art work of Xiang et al. (ICDCS’20) by 43× in computation and 1.23× in communication.}
}
@article{ZOHDI2022115315,
title = {Machine-learning and digital-twins for rapid evaluation and design of injected vaccine immune-system responses},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {401},
pages = {115315},
year = {2022},
note = {A Special Issue on Computational Modeling and Simulation of Infectious Diseases},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.115315},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522004169},
author = {T.I. Zohdi},
keywords = {Vaccine design, Digital-twins, Machine-learning},
abstract = {A computational framework is developed that researchers in the field can easily implement and subsequently use as an efficient tool to study the immune-response to a vaccine injection. There are three main components to this work: Part I-Digital-twin construction: An approach is developed that efficiently simulates the time-transient proliferation of cells/antibodies (proteins) and regulator/antigens (deactivated toxin) to an injected vaccine within tissue possessing complex heterogeneous microstructure. Here, we use the terms “cells” and “antibodies”, as well as “regulator” and “antigen” interchangeably. The approach utilizes two strongly-coupled conservation laws: (a) Conservation Law 1: comprises (a) rate of change of cells/antibodies, (b) cellular/antibody migration, (c) cellular/antibody proliferation controlled by a cell/antibody mitosis regulating chemical (antigen), (d) cell/antibody apoptosis and (b) Conservation Law 2: comprises (a) rate of change of the cell/antibody mitosis chemical regulator/antigen, (b) regulator/antigen diffusion, (c) regulator production by cells/antibody and (d) regulator/antigen decay. Part II-Efficient computation: A technique based on a voxel (3D “volume pixels”) representation of tissue microstructures and corresponding digital solution methods is developed for the calculations, which avoids computationally expensive steps involved in usual Finite Element procedures such as topologically conforming meshing, mapping, volume integration, stiffness matrix generation and matrix-based solution methods. The process proceeds by converting the tissue microstructure into voxels. The problem then becomes “digital” on a regular “voxel-grid”, directly manipulating voxel values, allowing extremely fast methods to be used to construct derivatives and to iteratively solve the system with minimal memory requirements. Part III-Machine-learning: The rapid and efficient computation allows for many vaccines to be tested quickly and uses a genomic-based Machine-Learning Algorithm to optimize the system. This is particularly useful for rapid design of next-generation vaccines and boosters for disease strain mutations. Numerical examples are provided to illustrate the results, with the overall goal being to provide a computational framework to rapidly design and deploy a vaccine for a targeted response.}
}
@article{ZHAO2022108454,
title = {Digital twin-enabled dynamic spatial-temporal knowledge graph for production logistics resource allocation},
journal = {Computers & Industrial Engineering},
volume = {171},
pages = {108454},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108454},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222004880},
author = {Zhiheng Zhao and Mengdi Zhang and Jian Chen and Ting Qu and George Q. Huang},
keywords = {Digital twin, Production logistics, Knowledge graph},
abstract = {Production logistics (PL) is increasingly receiving attention from supply chain research. The spatial disorder and temporal asynchrony of the PL resources due to the uncertainty and dynamicity pose great challenges to efficient resource allocation. The inability to obtain and rational use of PL resource spatial–temporal values causes unnecessary long travelling distances and excessive waiting time, which impede the sustainable performance of PL operations. In response, this research proposes a PL resource allocation approach based on the dynamic spatial–temporal knowledge graph (DSTKG). Internet of Things(IoT) signals data generated from large-scale deployed IoT devices are investigated and analysed to spatial–temporal values through deep neural networks. The DSTKG model is established for representing the digital twin replica with spatial–temporal consistency, followed by reasoning and completion of relationships based on PL task information. The PL resources are allocated efficiently through the graph algorithm from the directed and weighted graph. The case study is conducted to verify the feasibility and practicality of the proposed solution based on large-scale deployment. Finally, the result demonstrates the effectiveness of the proposed methodology.}
}
@article{KHAN2022105581,
title = {Knowledge distillation approach towards melanoma detection},
journal = {Computers in Biology and Medicine},
volume = {146},
pages = {105581},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105581},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522003730},
author = {Md Shakib Khan and Kazi Nabiul Alam and Abdur Rab Dhruba and Hasib Zunair and Nabeel Mohammed},
keywords = {Melanoma detection, Knowledge distillation, Skin lesion analysis, Deep learning},
abstract = {Melanoma is regarded as the most threatening among all skin cancers. There is a pressing need to build systems which can aid in the early detection of melanoma and enable timely treatment to patients. Recent methods are geared towards machine learning based systems where the task is posed as image recognition, tag dermoscopic images of skin lesions as melanoma or non-melanoma. Even though these methods show promising results in terms of accuracy, they are computationally quite expensive to train, that questions the ability of these models to be deployable in a clinical setting or memory constraint devices. To address this issue, we focus on building simple and performant models having few layers, less than ten compared to hundreds. As well as with fewer learnable parameters, 0.26 million (M) compared to 42.5 M using knowledge distillation with the goal to detect melanoma from dermoscopic images. First, we train a teacher model using a ResNet-50 to detect melanoma. Using the teacher model, we train the student model known as Distilled Student Network (DSNet) which has around 0.26 M parameters using knowledge distillation achieving an accuracy of 91.7%. We compare against ImageNet pre-trained models such MobileNet, VGG-16, Inception-V3, EfficientNet-B0, ResNet-50 and ResNet-101. We find that our approach works well in terms of inference runtime compared to other pre-trained models, 2.57 s compared to 14.55 s. We find that DSNet (0.26 M parameters), which is 15 times smaller, consistently performs better than EfficientNet-B0 (4 M parameters) in both melanoma and non-melanoma detection across Precision, Recall and F1 scores.}
}
@article{SHI2022104493,
title = {Data-driven construction of Three-dimensional subsurface geological models from limited Site-specific boreholes and prior geological knowledge for underground digital twin},
journal = {Tunnelling and Underground Space Technology},
volume = {126},
pages = {104493},
year = {2022},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2022.104493},
url = {https://www.sciencedirect.com/science/article/pii/S088677982200133X},
author = {Chao Shi and Yu Wang},
keywords = {3D geological model, Machine learning, Convolutional neural networks, XGBoost, Sparse measurements, Point cloud},
abstract = {A digital twin is a digital replica or virtual representations of 3D physical entities in the real world. In practice, it is challenging for 3D modelling of subsurface stratigraphy in an underground digital twin due to insufficient site-specific measurements and a lack of efficient 3D spatial prediction tools. In this study, a data-driven and deep learning method, called IC-XGBoost3D, is proposed to build a 3D geological model from limited site-specific boreholes and 2D training images reflecting prior geological knowledge. Anisotropic stratigraphic relationships are learned from two perpendicular 2D training images, and the extracted stratigraphic statistics serve as the input for pre-training a 2D simulation slice. A sequence of 2D simulation slices is then simulated with constraints by site-specific boreholes and subsequently reassembled into a 3D geological model. Note that the parameters of the deep learning method are calibrated with site-specific data and prior training images before being applied to develop the geological model. The model performance is demonstrated and validated using an illustrative example. The proposed method can efficiently generate an anisotropic 3D geological model as a point cloud from two perpendicular training images and sparse boreholes with a high prediction accuracy. More importantly, the proposed method not only infers the most probable 3D geological domain, but also provides a quantitative evaluation of associated 3D stratigraphic uncertainty. Effects of irregular borehole spacing and single training image on the simulation performance are also investigated.}
}
@article{GARCIA2022108463,
title = {Towards a connected Digital Twin Learning Ecosystem in manufacturing: Enablers and challenges},
journal = {Computers & Industrial Engineering},
volume = {171},
pages = {108463},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108463},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222004922},
author = {Álvaro García and Anibal Bregon and Miguel A. Martínez-Prieto},
keywords = {Digital twin, Learning ecosystem, Manufacturing, Human–machine collaboration, Learning factory, Cyber–physical system},
abstract = {The evolution of digital twin, leveraged by the progressive physical–digital convergence, has provided smart manufacturing systems with knowledge-generation ecosystems based on new models of collaboration between the workforce and industrial processes. Digital twin is expected to be a decision-making solution underpinned by real-time communication and data-driven enablers, entailing close cooperation between workers, systems and processes. But industry will need to face the challenges of building and supporting new technical and digital infrastructures, while workers’ skills development eventually manages to include the increased complexity of industrial processes. This paper is intended to reach a better understanding of learning opportunities offered by emerging Industry 4.0 digital twin ecosystems in manufacturing. Diverse learning approaches focused on the potential application of the digital twin concept in theoretical and real manufacturing ecosystems are reviewed. In addition, we propose an original definition of Digital Twin Learning Ecosystem and the conceptual layered architecture. Existing key enablers of the digital twin physical–digital convergence, such as collaborative frameworks, data-driven approaches and augmented interfaces, are also described. The role of the Learning Factory concept is highlighted, providing a common understanding between academia and industry. Academic applications and complex demonstration scenarios are combined in line with the enablement of connected adaptive systems and the empowerment of workforce skills and competences. The adoption of digital twin in production is still at an initial stage in the manufacturing industry, where specific human and technological challenges must be addressed. The research priorities presented in this work are considered as a recognised basis in industry, which should help digital twin with the objective of its progressive integration as a future learning ecosystem.}
}
@article{TEEKARAMAN2022108203,
title = {Abridged design with demand conventions for health care applications},
journal = {Computers and Electrical Engineering},
volume = {102},
pages = {108203},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108203},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200444X},
author = {Yuvaraja Teekaraman and Hariprasath Manoharan and Irina Kirpichnikova and Ramya Kuppusamy},
keywords = {Digital twins, Exploration units, Health care, Virtual representation},
abstract = {This article focuses on the process of improving health care in real-time experimental settings. Despite the fact that there are numerous solutions for health-care issues, no remote solution has been devised. As a result, a novel model has been devised in the suggested technique, in which all hospital administration processes are simplified through the use of a virtual representation digital twin process. When such technologies are used, a discrete protocol is followed, resulting in the identification and integration of a security model based on demand movement. A machine learning discriminant analysis has been combined with the process of digital twin functions efficiently based on input signals and state vectors. The cohesive model is put to the test in four case studies, including delay monitoring, predictive score analysis, communication units, and battery charge rate, with all case study outcomes proving to be more effective than existing systems by an average of 74%.}
}
@article{ZHOU2022103448,
title = {Uncertainty-aware consistency regularization for cross-domain semantic segmentation},
journal = {Computer Vision and Image Understanding},
volume = {221},
pages = {103448},
year = {2022},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2022.103448},
url = {https://www.sciencedirect.com/science/article/pii/S1077314222000625},
author = {Qianyu Zhou and Zhengyang Feng and Qiqi Gu and Guangliang Cheng and Xuequan Lu and Jianping Shi and Lizhuang Ma},
keywords = {Domain adaptation, Semantic segmentation, Transfer learning, Consistency regularization},
abstract = {Unsupervised domain adaptation (UDA) aims to adapt existing models of the source domain to a new target domain with only unlabeled data. Most existing methods suffer from noticeable negative transfer resulting from either the error-prone discriminator network or the unreasonable teacher model. Besides, the local regional consistency in UDA has been largely neglected, and only extracting the global-level pattern information is not powerful enough for feature alignment due to the abuse use of contexts. To this end, we propose an uncertainty-aware consistency regularization method for cross-domain semantic segmentation. Firstly, we introduce an uncertainty-guided consistency loss with a dynamic weighting scheme by exploiting the latent uncertainty information of the target samples. As such, more meaningful and reliable knowledge from the teacher model can be transferred to the student model. We further reveal the reason why the current consistency regularization is often unstable in minimizing the domain discrepancy. Besides, we design a ClassDrop mask generation algorithm to produce strong class-wise perturbations. Guided by this mask, we propose a ClassOut strategy to realize effective regional consistency in a fine-grained manner. Experiments demonstrate that our method outperforms the state-of-the-art methods on four domain adaptation benchmarks, i.e., GTAV → Cityscapes, SYNTHIA → Cityscapes, Virtual KITTI ⟶ KITTI and Cityscapes ⟶ KITTI.}
}
@article{SUN2022117680,
title = {Ensemble Knowledge Tracing: Modeling interactions in learning process},
journal = {Expert Systems with Applications},
volume = {207},
pages = {117680},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117680},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422009769},
author = {Jianwen Sun and Rui Zou and Ruxia Liang and Lu Gao and Sannyuya Liu and Qing Li and Kai Zhang and Lulu Jiang},
keywords = {Knowledge Tracing, Deep neural network, Learning interactions, User modeling},
abstract = {Knowledge Tracing (KT) aims to continuously estimate students’ evolving knowledge state during their learning process, which has attracted much research attention due to its potential for delivering personalized and optimal experiences to students in intelligent learning systems. The learning process is essentially the pairwise interactions of Students, Concepts, and Questions (S–C, S–Q, C–Q for short). Modeling all these interactions will improve the performance of KT. However, existing KT methods hardly exploit all the interactions in a single model. Specifically, Bayesian Knowledge Tracing (BKT) and most of its variants neglect C–Q; Deep Knowledge Tracing (DKT) and other deep neural network approaches mostly neglect S–Q and C–Q. We propose the Ensemble Knowledge Tracing (EnKT), which models all three types of interactions. The base model of EnKT is a hybrid of BKT and DKT. We also present an ensemble algorithm Recurrent Boosting (RB), which extends AdaBoost to deal with KT sequential data. Inspired by BKT, EnKT represents S–C and S–Q using learning and performance parameters, respectively. Besides, EnKT defines C–Q as the correlation complexity among the concepts involved in a question. Experiments show EnKT significantly outperforms state-of-the-art methods (by up to 6% in AUC in some cases) on four real-world benchmark datasets and illustrate better interpretability by several typical case studies.}
}
@article{XIAO2022107099,
title = {Efficient Combination of CNN and Transformer for Dual-Teacher Uncertainty-guided Semi-supervised Medical Image Segmentation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {226},
pages = {107099},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107099},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722004801},
author = {Zhiyong Xiao and Yixin Su and Zhaohong Deng and Weidong Zhang},
keywords = {Magnetic resonance imaging (MRI), Deep learning, Transformer, Semi-supervised learning, Image segmentation},
abstract = {Background and objective: Deep learning-based methods for fast target segmentation of magnetic resonance imaging (MRI) have become increasingly popular in recent years. Generally, the success of deep learning methods in medical image segmentation tasks relies on a large amount of labeled data. The time-consuming and labor-intensive problem of data annotation is a major challenge in medical image segmentation tasks. The aim of this work is to enhance the segmentation of MR images using a semi-supervised learning-based method using a small amount of labeled data and a large amount of unlabeled data. Methods: To utilize the effective information of the unlabeled data, we designed the method of guiding the Student segmentation model simultaneously by the Dual-Teacher structure of CNN and transformer forming the subject network. Both Teacher A and Student models are CNNs, and the TA-S module they form is a mean teacher structure with added data noise. In the TB-S module formed by the combination of Student and Teacher B models, their backbone networks CNN and transformer capture the local and global information of the image at the same time, respectively, to create pseudo labels for each other and perform cross-supervision. The Dual-Teacher guides the Student through synchronous training and performs knowledge rectification and communication with each other through consistent regular constraints, which better utilizes the valid information in the unlabeled data. In addition, the segmentation predictions of Teacher A and Student and Teacher A and Teacher B are screened for uncertainty assessment during the training process to enhance the prediction accuracy and generalization of the model. This method uses the mechanism of simultaneous training of the synthetic structure composed of TA-S and TB-S modules to jointly guide the optimization of the Student model to obtain better segmentation ability. Results: We evaluated the proposed method on a publicly available MRI dataset from a cardiac segmentation competition organized by MICCAI in 2017. Compared with several existing state-of-the-art semi-supervised segmentation methods, the method achieves better segmentation results in terms of Dice coefficient and HD distance evaluation metrics of 0.878 and 4.9 mm and 0.886 and 5.0 mm, respectively, using a training set containing only 10% and 20% of labeled data. Conclusion: This method fuses CNN and transformer to design a new Teacher-Student semi-supervised learning optimization strategy, which greatly improves the utilization of a large number of unlabeled medical images and the effectiveness of model segmentation results.}
}
@article{JI2022103533,
title = {Defending against attacks tailored to transfer learning via feature distancing},
journal = {Computer Vision and Image Understanding},
volume = {223},
pages = {103533},
year = {2022},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2022.103533},
url = {https://www.sciencedirect.com/science/article/pii/S107731422200114X},
author = {Sangwoo Ji and Namgyu Park and Dongbin Na and Bin Zhu and Jong Kim},
keywords = {Robust transfer learning, Adversarial example, Triplet loss, Mimic attack, Target-agnostic attack},
abstract = {Transfer learning is preferable for training a deep neural network with a small training dataset by leveraging a pre-trained teacher model. However, transfer learning opens a door for new attacks that generate adversarial examples using the pre-trained teacher model. In this paper, we propose a novel method called feature distancing to defend against adversarial attacks tailored to transfer learning. The method aims to train a student model with a distinct feature representation from the teacher model. We generate adversarial examples of the mimic attack with the teacher model, and the examples are used to train the student model. We use triplet loss to put the mimic attack examples close to their source images and far from their target images in the feature space of the student model. The proposed method is evaluated on three different transfer learning tasks with diverse attack configurations. It is the only method that achieves high “robust accuracy” and high “test accuracy” on every task we evaluate.}
}
@article{KARIMIALAVIJEH2022100040,
title = {Digitally enabled approaches for the scale up of mammalian cell bioreactors},
journal = {Digital Chemical Engineering},
volume = {4},
pages = {100040},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100040},
url = {https://www.sciencedirect.com/science/article/pii/S277250812200031X},
author = {Masih {Karimi Alavijeh} and Irene Baker and Yih Yean Lee and Sally L. Gras},
keywords = {Machine learning, Mechanistic modelling, Biomanufacturing, Bioreactor},
abstract = {With recent advances in digitisation and big data analytics, more pharmaceutical firms are adopting digital tools to achieve modernisation. The biological phenomena within bioreactors are a key target for such digital approaches, as these processes are often complicated and difficult to scale. Historically, rules of thumb have been used to match performance indicators across bioreactor scales. Although such methods are well-established and frequently employed by industry, no universal solution has been developed to overcome the many challenges faced in process development and scale-up. Several computer-based methodologies can potentially be applied to bioreactor scale-up, including knowledge driven and data-driven techniques. This review assesses the state of the art in digital advances in scaling bioreactors and the advantages and limitations of scaling techniques. Traditional approaches and their constraints are outlined. The application of knowledge-based techniques is then considered and compared to data-driven models. The ability to transfer processes across bioreactor scales, to compare data and predict process indicators across scales are then examined. Finally, the role of hybrid modelling and digital twins and their potential in bioprocess development are explored.}
}
@article{HUGHES2022109502,
title = {On robust risk-based active-learning algorithms for enhanced decision support},
journal = {Mechanical Systems and Signal Processing},
volume = {181},
pages = {109502},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109502},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022006124},
author = {A.J. Hughes and L.A. Bull and P. Gardner and N. Dervilis and K. Worden},
keywords = {Decision-making, Active learning, Value of information, Structural health monitoring, Sampling bias, Digital twins, Risk},
abstract = {Classification models are a fundamental component of physical-asset management technologies such as structural health monitoring (SHM) systems and digital twins. Previous work introduced risk-based active learning, an online approach for the development of statistical classifiers that takes into account the decision-support context in which they are applied. Decision-making is considered by preferentially querying data labels according to expected value of perfect information (EVPI). Although several benefits are gained by adopting a risk-based active learning approach, including improved decision-making performance, the algorithms suffer from issues relating to sampling bias as a result of the guided querying process. This sampling bias ultimately manifests as a decline in decision-making performance during the later stages of active learning, which in turn corresponds to lost resource/utility. The current paper proposes two novel approaches to counteract the effects of sampling bias: semi-supervised learning, and discriminative classification models. These approaches are first visualised using a synthetic dataset, then subsequently applied to an experimental case study, specifically, the Z24 Bridge dataset. The semi-supervised learning approach is shown to have variable performance; with robustness to sampling bias dependent on the suitability of the generative distributions selected for the model with respect to each dataset. In contrast, the discriminative classifiers are shown to have excellent robustness to the effects of sampling bias. Moreover, it was found that the number of inspections made during a monitoring campaign, and therefore resource expenditure, could be reduced with the careful selection of the statistical classifiers used within a decision-supporting monitoring system.}
}
@article{WAN2022109551,
title = {A dual learning-based recommendation approach},
journal = {Knowledge-Based Systems},
volume = {254},
pages = {109551},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109551},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122007791},
author = {Shanshan Wan and Ying Liu and Dongwei Qiu and James Chambua and Zhendong Niu},
keywords = {Recommender system, Dual learning, Data sparsity, Duality, Hybrid filtering recommendation},
abstract = {Data sparsity and cold start are two critical issues which need to be addressed in recommender systems (RSs). Currently, most methods address these issues by applying user history files or some side information to improve the user model and complete the rating matrix. However, such methods cannot perform well when labeled data is scarce or unavailable. In this paper, we propose a dual learning-based recommendation approach (DLRA). DLRA can trigger initial recommendation and improve the quality of recommendations by using the duality characteristics of RSs, even when the available labeled information is scarce. Specifically, DLRA regards the recommendation task as two independent subtasks — primal task and dual task, and these two tasks show strong duality in DLRA. The primal task is item-centered which aims to find users who can rate high for items, while the dual task is user-centered that aims to recommend the most favorite items to users. These two tasks have strong dualities in terms of the recommendation space, selection probability and recommendation basis. Based on these dualities, we design three dual learning strategies to couple the whole recommendation process and realize the self-tuning and self-improvement of each task model, and finally optimize the whole recommendation model. Based on the dataset of Movielens and BookCrossing, we simulate data sparsity and cold start recommendation scenarios, the experimental results show that DLRA achieves substantial improvement when the labeled data is scare, and it outperforms other hybrid recommendation approaches and deep learning strategies with a smaller predictive error as well as better recommendation accuracy.}
}
@article{BANNOUR2022104073,
title = {Privacy-preserving mimic models for clinical named entity recognition in French},
journal = {Journal of Biomedical Informatics},
volume = {130},
pages = {104073},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104073},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000892},
author = {Nesrine Bannour and Perceval Wajsbürt and Bastien Rance and Xavier Tannier and Aurélie Névéol},
keywords = {Confidentiality, Datasets as topic, Electronic health records, Mimic learning, Natural language processing, Neural networks, Computer},
abstract = {A vast amount of crucial information about patients resides solely in unstructured clinical narrative notes. There has been a growing interest in clinical Named Entity Recognition (NER) task using deep learning models. Such approaches require sufficient annotated data. However, there is little publicly available annotated corpora in the medical field due to the sensitive nature of the clinical text. In this paper, we tackle this problem by building privacy-preserving shareable models for French clinical Named Entity Recognition using the mimic learning approach to enable the knowledge transfer through a teacher model trained on a private corpus to a student model. This student model could be publicly shared without any access to the original sensitive data. We evaluated three privacy-preserving models using three medical corpora and compared the performance of our models to those of baseline models such as dictionary-based models. An overall macro F-measure of 70.6% could be achieved by a student model trained using silver annotations produced by the teacher model, compared to 85.7% for the original private teacher model. Our results revealed that these privacy-preserving mimic learning models offer a good compromise between performance and data privacy preservation.}
}
@article{FERDAUS2022108818,
title = {Significance of activation functions in developing an online classifier for semiconductor defect detection},
journal = {Knowledge-Based Systems},
volume = {248},
pages = {108818},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108818},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122003872},
author = {Md Meftahul Ferdaus and Bangjian Zhou and Ji Wei Yoon and Kain Lu Low and Jieming Pan and Joydeep Ghosh and Min Wu and Xiaoli Li and Aaron Voon-Yew Thean and J. Senthilnath},
keywords = {Leaky ReLU, Online learning, Defect detection, Prequential, Semiconductors},
abstract = {In anomaly detection problems for advanced semiconductor devices, non-visual defects occur frequently. Machine learning (ML) algorithms have the advantage of identifying such defects. However, in this real-world problem, data comes sequentially in a streaming fashion, thus, we may not have sufficient data to train an ML model in batch mode. In such a scenario, online ML models are useful to detect defects immediately since they work in a single-pass mode. Besides, when data is collected from more realistic non-stationary monitoring environments, online ML models with evolving architecture are more practical. Thus, evolving and online ML models are developed in this work to detect defects in technology computer-aided design (TCAD)-based digital twin model of advanced nano-scaled semiconductor devices such as a fin field-effect transistor (FinFET) and a gate-all-around field-effect transistor (GAA-FET). Activation functions (AFs) in deep neural networks (DNNs) and membership functions (MFs) in neuro-fuzzy systems (NFSs) play an important role in the performance of those ML models. This work focuses on analyzing the effects of various AFs/MFs in our developed online ML models while detecting defects in real-world nano-scaled semiconductor devices, where significant training samples are not available. From various semiconductor datasets having fewer samples, it has been observed that the proposed evolving neuro-fuzzy system (ENFS) with Leaky-ReLU MF performs better (improvement in the range of 1.9% to 30.8% considering overall classification accuracy) than the other DNN or ENFS-based online ML models. Having an evolving architecture and online learning mechanism, besides anomaly detection, the proposed model’s performance has also been evaluated for handling large data streams problems with concept drift. The performance of the proposed method has been compared with some recently developed baselines under the prequential test-then-train protocol. The classification rates of the proposed method has an improvement in the range of 1.1% to 65.9% than the existing methods. The code of this work has been made publicly available at https://github.com/MdFerdaus/LREC.}
}
@article{SERRANORUIZ2022185,
title = {Development of a multidimensional conceptual model for job shop smart manufacturing scheduling from the Industry 4.0 perspective},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {185-202},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000462},
author = {Julio C. Serrano-Ruiz and Josefa Mula and Raúl Poler},
keywords = {Industry 4.0, Job shop, Smart manufacturing scheduling, Digital twin, Zero-defect manufacturing},
abstract = {Based on a scientific literature review in the conceptual domain defined by smart manufacturing scheduling (SMS), this article identifies the benefits and limitations of the reviewed contributions, establishes and discusses a set of criteria with which to collect and structure its main synergistic attributes, and devises a conceptual framework that models SMS around three axes: a semantic ontology context, a hierarchical agent structure, and the deep reinforcement learning (DRL) method. The main purpose of such a modelling research is to establish a conceptual and structured relationship framework to improve the efficiency of the job shop scheduling process using the approach defined by SMS. The presented model orients the job shop scheduling process towards greater flexibility, through enhanced rescheduling capability, and towards autonomous operation, mainly supported by the use of machine learning technology. To the best of our knowledge, there are no other similar conceptual models in the literature that synergistically combine the potential of the specific set of Industry 4.0 principles and technologies that model SMS. This research can provide guidance for practitioners and researchers’ efforts to move toward the digital transformation of job shops.}
}
@article{SUN2022119739,
title = {Deep learning method based on graph neural network for performance prediction of supercritical CO2 power systems},
journal = {Applied Energy},
volume = {324},
pages = {119739},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119739},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922010273},
author = {Lei Sun and Tianyuan Liu and Ding Wang and Chengming Huang and Yonghui Xie},
keywords = {S-CO power system, Performance prediction, Thermodynamic characteristics, Digital twin, Graph neural network},
abstract = {Considering the increasing energy consumption and greenhouse gas emissions, the Supercritical CO2 (S-CO2) power system has attracted more and more attention. Due to the expensive computation resource and time cost, data-based solutions for performance prediction are urged. The surrogate model by machine learning is a promising alternative, but it only focuses on the objective functions and ignores the importance of topological structures and physical states of cycles. Aiming at providing a comprehensive model to predict physical states as well as thermodynamic characteristics, a deep learning method based on graph neural network (GNN) are devised in this paper. With the modeling calculation results as training dataset, a well-trained model can accurately reconstruct the physical states consisting of temperature, pressure, enthalpy, entropy (relative error of most samples <5 %) and exergy as well as thermal and exergy efficiency (relative error of most samples <5 %). Moreover, this model shows superior performance compared with traditional machine learning models including Regression Tree, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Gaussian Process Regression (GPR). Finally, the comparison between different training sizes demonstrate the model can help reduce sampling costs for complex systems. Overall, the presented deep learning model can provide a reliable and competitive choice for the digital twin of S-CO2 power system and other power systems.}
}
@article{HERRMANN202214,
title = {Hands-on kinetic measurements and simulation for chemical process engineering students},
journal = {Education for Chemical Engineers},
volume = {41},
pages = {14-21},
year = {2022},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772822000185},
author = {Stefan Herrmann and Daniel Felder and Maria Padligur and Sebastian Brosch and Matthias Geiger and Felix Stockmeier and Kristina Baitalow and Deniz Rall and Robert Femmer and Florian Roghmans and Martin Hauser and Jannik Mehlis and John Linkhorst and Matthias Wessling},
keywords = {Clock reaction, Laboratory class, Kinetic modeling, Computational fluid dynamics, Academic teaching, Hands-on learning},
abstract = {Hands-on experience in the laboratory is essential in chemical engineering education to enhance the understanding of abstract theories and their effect on chemical processes. In this work, we describe a laboratory class, which combines some of the main engineering concepts into a set of hands-on experiments and simulations. Students are introduced to an iodine clock reaction performed in multiple different reactor types and are instructed to determine the reaction kinetics. Subsequent analysis of the experimental data in Python teaches basic programming skills and the concepts of numeric integration and optimization. Finally, a digital twin of one of the reactors is developed in COMSOL Multiphysics to give the students an application-focused introduction to more-dimensional multiphysics modeling. The students thereby get practical insights into the different methods and stages of reactor and reaction engineering. Based on the students’ assignments, we consistently see a deeper understanding of reaction kinetics and reactor engineering than in the accompanying traditional lecture.}
}
@article{BAEK2022100097,
title = {Current state-of-the-art and utilities of machine learning for detection, monitoring, growth prediction, rupture risk assessment, and post-surgical management of abdominal aortic aneurysms},
journal = {Applications in Engineering Science},
volume = {10},
pages = {100097},
year = {2022},
issn = {2666-4968},
doi = {https://doi.org/10.1016/j.apples.2022.100097},
url = {https://www.sciencedirect.com/science/article/pii/S2666496822000152},
author = {Seungik Baek and Amirhossein Arzani},
keywords = {Data-driven approaches, EVAR, Circulating biomarkers, Pulse wave imaging, Physics-based machine learning, Digital twin},
abstract = {Ultrasound imaging has long been playing a central role in detecting abdominal aortic aneurysms (AAAs). With a recent trend of reducing prevalence of AAAs, ultrasound screening is only recommended for men aged 65 to 75 years with previous smoking history, and a national level of a screening program for women is currently not recommended in the US. In the 2000s, several research groups demonstrated the utility of finite element stress analysis using patient-specific images, which was promising for an accurate assessment of the rupture risk, but physical models remain to be enhanced by considering patient variability and multi-physical characteristics. This review aims to provide a survey of emerging and alternative technologies and new methodologies, such as personalized medicine and data-driven approaches, that may make potential breakthroughs on detection of small AAAs, monitoring of patients during the follow-ups, prediction of AAA growth, assessment of the rupture risk, and post-surgical prognosis for AAA patient management.}
}
@article{CHEN2022102974,
title = {A self-attention based global feature enhancing network for semantic segmentation of large-scale urban street-level point clouds},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {113},
pages = {102974},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102974},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222001662},
author = {Qi Chen and Zhenxin Zhang and Siyun Chen and Siyuan Wen and Hao Ma and Zhihua Xu},
keywords = {Urban street, Large-scale point clouds, Semantic segmentation, Attention mechanism, Global feature},
abstract = {Point clouds of large-scale urban street scenes contain large quantities of object categories and rich semantic information. The semantic segmentation is the basis and key to subsequent essential applications, such as digital twin engineering and city information model. The global feature of point clouds in large-scale scenes can provide long-range context information, which is critical to high-quality semantic segmentation. However, the learning of global spatial saliency considering class label constraints is often ignored in the feature representation of some deep learning models. With regard to this, we propose a Global Feature Self-Attention Encoding (GFSAE) module and a Weighted Semantic Mapping (WSM) module to make the semantic segmentation model of point clouds in large-scale urban street scene focus more on the global salient feature expression by self-attention enhancement channel by channel and take into account the constraints of semantic categories to learn a better semantic segmentation model for urban street scenes. The experiments are performed on the Semantic3D dataset and our own collected vehicle Mobile Laser Scanning (MLS) point cloud dataset. The segmentation results show that the GFSAE and the WSM proposed by us can improve the semantic segmentation of point clouds in large-scale urban street scenes and prove the effectiveness of our model compared with other state-of-the-art methods.}
}
@article{HAMID2022100044,
title = {Hybrid modelling for remote process monitoring and optimisation},
journal = {Digital Chemical Engineering},
volume = {4},
pages = {100044},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100044},
url = {https://www.sciencedirect.com/science/article/pii/S2772508122000333},
author = {Anuar Hamid and Anton Heryanto Hasan and Siti Nurfaqihah Azhari and Zalina Harun and Zulfan A. Putra},
keywords = {First principle modeling, Machine learning, Digital process engineering, Process modeling, Process simulation, Process optimization, Multi objective optimization, Dehydration unit, Natural gas, Water dew point, Reboiler duty},
abstract = {Process simulation is used to develop a digital twin representation of chemical processes typically for process optimization or what-if scenarios. However, it is known to be computationally expensive and there is an increasing need for process remote monitoring and optimization. This is where machine learning models shine, where they can run up to several orders of magnitude faster than their equivalent first principle process simulation models. Most of the previous work in this area tend to demonstrate the ability of machine learning models to accurately capture complex, non-linear relationships between process parameters in various chemical processes. Two important aspects are rarely discussed, namely the construction of machine learning models using operating data and the deployment of these models. In this paper, we address these two aspects and review the different information silos in industrial settings that need to be considered when working with hybrid models (combining process simulation and machine learning). This is illustrated by a case study for an operating natural gas dehydration unit, covering data management, process simulation, machine learning and visualization. We demonstrate how the hybrid models can be constructed and packaged as an online monitoring and a prediction dashboard. Several unique challenges are also highlighted including the reliability of field data and operational deviations due to operability and controllability - all of which need to be understood in order to successfully translate operating systems to process simulation and machine learning models that are reliable and accurate. While the exact implementation may vary from project to project, the current work serves as an example and highlights the important considerations to make when working with such systems.}
}
@article{CHRYSAFIADI2022109111,
title = {Cognitive-based adaptive scenarios in educational games using fuzzy reasoning},
journal = {Knowledge-Based Systems},
volume = {250},
pages = {109111},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109111},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122005482},
author = {Konstantina Chrysafiadi and Spyros Papadimitriou and Maria Virvou},
keywords = {Serious game, Educational game, Adaptivity, Fuzzy logic, Cognitive states},
abstract = {Nowadays, the use of educational games is gaining substantial popularity. Games offer immersive and fascinating environments that render them a powerful tool for achieving high-quality learning outcomes, like advancing students’ education through engaging activities. However, one crucial enhancement is that they should take into consideration important aspects of each individual student and adapt to them accordingly for improving the educational results. Given the above, the present work considers an educational adventure game that offers cognitive-based adaptivity in its educational content and scenarios. More specifically, it adapts both the difficulty level of the educational content and its plot dynamically to the knowledge level and learning needs of each individual student. The adaptation is realized using a fuzzy student model that detects the current cognitive state of the trainee and decides about the way in which the game’s plot has to be modified. It determines if its scenario will be dynamically extended or not, aiming to assist the trainee overcome her/his weaknesses and improve her/his educational performance. The gain of this is that through the plot’s adaptation, the game motivates the learners/players to be involved actively in the learning process and provides educational content tailored to their knowledge level and weaknesses, enhancing the educational results. The contribution of the game to the educational process and outcomes was thoroughly evaluated. The evaluation results indicate a high acceptance rate of the game by learners and teachers and underline its effectiveness in the educational results.}
}
@article{REJA2022104245,
title = {Computer vision-based construction progress monitoring},
journal = {Automation in Construction},
volume = {138},
pages = {104245},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104245},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522001182},
author = {Varun Kumar Reja and Koshy Varghese and Quang Phuc Ha},
keywords = {Progress monitoring, Computer vision, Automated construction, Data acquisition, 3D reconstruction, As-built modelling, Point cloud, Scan to BIM, Literature review, Digital Twin},
abstract = {Automating the process of construction progress monitoring through computer vision can enable effective control of projects. Systematic classification of available methods and technologies is necessary to structure this complex, multi-stage process. Using the PRISMA framework, relevant studies in the area were identified. The various concepts, tools, technologies, and algorithms reported by these studies were iteratively categorised, developing an integrated process framework for Computer-Vision-Based Construction Progress Monitoring (CV-CPM). This framework comprises: data acquisition and 3D-reconstruction, as-built modelling, and progress assessment. Each stage is discussed in detail, positioning key studies, and concurrently comparing the methods used therein. The four levels of progress monitoring are defined and found to strongly influence all stages of the framework. The need for benchmarking CV-CPM pipelines and components are discussed, and potential research questions within each stage are identified. The relevance of CV-CPM to support emerging areas such as Digital Twin is also discussed.}
}
@article{STERGIOU2022279,
title = {Digital twin intelligent system for industrial internet of things-based big data management and analysis in cloud environments},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {4},
pages = {279-291},
year = {2022},
note = {Virtual-reality and intelligent hardware in digital twins A)},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000444},
author = {Christos L. Stergiou and Kostas E. Psannis},
keywords = {Machine learning, IoT, Big data, Cloud computing, Management, Analytics, Digital twin Scenario, Energy efficiency},
abstract = {This work surveys and illustrates multiple open challenges in the field of industrial Internet of Things (IoT)-based big data management and analysis in cloud environments. Challenges arising from the fields of machine learning in cloud infrastructures, artificial intelligence techniques for big data analytics in cloud environments, and federated learning cloud systems are elucidated. Additionally, reinforcement learning, which is a novel technique that allows large cloud-based data centers, to allocate more energy-efficient resources is examined. Moreover, we propose an architecture that attempts to combine the features offered by several cloud providers to achieve an energy-efficient industrial IoT-based big data management framework (EEIBDM) established outside of every user in the cloud. IoT data can be integrated with techniques such as reinforcement and federated learning to achieve a digital twin scenario for the virtual representation of industrial IoT-based big data of machines and room temperatures. Furthermore, we propose an algorithm for determining the energy consumption of the infrastructure by evaluating the EEIBDM framework. Finally, future directions for the expansion of this research are discussed.}
}
@article{CHEN2022124040,
title = {Digital twins model and its updating method for heating, ventilation and air conditioning system using broad learning system algorithm},
journal = {Energy},
volume = {251},
pages = {124040},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124040},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222009434},
author = {Kang Chen and Xu Zhu and Burkay Anduv and Xinqiao Jin and Zhimin Du},
keywords = {Digital twins, Broad learning system, Incremental learning, Online model updating, HVAC system},
abstract = {Digital Twins (DT) can be used for the energy efficiency management of entire life cycle of HVAC systems. The existing chiller models usually can not update in real-time, so they are not suitable for real-time interactions between DT models and real physical systems. In this paper, an intelligent DT framework is proposed for HVAC systems, which includes the equipment, data, simulation, and application layers. Broad learning system (BLS) is presented to build the simulation layer of the chiller and its DT platform. The basic BLS model is optimized to reach the best performance by choosing linear rectification function as activation function and setting batch size to 64 by enumeration method. The real HVAC system located in Zhejiang province is selected to verify the proposed method. For the first half year operation, the average mean absolute error, root mean square error and coefficient of determination (R2) of Multi-BLS model for nine chillers can reach 9.04, 15.20 and 0.98 respectively. For the second half year operation, the proposed method can be updated in 4.63s and its R2 is 0.95. Compared with conventional models, the proposed Multi-BLS model has better prediction precision and can be updated in real-time within a shorter time.}
}
@article{WANG2022104515,
title = {Object verification based on deep learning point feature comparison for scan-to-BIM},
journal = {Automation in Construction},
volume = {142},
pages = {104515},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104515},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003880},
author = {Boyu Wang and Qian Wang and Jack C.P. Cheng and Chao Yin},
keywords = {Building information model (BIM), LiDAR point clouds, As-built modeling, Object verification, Deep learning},
abstract = {Building information models (BIMs) have been widely adopted in current construction projects to enhance the efficiency of facility maintenance operations. As-built BIMs can reflect the actual conditions of facilities and thus as-built BIM reconstruction has shown great significance in digital twin generation, building health monitoring, facility management and urban renewal. Laser scanners are capable to capture dense 3D measurements of the environment in a fast and highly accurate way. Therefore, laser scanning data have been widely used for as-built BIM generation. Although research efforts have been made on how to automatically achieve “Scan-to-BIM”, there are still gaps from applying current solutions to real scenarios. One of the challenges is that some irrelevant point clusters may be wrongly recognized as the desired object in the detection stage. This study presents a novel object verification approach based on deep learning point feature comparison to improve the accuracy of automated BIM reconstruction process. Firstly, a KPConv-based deep neural network is developed and trained to perform 3D point feature computation. Then through comparing point features calculated for extracted point clusters and as-designed BIM generated point clouds, point feature distance maps are generated. Afterwards, to automatically analyze the generated feature distance maps, a dataset including simulated positive and negative instances is created based on ModelNet40. And a tiny neural network is established and trained on the prepared dataset to acquire ability of distinguishment. To validate the feasibility of the proposed technique, experiments were conducted on both artificial point clouds and real scan data collected in one MEP room in a water treatment work in Hong Kong. It is demonstrated that the proposed technique can successfully filter out all the false positives in the Scan-to-BIM process, improving reconstruction accuracy significantly.}
}
@article{QIU2022102895,
title = {Low-cost mobile mapping system solution for traffic sign segmentation using Azure Kinect},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102895},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102895},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222000978},
author = {Zhouyan Qiu and Joaquín Martínez-Sánchez and Víctor Manuel Brea and Paula López and Pedro Arias},
keywords = {Time of Flight camera, Traffic sign detection and segmentation, Multi sensor system, Mobile mapping system},
abstract = {The mobile mapping system (MMS) could become the foundation of digital twins and 3D modeling, and is widely applicable in a variety of fields, such as infrastructure management, intelligent transportation systems, and smart cities. However, data collected by MMS is extensive and complex, making data processing difficult. We present a novel method for segmenting urban assets (specifically in this case study traffic signs) with a lower-cost Azure Kinect and automatic data processing workflows. First, it was necessary to verify the reliability of this approach using the Time of Flight (ToF) camera from Azure Kinect to detect road signs outdoors. Using the data generated by the ToF camera, we then extracted the Region of Interest (ROI) quickly and efficiently. After transforming the ROI to the RGB image, we obtained the traffic sign area through a hybrid color-shape based method. In addition, we calculated the distance between the traffic sign and Azure Kinect based on the depth image. The Coefficient of Variation cv averaged 1.1%. It is thus evident that Azure Kinect is reliable for outdoor traffic sign segmentation. Our algorithm has been compared with deep learning algorithms. According to our analysis, our algorithm has an accuracy of 0.8216, while the accuracy of deep learning is 0.7466, which indicates that our solution is more flexible and cost-effective.}
}
@article{SHARMA2022100383,
title = {Digital Twins: State of the art theory and practice, challenges, and open research questions},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100383},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100383},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000516},
author = {Angira Sharma and Edward Kosasih and Jie Zhang and Alexandra Brintrup and Anisoara Calinescu},
keywords = {Digital Twin, Internet of Things, Autonomous systems, Big data, Machine learning},
abstract = {Digital Twin was introduced over a decade ago, as an innovative all-encompassing tool, with perceived benefits including real-time monitoring, simulation, optimisation and accurate forecasting. However, the theoretical framework and practical implementations of digital twin (DT) are yet to fully achieve this vision at scale. Although an increasing number of successful implementations exist in research and industrial works, sufficient implementation details are not publicly available, making it difficult to fully assess their components and effectiveness, to draw comparisons, identify successful solutions, share lessons, and thus to jointly advance and benefit from the DT methodology. This work first presents a review of relevant DT research and industrial works, focusing on the key DT features, current approaches in different domains, and successful DT implementations, to infer the key DT components and properties, and to identify current limitations and reasons behind the delay in the widespread implementation and adoption of digital twin. This work identifies that the major reasons for this delay are: the fact the DT is still a fast evolving concept; the lack of a universal DT reference framework, e.g. DT standards are scarce and still evolving; problem- and domain-dependence; security concerns over shared data; lack of DT performance metrics; and reliance of digital twin on other fast-evolving technologies. Advancements in machine learning, Internet of Things (IoT) and big data have led to significant improvements in DT features such as real-time monitoring and accurate forecasting. Despite this progress and individual company-based efforts, certain research and implementation gaps exist in the field, which have so far prevented the widespread adoption of the DT concept and technology; these gaps are also discussed in this work. Based on reviews of past work and the identified gaps, this work then defines a conceptualisation of DT which includes its components and properties; these also validate the uniqueness of DT as a concept, when compared to similar concepts such as simulation, autonomous systems and optimisation. Real-life case studies are used to showcase the application of the conceptualisation. This work discusses the state-of-the-art in DT, addresses relevant and timely DT questions, and identifies novel research questions, thus contributing to a better understanding of the DT paradigm and advancing the theory and practice of DT and its allied technologies.}
}
@article{CONE2022103441,
title = {Learning with unobserved regimes},
journal = {Journal of Macroeconomics},
volume = {73},
pages = {103441},
year = {2022},
issn = {0164-0704},
doi = {https://doi.org/10.1016/j.jmacro.2022.103441},
url = {https://www.sciencedirect.com/science/article/pii/S0164070422000398},
author = {Thomas E. Cone},
keywords = {Adaptive learning, Economic dynamics, Expectations, New Keynesian model, Misspecification, Regime switching},
abstract = {This paper introduces new economic dynamics induced by learning. Under monetary policy with regime switches, private sector agents learn about output and inflation, which are functions of their beliefs about those variables. The learners’ models are misspecified because they do not observe the regimes. The novel dynamics occur if the regime switches are slow relative to the learning speed: Learners’ beliefs do not converge to the single process that usually describes their asymptotic beliefs, but to a Markov switching process. Checking for this possibility is crucial: When it occurs, the standard analysis gives incorrect results about the equilibria to which beliefs and other endogenous variables will converge. In an extension in which the learners choose their learning speed optimally, the result is strengthened.}
}
@article{HU2022102371,
title = {A grasps-generation-and-selection convolutional neural network for a digital twin of intelligent robotic grasping},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {77},
pages = {102371},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102371},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000588},
author = {Weifei Hu and Chuxuan Wang and Feixiang Liu and Xiang Peng and Pengwen Sun and Jianrong Tan},
keywords = {Intelligent robotic grasping, Digital twin, Convolutional neural network, Deep learning, RGB-D image},
abstract = {Robotic grasping plays an essential role in human-machine cooperation in various household and industrial applications. Although humans can instinctively execute grasps in an accurate, stable, and rapid way even under a constantly changing environment, intelligent grasping remains a challenging task for robots. As a prerequisite for grasping, robots need to correctly identify the best grasping location of unknown objects often based on an artificial intelligence approach, which is still a challenging problem. This paper proposes a new grasps-generation-and-selection convolutional neural network (GGS-CNN), which is trained and implemented in a digital twin of intelligent robotic grasping (DTIRG). By defining a grasp with 3-D position, rotation angle, and gripper width, the GGS-CNN generates grasp candidates by transforming the red–green-blue-depth images (RGB-D images) into feature maps and evaluating the quality of selected grasps. The GGS-CNN is trained in the virtual environment and the real world of the DTIRG to detect accurate grasps. In the grasping tests, the proposed GGS-CNN achieves grasping success rates of 96.7% and 93.8% for grasping single objects and cluttered objects, respectively, and obtains the best grasp from the RGB-D image in less than 40 ms.}
}
@article{YANG2022104519,
title = {Automated semantic segmentation of bridge components from large-scale point clouds using a weighted superpoint graph},
journal = {Automation in Construction},
volume = {142},
pages = {104519},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104519},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003922},
author = {Xiaofei Yang and Enrique {del Rey Castillo} and Yang Zou and Liam Wotherspoon and Yi Tan},
keywords = {Bridge component recognition, Deep learning, Semantic segmentation, Large-scale point clouds, Weighted Superpoint Graph},
abstract = {Deep learning techniques have the potential to provide versatile solutions for automated semantic segmentation of bridge point clouds, but previous studies were limited to small-scale bridge point clouds and focused on limited bridge component categories due to training sample scarcity. Additionally, no prior work considered the intrinsic data imbalance problem in the bridge dataset, with the points unequally distributed between the various components. This paper presents a weighted superpoint graph (WSPG) method, where bridge point clouds were firstly clustered into hundreds of semantically homogeneous superpoints that were then classified into different bridge components using PointNet and Graph Neural Networks. The WSPG method can recognize components directly from large-scale bridge point clouds and alleviate the data imbalance by leveraging a novel loss function that assigns weights according to the number of points contained in different bridge components. The effectiveness of the method was validated on both a real-world dataset with 5 categories of bridge components and a synthetic dataset with 8 categories of bridge components. Experiment results on the real-world dataset showed that the WSPG model achieved the best performance on all overall evaluation metrics of overall accuracy (OA: 99.43%), mean class accuracy (mAcc: 98.75%) and mean Intersection over Union (mIoU: 96.49%) compared to the existing cutting edge models such as PointNet, DGCNN and the original SPG. Additionally, the WSPG method also surpassed the cutting edge representatives in terms of mAcc and mIoU on the synthetic dataset, especially increasing the original SPG by 8.5% mAcc and 6.7% mIoU. The successful application of the proposed method will significantly improve upper-level tasks such as digital twining for existing bridges.}
}
@article{LENG2022101676,
title = {Digital twins-based flexible operating of open architecture production line for individualized manufacturing},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101676},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101676},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001379},
author = {Jiewu Leng and Ziying Chen and Weinan Sha and Zisheng Lin and Jun Lin and Qiang Liu},
keywords = {Individualized manufacturing, Digital twin, Open architecture production line, Flexible operating},
abstract = {Individualized manufacturing implies high flexibility of both the hardware and software of the production lines based on a fast physical and logical system (de)commissioning. This paper proposes an open architecture production line (OAPL) design together with a digital twins-based flexible operating approach for individualized manufacturing. Firstly, an OAPL is designed and implemented with physical reconfigurability by orchestrating different open architectural platforms together with open architecture machine tools (OAMTs). Secondly, an open architectural style modeling and configuration method is presented to enable the software reconfigurability of the controls of the OAPL. Thirdly, a digital twin-based online process emulating and multi-physics simulation is integrated to aid the comprehensive characterizing of the operation status of the OAPL. Based on the system reconfigurability and digital twins system, a triple-layer Learning-Optimization-Reacting approach together with an ensemble algorithm for flexible operating of the OAPL is proposed. The digital twins are formed with the ability to flexibly operate the OAPL for catering to different individualized requirements. A demonstrative implementation of a stepping-motor assembly OAPL is presented finally.}
}
@article{LI202233,
title = {Intelligent Drilling and Completion: A Review},
journal = {Engineering},
volume = {18},
pages = {33-48},
year = {2022},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S2095809922006257},
author = {Gensheng Li and Xianzhi Song and Shouceng Tian and Zhaopeng Zhu},
keywords = {Intelligent drilling and completion, Artificial intelligence, Intelligent application scenarios, Literature review, Systematic discuss},
abstract = {The application of artificial intelligence (AI) has become inevitable in the petroleum industry. In drilling and completion engineering, AI is regarded as a transformative technology that can lower costs and significantly improve drilling efficiency (DE). In recent years, numerous studies have focused on intelligent algorithms and their application. Advanced technologies, such as digital twins and physics-guided neural networks, are expected to play roles in drilling and completion engineering. However, many challenges remain to be addressed, such as the automatic processing of multi-source and multi-scale data. Additionally, in intelligent drilling and completion, methods for the fusion of data-driven and physics-based models, few-sample learning, uncertainty modeling, and the interpretability and transferability of intelligent algorithms are research frontiers. Based on intelligent application scenarios, this study comprehensively reviews the research status of intelligent drilling and completion and discusses key research areas in the future. This study aims to enhance the berthing of AI techniques in drilling and completion engineering.}
}
@article{UDUGAMA202294,
title = {Digitalisation in chemical engineering: Industrial needs, academic best practice, and curriculum limitations},
journal = {Education for Chemical Engineers},
volume = {39},
pages = {94-107},
year = {2022},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1749772822000112},
author = {Isuru A. Udugama and Christoph Bayer and Saeid Baroutian and Krist V. Gernaey and Wei Yu and Brent R. Young},
keywords = {Digitalisation, Digital twins, Big data analytics, internet of things, Industry survey, Expert survey, Staff survey},
abstract = {The digitalisation megatrend is currently changing the way whole sectors of the economy are operated and in the manufacturing sector, digitalisation promises productivity improvements with seemingly marginal investments in “steel and concrete”. These sector wide shifts will inevitably influence how a chemical engineering graduate would perform their job functions, irrespective if they are working in an area such as petrochemical manufacturing or in the financial and management sector of the economy. This manuscript presents the results from three targeted surveys carried out to get a better understanding of (1) what alumni from a chemical and materials engineering degree think these changes will mean for them, (2) expert opinions on the level of detail key topics of digital twins, big data and Internet of Things (IOT) should be covered by an undergraduate chemical engineering curriculum and (3) what staff from a chemical and materials engineering degree think the best way to embed these concepts into an undergraduate level chemical engineering education. Analysing the results from the survey highlighted the following aspects. (1) An overwhelming number of alumni reported that elements of digitalisation are already influencing their industry and job function and these influences are likely to get stronger overtime (2) The experts identified that chemical engineering graduates will likely drive the development of digital twins hence requiring a high level of understanding in this subject. The graduates would only need to apply Big Data analytics and likely not be involved with the IOT developments. (3) The staff identified that these requirements identified by the experts can be satisfied by taking a two-pronged approach of adding modules to current core courses while developing standalone elective courses to cover the more advanced concepts.}
}
@article{LI2022124440,
title = {Integrated graph deep learning framework for flow field reconstruction and performance prediction of turbomachinery},
journal = {Energy},
volume = {254},
pages = {124440},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124440},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222013433},
author = {Jinxing Li and Tianyuan Liu and Yuqi Wang and Yonghui Xie},
keywords = {Deep learning, Graph neural network, Field reconstruction, Performance prediction, Arbitrary structured/unstructured grids, Digital twin},
abstract = {The performance and reliability of turbomachinery directly affect the efficiency and safety of energy conversion systems. A dual graph neural network (DGNN) for turbomachinery flow field reconstruction and performance prediction is proposed, which utilizes flow field data at grid vertices and characterizes the neighborhood relationships of grids through the adjacency matrix. Different from previous work, this work extends deep learning methods to the reconstruction of global turbomachinery fields defined on arbitrary structured/unstructured grids. The flow field reconstruction and performance prediction of low aspect ratio rotors are used to verify the generalizability of DGNN. The proposed method can not only accurately predict performance parameters, but also achieve excellent performance in flow field reconstruction. The superiority of DGNN over the artificial neural network (ANN) in flow field reconstruction is clarified. The optimal GNN operator with the highest accuracy and lowest computation costs is obtained as SAGE. A well-trained DGNN can give the flow field distribution as well as the performance of turbines within 0.05 s. The proposed approach could be a real-time simulation and analysis approach to assist turbomachinery design and optimization. It may provide an efficient solution for establishing the digital twin system of turbomachinery in the future.}
}
@article{KIM2022109471,
title = {Digital twin approach for on-load tap changers using data-driven dynamic model updating and optimization-based operating condition estimation},
journal = {Mechanical Systems and Signal Processing},
volume = {181},
pages = {109471},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109471},
url = {https://www.sciencedirect.com/science/article/pii/S088832702200588X},
author = {Wongon Kim and Sunuwe Kim and Jingyo Jeong and Hyunjae Kim and Hyeonchan Lee and Byeng D. Youn},
keywords = {On-load tap changer (OLTC), Digital Twin, Prognostics and Health Management (PHM), Vibration Signal},
abstract = {On-load tap changers (OLTCs), which are found in power transformers, are mechanically operating components. The vibration signal of an OLTC can provide effective observed data for estimation of the mechanical state transition and a faulty operating condition. Data-driven methods (e.g., deep learning, machine learning) and physics-based methods (e.g., the finite element method, the lumped parameter model) for health-state estimation require sufficient prior knowledge – such as various observed data about fault states, modeling information (including geometry, material properties), and operating conditions – to build a valid digital twin approach. However, prior knowledge for various OLTCs and transformer models is hard to obtain. To begin to address the shortcomings of existing methods, this study proposes a digital twin approach for OLTCs using 1) pre-processing of the vibration signal, 2) data-driven dynamic model updating, and 3) optimization-based operating condition estimation. First, the time–frequency domain features are extracted from the reference signal using a minimum entropy deconvolution (MED) filter, to extract the impulsive vibration signal from OLTC operation. The initial operating conditions that arise from tap changing and diverter switching are assumed as impulsive force using extracted features. The dynamic model is driven by the numerical algorithm for subspace state-space system identification (N4SID), the reference signal, and the excitation impulse force. Next, the phase and magnitude modulation are updated to estimate uncertain operating condition and refine the dynamic model using optimization-based parameter tuning. Analysis results from the proposed digital twin approach are demonstrated for both a numerical and experimental example to verify the effectiveness of the proposed approach. In the numerical case study, i) the simplified physics-based modelling and ii) the joint-input state estimation method were compared with the proposed method. In the experimental case study, the proposed method was applied to an OLTC vibration signal of both inactive and active power transformers. The inflection points in the Dynamic Resistance Measurement (DRM) graph show synchronization with the experimental results of the estimated excitation forces derived using the proposed method.}
}
@article{BERGER2022113198,
title = {Multi-sensor spectral synergies for crop stress detection and monitoring in the optical domain: A review},
journal = {Remote Sensing of Environment},
volume = {280},
pages = {113198},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.113198},
url = {https://www.sciencedirect.com/science/article/pii/S003442572200308X},
author = {Katja Berger and Miriam Machwitz and Marlena Kycko and Shawn C. Kefauver and Shari {Van Wittenberghe} and Max Gerhards and Jochem Verrelst and Clement Atzberger and Christiaan {van der Tol} and Alexander Damm and Uwe Rascher and Ittai Herrmann and Veronica Sobejano Paz and Sven Fahrner and Roland Pieruschka and Egor Prikaziuk and Ma. Luisa Buchaillot and Andrej Halabuk and Marco Celesti and Gerbrand Koren and Esra Tunc Gormus and Micol Rossini and Michael Foerster and Bastian Siegmann and Asmaa Abdelbaki and Giulia Tagliabue and Tobias Hank and Roshanak Darvishzadeh and Helge Aasen and Monica Garcia and Isabel Pôças and Subhajit Bandopadhyay and Mauro Sulis and Enrico Tomelleri and Offer Rozenstein and Lachezar Filchev and Gheorghe Stancile and Martin Schlerf},
keywords = {Precision agriculture multi-modal solar-induced fluorescence satellite hyperspectral multispectral biotic and abiotic stress},
abstract = {Remote detection and monitoring of the vegetation responses to stress became relevant for sustainable agriculture. Ongoing developments in optical remote sensing technologies have provided tools to increase our understanding of stress-related physiological processes. Therefore, this study aimed to provide an overview of the main spectral technologies and retrieval approaches for detecting crop stress in agriculture. Firstly, we present integrated views on: i) biotic and abiotic stress factors, the phases of stress, and respective plant responses, and ii) the affected traits, appropriate spectral domains and corresponding methods for measuring traits remotely. Secondly, representative results of a systematic literature analysis are highlighted, identifying the current status and possible future trends in stress detection and monitoring. Distinct plant responses occurring under short-term, medium-term or severe chronic stress exposure can be captured with remote sensing due to specific light interaction processes, such as absorption and scattering manifested in the reflected radiance, i.e. visible (VIS), near infrared (NIR), shortwave infrared, and emitted radiance, i.e. solar-induced fluorescence and thermal infrared (TIR). From the analysis of 96 research papers, the following trends can be observed: increasing usage of satellite and unmanned aerial vehicle data in parallel with a shift in methods from simpler parametric approaches towards more advanced physically-based and hybrid models. Most study designs were largely driven by sensor availability and practical economic reasons, leading to the common usage of VIS-NIR-TIR sensor combinations. The majority of reviewed studies compared stress proxies calculated from single-source sensor domains rather than using data in a synergistic way. We identified new ways forward as guidance for improved synergistic usage of spectral domains for stress detection: (1) combined acquisition of data from multiple sensors for analysing multiple stress responses simultaneously (holistic view); (2) simultaneous retrieval of plant traits combining multi-domain radiative transfer models and machine learning methods; (3) assimilation of estimated plant traits from distinct spectral domains into integrated crop growth models. As a future outlook, we recommend combining multiple remote sensing data streams into crop model assimilation schemes to build up Digital Twins of agroecosystems, which may provide the most efficient way to detect the diversity of environmental and biotic stresses and thus enable respective management decisions.}
}
@article{BADUGE2022104440,
title = {Artificial intelligence and smart vision for building and construction 4.0: Machine and deep learning methods and applications},
journal = {Automation in Construction},
volume = {141},
pages = {104440},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104440},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003132},
author = {Shanaka Kristombu Baduge and Sadeep Thilakarathna and Jude Shalitha Perera and Mehrdad Arashpour and Pejman Sharafi and Bertrand Teodosio and Ankit Shringi and Priyan Mendis},
keywords = {Artificial intelligence, Machine learning, Deep learning, Automation, Internet of things, Building information modelling, Smart vision, Convolution neural network, Generative adversarial network, Artificial neural network},
abstract = {This article presents a state-of-the-art review of the applications of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) in building and construction industry 4.0 in the facets of architectural design and visualization; material design and optimization; structural design and analysis; offsite manufacturing and automation; construction management, progress monitoring, and safety; smart operation, building management and health monitoring; and durability, life cycle analysis, and circular economy. This paper presents a unique perspective on applications of AI/DL/ML in these domains for the complete building lifecycle, from conceptual stage, design stage, construction stage, operational and maintenance stage until the end of life. Furthermore, data collection strategies using smart vision and sensors, data cleaning methods (post-processing), data storage for developing these models are discussed, and the challenges in model development and strategies to overcome these challenges are elaborated. Future trends in these domains and possible research avenues are also presented.}
}
@article{KHAWANDAZOULAI2022934,
title = {The Patient Story in Palliative Care: A Thematic Analysis of Medical Students’ Reflections on a Narrative Medicine Assignment (S551)},
journal = {Journal of Pain and Symptom Management},
volume = {63},
number = {5},
pages = {934},
year = {2022},
issn = {0885-3924},
doi = {https://doi.org/10.1016/j.jpainsymman.2022.02.174},
url = {https://www.sciencedirect.com/science/article/pii/S0885392422002597},
author = {Mariana Khawand-Azoulai and Amanda Alladin and Kyra Lipman and Sarah Bland and Corinne Ferrari and Maria {van Zuilen}},
abstract = {Outcomes
1. Identify the benefits of a “patient story” exercise with a reflection component for medical students on a palliative care rotation 2. Describe the impact of a patient story exercise by reviewing common themes that were identified among medical students’ reflections 3. Discuss feasibility of integration of a patient story assignment with reflection as a routine part of palliative medicine rotations for learners of all stages
Original Research Background
Communication with seriously ill patients is a skill that requires practice; the traditional medical student model of learning to interview patients overlooks in-depth exploration of patients’ social background and personal history. This information is vital to determining goals of care and facilitates an authentic care partnership by building empathy and trust. We implemented a narrative medicine activity for students during their hospice and palliative medicine (HPM) rotation with the intent of highlighting the impact of humanism in the care of patients.
Research Objectives
Todetermine the impact of a narrative medicine exercise by a thematic analysis of students’ reflections on a “patient story” assignment.
Methods
Studentsreceived a 5-minute introduction to the “patient story” assignment. They invited patients to share their story and elicited “what matters most.” Students transcribed and shared this story with the HPM team and penned a personal reflection. We analyzed 100 reflections and used an inductive and iterative approach to thematic analysis that started with the development of codes from which broader themes were constructed.
Results
Four main themes were analyzed as being predominant with associated subthemes constructed and shown in parentheses: getting to know the patient (background, values, family, seeing the patient as more than a disease, resilience). student reaction to the experience (positive, negative, challenging), building blocks of the patient-physician relationship (empathy, trust, connection, listening), and student personal insights (personal reflection, lasting impact)
Conclusion
This activity demonstrated to students the value of uncovering patients’ backgrounds and values, reinforcing the multidomain approach of palliative medicine. The majority of students felt this was a positive, impactful exercise that would change their approach to patients in future practice.
Implications for Research, Policy, or Practice
A patient storyassignment is a feasible, well-received activity that creates an opportunity for students to reflect on the physician-patient relationship beyond the medical domain and enriches the care of those with serious illness.}
}
@article{WANG2022104464,
title = {Construction and maintenance of urban underground infrastructure with digital technologies},
journal = {Automation in Construction},
volume = {141},
pages = {104464},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104464},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003375},
author = {Mingzhu Wang and Xianfei Yin},
keywords = {Underground infrastructure, Literature review, Digital technologies, Inspection and maintenance, Condition assessment, Underground construction, Infrastructure operation & maintenance},
abstract = {Urban underground infrastructure is a critical component in cities to provide essential services to residents. Research efforts have been made to facilitate different activities of underground infrastructure projects using various methods, particularly digital technologies. To obtain deeper insights from existing research and provide directions for future research, this study conducts a comprehensive review of research on underground infrastructure construction and Operation & Maintenance (O&M) with a focus on digital technologies. The in-depth review was conducted based on 145 publications from the perspective of locating and mapping, construction and coordination, as well as O&M. Consequently, critical limitations and challenges are revealed, such as the lack of as-built and as-is information, the requirement of data quality and quantity for deep learning methods, the lack of fully automated robotic systems, etc. Afterwards, a status matrix was presented to identify the level of different digital technologies being studied and their future application potential for key activities of underground infrastructure projects. In the end, future research trends are proposed, including (1) digital twinning of underground infrastructure, (2) quality and uncertainty of inspection data, (3) data generation and semi-supervised learning, (4) predictive maintenance, and (5) fully automated robotic systems for inspection and maintenance. This study contributes to the body of knowledge by identifying the challenges and limitations of existing studies through a systematic review, providing a clear view of the achievements and potentials of digital technologies for underground infrastructure, and proposing future research directions to facilitate digital transformation in this area.}
}
@article{WANG2022108033,
title = {Digital twins supported equipment maintenance model in intelligent water conservancy},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {108033},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108033},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622002968},
author = {Zhoukai Wang and Weina Jia and Kening Wang and Yichuan Wang and Qiaozhi Hua},
keywords = {Digital twins, Transfer learning, Fault diagnose, Equipment maintenance, Intelligent water conservancy},
abstract = {With the rapid growth of China’s economy, applied research on hydropower engineering has received an increasing amount of attention. However, since hydraulic electromechanical devices often work in actual industrial manufacturing environments at high loads for a long time, their health status is hardly predicted. By introducing digital twins technique, this paper proposed a predictive maintenance model for electromechanical devices to solve the problems. Firstly, multiple sensors are implemented on critical parts of the hydraulic electromechanical devices to collect devices’ physical and spatial signals. Secondly, constructing the digital twins model of electromechanical devices with the sensing data and the devices’ structural characteristics. Finally, by transfer learning, a comprehensive and reliable fault diagnosis method is designed to predict the remaining life of the devices and make decisions for facilities maintenance. Experiments show that the proposed model performs the best accuracy rate compared with the other methods.}
}
@article{GUARINO2022118302,
title = {Adaptive talent journey: Optimization of talents’ growth path within a company via Deep Q-Learning},
journal = {Expert Systems with Applications},
volume = {209},
pages = {118302},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118302},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422014348},
author = {Alfonso Guarino and Delfina Malandrino and Francesco Marzullo and Antonio Torre and Rocco Zaccagnino},
keywords = {Talent journey, Deep Q-Learning, Digital Twin, User evaluation},
abstract = {In enterprise context, companies constantly aim to optimize their human resources and acquire new ones. Employees, also called talents, are required to achieve new skills for the company to stay competitive in the business. The talents’ ability to productively improve is a crucial factor for the success of a company. We propose Adaptive Talent Journey, a novel method for optimizing the growth path of talents within a company. The ultimate goal of Adaptive Talent Journey is to hold talent back inside the company. It exploits the notion of “digital twin” to define a digital representation of the talent, namely Talent Digital Twin, built on the basis of skills level and personal traits. Given a target company’s role, Adaptive Talent Journey proposes the most suitable path of work experiences (journey) to improve the skills of a talent so to achieve the target role requirements. Such a mechanism resonates with the Reinforcement Learning paradigm, and specifically with Deep Q-Learning. Specifically, the proposed method exploits: (i) two double Deep Q-Networks (DDQNs) for selecting the work experiences to be made; (ii) a transition module to support the DDQNs training and ensure good performance despite the limited availability of data. We implemented and deployed Adaptive Talent Journey in an intuitive Web application, namely ATJWeb. We evaluated both the effectiveness and efficiency of our proposal and the users’ satisfaction in using it, adopting, as a testbed, an IT company with its employees. Results proved that the Adaptive Talent Journey can optimize the growth path of talents, and that ATJWeb is pleasant and useful.}
}
@article{LEE2022101710,
title = {Digital twin-driven deep reinforcement learning for adaptive task allocation in robotic construction},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101710},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101710},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001689},
author = {Dongmin Lee and SangHyun Lee and Neda Masoud and M.S. Krishnan and Victor C. Li},
keywords = {Digital Twin, Proximal Policy Optimization (PPO), Deep Reinforcement Learning (DRL), Autonomous Robot, Adaptive Task Allocation},
abstract = {In order to accomplish diverse tasks successfully in a dynamic (i.e., changing over time) construction environment, robots should be able to prioritize assigned tasks to optimize their performance in a given state. Recently, a deep reinforcement learning (DRL) approach has shown potential for addressing such adaptive task allocation. It remains unanswered, however, whether or not DRL can address adaptive task allocation problems in dynamic robotic construction environments. In this paper, we developed and tested a digital twin-driven DRL learning method to explore the potential of DRL for adaptive task allocation in robotic construction environments. Specifically, the digital twin synthesizes sensory data from physical assets and is used to simulate a variety of dynamic robotic construction site conditions within which a DRL agent can interact. As a result, the agent can learn an adaptive task allocation strategy that increases project performance. We tested this method with a case project in which a virtual robotic construction project (i.e., interlocking concrete bricks are delivered and assembled by robots) was digitally twinned for DRL training and testing. Results indicated that the DRL model’s task allocation approach reduced construction time by 36% in three dynamic testing environments when compared to a rule-based imperative model. The proposed DRL learning method promises to be an effective tool for adaptive task allocation in dynamic robotic construction environments. Such an adaptive task allocation method can help construction robots cope with uncertainties and can ultimately improve construction project performance by efficiently prioritizing assigned tasks.}
}
@article{AREFI2022101389,
title = {Intelligent potato frying: Time to say goodbye to the “good old” processing strategies},
journal = {Thermal Science and Engineering Progress},
volume = {34},
pages = {101389},
year = {2022},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2022.101389},
url = {https://www.sciencedirect.com/science/article/pii/S2451904922001950},
author = {Arman Arefi and Oliver Hensel and Barbara Sturm},
keywords = {Acrylamide, Artificial Intelligence, Deep-fat frying, Digital Twin, Hyperspectral imaging, Machine learning, Intelligent processes, Optimal frying conditions, Smart frying},
abstract = {Potato chips production is a traditional food process. To achieve uniform product quality, raw materials are usually rigorously sorted. Traditionally, the process is conducted in a single stage approach leading to high quality losses. Recently, dynamically optimized frying processes have been found to result in higher product quality. Consequently, industrial continuous deep-fat fryers convey potato disks through several zones pre-set at different temperatures. However, these improved systems still do not take the variabilities in frying kinetics among potatoes into consideration. To address this issue and decrease uncertainties in end-product quality, frying conditions of each zone must be optimized, physiochemical properties of the various raw tubers and their frying kinetics taking into account. This paper, therefore, presents a novel approach for an intelligent frying process with embedded computer vision systems providing continuous monitoring of product quality and, therefore, facilitate dynamic control of frying conditions in order to meet desired quality attributes in the final product. An extensive literature review of the key physiochemical attributes of raw potato tubers is presented, followed by an introduction to novel pre-treatment technologies, and the importance of optimal frying conditions. An overview of the potentials for using computer vision systems for the assessment of said quality criteria is given, followed by a detailed description of the envisioned frying process. The paper concludes that the realization of intelligent frying processes necessitates the development of fully fledged digital twins of the process and the products, combining physics based and data driven modelling with real time sensing and control. Terminology: Chips refer to thin slices of potato while French fries refers to wedges/stripes.}
}
@article{KIM2022168,
title = {Cross-modal distillation with audio–text fusion for fine-grained emotion classification using BERT and Wav2vec 2.0},
journal = {Neurocomputing},
volume = {506},
pages = {168-183},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222008931},
author = {Donghwa Kim and Pilsung Kang},
keywords = {Multi-class emotion classification, Knowledge distillation, Transformer, BERT, Wav2Vec 2.0, Contrastive learning},
abstract = {Fine-grained emotion classification for mood- and emotion-related physical-characteristics detection and its application to computer technology using biometric sensors has been extensively researched in the field of affective computing. Although text modality has achieved a considerably high performance from the perspective of sentiment analysis, which simply classifies a positive or negative label, fine-grained emotion classification requires additional information besides text. An audio feature can be adopted as the additional information as it is closely associated with text, and the characteristics of the changes in sound pulses can be employed in fine-grained emotion classification. However, the multimodal datasets related to fine-grained emotion are limited, and the scalability and efficiency are insufficient for multimodal training to be applied extensively via the self-supervised learning (Self-SL) approach, which can adequately represent modality. To address these limitations, we propose cross-modal distillation (CMD), which induces the feature spaces of student models with a few parameters while receiving those of the teacher models that can adequately express each modality based on Self-SL. The proposed CMD performs the mapping of a feature space between teacher-student models based on contrastive learning, while two attention mechanisms—cross-attention between audio and text features and self-attention for features in modality—are performed during knowledge distillation. Wav2vec 2.0 and BERT, which are already adequately trained for audio and text via Self-SL, were adopted as teacher models; audio–text transformer models were used as student models. Accordingly, the CMD-based representation learning applies a lightweight model for IEMOCAP, MELD, and CMU–MOSEI datasets with the task of multi-class emotion classification, while exhibiting better fine-grained emotion classification performance than benchmark models with a considerably low uncertainty for prediction.}
}
@article{BONGKIM20221,
title = {A digital twin implementation architecture for wire + arc additive manufacturing based on ISO 23247},
journal = {Manufacturing Letters},
volume = {34},
pages = {1-5},
year = {2022},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2022.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S2213846322001808},
author = {Duck {Bong Kim} and Guodong Shao and Guejong Jo},
keywords = {Digital Twin, Wire + Arc Additive Manufacturing, ISO 23247, Data Analytics},
abstract = {Digital twin (DT) is an enabling technology characterized by integrating cyber and physical spaces. It is well-fitted to additive manufacturing since it can benefit from digitalized assets and data analytics for the process control. Wire + arc additive manufacturing (WAAM) is being increasingly recognized due to its fabrication of large-scale parts. This paper proposes a generalized DT implementation architecture for WAAM based on ISO 23247 to address integration and interoperability issues. It will enable manufacturers to leverage DT for the real-time decision-making and control. An application scenario of machine learning-based anomaly detection for WAAM is used to explain the architecture.}
}
@article{XIAO2022,
title = {AFSTGCN: Prediction for multivariate time series using an adaptive fused spatial-temporal graph convolutional network},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001419},
author = {Yuteng Xiao and Kaijian Xia and Hongsheng Yin and Yu-Dong Zhang and Zhenjiang Qian and Zhaoyang Liu and Yuehan Liang and Xiaodan Li},
keywords = {Adaptive adjacency matrix, Digital twin, Graph convolutional network, Multivariate time series prediction, Spatial-temporal graph},
abstract = {The prediction for Multivariate Time Series (MTS) explores the interrelationships among variables at historical moments, extracts their relevant characteristics, and is widely used in finance, weather, complex industries and other fields. Furthermore, it is important to construct a digital twin system. However, existing methods do not take full advantage of the potential properties of variables, which results in poor predicted accuracy. In this paper, we propose the Adaptive Fused Spatial-Temporal Graph Convolutional Network (AFSTGCN). First, to address the problem of the unknown spatial-temporal structure, we construct the Adaptive Fused Spatial-Temporal Graph (AFSTG) layer. Specifically, we fuse the spatial-temporal graph based on the interrelationship of spatial graphs. Simultaneously, we construct the adaptive adjacency matrix of the spatial-temporal graph using node embedding methods. Subsequently, to overcome the insufficient extraction of disordered correlation features, we construct the Adaptive Fused Spatial-Temporal Graph Convolutional (AFSTGC) module. The module forces the reordering of disordered temporal, spatial and spatial-temporal dependencies into rule-like data. AFSTGCN dynamically and synchronously acquires potential temporal, spatial and spatial-temporal correlations, thereby fully extracting rich hierarchical feature information to enhance the predicted accuracy. Experiments on different types of MTS datasets demonstrate that the model achieves state-of-the-art single-step and multi-step performance compared with eight other deep learning models.}
}
@article{HUO2022102508,
title = {Automatic Grading Assessments for Knee MRI Cartilage Defects via Self-ensembling Semi-supervised Learning with Dual-Consistency},
journal = {Medical Image Analysis},
volume = {80},
pages = {102508},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102508},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522001554},
author = {Jiayu Huo and Xi Ouyang and Liping Si and Kai Xuan and Sheng Wang and Weiwu Yao and Ying Liu and Jia Xu and Dahong Qian and Zhong Xue and Qian Wang and Dinggang Shen and Lichi Zhang},
keywords = {Knee cartilage defect, Semi-supervised learning, Dual consistency, Attention mechanism},
abstract = {Knee cartilage defects caused by osteoarthritis are major musculoskeletal disorders, leading to joint necrosis or even disability if not intervened at early stage. Deep learning has demonstrated its effectiveness in computer-aided diagnosis, but it is time-consuming to prepare a large set of well-annotated data by experienced radiologists for model training. In this paper, we propose a semi-supervised framework to effectively use unlabeled data for better evaluation of knee cartilage defect grading. Our framework is developed based on the widely-used mean-teacher classification model, by designing a novel dual-consistency strategy to boost the consistency between the teacher and student models. The main contributions are three-fold: (1) We define an attention loss function to make the network focus on the cartilage regions, which can both achieve accurate attention masks and boost classification performance simultaneously; (2) Besides enforcing the consistency of classification results, we further design a novel attention consistency mechanism to ensure the focusing of the student and teacher networks on the same defect regions; (3) We introduce an aggregation approach to ensemble the slice-level classification outcomes for deriving the final subject-level diagnosis. Experimental results show that our proposed method can significantly improve both classification and localization performances of knee cartilage defects. Our code is available on https://github.com/King-HAW/DC-MT.}
}
@article{YANG2022,
title = {A digital twins enabled underwater intelligent internet vehicle path planning system via reinforcement learning and edge computing},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000967},
author = {Jiachen Yang and Meng Xi and Jiabao Wen and Yang Li and Houbing Herbert Song},
keywords = {Digital twins, Reinforcement learning, Edge computing, Underwater intelligent internet vehicle, Path planning},
abstract = {The Autonomous Underwater Glider (AUG) is a kind of prevailing underwater intelligent internet vehicle and occupies a dominant position in industrial applications, in which path planning is an essential problem. Due to the complexity and variability of the ocean, accurate environment modeling and flexible path planning algorithms are pivotal challenges. The traditional models mainly utilize mathematical functions, which are not complete and reliable. Most existing path planning algorithms depend on the environment and lack flexibility. To overcome these challenges, we propose a path planning system for underwater intelligent internet vehicles. It applies digital twins and sensor data to map the real ocean environment to a virtual digital space, which provides a comprehensive and reliable environment for path simulation. We design a value-based reinforcement learning path planning algorithm and explore the optimal network structure parameters. The path simulation is controlled by a closed-loop model integrated into the terminal vehicle through edge computing. The integration of state input enriches the learning of neural networks and helps to improve generalization and flexibility. The task-related reward function promotes the rapid convergence of the training. The experimental results prove that our reinforcement learning based path planning algorithm has great flexibility and can effectively adapt to a variety of different ocean conditions.}
}
@article{DEON2022109578,
title = {Digital twin and machine learning for decision support in thermal power plant with combustion engines},
journal = {Knowledge-Based Systems},
volume = {253},
pages = {109578},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109578},
url = {https://www.sciencedirect.com/science/article/pii/S095070512200795X},
author = {B. Deon and K.P. Cotta and R.F.V. Silva and C.B. Batista and G.T. Justino and G.C. Freitas and A.M. Cordeiro and A.S. Barbosa and F.L. Loução and T. Simioni and A.M. Morais and I.E.A. Medeiros and R.J.S. Almeida and C.A.A. {Araújo Jr.} and C. Soares and N. Padoin},
keywords = {Digital twin, Machine learning, Predictive maintenance, Thermal power plant, Decision support},
abstract = {The reliability and performance of the generating machines in a thermal power plant are crucial to ensure agility and assertiveness in decision-making, maximize economic results, and ensure meeting the electricity sector demands. In this work, a decision support system (DSM) was developed to predict trends and operational deviations in thermal power plants with combustion engines in an automated and reliable way. It is based on digital twin models for thermoelectric generation engines and their subsystems associated with models of machine learning for predictive maintenance, allowing the classification of failures in the generating units of the plant. The models represent the mechanical, thermal, and electrical conditions and parameters of each piece of equipment under normal operating conditions, and the tool generates alerts when deviations from the base model occur. The benefits from event forecasting range from a reduction in operational issues to the company’s strategic objectives due to the reduction in corrective maintenance downtimes, resulting in reduced operation and maintenance costs. Considering the real-time execution character of the models, it is essential for the tool to meet the operation’s decision-making needs; so an on-premises application is necessary. The proposed architecture can be applied to any industrial sector that uses SCADA supervisors and can be adapted, expanded, and evolved to other generation technologies, such as thermal plants that use different fuels and small hydroelectric, wind, and thermonuclear plants. The techniques used in conjunction with the developed architecture can be replicated in other systems and energy sectors, such as distribution and transmission, and can also be applied to industry in general: chemical, petrochemical, oil and gas, and others.}
}
@article{KHANMOHAMMADI2022499,
title = {A framework of data modeling and artificial intelligence for environmental-friendly energy system: Application of Kalina cycle improved with fuel cell and thermoelectric module},
journal = {Process Safety and Environmental Protection},
volume = {164},
pages = {499-516},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0957582022005602},
author = {Shoaib Khanmohammadi and Farayi Musharavati and Rasikh Tariq},
keywords = {Artificial neural network, Computational intelligence, Scenario-based optimization, Efficient energy systems, Sensitivity analysis},
abstract = {Geothermal energy-driven systems with integrated waste heat recovery units such as the use of fuel cells and thermoelectric module can help to improve the renewable energy contribution in the energy mix. Data-driven optimization can improve their economic and environmental performance and their macro-projection can help in the achievement of net-zero plans. This article extends the use of a framework containing the usage of data modeling and artificial intelligence to conduct different optimization scenarios of the geothermal-driven energy system. It includes the improvement of the economic, exergetic, energetic, and environmental performance through the development of various optimization scenarios. This is done through the development of an extensive thermodynamic model and validation based upon energy, exergy, economic, and environmental evaluations. Different machine learning techniques are adapted for digital twinning of the six performance indicators as a function of nine design variables including operational, source, and economic variables. It is shown that the artificial neural network offers the best statistical fit as compared to the other machine learning techniques including RMSE: 0.1768, R2:0.9999, MSE:0.0312, and MAE:0.1107 for the total work output. Energy-efficient design has yielded a total work output of 1044.86 kW, with a first law efficiency of 0.3322. The economic design offers the lowest cost of electricity at only 34.004 $/hr. The sensitivity analysis has shown that the following parameters are the most sensitivity: turbine inlet temperature (18.19%) and pressure (18.23%), geothermal inlet temperature (16.34%) and pressure (18.00%), and the ammonia water concentration at the inlet of separator (15.96%).}
}
@article{AHMED2022292,
title = {Integrating digital twins and deep learning for medical image analysis in the era of COVID-19},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {4},
pages = {292-305},
year = {2022},
note = {Virtual-reality and intelligent hardware in digital twins A)},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000183},
author = {Imran Ahmed and Misbah Ahmad and Gwanggil Jeon},
keywords = {Digital twins, Deep learning, Healthcare, COVID-19, Chest X-rays, Artificial intelligence},
abstract = {Background
Digital twins are virtual representations of devices and processes that capture the physical properties of the environment and operational algorithms/techniques in the context of medical devices and technologies. Digital twins may allow healthcare organizations to determine methods of improving medical processes, enhancing patient experience, lowering operating expenses, and extending the value of care. During the present COVID-19 pandemic, various medical devices, such as X-rays and CT scan machines and processes, are constantly being used to collect and analyze medical images. When collecting and processing an extensive volume of data in the form of images, machines and processes sometimes suffer from system failures, creating critical issues for hospitals and patients.
Methods
To address this, we introduce a digital-twin-based smart healthcare system integrated with medical devices to collect information regarding the current health condition, configuration, and maintenance history of the device/machine/system. Furthermore, medical images, that is, X-rays, are analyzed by using a deep-learning model to detect the infection of COVID-19. The designed system is based on the cascade recurrent convolution neural network (RCNN) architecture. In this architecture, the detector stages are deeper and more sequentially selective against small and close false positives. This architecture is a multi-stage extension of the RCNN model and sequentially trained using the output of one stage for training the other. At each stage, the bounding boxes are adjusted to locate a suitable value of the nearest false positives during the training of the different stages. In this manner, the arrangement of detectors is adjusted to increase the intersection over union, overcoming the problem of overfitting. We train the model by using X-ray images as the model was previously trained on another dataset.
Results
The developed system achieves good accuracy during the detection phase of COVID-19. The experimental outcomes reveal the efficiency of the detection architecture, which yields a mean average precision rate of 0.94.}
}
@article{HAO2022102366,
title = {A self-training teacher-student model with an automatic label grader for abdominal skeletal muscle segmentation},
journal = {Artificial Intelligence in Medicine},
volume = {132},
pages = {102366},
year = {2022},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2022.102366},
url = {https://www.sciencedirect.com/science/article/pii/S0933365722001282},
author = {Degan Hao and Maaz Ahsan and Tariq Salim and Andres Duarte-Rojo and Dadashzadeh Esmaeel and Yudong Zhang and Dooman Arefan and Shandong Wu},
keywords = {Image segmentation, Semi-supervised learning, Self-attention, Teacher-student model, Skeletal muscle},
abstract = {Deep learning on a limited number of labels/annotations is a challenging task for medical imaging analysis. In this paper, we propose a novel self-training segmentation pipeline (Self-Seg in short) for segmenting skeletal muscle in CT images. Self-Seg starts with a small set of annotated images and then iteratively learns from unlabeled datasets to gradually improve the segmentation performance. Self-Seg follows a semi-supervised teacher-student learning scheme and there are two contributions: 1) we construct a self-attention UNet to improve segmentation over the classical UNet model, and 2) we implement an automatic label grader to implicitly incorporate medical knowledge for quality assurance of pseudo labels, from which good quality pseudo labels are identified to enhance learning of the segmentation model. We perform extensive experiments on three CT image datasets and show promising results on five evaluation settings, and we also compared our method to several baseline and related methods and achieved superior performance.}
}
@article{COLLINS2022100518,
title = {Review: Smart agri-systems for the pig industry},
journal = {animal},
volume = {16},
pages = {100518},
year = {2022},
note = {Manipulating Pig Production XVIII: Proceedings of the Eighteenth Biennial Conference of the Australasian Pig Science Association (APSA), 15-18 November 2021, Brisbane, Australia},
issn = {1751-7311},
doi = {https://doi.org/10.1016/j.animal.2022.100518},
url = {https://www.sciencedirect.com/science/article/pii/S1751731122000696},
author = {L.M. Collins and L.M. Smith},
keywords = {Digital farming, Pork, Precision Livestock Farming, Production, Systems approach},
abstract = {The projected rise in the global human population and the anticipated increase in demand for meat and animal products, albeit with a greatly reduced environmental footprint, offers a difficult set of challenges to the livestock sector. Primarily, how do we produce more, but in a way that is healthier for the animals, public, and the environment? Implementing a smart agri-systems approach, utilising multiplatform precision technologies, internet of things, data analytics, machine learning, digital twinning and other emerging technologies can support a more informed decision-making and forecasting position that will allow us to move towards greater sustainability in future. If we look to precision agronomy, there are a wide range of technologies available and examples of how digitalisation and integration of platform outputs can lead to advances in understanding the agricultural system and forecasting upcoming events and performance that have hitherto been impossible to achieve. There is much for the livestock sector and animal scientists to learn from the developments of precision technologies and smart agri-system approaches in the arable and horticultural contexts. However, there are several barriers the livestock sector must overcome: (i) the development and implementation of precision livestock farming technologies that can be easily integrated and analysed without the support of a dedicated data analyst in house; (ii) the lack of extensive validation of many developed and available precision livestock farming technologies means that reliability and accuracy are likely to be compromised when applied in commercial practice; (iii) the best smart agri-systems approaches are reliant on large quantities of data from across a wide variety of conditions, but at present the complications of data sharing, commercial sensitivities, data ownership, and permissions make it challenging to obtain or knit together data from different parts of the system into a comprehensive picture; and (iv) the high level of investment needed to develop and scale these technologies is substantial and represents significant risk for companies when a technology is emerging. Using a case study of the National Pig Centre (a flagship pig research facility in the UK) we discuss how a smart agri-systems approach can be applied in practice to investigate alternative future systems for production, and enable monitoring of these systems as a commercial demonstrator site for future pork production.}
}
@article{CATSOULIS2022104221,
title = {Integrating supervised learning and applied computational multi-fluid dynamics},
journal = {International Journal of Multiphase Flow},
volume = {157},
pages = {104221},
year = {2022},
issn = {0301-9322},
doi = {https://doi.org/10.1016/j.ijmultiphaseflow.2022.104221},
url = {https://www.sciencedirect.com/science/article/pii/S0301932222001987},
author = {Sotiris Catsoulis and Joel-Steven Singh and Chidambaram Narayanan and Djamel Lakehal},
keywords = {Computational Fluid Dynamics, Multiphase flow, Machine learning, Data-driven model, Simulation-based Digital Twin},
abstract = {The transition from iterative methods of engineering design towards physics-based modeling has been assisted by the advent of Computer-Aided-Engineering. However, post-processing of simulation results, based on a standard workflow providing base-case simulations complemented by selected operating conditions, has changed little. In this work, we propose a new paradigm for handling simulation data by deploying machine learning to encompass a wide spectrum of operating conditions, bypassing the need for additional simulations. This hybrid physics-based and data-driven modeling procedure yields to what we refer to as a Simulation-based Digital Twin. In this paper, we make the case for Computational Fluid Dynamics in multiphase flow systems, although the workflow can be generalized to any other computational engineering method. We quantify the computational speed-up to conclude that the combination of these two fields generates potential for improvement on the conventional methods used in the broad area of computational engineering.}
}
@article{WEI2022104356,
title = {Panorama-to-model registration through integration of image retrieval and semantic reprojection},
journal = {Automation in Construction},
volume = {140},
pages = {104356},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104356},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002291},
author = {Yujie Wei and Burcu Akinci},
keywords = {Image, Panorama, Image-to-BIM registration, Indoor localization, Semantic understanding, Deep learning, Convolutional neural network},
abstract = {Registering images to a building information model is an effective approach of associating as-built component status to as-designed information. However, registering a single still image to a digital twin is difficult due to pose ambiguities caused by the limited field of view in images. Several recent studies have started focusing on leveraging panoramic images to address problems, such as partial occlusions, repetitive facility components and textureless views, which could cause registration failures in existing image-based registration workflows. Even though having a bigger field of view helps in locating where the image was taken, registering a panoramic image to a building information model is still challenging due to inconsistent visual appearances, such as model vs. image visual differences caused by temporary objects in a scene, lighting condition changes, and different levels of details. In this paper, we present a novel method that registers panoramic images to a digital twin in a hierarchical way: the proposed method first performs rough registration through image retrieval using semantic segmentation, then localizes the image with a fine registration through minimizing semantic reprojection errors. Compared to existing methods, the developed method has the following contributions: 1) semantic-based image retrieval makes the rough registration process robust to lighting condition changes, texture differences, and temporary objects, 2) semantic-based image retrieval allows for bi-directional queries between a model and images, 3) reprojection-based fine registration further reduces the localization error due to the dimension reduction of features during image retrieval. Though the proposed method is developed for panoramic images, it can be generalized to monocular images at the cost of localization accuracy. The developed method was evaluated on a real-world academic building and a synthetic dataset, and the results showed that the proposed method can localize a panorama in less than a second and achieve sub-meter level localization error.}
}
@article{LI202261,
title = {Stochastic configuration networks for self-blast state recognition of glass insulators with adaptive depth and multi-scale representation},
journal = {Information Sciences},
volume = {604},
pages = {61-79},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522004133},
author = {Weitao Li and Qian Zhang and Dianhui Wang and Wei Sun and Qiyue Li},
keywords = {Self-blast state, Adaptive depth and multi-scale representation, Ensemble learning, Stochastic configuration networks, Feedback mechanism},
abstract = {The operating state of insulators is directly related to the stability of power transmission line. The existing methods for insulator state recognition cannot achieve satisfactory performance. In this paper, the self-blast state recognition of glass insulators is investigated by using an adaptive learning representation. To increase the adaptability of the network to different scales, we propose a solution based on multi-scale information throughout the entire process, beginning from a low-scale to high-scale subnetworks. The multi-scale information is aggregated in parallel way to take advantage of rich information representation. Then, an imitation of the human thinking pattern is employed. Utilizing entropy-based cost function, we update the parameters of the learner model in real-time. Based on the constraint of the evaluation index, adaptive depth representation for training glass insulators that are unsatisfied with the reliability evaluation is constructed to realize the self-optimizing regulation of feature space. Correspondingly, a stochastic configuration networks (SCNs) classifier is re-constructed to fit for the update multi-hierarchies knowledge space to carry out the re-recognition process. Finally, fuzzy integration is employed to ensemble multi-hierarchies network to improve the model’s generalization. The recognition results on aerial dataset of insulators images demonstrate the effectiveness of our proposed approach.}
}
@article{LV2022108956,
title = {Memory‐augmented neural networks based dynamic complex image segmentation in digital twins for self‐driving vehicle},
journal = {Pattern Recognition},
volume = {132},
pages = {108956},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108956},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004368},
author = {Zhihan Lv and Liang Qiao and Shuo Yang and Jinhua Li and Haibin Lv and Francesco Piccialli},
keywords = {Deep learning, Image segmentation, Memory-augmented neural networks, LSTM, Self-driving, Digital twins},
abstract = {With the continuous increase of the amount of information, people urgently need to identify the information in the image in more detail in order to obtain richer information from the image. This work explores the dynamic complex image segmentation of self-driving vehicle under Digital Twins (DTs) based on Memory-augmented Neural Networks (MANNs), so as to further improve the performance of self-driving in intelligent transportation. In view of the complexity of the environment and the dynamic changes of the scene in intelligent transportation, this work constructs a segmentation model for dynamic complex image of self-driving vehicle under DTs based on MANNs by optimizing the Deep Learning algorithm and further combining with the DTs technology, so as to recognize the information in the environment image during the self-driving. Finally, the performance of the constructed model is analyzed by experimenting with different image datasets (PASCALVOC 2012, NYUDv2, PASCAL CONTEXT, and real self-driving complex traffic image data). The results show that compared with other classical algorithms, the established MANN-based model has an accuracy of about 85.80%, the training time is shortened to 107.00 s, the test time is 0.70 s, and the speedup ratio is high. In addition, the average algorithm parameter of the given energy function α=0.06 reaches the maximum value. Therefore, it is found that the proposed model shows high accuracy and short training time, which can provide experimental reference for future image visual computing and intelligent information processing.}
}
@article{SOLOMON2022197,
title = {Predicting application usage based on latent contextual information},
journal = {Computer Communications},
volume = {192},
pages = {197-209},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002079},
author = {Adir Solomon and Bracha Shapira and Lior Rokach},
keywords = {Application predictions, User modeling, Deep learning},
abstract = {Predicting application usage is useful for offering personalized services, improving mobile energy consumption, and mobile system resource management optimization. Currently, however, there are many possible applications, and each user has his/her own preferences and usage patterns, which makes the application prediction task very challenging. In this study we use different representation methods to represent mobile users’ contextual information in order to predict application usage. We focus on the spatial information context (i.e., where the applications are used) and represent it with graph embeddings, which capture the locations users have visited based on their movement. We use multimodal embeddings to represent the temporal context, users’ identifiers, and previously used applications. Then, the contextual information’s latent representation is used in a deep learning framework composed of a GRU (gated recurrent unit), attention layer, and a softmax layer to provide application usage predictions. We evaluate our method on two real-world datasets comprised of data collected from mobile users’ devices. Our results show that the proposed application usage prediction method outperforms various machine learning models and state-of-the-art solutions. We also found that the spatial information’s latent representation derived from graph embeddings outperformed traditional and commonly used representation methods when predicting application usage. Our findings also reveal interesting usage patterns regarding users’ predictability, which can help us better understand users’ behavior.}
}
@article{ZHAO2022108741,
title = {Progressive privileged knowledge distillation for online action detection},
journal = {Pattern Recognition},
volume = {129},
pages = {108741},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108741},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002229},
author = {Peisen Zhao and Lingxi Xie and Jiajie Wang and Ya Zhang and Qi Tian},
keywords = {Online action detection, Knowledge distillation, Privileged information, Curriculum learning},
abstract = {Online Action Detection (OAD) in videos addresses the problem of real-time analysis for streaming videos, i.e., only the observed historical video frames are available at prediction time. Considering the future frames observable only at the training stage as a form of privileged information, this paper adopts the Learning Using Privileged Information (LUPI) paradigm. Knowledge distillation (KD) is employed to transfer the privileged information from the offline teacher to the online student. Note that this setting is different from conventional KD because the difference between the teacher and student models mostly lies in the input data rather than the network architecture. To relieves the input information gap for the LUPI, we propose a simple but effective Privileged Knowledge Distillation (PKD) method that enforce KD loss to partial hidden features of the student model. Moreover, we also schedules a curriculum learning procedure to gradually distill the privileged information. This approach is named as Progressive Privileged Knowledge Distillation (PPKD). Compared to some OAD methods that explicitly predict future frames or feature, our approach avoids predicting stage and achieves state-of-the-art accuracy on two popular OAD benchmarks, TVSeries and THUMOS14.}
}
@article{YAN2022105823,
title = {Digital twin-enabled dynamic scheduling with preventive maintenance using a double-layer Q-learning algorithm},
journal = {Computers & Operations Research},
volume = {144},
pages = {105823},
year = {2022},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2022.105823},
url = {https://www.sciencedirect.com/science/article/pii/S0305054822001046},
author = {Qi Yan and Hongfeng Wang and Fang Wu},
keywords = {Double-resource flexible job shop, Uncertain disturbances, Preventive maintenance, Digital twin, Reinforcement learning},
abstract = {Dynamic scheduling methods are essential and critical to manufacturing systems because of uncertain events in the production process, such as new job insertions, order cancellations, worker absences, and machine breakdowns. Emerging digital twin (DT) technology can help detect disturbances by continuously comparing physical space with virtual space and triggering a rescheduling policy immediately after a disturbance. This enables dynamic scheduling and greatly reduces the deviation between preschedules and actual schedules. This study focuses on a DT-enabled integrated optimisation problem of flexible job shop scheduling and flexible preventive maintenance (PM) considering both machine and worker resources. A double-layer Q-learning algorithm (DLQL) is designed as the underlying key optimisation method to simultaneously learn the selection process of machines and operations to achieve efficient real-time scheduling. The superior solution performance of DLQL was verified by comparing it with two well-known metaheuristic algorithms and a single-layer Q-learning algorithm under several benchmarks. Furthermore, different disturbance settings were designed to illustrate the DLQL-based dynamic scheduling process in detail. The proposed reinforcement learning (RL)-driven DT enables efficient collaborative scheduling between production and maintenance departments and helps manufacturing companies improve the real-time decision-making process under uncertain perturbations.}
}
@article{GURDURBROO2022104171,
title = {Design and implementation of a smart infrastructure digital twin},
journal = {Automation in Construction},
volume = {136},
pages = {104171},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104171},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522000449},
author = {Didem {Gürdür Broo} and Miguel Bravo-Haro and Jennifer Schooling},
keywords = {Data, Digital twins, Infrastructure, Smart infrastructure, Resilience},
abstract = {There is a critical need to make infrastructure systems more efficient, resilient, and sustainable. Infrastructure systems provide the basis for everyday life and enable the flow of goods, information, and services within urban and regional settings. Providing data-centric solutions to improve this flow is essential. This can only be achieved if we manage to transform passive infrastructure assets into cyber-physical systems. Digital twins bring the opportunity to turn passive infrastructure assets into data-centric systems of systems. This article aims to provide a summary of existing digital twin architectures and exemplify a digital twin design and implementation. To this end, a literature review of digital twin architecture is presented in addition to a case study of a digital twin implementation in smart infrastructure. The case study focuses on a digital twin implementation of a bridge and describes in detail the physical, cyber, integration, and service layers of this implementation. Later in the article, we discuss the learnings from this case study under three main categories – systems perspective, information perspective, and organisational perspective. The findings show the importance of acquiring a systems perspective when designing digital twins today to enable interoperable systems of systems in the future. Furthermore, the findings highlight the vital necessity of data and information management while also considering the multidisciplinary aspects of digital twin design and implementation.}
}
@article{ARUNACHALAM20225998,
title = {Deep learning and optimisation for quality of service modelling},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part B},
pages = {5998-6007},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822000295},
author = {Krishnakumar Arunachalam and Senthilkumaran Thangamuthu and Vijayanand Shanmugam and Mukesh Raju and Kamali Premraj},
keywords = {Optimisation, Deep learning, Machine learning, Time-series modelling},
abstract = {Machine learning is increasingly used to create digital twins for data collected from various underlying engineering processes. Such digital twins can be used in a wide variety of activities such as optimisation, forecasting of future data, etc. In this respect, forecasting the evolution of time-series data in the future time-steps is often encountered in various engineering systems and applications. In particular, probabilistic forecasting of time-series data over point-based predictions is often encouraged, but challenging to achieve though. In this work, deep learning (DR) technology is combined with various state-of-the-art mathematical optimisation algorithms in order to effectively achieve the ’confidence-based’ probabilistic predictions of Quality of Service (QoS) data emanating from various low-powered Internet of Things (IoT) devices. The results demonstrate that Deep Neural Networks (DNN), if combined with right mathematical optimisation algorithm, can help generating accurate probabilistic forecasts for both single time-series and a combination of multiple time-series data.}
}
@article{SLEITI20223704,
title = {Digital twin in energy industry: Proposed robust digital twin for power plant and other complex capital-intensive large engineering systems},
journal = {Energy Reports},
volume = {8},
pages = {3704-3726},
year = {2022},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.02.305},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722005522},
author = {Ahmad K. Sleiti and Jayanta S. Kapat and Ladislav Vesely},
keywords = {Digital twin, Energy savings, Power plant, Dynamic system model (DSM), Anomaly Detection and deep Learning (ADL), Sensor network, Energy cyber–physical systems.},
abstract = {The complex future power plants require digital twin (DT) architecture to achieve high reliability, availability and maintainability at lower cost. The available research on DT for power plants is limited and lacks details on DT comprehensiveness and robustness. The main focus of the present study is to propose a comprehensive and robust DT architecture for power plants that can also be used for other similar complex capital-intensive large engineering systems. First, overviews are conducted for DT key research and development for power plants and related energy savings applications to provide current status, guidelines and research gaps. Then, the requirements and rules for the power plant DT are established and the major DT components are determined. These components include the physics-based formulations; the statistical analysis of data from the sensor network; the real-time data; the pre-performed localized in-depth simulations to predict activities of the corresponding physical twin; and the system Genome with a digital thread that connects all these components together. Recommendations and future directions are made for the power plant DT development including the need for real data and physical description of the overall system focusing on each component individually and on the overall connections. Data-driven algorithms with capabilities to predict the system’s dynamic behavior still need to be developed. The data-driven approach alone is not sufficient and a low-order physics based model should operate in tandem with the updated latest system parameters to allow interpretation and enhancing the results from the data-driven process. Discrepancies between the dynamic system models (DSM) and anomaly detection and deep learning (ADL) require in-depth localized off-line simulations. Furthermore, this paper demonstrates the advantages of the developed ADL algorithm approach and DSM prediction of the DT using vector autoregressive model for anomaly detection in utility gas turbines with data from an operational power plant.}
}
@article{MATSUDA2022103667,
title = {Development of a prediction model tuning method with a dual-structured optimization framework for an entire heating, ventilation and air-conditioning system},
journal = {Sustainable Cities and Society},
volume = {79},
pages = {103667},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103667},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722000014},
author = {Yuki Matsuda and Ryozo Ooka},
keywords = {Artificial neural network, Digital twin, Machine learning, Data-driven modeling, Thermally activated building system, Building energy management},
abstract = {Heating, ventilation, and air-conditioning (HVAC) account for a large proportion of energy consumption. Improving the energy efficiency of HVAC and utilizing increased amounts of renewable energy are effective strategies for achieving decarbonization. While thermal storage is one of the key technologies that solves a renewable energy issue that has the temporal and geographical gaps between supply and demand, operation planning using optimization methods, such as model predictive control, is important owing to the complexity of the system. A prediction model with high accuracy and low computational load is required because the performance of model predictive control typically depends on it. Further, to facilitate its extensive use in common buildings, it is necessary to develop a simple modeling method that requires no expertise. This study proposes a framework based on a dual-structured optimization process as a modeling method to create a prediction model. This method was applied to an actual small-scale office building. It was confirmed that such a model which accurately predicts up to 24 h ahead within approximately 1 s can be created.11AHP: air source heat pump chiller; ANN: artificial neural network; DB: dry bulb; DNN: deep neural network; GTHP: geothermal heat pump; HVAC: heating, ventilation and air-conditioning; MAE: mean absolute error; MLR: multi linear regression; MPC: model predictive control; m-PSO: particle swarm optimization incorporating mutation method; PSO: particle swarm optimization; ReLU: rectified linear unit; RMSE: root mean square error; TABS: thermally activated building system; WB: wet bulb; a: weight for DNN; b: bias for DNN; c: coefficient for m-PSO, c1 = c2 = 1.49618; fDNN: function of deep neural network; mrate: mutation rate for m-PSO; n: number of items/elements; r: random value r∈[0,1]; t: time step; tH: prediction horizon; vi: velocity of ith element for m-PSO; w: inertial coefficient for m-PSO, w = 0.7298; x: input data for functions; xgb: global best positions for m-PSO; xpb: personal best positions for m-PSO; yi: observed value of ith element; y^i: predicted value of ith element; φ: activation function; U: uniform distribution value in the available range.}
}
@article{BALCO2022313,
title = {Virtual and Augmented Reality in Manufacturing Companies in Slovakia},
journal = {Procedia Computer Science},
volume = {201},
pages = {313-320},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004550},
author = {Peter Balco and Peter Bajzík and Klára Škovierová},
keywords = {Augmented reality, Virtual reality, Manufacturing},
abstract = {Virtual and augmented reality technologies are gaining a stable presence in many industries, while supporting the efficiency and productivity of individual business processes. Education, virtual prototyping, production system design, remote maintenance, digital twin scenario simulations, digital tutorials are areas in which these technologies have found their place and at the same time record visible development. These are solutions that shorten product development time, reduce prototyping costs, streamline training and assembly processes, and can be used as a decision support tool through simulations of various production scenarios. Slovakia, as a pre-developed country, has the ambition to implement modern technologies in industry and education. Reflecting on this strategy, the aim of our paper was to quantify and analyze the level of knowledge and interest in these technologies in Slovak manufacturing companies.}
}
@article{MELESSE202213,
title = {Machine Learning-Based Digital Twin for Monitoring Fruit Quality Evolution},
journal = {Procedia Computer Science},
volume = {200},
pages = {13-20},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.200},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002095},
author = {Tsega Y. Melesse and Matteo Bollo and Valentina Di Pasquale and Francesco Centro and Stefano Riemma},
keywords = {digital twin, thermal images, machine learning, image classification, food waste},
abstract = {A technological gap to monitor fruit quality evolution in the food supply chain is causing a huge waste of fruits. A digital twin is a promising tool to minimize fruit waste by monitoring and predicting the status of fresh produce throughout its life. In post-harvest engineering, the digital twin could be defined as a virtual representation of real produce. The objective of this work is to present a new approach to create a machine learning-based digital twin of banana fruit to monitor its quality changes throughout storage. The thermal camera has been used as a data acquisition tool due to its capability to detect the surface and physiological changes of fruits throughout the storage. In this study, after constructing the dataset of thermal data belonging to four classes, the training of the model has been performed using intelligent technologies from SAP. The solution has applied a deep convolutional neural network to monitor the fruit status based on the thermal information, and the training process has shown higher accuracy. Thus, 99% of prediction accuracy has been achieved which is proved to be a promising technique for the development of fruit digital twins. The application of thermal imaging techniques can be used as a data source to create a machine learning-based digital twin of fruit that can minimize waste in the food supply chain.}
}
@article{LI2022390,
title = {Knowledge Distillation for Energy Consumption Prediction in Additive Manufacturing},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {390-395},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.225},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322002269},
author = {Yixin Li and Fu Hu and Michael Ryan and Ray Wang and Ying Liu},
keywords = {Additive manufacturing, Knowledge distillation, Energy consumption, Machine learning},
abstract = {Owing to the advances of data sensing and collecting technologies, more production data of additive manufacturing (AM) systems is available and advanced data analytics techniques are increasingly employed for improving energy management. Current supervised learning-based analytical methods, however, typically require extracting and learning valuable information from a significant amount of data during training. It is difficult to make a trade-off between latency and computing resources to implement the analytical models. As such, this paper developed a method utilizing the knowledge distillation (KD) technique for predicting AM energy consumption based on product geometry information to reduce computational burdens while simultaneously retaining model performance. Through a teacher-student architecture, layer-by-layer images of products and energy consumption datasets are used to train a teacher model from which the knowledge is extracted and used to build a student model to predict energy consumption. A case study was conducted to demonstrate the feasibility and effectiveness of the proposed approach using real-world data from a selective laser sintering (SLS) system. Comparisons between distilled and independently trained student models were made in terms of the root mean square error (RMSE) and training time. The distilled student model performed better (14.3947KWh/kg) and required a shorter training time (34s) than the complex teacher model.}
}
@article{NOVIKOV20222330,
title = {Integrated Resource Management in the Digital Ecosystem of the Enterprise Based on Intelligent Consorts},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2330-2335},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.056},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322020651},
author = {Dmitry Novikov and Natalya Bakhtadze and Denis Elpashev and Alexander Suleykin},
keywords = {Digital consorts, digital ecosystem (DES), proactive management, integrated resource management systems, digital twin, machine learning, search for association rules},
abstract = {An approach to the development of a digital ecosystem (DES) which comprises interacting service ssubsystems (consorts) is presented. Methods for analyzing and prediction of the production situations using machine learning algorithms and inductive knowledge bases are proposed. The possibility of using these methods for predicting the state of the digital infrastructure resources of industrial enterprises in real time is demonstrated. The paper shows that the methods of proactive decision support for situational control can help production companies to prevent abnormal situations. Case studies are included to illustrate the creation of association rules in the state prediction problem for an enterprise resource complex.}
}
@article{ULMER2022675,
title = {Usage of digital twins for gamification applications in manufacturing},
journal = {Procedia CIRP},
volume = {107},
pages = {675-680},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.044},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003286},
author = {Jessica Ulmer and Sebastian Braun and Chi-Tsun Cheng and Steve Dowey and Jörg Wollert},
keywords = {Gamification, Digital Twin, Support System},
abstract = {Gamification applications are on the rise in the manufacturing sector to customize working scenarios, offer user-specific feedback, and provide personalized learning offerings. Commonly, different sensors are integrated into work environments to track workers’ actions. Game elements are selected according to the work task and users’ preferences. However, implementing gamified workplaces remains challenging as different data sources must be established, evaluated, and connected. Developers often require information from several areas of the companies to offer meaningful gamification strategies for their employees. Moreover, work environments and the associated support systems are usually not flexible enough to adapt to personal needs. Digital twins are one primary possibility to create a uniform data approach that can provide semantic information to gamification applications. Frequently, several digital twins have to interact with each other to provide information about the workplace, the manufacturing process, and the knowledge of the employees. This research aims to create an overview of existing digital twin approaches for digital support systems and presents a concept to use digital twins for gamified support and training systems. The concept is based upon the Reference Architecture Industry 4.0 (RAMI 4.0) and includes information about the whole life cycle of the assets. It is applied to an existing gamified training system and evaluated in the Industry 4.0 model factory by an example of a handle mounting.}
}
@article{LOGHIN2022331,
title = {3D FEA based surrogate modeling in fatigue crack growth life assessment},
journal = {Procedia Structural Integrity},
volume = {38},
pages = {331-341},
year = {2022},
note = {Fatigue Design 2021, International Conference Proceedings, 9th Edition},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2022.03.034},
url = {https://www.sciencedirect.com/science/article/pii/S2452321622002475},
author = {Adrian Loghin and Shakhrukh Ismonov},
keywords = {Fatigue crack growth simulation, uncertainty quantification, response surface modeling, remaining useful life, 3D finite element modeling, probabilistic structural life assessment},
abstract = {Runtime efficient models designed for damage tolerant life assessment are desired in Structural Health Management and Digital Twin development. While FEM is commonly used in the industry to assess health of a nominal structure design while in service, in probabilistic assessments, reduced order models are preferred due to lower runtime compared to the deterministic models but at the cost of solution accuracy. Readily available machine learning algorithms coupled with deterministic 3D simulations for modeling fatigue crack growth provide a feasible path to reach a better runtime-accuracy compromise. In this study, a fatigue crack growth testing procedure along with measurement data are used for validation purposes and for laying out details of the modeling process. Accuracy and solution runtime of the 3D FEA based surrogate models are assessed to demonstrate the efficiency of the method.}
}
@article{RASOVSKA20221938,
title = {Learning factory FleXtory: Interactive loops between real and virtual factory through digital twin},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1938-1943},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.682},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322020006},
author = {I. Rasovska and I. Deniaud and F. Marmier and J.-L. Michalak},
keywords = {Industry 4.0, Learning factory architecture, Virtual factory, Digital twin},
abstract = {The digitalization increase in industrial processes is perceived as an opportunity to grow up the competitiveness of companies. Data is more and more accessible, potentially allowing making better decisions at all levels of the company. Job profiles are then changing and requiring new skills, more focused on new technologies and information systems. The most effective way to acquire necessary skills is a “learning by doing” way in industrial projects and processes. The learning factory FleXtory was designed and produced in this objective and at the crossroad of the academic and the industrial environment. It allows running combinations of theoretical and applied tools in the context of industry 4.0. This is based on interactive loops between the real and the virtual world passing through the digital twin of the learning factory. The pedagogical modules developed within this learning factory address the evolution of professional competencies and skills in according to the transition to industry 4.0. This transformation supposes the development of the ability for professionals to work within a digital environment. In this paper we propose an architecture model of FleXtory favoring the return on experience/information loop within the digital transformation of the learning factory. The originality of our work is to consider this architecture from the point of view of the pedagogical specifications of the proposed learning models and the future perspective of their use.}
}
@article{MASCHLER2022220,
title = {Transfer Learning as an Enhancement for Reconfiguration Management of Cyber-Physical Production Systems},
journal = {Procedia CIRP},
volume = {112},
pages = {220-225},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.095},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122012604},
author = {Benjamin Maschler and Timo Müller and Andreas Löcklin and Michael Weyrich},
keywords = {Case Study, Cyber-physical Production System, Deep Learning, Intelligent Digital Twin, Industrial Transfer Learning, Lifecycle Management, Qualitative Analysis, Real2Sim, Reconfiguration Management, Sim2Sim, Sim2Real, Use Cases},
abstract = {Reconfiguration demand is increasing due to frequent requirement changes for manufacturing systems. Recent approaches aim at investigating feasible configuration alternatives from which they select the optimal one. This relies on processes whose behavior is not reliant on e.g. the production sequence. However, when machine learning is used, components’ behavior depends on the process’ specifics, requiring additional concepts to successfully conduct reconfiguration management. Therefore, we propose the enhancement of the comprehensive reconfiguration management with transfer learning. This provides the ability to assess the machine learning dependent behavior of the different CPPS configurations with reduced effort and further assists the recommissioning of the chosen one. A real cyber-physical production system from the discrete manufacturing domain is utilized to demonstrate the aforementioned proposal.}
}
@incollection{BADRA2022125,
title = {Chapter 6 - A machine learning-genetic algorithm approach for rapid optimization of internal combustion engines},
editor = {Jihad Badra and Pinaki Pal and Yuanjiang Pei and Sibendu Som},
booktitle = {Artificial Intelligence and Data Driven Optimization of Internal Combustion Engines},
publisher = {Elsevier},
pages = {125-158},
year = {2022},
isbn = {978-0-323-88457-0},
doi = {https://doi.org/10.1016/B978-0-323-88457-0.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884570000035},
author = {Jihad Badra and Opeoluwa Owoyele and Pinaki Pal and Sibendu Som},
keywords = {Automated machine learning, Genetic algorithm, Hyperparameter optimization, Internal combustion engines, Machine learning},
abstract = {The optimization of internal combustion (IC) engine is a highly complex problem because of the high-dimensionality and nonlinear interactions among design parameters. Machine learning (ML) offers a pathway to transform complex physical processes that occur in a combustion engine into compact informational processes. A careful definition of the objective (merit) function for optimization is a critical step. Training data for these ML algorithms must be cleverly prepared to improve the prediction efficiency. Global optimum search optimization methods must be adopted to avoid local optimum designs with reduced merit value. Postprocessing of the optimization outputs is also needed to evaluate the recommended design by employing sensitivity and robustness analysis. In this chapter, a machine learning-genetic algorithm (ML-GA) framework is discussed to optimize the performance of IC engines. ML-GA is comprised of a robust Super Learner approach to build the surrogate model (based on simulation or experimental data), wherein multiple ML algorithms are pooled together instead of a single learner. This Super Learner surrogate is then used to replace expensive simulations or experiments during the course of a GA optimization to rapidly arrive at the optimal design parameters at significantly lower cost. The efficiency and robustness of ML-GA is further enhanced by way of automated hyperparameter selection coupled with an active learning approach. In particular, a Bayesian approach is employed to optimize the hyperparameters of the base learners that make up a Super Learner model to obtain better test performance. In addition to performing hyperparameter optimization (HPO), an active learning approach is leveraged, where the process of data generation, ML training, and surrogate optimization, is performed repeatedly to refine the solution in the vicinity of the predicted optimum. The ML-GA algorithm has been demonstrated for simulation-driven design optimization of advanced IC engines with significant savings in design time and cost compared to traditional optimization methods, as highlighted in this chapter.}
}
@article{UMEDA20221,
title = {Digital Triplet and its Implementation on Learning Factory},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {1-6},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.160},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001604},
author = {Yasushi Umeda and Yuki Hongo and Jumpei Goto and Shinsuke Kondoh},
keywords = {Digital Twin, Digital Triplet, Cyber Physical Production System, Learning Factory, Kaizen},
abstract = {To support engineers in cyber-physical production system (CPPS), we propose the ‘digital triplet’ (D3) concept consisting of physical, cyber, and intelligent activity worlds. This study presents a prototype system of our D3 type learning factory for embodying the D3 concept and verifying its potential. Specifically, we propose a method for structurally describing engineering processes by mapping intention and judgment of engineers to operations on CPPS. We then performed an experiment of performance improvement by engineers on the prototype system. The experiment verified that the D3 concept is successfully realized and that the proposed method is effective at representing the engineering processes of engineers.}
}
@article{DETTORI2022301,
title = {Optimizing integrated steelworks process off-gas distribution through Economic Hybrid Model Predictive Control and Echo State Networks},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {40},
pages = {301-306},
year = {2022},
note = {1st IFAC Workshop on Control of Complex Systems COSY 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.01.089},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323000964},
author = {S. Dettori and I. Matino and V. Colla and A. Wolff and M. Neuer and V. Baric and D. Schroeder and V. Utkin and F. Schaub},
keywords = {Economic Hybrid Model Predictive Control, Artificial Intelligence, Machine Learning, Process off-gas distribution, Integrated Steelworks, Reservoir computing},
abstract = {Steel production in integrated steelworks involves the simultaneous production of various byproducts, including process off-gases that are usually exploited for generating electricity in the internal power plant, heat and steam. Their discontinuous production is managed through complex network, gasholders and torches, which must be managed with stringent operational constraints. In this paper we present a supervision and control system designed to optimize the economic management of the distribution of process off-gases that also allows minimizing the environmental impact. The system implements a digital twin based mainly on machine learning techniques, including Echo State Networks, and a hierarchical optimization system, which first level is based on an economic model predictive approach and the second level is based on the economic hybrid model predictive control. This system allows to effectively maximize the use of off-gases while minimizing the environmental impact of their use up to 97%.}
}
@article{QUINN2022631,
title = {A Learning Factory Framework: Challenges and Solutions for an Irish University*},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {631-636},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.471},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322017645},
author = {William Quinn and Victor Cionca and Kritchai Witheephanich and Cemalettin Ozturk},
keywords = {Learning Factory, Energy Efficient Manufacturing, Distributed Wireless Networked Control Systems, Digital Twin},
abstract = {After its first decade since initialisation, Industry 4.0 is now becoming a standard for manufacturing industry. This new standard necessitates a new era of skilled workforce from all levels with literacy of digital technologies and manufacturing automation. Learning factories concept brings new opportunities to research institutes for developing a test bed for Industry 4.0 standards that can be used for both developing necessary skills and demonstration for companies moving forward to their digitisation journey. In this paper, we outline challenges and our roadmap to address for a successful implementation of a learning factory with a case of an Irish University.}
}
@article{CHRISTOU2022103591,
title = {End-to-end industrial IoT platform for Quality 4.0 applications},
journal = {Computers in Industry},
volume = {137},
pages = {103591},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103591},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001986},
author = {Ioannis T. Christou and Nikos Kefalakis and John K. Soldatos and Angela-Maria Despotopoulou},
keywords = {Predictive maintenance, Zero-defect manufacturing, Quality management, Artificial intelligence, Predictive control, Big data, Industrial IoT, Configurable analytics},
abstract = {Predictive maintenance, quality management, and zero-defect manufacturing are among the most prominent smart manufacturing use cases in the Industry4.0 era. Nevertheless, the development of such systems is still challenging because of the need to integrate multiple fragmented data sources, to apply advanced machine learning techniques for multi-objective optimizations, and to implement configurable digital twins that can flexibly adapt to changing industrial configurations. This paper presents the architecture, design, practical implementation, and evaluation of an end-to-end platform that addresses these challenges. The platform provides the means for collecting, managing, and routing data streams from heterogeneous cyber physical production systems, in configurable and interoperable ways. Moreover, it supports advanced data analytics by means of a novel machine learning framework that leverages quantitative rule mining. The presented platform has been successfully deployed in various industrial settings and has been positively evaluated in terms of its ability to accelerate application development, reduce unscheduled downtimes, provide increased Overall Equipment Efficiency (OEE), compute production process parameter configurations that lower the percentage of product defects, and predict product defects before they occur.}
}
@article{HOOSHYAR2022116670,
title = {GameDKT: Deep knowledge tracing in educational games},
journal = {Expert Systems with Applications},
volume = {196},
pages = {116670},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116670},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422001555},
author = {Danial Hooshyar and Yueh-Min Huang and Yeongwook Yang},
keywords = {Learner model, Deep knowledge tracing, Educational game, Prediction of player performance, Deep learning},
abstract = {Despite the multiple deep knowledge tracing (DKT) methods developed for intelligent tutoring systems and online learning environments, there exists only a few applications of such methods in educational computer games. One key challenge is that a player may deploy several interweaved and overlapped skills during gameplay, making the assessment task nontrivial. In this research, we present a generalizable DKT approach called GameDKT that integrates state-of-the-art machine learning with domain knowledge to model the learners’ knowledge state during gameplay, in an attempt to monitor and trace their proficiency level for the different skills required for educational games. Our findings reveal that GameDKT approach could successfully predict the performance of players in the coming game task using the cross-validated CNN model with accuracy and AUC of roughly 85% and 0.913, respectively, thus outperforming the MLP baseline model by up to 14%. When the performance of players is forecasted for up to four game tasks in advance, results show that the CNN model can achieve more than 70% accuracy. Interestingly, this model seems to be better and faster at identifying local patterns and it could achieve a higher performance compared to RNN and LSTM in both one-step and multi-step prediction of learners’ performance in game tasks.}
}
@article{SOHR2022371,
title = {Decision Modeling for an ISA-95 based Production Ontology},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {371-376},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.421},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322017074},
author = {Annelie Sohr and Franz Georg Listl and Katharina Ecker and Jan Fischer and Jan Christoph Wehrstedt and Michael Weyrich},
keywords = {Decision Support, Digital Twin, Simulation, Shop Floor Management, Production Planning, Scheduling, Domain Model, Semantic Model},
abstract = {Decision support systems in production and manufacturing help to achieve economic and ecological objectives and increase flexibility and resilience. Semantic models as knowledge base for these systems enable and empower analytical components such as machine learning, simulation, and optimization algorithms. This paper discusses some of the relevant aspects of such semantic models. Existing solutions and standards for common data models in manufacturing are explored, and based on that, alternatives and decisions for managing manufacturing operations are modeled in an ontology. Specifically, a general ontology for handling decisions in any type of factory is introduced.}
}
@article{PABOLU20221887,
title = {A digital-twin based worker's work allocation framework for a collaborative assembly system},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1887-1892},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.674},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322019929},
author = {Venkata Krishna Rao Pabolu and Divya Shrivastava and Makarand S. Kulkarni},
keywords = {Assembly line digital-twin, Collaborative assembly system, Human-Robot Collaboration, ALWABP work allocation},
abstract = {Assigning the right task to the right worker remains a challenge to an assembly line manager. This work is proposed to use digital-twin technologies for Assembly line worker assignment and balancing problems (ALWABP) for a collaborative assembly system. Applications of the Internet of Things, Data analytics and Learning algorithms are proposed to estimate the worker's work skill and task performance. Assembly line Digital Twin and collaborative robot (CoBot) work simulation tools are proposed to estimate the CoBot's assembly task performing ability and execution time. ALWABP framework is proposed to optimize the work allocation between the worker and the CoBot. Assembly line digital twin is furthermore proposed to simulate the ALWABP output during the decision-making process of the assembly line manager by considering the Human-in-the-loop decision-making system.}
}
@article{THURER2022710,
title = {Digital Twin Architecture for Production Logistics: The Critical Role of Programmable Logic Controllers (PLCs)},
journal = {Procedia Computer Science},
volume = {200},
pages = {710-717},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.269},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002782},
author = {Matthias Thürer and Shan Shan Li and Ting Qu},
keywords = {Digital Twin, Programmable Logic Controller (PLC), Digital Encapsulated Information, Action Design Research, Smart Manufacturing},
abstract = {Digital Twin is a key enabling technology of Industry 4.0, Smart Manufacturing and Made in China 2025, and a broad literature emerged. But existing literature tends to focus on large-scale equipment or large products, often in a fixed position layout. This study argues that this is due to the use of centralized data and system models. It proposes new architectures for digital twins based on local product and resource twins that use digital encapsulated information to create higher level system twins. Using Action Design Research, a first tentative to develop a prototype of this new architecture is presented. A main learning outcome is that the Programmable Logic Controller (PLC) is an essential part of a digital twin implementation, which receives insufficient research attention. The capability of the PLC largely determines the architecture of a digital twin.}
}
@article{MIHAI20221082,
title = {Multimodal emotion detection from multiple data streams for improved decision making},
journal = {Procedia Computer Science},
volume = {214},
pages = {1082-1089},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.281},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019937},
author = {Neghina Mihai and Matei Alexandru and Zamfirescu Bala-Constantin},
keywords = {emotion detection, sensor fusion, multimodal, affect},
abstract = {Recent neurological studies shows that emotions are tightly connected to the thinking and cognitive actions, being part of the decision-making process. Considering this, having a way to help decision making processes based on current emotion of the user or to consider the potential emotional impact if a decision is made, would be beneficial. This paper introduces a novel method for fusing multiple emotional signals, using a weighted average, where each weight value adapts to real time conditions, based on signal type, presence, and quality. In the context of a training station for manual operation, we implemented and tested separately several emotion detection methods, each based on a different signal acquired from audio, video, and galvanic skin response data streams. The final goal is to include the proposed method together with state of the art emotion detection machine learning algorithms as part of the digital twin training station for manual operation.}
}
@article{TADA20221927,
title = {Architecture PBL for designing and constructing on-campus facilities by technical college students using digital twins},
journal = {Procedia Computer Science},
volume = {207},
pages = {1927-1932},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.251},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922011371},
author = {Dr. Yutaka TADA and Dr. Takashi MATSUMOTO and Dr. Nariyuki KAWABATA},
keywords = {Building Information Modeling (BIM), design, estimate, construction},
abstract = {In this study, we examined the role and issues of digital twins in architectural Problem Based Learning (PBL) for students, including not only design but also estimate and construction, based on actual cases. As an example, we examined three architectural PBLs-interior renovation and outdoor furniture installation-implemented on the campus of National Institute of Technology, Anan College from the 2020 to 2022 fiscal year. As a result, it was found that the expression of the architectural space, the safety of the construction, and the accuracy of the estimation were improved. An issue we found was that students could not get a realistic sense of scale only on Building Information Modeling (BIM), and it was important to actually perform construction with PBL.}
}
@article{ALLAMAA2022385,
title = {Sim2real for Autonomous Vehicle Control using Executable Digital Twin},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {24},
pages = {385-391},
year = {2022},
note = {10th IFAC Symposium on Advances in Automotive Control AAC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.314},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322023461},
author = {Jean Pierre Allamaa and Panagiotis Patrinos and Herman {Van der Auweraer} and Tong Duy Son},
keywords = {Sim2Real, ADAS, model predictive control, domain randomization},
abstract = {In this work, we propose a sim2real method to transfer and adapt a nonlinear model predictive controller (NMPC) from simulation to the real target system based on executable digital twin (xDT). The xDT model is a high fidelity vehicle dynamics simulator, executable online in the control parameter randomization and learning process. The parameters are adapted to gradually improve control performance and deal with changing real-world environment. In particular, the performance metric is not required to be differentiable nor analytical with respect to the control parameters and system dynamics are not necessary linearized. Eventually, the proposed sim2real framework leverages altogether online high fidelity simulator, data-driven estimations, and simulation based optimization to transfer and adapt efficiently a controller developed in simulation environment to the real platform. Our experiment demonstrates that a high control performance is achieved without tedious time and labor consuming tuning.}
}
@incollection{CASINI2022583,
title = {Chapter 11 - Advanced facility management},
editor = {Marco Casini},
booktitle = {Construction 4.0},
publisher = {Woodhead Publishing},
pages = {583-605},
year = {2022},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-12-821797-9},
doi = {https://doi.org/10.1016/B978-0-12-821797-9.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217979000039},
author = {Marco Casini},
keywords = {Digital Twin, predictive maintenance, building facility management, computer-aided facility management (CAFM), BIM-enhanced facility management (FM-enabled BIM), predictive building maintenance, extended reality, VR in maintenance, AR in maintenance, MR in maintenance, digital building twin},
abstract = {The chapter illustrates the most advanced building facility management tools and methods, focusing on facility operation and maintenance activities. After an overview on main building maintenance policies and strategies, the chapter describes computer-aided facilities management software for planning, managing, reporting, and tracking maintenance operations with particular reference to their integration with BIM (FM-enabled BIM), illustrating applications and advantages. The use of Internet of Things and machine learning is investigated, illustrating the new concept of building predictive maintenance for early fault detection, fault identification, health assessment of equipment, as well as to predict accurately the future state of systems. Applications of extended reality (virtual reality, augmented reality, and mixed reality) technology in building and systems maintenance are described, showing advantages in supporting technical maintenance tasks, as well as improving the management of maintenance operations and supporting strategic decision-making. Finally, the new Digital twin technology, which promises to revolutionize the way buildings are managed and maintained, is thoroughly investigated.}
}
@article{CARAMIHAI20221152,
title = {Decision Support Collaborative Platform for e-Health Integration in Smart Communities Context},
journal = {Procedia Computer Science},
volume = {214},
pages = {1152-1159},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.290},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020026},
author = {Simona Iuliana Caramihai and Ioan Dumitrache and Mihnea Alexandru Moisescu and Ioan Stefan Sacala},
keywords = {Digital Medical Twins, Healthcare systems, Neuromorphic System},
abstract = {Data gathering related to biological parameters evolution using sensor networks associated to the patient, correlated with the analysis and data processing, together with a priori information and knowledge related with the patient, will allow developing new digital models for the characterization of healthcare state. By the help of algorithms for automated learning or neuromorphic devices, one can build a library for patients’ Digital Twins based on its corresponding health state. In this papers authors present the main components of an integrated informatic platform which assure an efficient access for patients to various medical health services. Also, the paper presents a model for each patient health state using specific artificial intelligence techniques as a support system for every family doctor's decision and medical experts.}
}
@article{SALAZAR2022466,
title = {Neuro-Fuzzy Digital Twin of a High Temperature Generator},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {9},
pages = {466-471},
year = {2022},
note = {11th IFAC Symposium on Control of Power and Energy Systems CPES 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.081},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322004669},
author = {William Chicaiza Salazar and Diogo Ortiz Machado and Antonio Javier Gallego Len and Juan Manuel Escaño Gonzalez and Carlos Bordons Alba and Gustavo Artur {de Andrade} and Julio Elias Normey-Rico},
keywords = {Solar Energy, Fresnel Solar Collector, ANFIS, High Pressure Generator, Absorption Chiller, Lithium-Bromide},
abstract = {Solar absorption plants are renewable energy systems with a special advantage: the cooling demand follows the solar energy source. The problem is that this plant presents solar intermittency, phenomenological complexity, and nonlinearities. That results in a challenge for control and energy management. In this context, this paper develops a Digital Twin of an absorption chiller High Temperature Generator (HTG) seeking accuracy and low computational efort for control and management purposes. A neuro-fuzzy technique is applied to describe HTG, internal Lithium-Bromide temperature, and water outlet temperature. Two Adaptative Neuro-Fuzzy Inference Systems (ANFIS) are trained considering real data of eight days of operation. Then, the obtained model is validated considering two days of real data. The validation shows a RMSE of 1.65e−2 for the internal normalized temperature, and 2.05e−2 for the outlet normalized temperature. Therefore, the obtained Digital Twin presents a good performance capturing the dynamics of the HTG with adaptive capabilities considering that each day can update the learning step.}
}
@article{LUKAC2022351,
title = {Using Digital Twins for the Optimization of Control Cabinet Engineering},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {4},
pages = {351-355},
year = {2022},
note = {17th IFAC Conference on Programmable Devices and Embedded Systems PDES 2022 — Sarajevo, Bosnia and Herzegovina, 17-19 May 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.06.058},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322003731},
author = {Dr. Dusko Lukac},
keywords = {EPLAN, Education, CCEC, Digital Twin, ProPanel, Electric P8},
abstract = {The digital twin adds a great value to the interlinking of work steps in control cabinet construction. The model is created in electrical design and during assembly planning. The twin then accompanies the entire production process and provides the individual departments with their information. The unique feature: customer, switchgear manufacturers, and their suppliers work with the same model. The device manufacturers supply macros in which all the properties of their products are mapped. The electrical designers insert these devices into their schematic and thus generate a logical version of the model. The layout planner then puts the model into physical form by arranging the devices in the virtual switchgear cabinet. The finished model includes all dimensions and shows the individual devices inside the realistic 3D digital layout. Also included is the entire wiring with all wire paths, cable ducts, top-hat rails (position and length), and all drill holes and cutouts. The haptic representation simplifies many activities, as individual positions in the cabinet are more easily recognized. As a result, production is less time-consuming, and the susceptibility to errors is significantly reduced. Such a solution is presented using the EPLAN platform and the products Data Portal, Electric P8, Pro Panel, including the connection of the software to the actual machines, which have the task of creating a real construction out of the simulation of a digital twin.}
}
@article{BADAKHSHAN20221980,
title = {Using digital twins for inventory and cash management in supply chains},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1980-1985},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.689},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322020079},
author = {Ehsan Badakhshan and Peter Ball and Ali Badakhshan},
keywords = {Digital twins, Machine learning, Cash flow management, Simulation, Inventory management},
abstract = {In this paper, we investigate the potential of a supply chain (SC) digital twin framework to help decision makers in managing inventory and cash flows throughout the SCs. The proposed SC digital twin framework integrates machine learning (ML) and simulation to identify the inventory replenishment policies that minimize the cash conversion cycle of an SC, currently absent from the literature. The results show that the upstream member of an SC plays a pivotal role in mitigating the bullwhip effect and consequently minimizing the cash conversion cycle of the SC.}
}
@article{CHABANET2022378,
title = {Toward a self-adaptive digital twin based Active learning method: an application to the lumber industry},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {378-383},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.223},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322002245},
author = {Sylvain Chabanet and Hind Bril El-Haouzi and Philippe Thomas},
keywords = {Digital Twins, Active Learning, Artificial Intelligence, Sawmill Industry, Industry 4.0},
abstract = {Digital Twins (DT) is an extremely promising framework developed in the context of Industry 4.0 to facilitate the convergence of the physical and digital spaces. Numerous challenges remain, however, in terms of development, deployment, and self-adaptability of the DT faced with changes from its physical twin. Concerning this last point in particular, the set of Machine Learning (ML) methods known as Active Learning appears promising. This framework allows the DT to play an active role in the selection of the data samples used to train supervised ML models. This paper proposes a use-case inspired from the sawmill industry to illustrate the interest of these method in the presence of various changes in the flow of data gathered by the DT.}
}
@article{MULLER2022126,
title = {Self-improving Models for the Intelligent Digital Twin: Towards Closing the Reality-to-Simulation Gap},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {126-131},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.181},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001823},
author = {Manuel S. Müller and Nasser Jazdi and Michael Weyrich},
keywords = {, , , , , , , },
abstract = {This paper presents a novel approach to ensure the quality of the Digital Twin models that modern Cyber-Physical Manufacturing Systems (CPMS) rely on. CPMS are configurable and intelligent. Environmental and system parameters change frequently. Thus, static models are inadequate. Autonomous mobile robots and the simulation of their movement are important elements of these CPMS. Based on our reinforcement learning-based methodology, we use these robots as an example to show how the Digital Twin automatically improves models that do not perfectly represent the physical asset, making it an intelligent Digital Twin. In our scenario, the behavior of the asset deviates from the simulated prediction, i.e., a simulation gap occurs. The presented approach closes this simulation gap through a three-step mechanism. First, it makes the simulated data and the real data comparable and synchronizes it. Second, it applies reinforcement learning to find patterns in the deviations between the simulated and real data. Third, it learns to compensate for them. The evaluation of this example shows promising results.}
}
@article{CECH202267,
title = {Digital Twins and HIL Simulators in Control Education – Industrial Perspective},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {17},
pages = {67-72},
year = {2022},
note = {13th IFAC Symposium on Advances in Control Education ACE 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.226},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014513},
author = {M. Čech and M. Vosáhlo},
keywords = {Digital twin, XIL standard, HIL simulator, control education, feedback control, vibration damping, overhead crane, STEM, 4th generation university},
abstract = {Novel concepts like digital twins and X-in-the-Loop (XIL) simulations are being adopted in many new application areas by all entities in supply chains, including enterprises of all sizes as well as research institutes. However, they are not sufficiently addressed during control education in standard bachelor and master courses. The main cause is the high price of required equipment and SW toolchains. In addition, clear vision and common understanding of the role of digital twins in individual XIL stages should be created. In this paper, cost effective tools are presented and their utilization is demonstrated with a simple gantry crane model with special focus on load swinging attenuation. The authors believe that the presented tools and ideas would bring wider competences to the students and thus bridge the gap between industrial needs and academic practice and shorten the way towards 4th generation universities.}
}
@article{SPINTI2022118436,
title = {Atikokan Digital Twin: Machine learning in a biomass energy system},
journal = {Applied Energy},
volume = {310},
pages = {118436},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.118436},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921016627},
author = {Jennifer P. Spinti and Philip J. Smith and Sean T. Smith},
keywords = {Bayesian machine learning, Digital twin, Uncertainty quantification, Science-based models, Biomass boiler},
abstract = {The Atikokan Generating Station, operated by Ontario Power Generation, has a 200 MW, biomass-fired tower boiler that operates on a dispatch schedule with a five-minute cycle. The boiler is generally operated in the range of 40–100 MW using two of five burner levels. In order to optimize boiler performance, we propose the implementation of a unique digital twin. Our digital twin abstraction couples Bayesian inference from science-based models and from observations (machine learning) with decision theory to predict operating-variable set points that optimize the physical asset (the boiler) in the presence of uncertainty (artificial intelligence). We focus this paper on the continuous Bayesian machine learning part of the Atikokan Digital Twin; we discuss decision theory in a companion paper. We identify and learn about 12 operational, model, and measured-output parameters and their uncertainties from high-fidelity, science-based simulations of the Atikokan boiler and from the observed measurements at the power plant. Since the goal of the Atikokan Digital Twin is to implement it online in real time, we require fast function evaluations for the quantities of interest extracted from the simulations in the Bayesian analysis. We use Gaussian process regression/interpolation to create accurate, robust surrogate models. We define the Bayesian priors and likelihood function and solve for the posterior distributions of the 12 parameters. We then propagate these distributions (i.e., parameters with uncertainty) into the predicted distributions of 790 quantities of interest to learn about the relative importance of various sources of error including experimental, model, and operating-parameter errors.}
}
@article{LIU2022567,
title = {Ability boosted knowledge tracing},
journal = {Information Sciences},
volume = {596},
pages = {567-587},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522001876},
author = {Sannyuya Liu and Jianwei Yu and Qing Li and Ruxia Liang and Yunhan Zhang and Xiaoxuan Shen and Jianwen Sun},
keywords = {Knowledge tracing, Learner modeling, Artificial intelligence in education, Ensemble learning, Matrix factorization, Graph neural networks},
abstract = {Knowledge tracing (KT) has become an increasingly relevant problem in intelligent education services, which estimates and traces the degree of learner’s mastery of concepts based on students’ responses to learning resources. The existing mainstream KT models, only attribute learners’ feedback to the degree of knowledge mastery and leave the influence of mental ability factors out of consideration. Although ability is an essential component of the problem-solving process, these knowledge-centered models cause a contradiction between data fitting and rationalization of the model decision-making process, making it difficult to achieve high precision and readability simultaneously. In this paper, an innovative KT model, ability boosted knowledge tracing (ABKT)11Our implementations are available inhttps://github.com/ccnu-mathits/ABKT. is proposed, which introduces the ability factor into learning feedback attribution to enable the model to analyze the learning process from two perspectives, knowledge and ability, simultaneously. Based on constructive learning theory, continuous matrix factorization (CMF) model is proposed to simulate the knowledge internalization process, following the initiative growth and stationarity principles. In addition, the linear graph latent ability (LGLA) model is proposed to construct learner and item latent ability features, from graph-structured learner interaction data. Then, the knowledge and ability dual-tracing framework is constructed to integrate the knowledge and ability modules. Experimental results on four public databases indicate that the proposed methods perform better than state-of-the-art knowledge tracing algorithms in terms of prediction accuracy in quantitative assessments, displaying some advantages in model interpretability and intelligibility.}
}
@article{QIE20221,
title = {Data-driven deviation generation for non-ideal surfaces of Skin Model Shapes},
journal = {Procedia CIRP},
volume = {109},
pages = {1-6},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.205},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122006539},
author = {Yifan Qie and Nabil Anwer},
keywords = {Skin Model Shapes, Geometric deviations, Transfer learning},
abstract = {The digitalization techniques enable quality control as well as performance simulation of mechanical products in the Digital Twin era. In the product design process, realistic models regarding surface geometrical deviations are essential for further functional analysis. In the context of ISO standards on geometrical product specifications and verification (GPS), the Skin Model Shapes (SMSs) concept stemmed from the Skin Models paradigm is put forward to represent deviations of mechanical parts in accordance with the nature of geometric deviations on the surfaces. Contributions have been made to generate SMSs by combining both systematic deviations and random deviations on the nominal model. However, the non-ideal surface generation process is limited by the reuse of geometric deviation’s knowledge. Existing geometric deviations that are predicted or observed on the parts cannot be applied for other SMSs generation without new parameter settings using different methods. In this paper, a framework for data-driven geometric deviation generation is proposed for non-ideal surface modeling in tolerancing. A database that contains a variety of deviations generated by different methods is constructed. Deviations that are classified as systematic and random types are represented and stored as samples in the dataset. Deviation pattern identification is addressed by transfer learning using AlexNet and it is used for different types of non-ideal surface generation. Different approaches regarding both prediction and observation stage within SMS schema are included for constructing the deviation samples. The mapping process for both systematic and random deviation samples are considered. Therefore, there is no need to re-design deviations with parameters for the surfaces. A case study is used to illustrate the non-ideal surface generation process by the propose method.}
}
@incollection{PASSALIS2022165,
title = {Chapter 8 - Knowledge distillation},
editor = {Alexandros Iosifidis and Anastasios Tefas},
booktitle = {Deep Learning for Robot Perception and Cognition},
publisher = {Academic Press},
pages = {165-186},
year = {2022},
isbn = {978-0-323-85787-1},
doi = {https://doi.org/10.1016/B978-0-32-385787-1.00013-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857871000130},
author = {Nikolaos Passalis and Maria Tzelepi and Anastasios Tefas},
keywords = {Knowledge distillation, Knowledge transfer, Neural network distillation, Probabilistic knowledge transfer, Flow of solution procedure},
abstract = {The need to develop faster, more lightweight and flexible Deep Learning (DL) models has led to the development of a wide variety of methods. Among the most well-known methods for improving the accuracy of lightweight DL models is knowledge distillation, also known as knowledge transfer. Knowledge distillation is capable of improving the effectiveness of the training process by transferring the knowledge encoded in a large and complex neural network into a smaller and faster one. This chapter aims to provide an introduction to knowledge distillation approaches by presenting some of the most representative methods that will equip the reader with the necessary knowledge and tools to apply these methods in practice, as well as to follow this rapidly advancing field. First, we present the seminal neural network distillation approach, which kick–started the field. Then we present a generalization of this approach, which provides a general probabilistic view on knowledge distillation, allowing for going beyond classification tasks and overcoming some significant limitations of earlier methods. Multilayer knowledge distillation approaches, which employ multiple layers for the process of knowledge transfer and further improve the effectiveness of distillation, are also presented and discussed. Finally, we present and discuss some more advanced ways to train the teacher model, allowing for deriving more effective distillation methods, such as online distillation methods, which allow for simultaneously training both the teacher and student models, as well as, self-distillation methods, which do not even use a different teacher model and are capable of reusing the knowledge extracted from the student model.}
}
@article{GONNHEIMER2022734,
title = {Generation of identifiable CNC reference runs with high information content for machine learning and analytic approaches to parameter identification},
journal = {Procedia CIRP},
volume = {107},
pages = {734-739},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.054},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003389},
author = {Philipp Gönnheimer and Robin Ströbel and Markus Netzer and Jürgen Fleischer},
keywords = {Artificial intelligence, Automation, Computer numerical control (CNC), Digital manufacturing system, Identification, Machine tool},
abstract = {As a result of the change to Industry 4.0, the requirements for information models and digital twins are steadily increasing. Thus, reliable methods to identification and assignment of data sources in CNC machines are required. AI-based approaches are already capable of identifying individual signal groups but are increasingly reaching their limits due to the small size of existing datasets. Furthermore, the low information content of the timeseries used to build the learning datasets represents an additional limitation. In this paper, an approach is presented and examined by means of which identifiable CNC reference runs with particularly high information content can be generated to create a suitable database for machine learning approaches. Moreover, due to the uniqueness of the generated trajectories, the reference runs represent a particularly suitable basis for analytical methods to parameter identification.}
}
@article{TAPPE2022421,
title = {Neural ODEs and differential flatness for total least squares parameter estimation},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {20},
pages = {421-426},
year = {2022},
note = {10th Vienna International Conference on Mathematical Modelling MATHMOD 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.131},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322013301},
author = {Aike Aline Tappe and Moritz Schulze and René Schenkendorf},
keywords = {total least squares, neural ordinary differential equations, differential flatness, model inversion, kinetic parameter estimation, parameter uncertainties},
abstract = {In (bio)chemical process engineering, first-principles process models have played a central role for some time in better understanding, monitoring, and controlling these complex processes. Dynamic process models have become even more critical in the context of Industry 4.0 and the use of digital twins in the last decade. However, the quality and the technology readiness level of digital process models depend crucially on the reliability of the model predictions. In addition to a suitable model structure/hypothesis, the model parameters of the implemented kinetics are of paramount importance. The accuracy of the parameter estimation, in turn, depends on the quantity and quality of the data as well as on the employed parameter identification solving strategies, where ordinary least squares concepts are still the standard. We propose a novel parameter identification concept that combines systems theory and machine learning principles. The parameter identification problem is formulated as a total least squares optimization problem that uses neural ordinary differential equations for surrogate modeling and recalculates the model control inputs with the algebraic differential flatness framework for model inversion. The usefulness of the proposed concept for more precise kinetic parameters is demonstrated with a simulation study of an enzyme-catalyzed biochemical process, where the total least squares approach leads to lower parameter uncertainties compared to the standard concept based on ordinary least squares using the same amount of data.}
}
@article{RITTO2022109000,
title = {A transfer learning-based digital twin for detecting localised torsional friction in deviated wells},
journal = {Mechanical Systems and Signal Processing},
volume = {173},
pages = {109000},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109000},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022001789},
author = {T.G. Ritto and K. Worden and D.J. Wagg and F.A. Rochinha and P. Gardner},
keywords = {Digital twin, Drill-string vibration, Transfer learning, Domain adaptation},
abstract = {Digital twins seek to replicate a physical structure in a digital domain. For a digital twin to have close correspondence to its physical twin, data are required. However, it is not always possible, or cost-effective, to collect a complete set of data for a structure in all configurations of interest. It is nonetheless useful to repurpose data to help validate predictions for different configurations and scenarios. This statement is true in drilling applications, where, for example, the length of the drill string is altered throughout operation. This paper demonstrates how transfer learning, in the form of three domain-adaptation methods, — transfer component analysis (TCA), maximum independence domain adaptation (MIDA) and geodesic flow kernel (GFK) — can be used to construct a digital twin for localising torsional friction in deviated wells under structural changes (e.g., when the drill column gets longer). The method uses a physics-based torsional model to train a machine-learning classifier that can localise torsional friction for a given drill string length and diameter, where friction localisation labels are known (source). As the length or diameter of the drill string are altered in the field, transfer learning is utilised to map the classifier from the labelled (source) scenario onto these unlabelled (target) scenarios. As a result, transfer learning improves the performance of the classifier when applied to the target data, and increases the domain of validity for the classifier. The performance of the classifier, and therefore its suitability to new drill-string configurations, is estimated by utilising two different distance metrics between the source and a proposed target dataset.}
}
@article{THOMAS2022109293,
title = {A machine learning approach to determine the elastic properties of printed fiber-reinforced polymers},
journal = {Composites Science and Technology},
volume = {220},
pages = {109293},
year = {2022},
issn = {0266-3538},
doi = {https://doi.org/10.1016/j.compscitech.2022.109293},
url = {https://www.sciencedirect.com/science/article/pii/S0266353822000355},
author = {Akshay J. Thomas and Eduardo Barocio and R. Byron Pipes},
keywords = {Inverse determination, Short fiber composites, Elastic properties, Fiber orientation state, Extrusion deposition additive manufacturing, Support vector regression},
abstract = {This work focuses on the simultaneous determination of the elastic constants and the fiber orientation state for a short fiber-reinforced polymer composite by performing a minimum of experimental tests. We introduce a methodology that enables the inverse determination of fiber orientation state and the in-situ polymer properties by performing tensile tests at the composite coupon level. We demonstrate the approach for the extrusion deposition additive manufacturing (EDAM) process to illustrate one application of the methodology, but the development is such that it can be applied to short fiber-reinforced polymer (SFRP) systems processed via other methods. Currently, developing composites additive manufacturing digital twins require extensive material characterization. In particular, the mechanical characterization of the orthotropic elastic properties of a composite involves extensive sample preparation and testing, therefore the elasticity tensor is generally populated using a micromechanics model. This, however, requires measuring the fiber orientation state in addition to knowing the constituent material properties. Experimentally measuring the fiber orientation state can be tedious and time consuming. Further, optical methods are limited to resolving the orientation of cylindrical fibers or cluster of non-cylindrical fibers, and computed tomography (CT) methods scan regions of volume that are much smaller than a full printed bead. Therefore, we propose a methodology, accelerated by machine learning, to identify the anisotropic mechanical properties and fiber orientation state at the same time. Early results show that inference of the fiber orientation and composite properties is possible with as few as three tensile tests. Our results show that a combination of the choice of the micromechanics model and reliable set of experiments can yield the nine elastic constants, as well as, the fiber orientation state.}
}
@article{KHOSRAVI2022100074,
title = {Explainable Artificial Intelligence in education},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100074},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100074},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000297},
author = {Hassan Khosravi and Simon Buckingham Shum and Guanliang Chen and Cristina Conati and Yi-Shan Tsai and Judy Kay and Simon Knight and Roberto Martinez-Maldonado and Shazia Sadiq and Dragan Gašević},
keywords = {Explainable AI, AI in Education, Open learner models},
abstract = {There are emerging concerns about the Fairness, Accountability, Transparency, and Ethics (FATE) of educational interventions supported by the use of Artificial Intelligence (AI) algorithms. One of the emerging methods for increasing trust in AI systems is to use eXplainable AI (XAI), which promotes the use of methods that produce transparent explanations and reasons for decisions AI systems make. Considering the existing literature on XAI, this paper argues that XAI in education has commonalities with the broader use of AI but also has distinctive needs. Accordingly, we first present a framework, referred to as XAI-ED, that considers six key aspects in relation to explainability for studying, designing and developing educational AI tools. These key aspects focus on the stakeholders, benefits, approaches for presenting explanations, widely used classes of AI models, human-centred designs of the AI interfaces and potential pitfalls of providing explanations within education. We then present four comprehensive case studies that illustrate the application of XAI-ED in four different educational AI tools. The paper concludes by discussing opportunities, challenges and future research needs for the effective incorporation of XAI in education.}
}
@article{RAMU2022103663,
title = {Federated learning enabled digital twins for smart cities: Concepts, recent advances, and future directions},
journal = {Sustainable Cities and Society},
volume = {79},
pages = {103663},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103663},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721009264},
author = {Swarna Priya Ramu and Parimala Boopalan and Quoc-Viet Pham and Praveen Kumar Reddy Maddikunta and Thien Huynh-The and Mamoun Alazab and Thanh Thi Nguyen and Thippa Reddy Gadekallu},
keywords = {Digital Twin, Federated Learning, Internet of Things, Virtual replica, Smart city},
abstract = {Recent advances in Artificial Intelligence (AI) and the Internet of Things (IoT) have facilitated continuous improvement in smart city based applications such as smart healthcare, transportation, and environmental management. Digital Twin (DT) is an AI-based virtual replica of the real-world physical entity. DTs have been successfully adopted in manufacturing and industrial sectors, they are however still at the early stage in smart city based applications. The major reason for this lag is the lack of trust and privacy issues in sharing sensitive data. Federated Learning (FL) is a technology that could be integrated along with DT to ensure privacy preservation and trustworthiness. This paper focuses on the integration of these two promising technologies for adoption in real-time and life-critical scenarios, as well as for ease of governance in smart city based applications. We present an extensive survey on the various smart city based applications of FL models in DTs. Based on the study, some prominent challenges and future directions are presented for better FL–DT integration in future applications.}
}
@article{SCREPANTI2022267,
title = {Control Engineering and Robotics since Primary School: an Infrastructure for creating the Digital Twin model of the Learning Class.},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {17},
pages = {267-272},
year = {2022},
note = {13th IFAC Symposium on Advances in Control Education ACE 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.290},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322015270},
author = {L. Screpanti and D. Scaradozzi and R.N. Gulesin and N. Ciuccoli},
keywords = {Robotics, Educational environment, Educational robotics, Cyber-Physical Human System, CPHS, Cyber-Physical System, CPS, Digital Twin, MAS},
abstract = {Control engineering has a cross-boundary nature because its applications span over a wide range of fields, among which science, technology, engineering, and mathematics (STEM). Creating an automation literacy from the Primary School's age is beneficial for pupils because it supports the development of valuable skills that allow the comprehension and analysis of real-world phenomena. Even if many researchers and policymakers have advocated for engineering education since early education, it is usually kept for undergraduate and graduate-level education. What prevents systems theory and control education from being integrated into K12 education is the lack of available educational resources and the lack of indicators to represent the learning gain of students. To help teachers in the latter aspect, a solution can be studying the entire process as a cyber-physical human system (CPHS). The paper consists of a brief report about the work carried out by authors to represent the entire classroom as a CPHS where the physical robots designed by students, humans (teachers and learners), and cybertechnologies are interconnected to accomplish a goal which is learning. The entire infrastructure could be seamlessly deployed into the classroom, supporting learning assessment and the feedback process starting from the deployment of a (quasi) real-time intelligent collection system.}
}
@article{SEINMINN2022100050,
title = {AI-assisted knowledge assessment techniques for adaptive learning environments},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100050},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100050},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000054},
author = { {Sein Minn}},
keywords = {Adaptive learning environments, Intelligent tutoring systems, Educational data mining, Formative assessment, Summative assessment, Knowledge tracing, Cognitive diagnosis modelling},
abstract = {The growth of online learning, enabled by the availability on the Internet of different forms of didactic materials such as MOOCs and Intelligent Tutoring Systems (ITS), in turn, increases the relevance of personalized instructions for students in an adaptive learning environment. There are increasing interests as well as many challenges in the application of Artificial Intelligence (AI) techniques in educational settings to provide adaptive learning content to learners. Knowledge assessment is necessary for providing an adaptive learning environment. A student model serves as a fundamental building block of knowledge assessment in an adaptive learning environment. This paper intends to review the development of dominant families of student models with psychometric theory in early educational research, recent adaptations, and advances with machine learning and deep learning techniques. Our review covers not only the important families of student models but also why they were invented from both theoretical and practical viewpoints with AI and educational perspectives. We believe that the discussion covered in this review will be a valuable reference of introductory insights to AI for educational researchers, as well as an endeavor of introducing basic psychometric perspectives to AI experts for knowledge assessment in the field of learning science. Finally, we provide recent challenges and some potential directions for developing efficient knowledge assessment techniques in future adaptive learning ecosystems.}
}
@article{MULLER202272,
title = {Situational Risk Assessment Design for Autonomous Mobile Robots},
journal = {Procedia CIRP},
volume = {109},
pages = {72-77},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.216},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122006643},
author = {Manuel Müller and Golsa Ghasemi and Nasser Jazdi and Michael Weyrich},
keywords = {Situational Risk Assessment, Digital Twin, Design Approach, Complexity, System Theoretic Process Analysis (STPA)},
abstract = {The emerging autonomous mobile robots promise a new level of efficiency and flexibility. However, because these types of systems operate in the same space as humans, mobile robots must cope with dynamic changes and heterogeneously structured environments. To ensure safety, new approaches are needed that model risk at runtime. This risk depends on the situation and is therefore a situational risk. In this paper, we propose a new methodology to model situational risk based on multi-agent adversarial reinforcement learning. In this methodology, two competing groups of reinforcement learning agents, namely the protagonists and the adversaries, fight against each other in the simulation. The adversaries represent the disruptive and destabilizing factors, while the protagonists try to compensate for them. The situational risk is then derived from the outcome of the simulated struggle. At this point, the system’s Digital Twin provides up-to-date and relevant models for simulation and synchronizes the simulation with the real asset. Our risk modeling differentiates the four steps of intelligent information processing: sense, analyze, process, and execute. To find the appropriate adversaries and actors for each of these steps, this methodology builds on Systems Theoretic Process Analysis (STPA). Using STPA, we identify critical signals that lead to losses when a disturbance under certain conditions or in certain situations occurs. At this point, the challenge of managing the complexity arises. We face this issue using training effort as a metric to evaluate it. Through statistical analysis of the identified signals, we derive a procedure for defining action spaces and rewards for the agents in question. We validate the methodology using the example of a Robotino 3 Premium from Festo, an autonomous mobile robot.}
}
@article{KUMARI2022199,
title = {MetaAnalyser - A Concept and Toolkit for Enablement of Digital Twin},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {199-204},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.193},
url = {https://www.sciencedirect.com/science/article/pii/S240589632200194X},
author = {Jaya Kumari and Ramin Karim and Kevin Karim and Martin Arenbro},
keywords = {digital twin, artificial intelligence, automated machine learning, MetaAnalyser, feature selection, model selection},
abstract = {Digital Twin (DT) has promising impact on the life cycle management of assets in manufacturing industry. The concept of DT has become possible with digitalisation and Artificial Intelligence (AI). Data driven Machine Learning (ML) capabilities, can enhance the performance of the DT. To replicate a dynamic system, the DT should continuously receive and process incoming data in real-time. However, every time that the system receives new incoming datasets, the challenges of ML such as data preparation, feature selection, model selection and performance evaluation, slow down the development process of DT. This paper proposes a MetaAnalyser platform that automates these steps for incoming datasets in real-time. The MetaAnalyser platform through automating data preparation, feature selection, model selection and performance evaluation, is expected to increase the level of agility in the development process of DT and the efficiency of the DT during its lifecycle. The MetaAnalyser platform is demonstrated in this paper by ranking the features that affect the arrival delays in trains and ranking regression models based on their performance on the dataset.}
}
@article{PANAGOU2022132,
title = {Feature investigation with Digital Twin for predictive maintenance following a machine learning approach},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {132-137},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.182},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001835},
author = {Sotirios Panagou and Fabio Fruggiero and Marida Lerra and Carmen del Vecchio and Fernando Menchetti and Luca Piedimonte and Oreste Riccardo Natale and Salvatore Passariello},
keywords = {, , , , },
abstract = {Predictive Maintenance is gathering a lot of interest both from research and industries. The combination of Digital Twin models and Machine Learning provides the mixture of past and featured values for application in the prediction of failures in correlation with production plans. In this work, we explored the use of Machine Learning to extract, through the important features selection, information on which sensors/data - used in a steel industry production line - can be considered “principal” through data obtained from the integration of real-time monitoring and Digital Twin elaboration. The analysis of the data, collected from a period of six months, provided information on anomalies and main signal correlation. The data from Digital Twin and Machine Learning predicted normal and in need of observation states along with the anomalies. Further investigation using Machine Learning, provided the sensors that reported the anomalies and gathered principal components. The sensors’ signal data are currently used for real-time monitoring and Predictive Maintenance plans and integrated in a cloud based platform.}
}
@article{LI2022106712,
title = {A single view leaf reconstruction method based on the fusion of ResNet and differentiable render in plantgrowthdigitaltwinsystem},
journal = {Computers and Electronics in Agriculture},
volume = {193},
pages = {106712},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106712},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922000291},
author = {Wei Li and Deli Zhu and Qing Wang},
keywords = {Deep learning, 3d leaf reconstruction, Resnet, Differentiable render, Plant growth digital twin system},
abstract = {In modern agriculture, plant growth digital twin system helps breeders monitor plant growth, increase yield, and provide growth management advice. Research on the single view leaf 3D reconstruction in digital twin systems has achieved relative success. However, in traditional single-view reconstruction algorithms, the leaf reconstruction often contains the problems of low precision, achieving complexity, and slow speed, making it difficult for recovering three-dimensional information about leaves. Consequently, the reconstruction precision is significantly reduced, which further affects the accuracy of single-view leaf 3D reconstruction. In response to this problem, this study proposed a single-view leaf reconstruction approach in plant growth digital twin systems based on deep learning. The method in this paper mainly fuses the advantages of ResNet and differentiable rendering, and the model is used for further enhancing feature extraction capability and reconstruction precision. Finally, the experiment presented in this paper suggests that the method allows for the 3D reconstruction of plant leaves with different shapes using a single view. Moreover, the experiment results show that the F-Score, CD, EMD reached 76.192, 0.808, and 3.567. Compared with other models, the proposed model in this study has higher reconstruction accuracy, 3D evaluation indicators, and prediction results, providing important ideas and methods for recovering the leaves from a single view in a plant growth digital twin system.}
}
@article{SCHNURER20222416,
title = {Offline digital twin synchronization using measurement data and machine learning methods},
journal = {Materials Today: Proceedings},
volume = {62},
pages = {2416-2420},
year = {2022},
note = {37th Danubia Adria Symposium on Advances in Experimental Mechanics},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.02.566},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322012275},
author = {Dominik Schnürer and Franz Hammelmüller and Helmut J. Holl and Wolfgang Kunze},
keywords = {Digital twin, Machine learning, Automatic differentiation, Parameter identification, Compliances},
abstract = {Digital Twins play an important role in modeling production processes to adapt parameters according to predicted situations. Panel bending machines from Salvagnini use this technology to ensure safe operating conditions and to guarantee accurate results for different settings, even with highly variable material properties. Due to constantly increasing accuracy requirements, digital twins have to increase accuracy on the one hand and adapt to new machine generations on the other hand. This work shows how machine learning tools can be used to synchronize digital twins accurately and efficiently with real-world behavior by learning parameter values with measurement data while maintaining interpretable and robust analytical models.}
}
@incollection{CASINI2022151,
title = {Chapter 3 - Building digital revolution},
editor = {Marco Casini},
booktitle = {Construction 4.0},
publisher = {Woodhead Publishing},
pages = {151-186},
year = {2022},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-12-821797-9},
doi = {https://doi.org/10.1016/B978-0-12-821797-9.00013-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217979000131},
author = {Marco Casini},
keywords = {Construction 4.0, building information modeling, cloud computing, edge computing, Internet of things, 5G network, artificial intelligence, machine learning, Big Data, advanced analytics, nanotechnology, digital twin, digital building life cycle, augmented digital design, data-driven design, connected construction, building automation, automated construction site},
abstract = {The chapter provides and overview of the digital revolution that is changing the Architecture, Engineering, Construction and Operation (AECO) industry and leading to the Construction 4.0 model, based on augmented digital design, connected and automated construction processes, and smart building operations and maintenance. The key technology drivers—building information modeling, cloud and edge computing, Internet of things, 5G networks, artificial intelligence and machine learning, Big Data and advanced analytics, and nanotechnology—are described, highlighting their role in the digital transformation of the construction sector and the new opportunities brought in terms of higher productivity and building quality. A thorough analysis of Construction 4.0 tools and methods is given, describing the applications and advantages in the whole value chain of the new “digital twin building life cycle” achievable with the full integration of all these digital technologies.}
}
@article{WOLF2022366,
title = {Real Time Locating Systems for Human Centered Production Planning and Monitoring},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {366-371},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.221},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322002221},
author = {M. Wolf and M. Rantschl and E. Auberger and H. Preising and A. Sbaragli and F. Pilati and C. Ramsauer},
keywords = {Production planning, control, human centered design, digital twin, real-time feedback, RTLS},
abstract = {Production companies often operate in a dynamic and volatile environment. This leads to an increasing demand for continuous changes in their production systems and processes. Furthermore, decreasing product life cycle times and rising market demand for product variety and individualized products is bringing about the necessity for the monitoring and coordination of processes in the operations phase. As a result of these developments production management is facing new challenges in decision making for the optimal settings of the production system design and the related coordination of production. All of this demands enormous efforts to maintain a consistent and reliable database for the ongoing configuration and coordination of the production system. It is thus a remarkable challenge for industrial companies. Real time locating systems (RTLS) with their ability to continuously monitor the current position and parameters (speed, direction, etc.) of process resources (operators, equipment, products, etc.) offer several potential benefits for the manufacturing industry. The potential areas of application can be identified in different layers of production management. They range from data acquisition on shop floor level through reconfiguration of production systems to providing real time feedback for the blue collars. Furthermore, it allows dynamic coordination of production orders for industrial plants via appropriate digital twin (DT) technologies. This paper proposes an original framework for RTLS in industrial environments and presents a case study for framework application at the TU Graz Learning Factory.}
}
@article{RAHAYU2022100047,
title = {A systematic review of ontology use in E-Learning recommender system},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100047},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100047},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000029},
author = {Nur W. Rahayu and Ridi Ferdiana and Sri S. Kusumawardani},
keywords = {e-learning recommendation item, Learning object, Ontology-based recommender system, Ontology evaluation, Ontology methodology, Ontology use},
abstract = {Ontology and knowledge-based systems typically provide e-learning recommender systems. However, ontology use in such systems is not well studied in systematic detail. Therefore, this research examines the development and evaluation of ontology-based recommender systems. The study also discusses technical ontology use and the recommendation process. We identified multidisciplinary ontology-based recommender systems in 28 journal articles. These systems combined ontology with artificial intelligence, computing technology, education, education psychology, and social sciences. Student models and learning objects remain the primary ontology use, followed by feedback, assessments, and context data. Currently, the most popular recommendation item is the learning object, but learning path, feedback, and learning device could be the future considerations. This recommendation process is reciprocal and can be initiated either by the system or students. Standard ontology languages are commonly used, but standards for student profiles and learning object metadata are rarely adopted. Moreover, ontology-based recommender systems seldom use the methodology of building ontologies and hardly use other ontology methodologies. Similarly, none of the primary studies described ontology evaluation methodologies, but the systems are evaluated by nonreal students, algorithmic performance tests, statistics, questionnaires, and qualitative observations. In conclusion, the findings support the implementation of ontology methodologies and the integration of ontology-based recommendations into existing learning technologies. The study also promotes the use of recommender systems in social science and humanities courses, non-higher education, and open learning environments.}
}
@article{HERMANN202283,
title = {A Digital Twin Approach for the Prediction of the Geometry of Single Tracks Produced by Laser Metal Deposition},
journal = {Procedia CIRP},
volume = {107},
pages = {83-88},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S221282712200230X},
author = {Florian Hermann and Bowen Chen and Golsa Ghasemi and Valentin Stegmaier and Thomas Ackermann and Peter Reimann and Sabrina Vogt and Thomas Graf and Michael Weyrich},
keywords = {Laser metal deposition, Software defined manufacturing, Digital Twin, Asset Administration Shell},
abstract = {Flexible manufacturing processes such as laser metal deposition exhibit high potential for a production solely defined by software to cope with the current challenges of production systems. The determination of suitable machine parameters for the production of novel materials and geometries however requires extensive experimental effort. Existing simulative approaches do not offer sufficient accuracy to predict the relevant machine parameters in a satisfactory way. This paper presents a new concept, in which we apply a digital twin to provide a step towards a fully software-defined and predictable laser metal deposition process. The presented concept includes relevant data of the machines as well as data-driven machine learning models and physics-based simulation models. This enables a more reliable prediction of geometries of single tracks which was validated on a laser metal deposition machine.}
}
@article{OEDA20221992,
title = {Latent State Estimation Method for Students Using Knowledge Tracing Combining with IRT},
journal = {Procedia Computer Science},
volume = {207},
pages = {1992-1999},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.258},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922011449},
author = {Shinichi Oeda and Toma Kakizaki},
keywords = {Intelligent Tutoring System, Educational Data Mining, Student Modeling, Knowledge Tracing, Item Response Theory},
abstract = {The Intelligent Tutoring Systems (ITSs) are a system that provides an efficient learning environment by assigning questions that are suitable to the learner's skill state. To improve the performance of ITSs, conventional studies have proposed student models that can estimate skill states with high accuracy. However, these models were based on the assumption that the log data was correct. In other words, they do not take into account the possibility of students cheating or memorizing answers, assuming that the test was conducted fairly and impartially. In recent years, many examinations have been conducted online as a countermeasure against COVID-19 (coronavirus) infections, and students may be able to obtain hints for the examinations using the internet or books. In this way, it is important to know how students approach learning in order to estimate their skill state. In this study, we propose a method for estimating the latent state of learners using a model that combines Knowledge Tracing, the de-facto standard for student modeling method, and Item Response Theory.}
}
@article{HOSAMO2022111988,
title = {A Digital Twin predictive maintenance framework of air handling units based on automatic fault detection and diagnostics},
journal = {Energy and Buildings},
volume = {261},
pages = {111988},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.111988},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822001591},
author = {Haidar Hosamo Hosamo and Paul Ragnar Svennevig and Kjeld Svidt and Daguang Han and Henrik Kofoed Nielsen},
keywords = {Digita twin, Facility management, Internet of things, Predictive maintenance, APAR, Air handling unit},
abstract = {The building industry consumes the most energy globally, making it a priority in energy efficiency initiatives. Heating, ventilation, and air conditioning (HVAC) systems create the heart of buildings. Stable air handling unit (AHU) functioning is vital to ensuring high efficiency and extending the life of HVAC systems. This research proposes a Digital Twin predictive maintenance framework of AHU to overcome the limitations of facility maintenance management (FMM) systems now in use in buildings. Digital Twin technology, which is still at an initial stage in the facility management industry, use Building Information Modeling (BIM), Internet of things (IoT) and semantic technologies to create a better maintenance strategy for building facilities. Three modules are implemented to perform a predictive maintenance framework: operating fault detection in AHU based on the APAR (Air Handling Unit Performance Assessment Rules) method, condition prediction using machine learning techniques, and maintenance planning. Furthermore, the proposed framework was tested in a real-world case study with data between August 2019 and October 2021 for an educational building in Norway to validate that the method was feasible. Inspection information and previous maintenance records are also obtained through the FM system. The results demonstrate that the continually updated data combined with APAR and machine learning algorithms can detect faults and predict the future state of Air Handling Unit (AHU) components, which may assist in maintenance scheduling. Removing the detected operating faults resulted in annual energy savings of several thousand dollars due to eliminating the identified operating faults.}
}
@article{MARCON2022139,
title = {An Experimental Training Production Line to Demonstrate the Basics of Industry 4.0},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {4},
pages = {139-144},
year = {2022},
note = {17th IFAC Conference on Programmable Devices and Embedded Systems PDES 2022 — Sarajevo, Bosnia and Herzegovina, 17-19 May 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.06.023},
url = {https://www.sciencedirect.com/science/article/pii/S240589632200338X},
author = {P. Marcon and J. Jirsa and L. Venkrbec and F. Zezulka and T. Benesl and V. Kaczmarczyk and J. Arm},
keywords = {Asset administration shell, digital twin, decentralized control, Industry 4.0},
abstract = {The paper deals with appropriate possibilities, instrumentation, and technologies for purposes of education, presentation, teaching and research, and development of technologies and procedures of Industry 4.0 ideas, and their implementation in case studies of Factories of the Future. Through the physical module-based production lines of a FESTO Didactic company authors present technologies, standards, and principles of Industry 4.0. They are persuaded, that after some years of simple presentation of ideas and visions, it is necessary to provide a step from theory to the praxis of the Industry 4.0 technologies in the fully standardized matter. Paper presents a state of the art of an enhanced Experimental Education production line, the Cyber-Physical (CP) Factory of the company Festo Didactic. They show existing properties of the technology, stemming from 2016, its disadvantages and provide a proposal on how to enhance CP Factory towards a physical system, implementing and providing properties of fully decentralized control of automatic production systems in a standardized way.}
}
@incollection{HABASH2022129,
title = {5 - Building as a human-cyber-physical system},
editor = {Riadh Habash},
booktitle = {Sustainability and Health in Intelligent Buildings},
publisher = {Woodhead Publishing},
pages = {129-160},
year = {2022},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-323-98826-1},
doi = {https://doi.org/10.1016/B978-0-323-98826-1.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988261000053},
author = {Riadh Habash},
keywords = {Intelligent efficiency, Digital sustainability, Digital twin, User experience, Intelligent architecture, Artificial intelligence of things, Machine learning, Building information modeling, Surveillance and privacy, Cyber-physical security},
abstract = {An intelligent building is a composite structure comprising human-cyber-physical systems aiming to achieve specific goals within unfolding digital transformation. Fast urbanization, persistent lessening of limited resources, and occupants’ expectations of building capabilities are driving the need for the deployment of intelligent data-driven solutions. This in turn takes a step beyond delivering insights toward achieving optimized decision-making based on artificial intelligence and machine learning to universal design platforms such as building information modeling and digital twin. Integration of architecture, engineering, and building science with digital sustainability happens on a contributory intensity where intelligence is correlated with making the design and construction process more efficient. A critical consideration in this chapter is the consolidation of all aspects of digitization into one operational model. This is where digital sustainability with a human-cyber-physical scenario comes in to enhance performance and improve living conditions in the built environment. The applicability of intelligent technologies that incorporate distributed sensing and big data addresses characteristics like energy efficiency and heathy living. The chapter concludes with possible solutions for a safe and secure scenario, with the employment of blockchain technology as a measure to fortify and control the design and operation tools.}
}
@article{RAUH2022576,
title = {Towards AI Lifecycle Management in Manufacturing Using the Asset Administration Shell (AAS)},
journal = {Procedia CIRP},
volume = {107},
pages = {576-581},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003122},
author = {Lukas Rauh and Sascha Gärtner and David Brandt and Michael Oberle and Daniel Stock and Thomas Bauernhansl},
keywords = {AI Lifecycle, AI Asset, Industry 4.0, Digital Twin, Asset Administration Shell},
abstract = {Driven by the digital transformation, manufacturing companies face the challenge of managing, but more importantly, enabling rapid operationalization of AI to achieve the full advantage of the exponential data growth. Heterogeneous data structures, the continuously growing variety of implementation frameworks, and the lack of standards for the semantic description of AI solution components, the effort required to manage and share datasets and models between stakeholders impedes efficient and reproducible progression. This paper addresses the current challenges in industrial AI applications currently hindering their acceptance and widespread adoption in manufacturing. Based on an overview of the AI application lifecycle, we present our approach for AI asset meta-data management utilizing the technical concept of the Asset Administration Shell (AAS). Following the definition of the AAS as a reference implementation of the digital twin that provides a digital representation of physical assets and their properties, we propose an AAS for AI assets. The AI AAS maps relevant properties of an AI model together with properties of the corresponding dataset and learning algorithm in order to integrate the AI lifecycle in the Industry4.0 ecosphere.}
}
@article{KOLIBAROV2022173,
title = {Roof Segmentation Towards Digital Twin Generation in LoD2+ Using Deep Learning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {11},
pages = {173-178},
year = {2022},
note = {IFAC Workshop on Control for Smart Cities CSC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.08.068},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322011582},
author = {N. Kolibarov and D. Wästberg and V. Naserentin and D. Petrova-Antonova and S. Ilieva and A. Logg},
keywords = {Digital Twin Cities, LoD2+, Deep Learning, Convolutional Neural Networks, Roof Segmentation},
abstract = {There is an increasing need for digital twins of cities and their base maps, 3D city models. Creating and updating these twins is not an easy task, so automating and streamlining the process is a field of active research. A significant part of the urban geometry is residential buildings and their roofs. Modeling of roofs for urban buildings can be divided into three main areas - building detection, roof recognition and building reconstruction. The building and roofs are segmented with the help of machine learning and image processing. Afterwards the extracted information is used to generate parametric models for the roofs using methods from computational geometry. The goal is to create correct virtual models of roofs belonging to many different types of buildings. In this study, a supervised deep learning approach is proposed for the segmentation of roof edges from a single orthophoto. The predicted features include the linear elements of roofs. The experiments show that, despite the small amount of training data, even in the presence of noise, the proposed method performs well on semantic segmentation of roofs with different shapes and complexities. The quality of the extracted roof elements for the test area is about 56% and 71% for mean intersection over union (IOU) and Dice metric scores, respectively.}
}
@article{ELAMBASSERIL2022461,
title = {Artificial intelligence: way forward to empower metal additive manufacturing product development – an overview},
journal = {Materials Today: Proceedings},
volume = {58},
pages = {461-465},
year = {2022},
note = {International Conference on Artificial Intelligence & Energy Systems},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.02.485},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322011415},
author = {Joe Elambasseril and Milan Brandt},
keywords = {Additive manufacturing, Digital Twin, Artificial intelligence, Machine learning, powder bed fusion, Digital twin},
abstract = {Metal additive manufacturing (MAM) has emerged as a promising technology to fabricate parts that are nearly impossible to do so by traditional methods. Since MAM is considered an open-loop system, it is not as efficient at fabricating repeatable and high-quality parts as a closed system. Hence, changing the MAM open-loop system to a closed system proves to be an imperative step for producing reliable parts. The direct application of physics or heat transfer or fluid dynamics equations to a MAM system is impossible as several uncontrolled parameters impact each layer fabrication. Thus, leading researchers are focusing on developing a digital twin, a real-time virtual model or digital representation to provide better control of the MAM process. Currently, the digital twin for MAM is in the development stage as the data capture through real-time monitoring, experimental values, and simulations from layer fabrication need to be sorted and stored in a structured manner. The way forward for a consistent quality MAM part is achieved through a digital twin which is explicitly driven by subsets of artificial intelligence such as machine learning and deep learning. More data must be further acquired and utilized to train the machine to learn the algorithm. An overview of artificial intelligence in additive manufacturing for a closed-loop system is presented.}
}
@article{IOSHCHIKHES20221232,
title = {Assessing Energy Efficiency Measures for Hydraulic Systems using a Digital Twin},
journal = {Procedia CIRP},
volume = {107},
pages = {1232-1237},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.137},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004218},
author = {Borys Ioshchikhes and Fabian Borst and Matthias Weigold},
keywords = {expert system, real-time simulation, machine learning, predicitve efficiency},
abstract = {As manufacturing companies around the world face the challenge of reducing CO2 emissions and achieving their climate goals, increasing energy efficiency provides a promising solution while potentially reducing costs. Hydraulic systems are used in a wide range of applications such as heating, ventilation, air conditioning or machine tools and account for approximately 11 % of the electric energy demand in the German industry in 2017. Furthermore, up to 25 million tons of CO2 are emitted annually in Germany as a result of their operation. Against this background, the following paper aims to increase the energy efficiency of hydraulic systems through automated assessment of energy efficiency measures during system operation. Therefore, we present a modular approach for real-time assessing of energy efficiency measures using a digital twin, which contains an expert system combined with real-time simulation models. To detect inefficiencies without time consuming analysis and substantial user expertise, the expert system automatically identifies system leakage and increased flow resistance using a multi-output regression model. Finally, the expert system aims at engaging operators to implement energy efficiency measures by quantifying their respective energy saving potentials. The proposed measures are applied to the virtual representation of a hydraulic system in real-time. Therefore, a Modelica simulation model is developed, which is exported as a functional mock-up unit (FMU) and integrated into a Python framework. If measures lead to an improvement in energy efficiency, these are recommended to the operator. The overall concept is validated using a physical hydraulic system within the ETA Research Factory. The validation of the prototype shows that the developed approach can be applied to industrial applications and help in reducing their energy consumption.}
}
@article{SEPAHVAND2022105413,
title = {Overcoming limitation of dissociation between MD and MI classifications of breast cancer histopathological images through a novel decomposed feature-based knowledge distillation method},
journal = {Computers in Biology and Medicine},
volume = {145},
pages = {105413},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105413},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522002050},
author = {Majid Sepahvand and Fardin Abdali-Mohammadi},
keywords = {Breast cancer, Histopathology, BreakHis, Knowledge distillation, Convolutional neural network},
abstract = {Magnification-independent (MI) classification is considered a promising method for detecting the histopathological images of breast cancer. However, it has too many parameters for real implementation due to dependence on input images in different magnification factors. In addition, magnification-dependent (MD) classification usually performs poorly on unseen samples, although it has lower input image sizes and fewer parameters. This paper proposes a novel method based on knowledge distillation (KD) to overcome the limitation of dissociation between MI classification and MD classification of breast cancer in histopathological images. The proposed KD method includes a pre-trained MI teacher model that is responsible for training an unprepared MD student model developed through only one magnification factor. In the proposed method, the decomposed feature maps of a teacher's intermediate layers are transferred as dark knowledge to a student. According to the experimental results, the student model developed through 40X images yielded accuracy rates of 99.41%, 99.26%, 99.14%, and 99.09% in response to unseen samples of 40X, 100X, 200X, and 400X images, respectively. Moreover, comparison results indicated the competitive performance of the proposed student model as opposed to the state-of-the-art method based on deep learning on BreakHis.}
}
@incollection{GALEAZZI20221543,
title = {A Methodology for The Optimal Surrogate Modelling of Digital Twins Using Machine Learning},
editor = {Ludovic Montastruc and Stephane Negny},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {51},
pages = {1543-1548},
year = {2022},
booktitle = {32nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-95879-0.50258-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323958790502587},
author = {Andrea Galeazzi and Kristiano Prifti and Francesco Gallo and Flavio Manenti},
keywords = {surrogate model, machine learning, digital twin, amine washing},
abstract = {Process simulation and digital twins are of paramount importance to design new processes or optimize already existing plants and equipment. The main drawback with current process simulation software is the computational time required to obtain a solution convergence towards a new steady-state. Especially when the input or the output to a system are perturbated. This procedure may take up to minutes in large systems or with strong non-linear recycles. Surrogate modelling of digital twins offers the possibility to speed up the time to convergence required by process simulators by substituting the fundamental or rigorous models with machine learning methods and models. In this work, a surrogate modelling methodology is described for extracting a useful amount of data in a domain near the nominal steady-state of the plant for which a digital twin has been created. For each process variable, a plethora of machine learning models are trained and compared. The best-performing models are chosen to predict the behaviour of such process variables. The application of the surrogate modelling framework thus created has been successfully applied to a steady-state simulation, i.e. digital twin, of an acid gas dimethylamine washing process at the Itelyum exhausted oil refinery in Pieve Fissiraga (LO), Italy.}
}
@article{MARVIN2022344,
title = {Digitalisation and Artificial Intelligence for sustainable food systems},
journal = {Trends in Food Science & Technology},
volume = {120},
pages = {344-348},
year = {2022},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2022.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0924224422000280},
author = {Hans J.P. Marvin and Yamine Bouzembrak and H.J. {van der Fels-Klerx} and Corné Kempenaar and Roel Veerkamp and Aneesh Chauhan and Sanne Stroosnijder and Jan Top and Görkem Simsek-Senel and Hans Vrolijk and Willem Jan Knibbe and Lu Zhang and Remko Boom and Bedir Tekinerdogan},
keywords = {Systems approach, Digital twins, Machine learning, Internet of things, Big Data},
abstract = {Background
The European Commission (EC) has launched the European Green Deal communication, setting out the path for a fundamental transformation of Europe. Key element in this policy is a fully sustainable food system outlined in the farm-to-fork strategy. Such strategy requires a systems approach in which all aspects related to the production and consumption of sufficient and healthy food are considered, including economic, environmental (climate, ecosystems) and social aspects.
Scope and approach
Here, we present the systems approach concept for food production, following the farm-to-fork principle as embraced by the EC, and elaborate on how digitalisation and Artificial Intelligence (AI) can solve the challenges that a sustainable food system imposes.
Key findings and conclusions
We present a number of research and innovation challenges and illustrate these by some specific examples. It is concluded that AI and digitalisation show great potential to support the transition towards a sustainable food system. This development will impact the roles and interactions of the actors in the entire value chain from farmers to consumers. Policy recommendations are made for a successful future implementation of AI in sustainable food production.}
}
@article{KONG2022100,
title = {The status of delivery of ISO GPS in China: A survey},
journal = {Procedia CIRP},
volume = {114},
pages = {100-105},
year = {2022},
note = {17th CIRP Conference on Computer Aided Tolerancing (CAT2022)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122014391},
author = {Chao Kong and Tukun Li and Zongzheng Zhang and Yuanping Xu and Juning Luo and Hongsheng Fu and Yue Zhu and Cuixin Ming and Jichang Yu},
keywords = {Engineer Education, ISO GPS, Survey, Industrial Internet},
abstract = {Engineers worldwide use the ISO GPS (Geometrical Production Specification and Verification) as a common language to express and exchange the geometrical information of a product and its digital twin. This paper presents a survey of engineering education of the Geometrical Production Specification and Verification (ISO GPS) in China. The research was conducted in 2021, and the purpose is to better understand the current situation in the delivery and usage of ISO GPS. It consists of a literature review of 66 published textbooks, an online questionnaire from 325 educators/students/engineers, who are from five universities and a standardisation committee. The results of this survey are presented and discussed graphically. The results show that:1) there is a significant gap between the textbook teaching and the latest ISO GPS standards; 2) the frequency of the latest ISO GPS usage is relatively low; 3) the current delivery methods cannot address the requirements of end-users soundly. A discussion of the possible solution is presented as well.}
}
@incollection{KALIWAL2022255,
title = {Chapter 16 - Evaluate learner level assessment in intelligent e-learning systems using probabilistic network model},
editor = {Rajiv Pandey and Sunil Kumar Khatri and Neeraj kumar Singh and Parul Verma},
booktitle = {Artificial Intelligence and Machine Learning for EDGE Computing},
publisher = {Academic Press},
pages = {255-265},
year = {2022},
isbn = {978-0-12-824054-0},
doi = {https://doi.org/10.1016/B978-0-12-824054-0.00029-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240540000290},
author = {Rohit B. Kaliwal and Santosh L. Deshpande},
keywords = {Intelligent tutoring systems (ITSs), Intelligent e-learning systems, Bayesian network (BN), E-learning, Learner},
abstract = {E-learning is a broader term than “m-learning” and “online learning.” The originality is that it provides the learner with the chance to learn anywhere and anytime. As technology is improving, teaching-learning methodologies are also changing by making electronic learning one of the leading trends in the education system. Due to the complexity of the education system, the prospective dynamics of e-learning require the development of a knowledge delivery and evaluation system. In the history of the education system, a pandemic situation created major disruption affecting 1.6 billion learners in 190 countries all over the globe. In this chapter, an evaluation method for assessing the learner assessment in an intelligent e-learning system is presented. Our method is called the probabilistic network (PN) model and is a well-known evaluation method for the learners in a framework. However, evaluation of the learner’s learning and outcomes still remains a challenge. The main objective of this chapter is to add its dimension toward the evaluation of learning outcomes especially for the learners of intelligent e-learning systems. In this case, intelligent tutoring systems (ITSs) were the bone for the smooth running of teaching and have been playing an important role in the e-learning system. An ITS is a type of knowledge-based system whose main objective is to effectively complement a human tutor with a computer. Unlike conventional classroom teaching, ITSs have the potential to fit according to an individual learner’s needs. A probability-based ITS structure is suggested in this chapter, consisting of four models, specifically the learner’s action model, the pedagogical model, the knowledge base model, and the learner assessment model. The importance is being given to test the learner level model where an element of complexity has been introduced and handled by the probabilistic network or Bayesian network model. The purpose of this assessment learner model is to correctly determine the level of knowledge of each learner based on their response to the level of questions, where the level of questions is random to the evaluation process of the learner. With the aid of Bayes’ law, the method has integrated into it the complexity and has found positive results that take the possibility of learners into account.}
}
@article{SCHIRMANN2022102916,
title = {Data-driven models for vessel motion prediction and the benefits of physics-based information},
journal = {Applied Ocean Research},
volume = {120},
pages = {102916},
year = {2022},
issn = {0141-1187},
doi = {https://doi.org/10.1016/j.apor.2021.102916},
url = {https://www.sciencedirect.com/science/article/pii/S0141118721003850},
author = {Matthew L. Schirmann and Matthew D. Collette and James W. Gose},
keywords = {Data-driven models, Digital twin, Machine learning, Motion forecasting, Neural networks, Physics-based models},
abstract = {Machine learning approaches, onboard measurements, and widely available wave forecast and hindcast data present an opportunity to develop predictive models for vessel motion forecasting. Detailed vessel motion forecasts would support underway and deployment decisions for safer and more efficient vessel operation. To demonstrate this application, ridge regression and neural network models for heave, pitch, and roll prediction were trained and tested using time-and-place specific, multidirectional wave model parameters as input. Additionally, the performance benefits of providing these predictive models with computationally efficient, physics-based model predictions (PBMPs) of heave, pitch, and roll as additional inputs were examined. Data from approximately 13,500 30-minute windows, measured aboard an operational research vessel, were used to train and test the data-driven models. Data from over 2,500 additional 30-minute windows, measured aboard a sister vessel, were also used to test the versatility of the trained models. The results of this study showed effective reduction of motion amplitude mean-squared error (MSE) values on multiple test datasets relative to the PBMPs alone. The results also showed that inclusion of PBMPs as input to the data-driven models was typically beneficial in terms of MSE reduction, stressing the importance of retaining physics-based information in data-driven models.}
}
@article{WANG2022164,
title = {Research on a Risk Control Model Based on Selective Ensemble Algorithm of Hierarchical Clustering and Simulated Annealing},
journal = {Procedia Computer Science},
volume = {202},
pages = {164-171},
year = {2022},
note = {International Conference on Identification, Information and Knowledge in the internet of Things, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922005579},
author = {Maoguang Wang and Haoyue Ji},
keywords = {, , , },
abstract = {In order to solve the problems in the traditional ensemble learning model, selective ensemble algorithms are often used to optimize the ensemble learning model. To settle the shortcomings of the current selective ensemble algorithm existing in the selection efficiency and predict outcomes, this paper proposes selective ensemble algorithm basing on stacking ensemble framework. The algorithm mainly uses the agglomerated hierarchical clustering (AHC) algorithm and the metropolis criterion of simulated annealing to select the type and number of base learners. In terms of empirical analysis, Lending-Club data is used to build a multi-classification model. The experimental results show that compared with the single learner model, the AHC-Metropolis selective ensemble algorithm has better performance and stability in predicting multi-classification problems, and can provide a basis for improving the financial risk control system and ensuring national security.}
}
@article{ABDOUNE20222545,
title = {Integration of Artificial Intelligence in the life cycle of industrial Digital Twins},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2545-2550},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.092},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021012},
author = {Farah Abdoune and Maroua Nouiri and Olivier Cardin and Pierre Castagna},
keywords = {Digital Twin, Lifecycle, Artificial Intelligence, Machine Learning, Reinforcement learning, Unsupervised learning},
abstract = {Digital twins (DT) constitute a major concept of future industrial systems. They are expected to enable efficient virtualization of manufacturing systems and enhance various decision-making processes. In parallel, many initiatives exhibited how artificial intelligence (AI) could increase the performance of the DT on specific applications. By reviewing the literature combining AI and DT, a lack of contributions on the whole life cycle of the DT was exhibited. Therefore, the main contribution of this paper is to define a global integration framework of AI into DT, focused on the exploitation phase of the DT. A case study, using a relatively simple physical twin, illustrates the potential of such integration for the response of the DT to unpredictable modifications of the physical twin.}
}
@article{YI2022552,
title = {A vision-based human-robot collaborative system for digital twin},
journal = {Procedia CIRP},
volume = {107},
pages = {552-557},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003080},
author = {Shuming Yi and Sichao Liu and Xiaohu Xu and Xi Vincent Wang and Sijie Yan and Lihui Wang},
keywords = {Robot, assembly, human-robot collaboration, digital twin},
abstract = {Flexible and safe human-robot collaboration depends on accurately capturing the three-dimensional motion of humans and robots in the field of smart manufacturing. In this paper, a novel approach to developing a human-robot collaborative assembly system is proposed and applied to the field of digital twins. Within the context, a deep learning-based model is explored to develop a depth camera-based human recognition system for accurate prediction of key points for human skeletons model and high-precision human localisation in a human-robot collaborative setting. After the functional mapping of robot calibration, a collision warning module leverages coordinates of key human-robot points to facilitate efficient and safe human-robot collaborative assembly.}
}
@article{MUKHOPADHYAY202255,
title = {Virtual-reality-based digital twin of office spaces with social distance measurement feature},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {1},
pages = {55-75},
year = {2022},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000043},
author = {Abhishek Mukhopadhyay and GS Rajshekar Reddy and KamalPreet Singh Saluja and Subhankar Ghosh and Anasol Peña-Rios and Gokul Gopal and Pradipta Biswas},
keywords = {Virtual environment, Digital twin, 3D visualization, Convolutional neural network, Object detection, Social distancing},
abstract = {Background
Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
Methods
In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
Results
We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
Conclusions
A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.}
}
@article{PATWARDHAN2022114,
title = {Federated Learning for Enablement of Digital Twin},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {114-119},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.179},
url = {https://www.sciencedirect.com/science/article/pii/S240589632200180X},
author = {Amit Patwardhan and Adithya Thaduri and Ramin Karim and Miguel Castano},
keywords = {Digital twin, federated learning, LiDAR, point cloud, railway catenary},
abstract = {Creation, maintenance, and update of digital twins are costly and time-consuming mechanisms. The required effort can be optimized with the use of LiDAR technologies, which support the process of collecting data related to spatial information such as location, geometry, and position. Sharing such data in multi-stakeholder environments is hindered due to competition, confidentiality, and security requirements. Multi-stakeholder environments favor the use of decentralized creation and update mechanisms with reduced data exchange. Such mechanisms are facilitated by Federated Learning, where the learning process is performed at the data owner’s location. Two case studies are presented in this paper, where LiDAR is used to extract information from industrial equipment as a part of the creation of a digital twin.}
}
@article{BAKHTADZE20221775,
title = {Digital Predictive Twins for Virtual Stability Analyzers},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1775-1780},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.655},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322019735},
author = {Natalia N. Bakhtadze and Igor B. Yadykin and Evgeny M. Maximov},
keywords = {Lyapunov equation, Gramians, companion forms, spectral decomposing, associative research, digital twins, virtual analyzer},
abstract = {Identification methods are presented for real-time development of discrete intelligent predictive models of dynamic processes for electric power systems. It is shown that digital models created at each time instant based on machine learning can effectively predict the possibility of stability loss for a wide class of nonlinear dynamic processes. The stability of discrete systems is studied on the basis of the Gramian method. In this paper, the stability indices of systems are determined using energy functional. Spectral expansions of functional are obtained, which makes it possible to reveal dominant modes that affect the energy of oscillations in the modes of operation of systems near the stability boundary.}
}
@article{LI2022102321,
title = {AR-assisted digital twin-enabled robot collaborative manufacturing system with human-in-the-loop},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {76},
pages = {102321},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102321},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000102},
author = {Chengxi Li and Pai Zheng and Shufei Li and Yatming Pang and Carman K.M. Lee},
keywords = {Augmented reality, Digital twin, Collaborative manufacturing system, Reinforcement learning, Human-in-the-loop control},
abstract = {The teleoperation and coordination of multiple industrial robots play an important role in today’s industrial internet-based collaborative manufacturing systems. The user-friendly teleoperation approach allows operators from different manufacturing domains to reduce redundant learning costs and intuitively control the robot in advance. Nevertheless, only a few preliminary works have been introduced very recently, let alone its effective implementation in the manufacturing scenarios. To address the gap, this research proposes a novel multi-robot collaborative manufacturing system with human-in-the-loop control by leveraging the cutting-edge augmented reality (AR) and digital twin (DT) techniques. In the proposed system, the DTs of industrial robots are firstly mapped to physical robots and visualize them in the AR glasses. Meanwhile, a multi-robot communication mechanism is designed and implemented, to synchronize the state of robots in the twin. Moreover, a reinforcement learning algorithm is integrated into the robot motion planning to replace the conventional kinematics-based robot movement with corresponding target positions. Finally, three interactive AR-assisted DT modes, including real-time motion control, planned motion control, and robot monitoring mode are generated, which can be readily switched by human operators. Two experimental studies are conducted on (1) a single robot with a commonly used peg-in-hole experiment, and (2) the motion planning of multi-robot collaborative tasks, respectively. From the experimental results, it can be found that the proposed system can well handle the multi-robot teleoperation tasks with high efficiency and owns great potentials to be adopted in other complicated manufacturing scenarios in the near future.}
}
@article{ZHANG2022359,
title = {Reinforcement learning and digital twin-based real-time scheduling method in intelligent manufacturing systems},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {359-364},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.413},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322016986},
author = {Lixiang Zhang and Yan Yan and Yaoguang Hu and Weibo Ren},
keywords = {Real-time scheduling, reinforcement learning, digital twin, intelligent manufacturing},
abstract = {Optimization efficiency and decision-making responsiveness are two conflicting objectives to be considered in intelligent manufacturing. Therefore, we proposed a reinforcement learning and digital twin-based real-time scheduling method, called twins learning, to satisfy multiple objectives simultaneously. First, the interaction of multiple resources is constructed in a virtual twin, including physics, behaviors, and rules to support the decision-making. Then, the real-time scheduling problems are modeled as Markov Decision Process and reinforcement learning algorithms are developed to learn better scheduling policies. The case study indicates the proposed method has excellent adaptability and learning capacity in intelligent manufacturing.}
}
@article{KALANTARI2022104140,
title = {Developing and user-testing a “Digital Twins” prototyping tool for architectural design},
journal = {Automation in Construction},
volume = {135},
pages = {104140},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104140},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522000139},
author = {Saleh Kalantari and Sina Pourjabar and Tong Bill Xu and Julia Kan},
keywords = {Design tool, Physical to digital, Prototyping, Human–computer interaction, Design Process, Computer-aided design},
abstract = {The researchers developed and tested a hybrid physical/digital toolset for architectural prototyping, named “Ph2D,” which allows adjustments in a physical floorplan model to be mirrored and analyzed in a digital platform. The toolset uses a modular approach based on interconnectable tiles (e.g., “wall section with exterior door”). Additional tile pieces beyond the base toolset can be 3D-printed and/or digitally defined by users, allowing for a large degree of customization. The “digital twin” approach combines the utility of hand-on experimentation with automatic digital representation, and it enables analytical tools such as connectivity analysis and energy performance simulation to be conducted simultaneously with the physical design ideation. User-testing (n = 182) indicated significant enthusiasm for the toolset's usability, and showed an association between exposure to the toolset and higher regard for physical prototyping in design. Non-designers expressed a strong interest in experimenting with the tool, indicating potentially effective roles in design education and team-communication.}
}
@article{LI2022123390,
title = {Deep learning based real-time energy extraction system modeling for flapping foil},
journal = {Energy},
volume = {246},
pages = {123390},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.123390},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222002936},
author = {Yunzhu Li and Tianyuan Liu and Yuqi Wang and Yonghui Xie},
keywords = {Flapping foil, Field prediction, Transient flow, Aerodynamics characteristics, Digital twin, Convolutional neural network},
abstract = {Considering the increasing energy consumption and greenhouse gas emissions, the promising energy extraction system via flapping foil for flow and wind energy has attracted more and more attention. Due to the expensive computation resource and time cost, the CFD method impedes the realization of real-time modeling for flapping foil. The surrogate model by machine learning is a promising alternative, but it only focuses on the objective functions and ignores the importance of physical fields. Aiming at providing a comprehensive model to predict the aerodynamic characteristics as well as the physical fields, a deep learning based real-time model containing two modular convolutional neural networks are devised in this paper. With the numerical simulations as training dataset, a well-trained model can accurately predict the pressure and velocity fields as well as the lift and moment coefficients in millisecond. Moreover, the global sensitivity analysis and the optimizations are conducted based on this model. By leveraging the automatic differential mechanics in deep learning method, the time consumption for kinematic optimization is accelerated into a minute, which further demonstrates the real-time capability. Overall, the presented deep learning model can provide a reliable and competitive choice for the digital twin of flapping foil energy extraction system.}
}
@article{KAY2022100069,
title = {Enhancing learning by Open Learner Model (OLM) driven data design},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100069},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100069},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000248},
author = {Judy Kay and Kathryn Bartimote and Kirsty Kitto and Bob Kummerfeld and Danny Liu and Peter Reimann},
keywords = {Open Learner Models (OLMs), Learner models, Student models, Scrutability, Ontologies, Self-regulated learning, Learning analytics},
abstract = {There is a huge and growing amount of data that is already captured in the many, diverse digital tools that support learning. Additionally, learning data is often inaccessible to teachers or served in a manner that fails to support or inform their teaching and design practice. We need systematic, learner-centred ways for teachers to design learning data that supports them. Drawing on decades of Artificial Intelligence in Education (AIED) research, we show how to make use of important AIED concepts: (1) learner models; (2) Open Learner Models (OLMs); (3) scrutability and (4) Ontologies. We show how these concepts can be used in the design of OLMs, interfaces that enable a learner to see and interact with an externalised representation of their learning progress. We extend this important work by demonstrating how OLMs can also drive a learner-centred design process of learning data. We draw on the work of Biggs on constructive alignment (Biggs, 1996, 1999, 2011), which has been so influential in education. Like Biggs, we propose a way for teachers to design the learning data in their subjects and we illustrate the approach with case studies. We show how teachers can use this approach today, essentially integrating the design of learning data along with the learning design for their subjects. We outline a research agenda for designing the collection of richer learning data. There are three core contributions of this paper. First, we present the terms OLM, learner model, scrutability and ontologies, as thinking tools for systematic design of learning data. Second, we show how to integrate this into the design and refinement of a subject. Finally, we present a research agenda for making this process both easier and more powerful.}
}
@incollection{JONES20221525,
title = {Pilot Plant 4.0: A Review of Digitalization Efforts of the Chemical and Biochemical Engineering Department at the Technical University of Denmark (DTU)},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1525-1530},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50254-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596502542},
author = {Mark Nicholas Jones and Mads Stevnsborg and Rasmus Fjordbak Nielsen and Deborah Carberry and Khosrow Bagherpour and Seyed Soheil Mansouri and Steen Larsen and Krist V. Gernaey and Jochen Dreyer and John Woodley and Jakob Kjøbsted Huusom and Kim Dam-Johansen},
keywords = {Digitalization, Database, Process Control, Digital Twin, Machine Learning},
abstract = {The pilot plant at the Chemical and Biochemical Engineering Department at DTU (DTU Kemiteknik, abbrv. DTU KT) serves as a facility for research & education with access to various process equipment, commonly employed in up-and down-stream processes. Among the available equipment are fermenters, membranes, distillation columns, absorbers, desorbers, extractors, crystallizers, chromatography columns and all kind of high-temperature reactors and process equipment for particulates. The equipment is supplemented by mobile demonstration units for use at industrial sites and a large-scale maritime test station. These units are perfectly suited in combination with laboratory facilities to perform scale up studies together with the capabilities of a modern digital infrastructure. Some of the units are only operated manually while other units can be operated through human machine interfaces (HMI). In line with DTU’s strategic objectives, DTU KT focuses on the development and application of an Industry 4.0 framework for its research and educational activities. Therefore, the pilot plant and laboratory facilities are going through a digital transformation, creating a suitable infrastructure that provides remote accessibility to all research and operational data. These efforts are presented in this work.}
}
@article{CANDON2022108809,
title = {Advanced multi-input system identification for next generation aircraft loads monitoring using linear regression, neural networks and deep learning},
journal = {Mechanical Systems and Signal Processing},
volume = {171},
pages = {108809},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.108809},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022000085},
author = {Michael Candon and Marco Esposito and Haytham Fayek and Oleg Levinski and Stephan Koschel and Nish Joseph and Robert Carrese and Pier Marzocca},
keywords = {Structural Health Monitoring, MISO loads monitoring, Dynamic loads, Quasi-static loads, Artificial neural networks, Deep learning, Linear regression},
abstract = {Over the past decade, the ideologies surrounding Structural Health Monitoring (SHM) have shifted drastically within the aerospace engineering disciplines, predominantly onus to rapid advancements in machine intelligence. While traditional SHM practices are based on scheduled and pre-emptive maintenance, the NextGen SHM system, known commonly as Prognostics and Health Management (PHM), has a focus on pro-active condition-based maintenance, forecasting and prognostics — a milestone on the trajectory towards Digital Twin technology. In aircraft, particularly defense fighter air platforms, fatigue-critical high-amplitude cyclic behavior is unavoidable, where rapid fatigue life consumption due to an airframe buffet is one of the most problematic phenomena that engineers have encountered throughout the 4th and 5th generation fighter programs. This paper serves as a point-of-reference consolidating a range of machine learning models, under a single benchmark aircraft Multi-Input Single-Output (MISO) loads monitoring problem. Linear regression models, traditional (shallow) artificial neural networks, and deep learning strategies are all explored, where strain sensors are used as inputs to predict representative bending and torsional dynamic (buffet) and quasi-static (maneuver) load spectra on an aircraft wing during transonic buffeting maneuvers. For the benchmark system considered herein, the MISO coherence ranges from high to very weak depending on the load case, hereby providing a unique opportunity to rigorously explore the time-series modeling requirements and make valuable recommendations across a wide range of data-qualities that are likely to be encountered in traditional or modern aircraft data-acquisition systems or, for that matter, in any mechanical systems plagued by fatigue.}
}
@article{BASU20221,
title = {Biomaterialomics: Data science-driven pathways to develop fourth-generation biomaterials},
journal = {Acta Biomaterialia},
volume = {143},
pages = {1-25},
year = {2022},
issn = {1742-7061},
doi = {https://doi.org/10.1016/j.actbio.2022.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S1742706122001039},
author = {Bikramjit Basu and N.H. Gowtham and Yang Xiao and Surya R. Kalidindi and Kam W. Leong},
abstract = {Conventional approaches to developing biomaterials and implants require intuitive tailoring of manufacturing protocols and biocompatibility assessment. This leads to longer development cycles, and high costs. To meet existing and unmet clinical needs, it is critical to accelerate the production of implantable biomaterials, implants and biomedical devices. Building on the Materials Genome Initiative, we define the concept ‘biomaterialomics’ as the integration of multi-omics data and high-dimensional analysis with artificial intelligence (AI) tools throughout the entire pipeline of biomaterials development. The Data Science-driven approach is envisioned to bring together on a single platform, the computational tools, databases, experimental methods, machine learning, and advanced manufacturing (e.g., 3D printing) to develop the fourth-generation biomaterials and implants, whose clinical performance will be predicted using ‘digital twins’. While analysing the key elements of the concept of ‘biomaterialomics’, significant emphasis has been put forward to effectively utilize high-throughput biocompatibility data together with multiscale physics-based models, E-platform/online databases of clinical studies, data science approaches, including metadata management, AI/ Machine Learning (ML) algorithms and uncertainty predictions. Such integrated formulation will allow one to adopt cross-disciplinary approaches to establish processing-structure-property (PSP) linkages. A few published studies from the lead author's research group serve as representative examples to illustrate the formulation and relevance of the ‘Biomaterialomics’ approaches for three emerging research themes, i.e. patient-specific implants, additive manufacturing, and bioelectronic medicine. The increased adaptability of AI/ML tools in biomaterials science along with the training of the next generation researchers in data science are strongly recommended.
Statement of significance
This leading opinion review paper emphasizes the need to integrate the concepts and algorithms of the data science with biomaterials science. Also, this paper emphasizes the need to establish a mathematically rigorous cross-disciplinary framework that will allow a systematic quantitative exploration and curation of critical biomaterials knowledge needed to drive objectively the innovation efforts within a suitable uncertainty quantification framework, as embodied in ‘biomaterialomics’ concept, which integrates multi-omics data and high-dimensional analysis with artificial intelligence (AI) tools, like machine learning. The formulation of this approach has been demonstrated for patient-specific implants, additive manufacturing, and bioelectronic medicine.}
}
@article{GREIS20221912,
title = {Towards Learning-Enabled Digital Twin with Augmented Reality for Resilient Production Scheduling},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1912-1917},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.678},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322019966},
author = {Noel P. Greis and Monica L. Nogueira and Wolfgang Rohde},
keywords = {Digital twin, Augmented reality, Machine learning, Cognitive digital twin, Production},
abstract = {This paper explores the current roles and future potential of augmented reality and cognitive capabilities for digital twins. Machine learning models are increasingly powering digital twins because of their ability to capture the complexity of the physical world with fidelity. However, machine learning models fall short when predicting beyond the experience of past data. Today's production systems and supply chains are navigating disruptions that they have not experienced previously. This research is motivated by the necessity to explore methods for managing large-scale disruptions outside the “learned” experience of both the data and the control strategy of the production system. To explore the interaction of human and machine intelligence in managing disruptions, this paper builds on ongoing work by the authors to develop a framework integrating production and logistics processes that employs a machine learning-enabled digital twin to ensure adaptive production scheduling and resilient supply chain operations.}
}
@article{LIN2022115305,
title = {Structure health monitoring of a composite wing based on flight load and strain data using deep learning method},
journal = {Composite Structures},
volume = {286},
pages = {115305},
year = {2022},
issn = {0263-8223},
doi = {https://doi.org/10.1016/j.compstruct.2022.115305},
url = {https://www.sciencedirect.com/science/article/pii/S026382232200112X},
author = {Minxiao Lin and Shijun Guo and Shun He and Wenhao Li and Daqing Yang},
keywords = {Composite wing, Structural health monitoring, Convolutional neural network, Digital twin},
abstract = {An investigation was made into a method for Structural Health Monitoring (SHM) of a composite wing using Convolutional Neural Network (CNN) model. In this method, various aerodynamic loads of an aircraft during flight and corresponding strain data were used for CNN model training. The proposed method was demonstrated by numerical simulation using vortex lattice method for aerodynamic loads of an A350-type aircraft in over a thousand flight conditions and a Finite Element (FE) model as a digital twin of the full-scale composite wing. To represent the measurement of 324 sensors mounted in the 18 skin-rib joints of the inboard wing, strain data from the 18 × 18 elements of the FE model in the sensor locations were calculated corresponding to the flight loadings. The strain data from the original structure FE model were employed to train a CNN model that was classified as healthy samples. Damaged elements were then introduced in random locations to produce data samples corresponding to the same set of flight loads for the CNN model training. In the subsequent damage detection process using the trained CNN model, confusion matrix, uncertainty and sensitivity analysis were evaluated. The study results show that robust damage detection results can be obtained with 99% accuracy without noise and 97% accuracy with 2% Gaussian noise. In the damage localization process, threshold value was set at 1.5, 2 or 2.5, and 83% overall accuracy was achieved using the CNN model when the threshold value was 1.5. The study demonstrated that the proposed method is efficient, accurate and robust.}
}
@article{HU2022101562,
title = {Mutual information-enhanced digital twin promotes vision-guided robotic grasping},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101562},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101562},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622000350},
author = {Fuwen Hu},
keywords = {Robotic grasping, Machine vision, Digital twin, Mutual information, Smart manufacturing},
abstract = {Vision-guided learning for autonomous robotic manipulations is a wide-ranging and high-impact topic in the context of smart manufacturing. Most learning strategies are object-centered or prior information-dependent, which likely lead to the problems of generalization across objects or scenes. To alleviate this, this work presented an embodiment decision-making method by the marriage between the digital twin epistemology and information-theoretic approach. The initial insight was that the mutual information generated in the interactions between the available vision models and real-world perceptions could decrease the uncertainty of sensing-action processes. Further, the real-time interactive information gains and visual templates constitute the digital twin through bidirectional data flowing and real-time optimization. As a demonstration of concept, on the conveyor-based and vision-guided robotic grasping platform, the robotic grasping experiments of freely placed and moving parts were performed. Experimental results indicated that the autonomous and real-time optimization of the conveyor-based and vision-guided robotic grasping system happens and the adaptability to the real-world changes had been clearly increased. This research suggested that the representation and dynamic capture of the complex interactions between both sides of cyber-physical system could generate new possibilities to the evolution of decision-making paradigm in more complex industrial processes.}
}
@article{SHI20221867,
title = {A Cognitive Digital Twins Framework for Human-Robot Collaboration},
journal = {Procedia Computer Science},
volume = {200},
pages = {1867-1874},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.387},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003969},
author = {Yanjun Shi and Weiming Shen and Lihui Wang and Francesco Longo and Letizia Nicoletti and Antonio Padovano},
keywords = {Digital twin, human-robot collaboration, human cyber-physical system, 5G communication network, edge-cloud collaboration},
abstract = {This paper proposes a cognitive digital twin framework for smart manufacturing, and especially for human-robot-collaboration cases. The proposed framework comprises three layers (field, edge, and cloud layers) based on the 5G communication network. In the field layer, the physical twin’s data from the physical machine and human operators are transmitted through the edge layer and then to the cloud layer to virtualize the digital twin. The cloud layer generates inference model generation by deep learning training and updates the inference model in the edge layer to make the field’s machine smart. Especially, human operators’ models are built based on the multimodal fusion in the cloud layer for cognitive function. Also, edge-cloud collaborative computing is presented to implement the proposed framework. Finally, the study is validated with a human-robot-collaboration case involving 5G edge computing.}
}
@article{NISTALNUNO2022100077,
title = {Medication recommendation system for online pharmacy using an adaptive user interface},
journal = {Computer Methods and Programs in Biomedicine Update},
volume = {2},
pages = {100077},
year = {2022},
issn = {2666-9900},
doi = {https://doi.org/10.1016/j.cmpbup.2022.100077},
url = {https://www.sciencedirect.com/science/article/pii/S2666990022000283},
author = {Beatriz Nistal-Nuño},
keywords = {Artificial intelligence, Medication, Recommender system, User-adaptive system, Bayesian network},
abstract = {Background
This article proposes a prototype of a user-adaptive system for helping patients to obtain their ambulatory prescribed medications when purchasing online in a more convenient manner than traditional methods, and the adoption of artificial intelligence to achieve improvements. The system developed simulates an online pharmacy with an introductory adaptive user interface using Bayesian user modeling for predicting the medication needs of patients. This program is used to show its step-by-step design and functioning.
Methods
The introductory adaptive user interface was developed on Visual C++ of Microsoft Visual Studio. The patient model acquisition and application implementing the learning and inference was performed with a Bayesian Network. The Bayesian network was elaborated with the GeNIe Modeler software, Version 2.3.R4, provided by BayesFusion, LLC. Synthetic data from a synthetically generated dataset of anonymous patients was used. The performance of the system was evaluated through simulations using testing data from the synthetic dataset. The Accuracy of predictions was analyzed.
Results
The Average accuracy was estimated with the average correct recommendations of medications, for different numbers of purchased medications per session. The Average accuracy increased with the number of purchased medications, from 86.3529% up to 92.5303%. The Average wrong recommendations decreased with the increase in the number of purchased medications, from an average of 3.4117 up to 1.5686.
Conclusion
The system quickly and consistently attained high accuracy in predicting the medication categories needed by the patients, potentially being able to save time and effort for the patients by relying on the system's recommendations.}
}
@article{REGONA202216,
title = {Artificial Intelligent Technologies for the Construction Industry: How Are They Perceived and Utilized in Australia?},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {8},
number = {1},
pages = {16},
year = {2022},
issn = {2199-8531},
doi = {https://doi.org/10.3390/joitmc8010016},
url = {https://www.sciencedirect.com/science/article/pii/S2199853122010253},
author = {Massimo Regona and Tan Yigitcanlar and Bo Xia and Rita Yi Man Li},
keywords = {industry 4.0, automation, robotics, digital twin, machine learning, big data, social media analytics},
abstract = {ABSTRACT
Artificial intelligence (AI) is a powerful technology that can be utilized throughout a construction project lifecycle. Transition to incorporate AI technologies in the construction industry has been delayed due to the lack of know-how and research. There is also a knowledge gap regarding how the public perceives AI technologies, their areas of application, prospects, and constraints in the construction industry. This study aims to explore AI technology adoption prospects and constraints in the Australian construction industry by analyzing social media data. This study adopted social media analytics, along with sentiment and content analyses of Twitter messages (n = 7906), as the methodological approach. The results revealed that: (a) robotics, internet-of-things, and machine learning are the most popular AI technologies in Australia; (b) Australian public sentiments toward AI are mostly positive, whilst some negative perceptions exist; (c) there are distinctive views on the opportunities and constraints of AI among the Australian states/territories; (d) timesaving, innovation, and digitalization are the most common AI prospects; and (e) project risk, security of data, and lack of capabilities are the most common AI constraints. This study is the first to explore AI technology adoption prospects and constraints in the Australian construction industry by analyzing social media data. The findings inform the construction industry on public perceptions and prospects and constraints of AI adoption. In addition, it advocates the search for finding the most efficient means to utilize AI technologies. The study helps public perceptions and prospects and constraints of AI adoption to be factored in construction industry technology adoption.}
}
@article{PANAGOU2022289,
title = {Explorative hybrid digital twin framework for predictive maintenance in steel industry},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {40},
pages = {289-294},
year = {2022},
note = {1st IFAC Workshop on Control of Complex Systems COSY 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.01.087},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323000940},
author = {Sotirios Panagou and Fabio Fruggiero and Carmen {del Vecchio} and Kisan Sarda and Fernando Menchetti and Luca Piedimonte and Oreste Riccardo Natale and Salvatore Passariello},
keywords = {Digital Twin, Machine Learning, steel industry, Preventive Maintenance, Industrial Internet of Things},
abstract = {Manufacturing systems in steel industries are characterized by high complexity and high temperature and pressure conditions during production. Industries have to speed up their production to meet the market's demand for products in a fast changing economy. To prevent breakdowns in the manufacturing lines and further economic loss, steel industries utilize preventive maintenance approach and early replacement of equipment, which is expensive and not optimal. Preventive maintenance can be beneficial in the steel industry and reduce costs, if it is supported by information gathered from previous breakdowns in the production line, such as condition of equipment, environment and further data that can be collected. In this work, historical data and data collected from a digital twin representation of the manufacturing line from Pittini, a steel making industry in Italy, were utilized to gain information on the conditions before a breakdown in the production line. Furthermore, we present a cloud based framework created by utilizing the information and data for optimization and real-time driven preventive maintenance approach and remote control.}
}
@article{ZEB2022103309,
title = {Industrial digital twins at the nexus of NextG wireless networks and computational intelligence: A survey},
journal = {Journal of Network and Computer Applications},
volume = {200},
pages = {103309},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103309},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002988},
author = {Shah Zeb and Aamir Mahmood and Syed Ali Hassan and MD. Jalil Piran and Mikael Gidlund and Mohsen Guizani},
keywords = {Industry 4.0, Digital twin, Industrial Internet of things, Cyber–physical systems, Machine learning, Artificial intelligence, Computational intelligence, Multi-access edge computing, 5G-and-Beyond/6G, Green communication, Age of information},
abstract = {By amalgamating recent communication and control technologies, computing and data analytics techniques, and modular manufacturing, Industry 4.0 promotes integrating cyber–physical worlds through cyber–physical systems (CPS) and digital twin (DT) for monitoring, optimization, and prognostics of industrial processes. A DT enables interaction with the digital image of the industrial physical objects/processes to simulate, analyze, and control their real-time operation. DT is rapidly diffusing in numerous industries with the interdisciplinary advances in the industrial Internet of things (IIoT), edge and cloud computing, machine learning, artificial intelligence, and advanced data analytics. However, the existing literature lacks in identifying and discussing the role and requirements of these technologies in DT-enabled industries from the communication and computing perspective. In this article, we first present the functional aspects, appeal, and innovative use of DT in smart industries. Then, we elaborate on this perspective by systematically reviewing and reflecting on recent research trends in next-generation (NextG) wireless technologies (e.g., 5G-and-Beyond networks) and design tools, and current computational intelligence paradigms (e.g., edge and cloud computing-enabled data analytics, federated learning). Moreover, we discuss the DT deployment strategies at different communication layers to meet the monitoring and control requirements of industrial applications. We also outline several key reflections and future research challenges and directions to facilitate industrial DT’s adoption.}
}
@article{ZHAN2022105766,
title = {Industrial internet of things and unsupervised deep learning enabled real-time occupational safety monitoring in cold storage warehouse},
journal = {Safety Science},
volume = {152},
pages = {105766},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105766},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522001060},
author = {Xuegang Zhan and Wei Wu and Leidi Shen and Wangyunyan Liao and Zhiheng Zhao and Jing Xia},
keywords = {Occupational safety management, Abnormal stationary detection, Deep learning, Indoor positioning, Internet of things, Digital twin, Cold chain logistics},
abstract = {Occupational safety and health (OSH) has always been a big concern in the labor-intensive warehouse industry, especially under peculiar circumstances like a low temperature. Accordingly, this paper aims to propose a framework of a smart system using the Industrial Internet of Things (IIoT) and digital twin (DT) technologies to realize real-time occupational safety monitoring in the warehouse and ensure synchronized cyber-physical spaces for information traceability and visibility. The unsupervised deep neural structure of stacked auto-encoder (SAE) is designed to identify abnormal stationary from human motion status, which is perceived as a sign of potential accident. The model is developed to automatically update online by cooperating with calibration samples so as to keep in accordance with the evolution of surroundings. The Bluetooth low energy (BLE) and a log-distance path loss model are used to fulfill indoor localization in order for managers to promptly respond to an incident on site. Besides, some intelligent services are enabled to promote the efficiency of safety management. A real-life case study is carried out in an air cargo cold storage warehouse to illustrate the viability and rationality of the proposed system and methods. The elaboration of the implementation is envisioned to facilitate replication and reproduction effectively. The impact of learning features concerned with distance and vibration on the performance of anomaly detection has also been analyzed by experiments. The insights and lessons gained in this study hold the promise of providing a reference or sparking new ideas for researchers and practitioners to meet similar needs in practice.}
}
@article{ZHAO2022433,
title = {Communication-Efficient Federated Learning for Digital Twin Systems of Industrial Internet of Things},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {433-438},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.232},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322002336},
author = {Yunming Zhao and Li Li and Ying Liu and Yuxi Fan and Kuo-Yi Lin},
keywords = {Federated Learning, Digital Twins, Industrial Internet of Things, Communication-Efficient, Intelligent Manufacturing},
abstract = {With the rapid development and deployment of Industrial Internet of Things technology, it promotes interconnection and edge applications in smart manufacturing. However, challenges remain, such as yet-to-improve communication efficiency and trade-offs between computing power and energy consumption, which limits the application and further development of IIoT technology. This paper proposes the digital twin systems into the IIoT to build model between physical objects and digital virtual systems to optimize the structure of IIoT. And we further introduce federal learning to train the digital twins model and to improve the communication efficiency of IIoT. In this paper, we first establish the digital twins model of IIoT based on industrial scenario. Moreover, to optimize the communication overhead allocation problem, this paper proposes an improved communication-efficient distribution algorithm, which speeds up the training performance of federated model and ensures the performance of industrial system model by changing the update training mode of client and server and allowing some industrial equipment to participate in federated training. This paper simulates the real-word intelligent camera detection to validate the proposed method. Comparing our proposed method with the existing traditional methods, the results show the advantages of the proposed method can improve the communication performance of the training model.}
}
@article{OEDA20222283,
title = {Data Preprocessing with IRM for skill extraction using NMF},
journal = {Procedia Computer Science},
volume = {207},
pages = {2283-2290},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.287},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922011759},
author = {Shinichi Oeda and Shunsuke Iizumi},
keywords = {NMF(Non-negative Matrix Factorization), IRM(Infinite Relational Model), Co-clustering, Q-matrix, Student-Modeling, Skill-Modeling},
abstract = {To provide examinees with appropriate question items, identifying and evaluating users' latent and acquired skills is important in e-learning systems. These latent skills are defined in terms of a Q-matrix. Constructing Q-matrices manually is relatively labor-intensive, because doing so requires expert insight as well as verbal examination to identify which skills students have mastered. Methods to extract a Q-matrix from examination results automatically using nonnegative matrix factorization (NMF) have been explored, and improved methods such as online NMF have been proposed, which extracts an immutable Q-matrix from time-series data. In this study, we propose a data preprocessing method to improve the accuracy of NMF using a co-clustering method called an infinite relational model (IRM). We also describe experiments conducted using synthetic data, in which learners' skills and the bias in the number of their responses were evaluated, and show that the results demonstrate the efficacy of the proposed approach.}
}
@article{DRZAJIC202217,
title = {Virtual Operators with Self and Transfer Learning Ability in EDM},
journal = {Procedia CIRP},
volume = {113},
pages = {17-22},
year = {2022},
note = {21st CIRP CONFERENCE ON ELECTRO PHYSICAL AND CHEMICAL MACHINING, ISEM XXIJune, 14 to 17, 2022 in Zurich},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.113},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122013129},
author = {D. Držajić and M. Wiessner and U. Maradia and D. Piga},
keywords = {Iterative Machine Learning, Blackbox Optimization, Electrical Discharge Machining (EDM), Artificial Intelligence},
abstract = {Increasing the manufacturing resource efficiency requires pushing the process limits by customizing the machining parameters to the job at hand. Such optimizations performed usually based on empirical methods and experience due to the sheer complexity of physical modeling. Current research presents a system comprised of advanced machine learning methods, where model-based optimization employed to simultaneously create a digital twin and optimize the process as a self-learning virtual operator using a case study with the EDM processes.}
}
@incollection{KEPRATE2022561,
title = {Chapter Sixteen - Use of digital twins for process safety management},
editor = {Faisal Khan and Hans Pasman and Ming Yang},
series = {Methods in Chemical Process Safety},
publisher = {Elsevier},
volume = {6},
pages = {561-589},
year = {2022},
booktitle = {Methods to Assess and Manage Process Safety in Digitalized Process System},
issn = {2468-6514},
doi = {https://doi.org/10.1016/bs.mcps.2022.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468651422000113},
author = {Arvind Keprate and Nikhil Bagalkot},
keywords = {Process safety management, Digital twin, Physics based modeling, Surrogate modeling, Bayesian network},
abstract = {The chapter discusses the framework for developing a Digital Twin (DT) for the process safety management (PSM) of small-bore piping (SBP) on a typical offshore platform. One of the important problems during the PSM of SBP is that due to significantly large number of SBPs on a process facility it is very difficult to place sensors at small bore connection (SBC) for stress estimation. In absence of the stress values, it is difficult to estimate the remaining fatigue life (RFL) of SBC which further impedes the inspection planning. Thus, in this chapter, a methodology comprising of CFD, FEA and Machine Learning is used to obtain a virtual sensor for estimating stress at the SBC. The input to the virtual sensor is process parameters such as pressure and flow velocity, while the output is the maximum Von-Mises stress at the SBC. Thereafter, probabilistic crack growth law coupled with Bayesian Network is used to develop a DT for RFL estimation of SBP, which in turn is used to obtain reliability curves and inspection plans. Online deployment of the developed DT will give an up-to-date RFL estimates and inspection plans which can be used then be used for PSM of the SBP.}
}
@article{CHRISTOPOULOS20222908,
title = {Transformation of Robotics Education in the Era of Covid-19: Challenges and Opportunities},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2908-2913},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.173},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021875},
author = {Athanasios Christopoulos and Guido Coppo and Salvatore Andolina and Simone Lo Priore and Dario Antonelli and Dimitrios Salmas and Chrysostomos Stylios and Mikko-Jussi Laakso},
keywords = {Robotics, STEM, Higher Education, Blended Learning, Virtual Reality, COVID-19},
abstract = {The COVID-19 pandemic has significantly impacted many aspects of our social and professional life. To this end, Higher Education institutions reacted rather vastly to this unpreceded situation although many issues have been reported in the international literature since the emergence of the first global lockdown. As we are now transitioning back to the ‘normality’, universities and businesses consider the so-called ‘blended’ or ‘hybrid’ model as a means of facilitating the transition phase. In view of this decision, several studies can be identified wherein blended learning scenarios are proposed and described. The present work constitutes such an effort. Precisely, while adjusting the lens to the didactic of Robotics courses, we propose a blended learning model via which the laboratory activities are performed without the physical presence of the students in the physical context. The aforementioned objective is attained under the aid of the Virtual Reality technology coupled with the Digital Twin model. We hope that the ideas presented in this manuscript will motivate and inspire more researchers, instructional designers, and educators to consider the adoption of such alternative instructional techniques to mitigate the shortcomings that the remote education setting brings and further to improve the overall learning experience.}
}
@article{MALEKI202279,
title = {Intelligent Digital Twin in Health Sector: Realization of a Software-Service for Requirements- and Model-based-Systems-Engineering},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {79-84},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.187},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014021},
author = {Samira Maleki and Nasser Jazdi and Behrang Ashtari},
keywords = {Digital Twin, intelligent Digital Twin, Software service, Requirements-Engineering, Model-},
abstract = {The concept of the intelligent Digital Twin focuses on automatic model creation and model extension for a product throughout its lifecycle using artificial intelligence. To create an intelligent Digital Twin, the Digital Twin architecture must also include software services that support machine learning algorithms. These software services must have two characteristics. First, an interface to the real environment to dynamically receive feedback from an existing product or a product under development. Second, machine learning algorithms that analyze and manage the Digital Twin's models based on feedback from the real product. In this paper, we present the concept and implementation of a software service within the intelligent Digital Twin's architecture for automated requirements modeling as well as for modeling of the product system architecture. Since medical devices differ significantly in their applied technology, area of use, and application, the requirement engineering of medical devices needs to identify and specify the critical design requirements for each product individually. This makes automated requirement engineering and Model-based-Systems Engineering of medical devices very challenging. The implemented intelligent software service meets these challenges for product design in the healthcare sector.}
}
@incollection{KARAASLAN2022261,
title = {Chapter 10 - Mixed reality-assisted smart bridge inspection for future smart cities},
editor = {Amir H. Alavi and Maria Q. Feng and Pengcheng Jiao and Zahra Sharif-Khodaei},
booktitle = {The Rise of Smart Cities},
publisher = {Butterworth-Heinemann},
pages = {261-280},
year = {2022},
isbn = {978-0-12-817784-6},
doi = {https://doi.org/10.1016/B978-0-12-817784-6.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128177846000023},
author = {Enes Karaaslan and Mahta Zakaria and F. Necati Catbas},
keywords = {Real-time machine learning, Mixed reality, Human-computer interaction, Structural health monitoring, Smart cities, Smart infrastructure inspection},
abstract = {Smart infrastructures aim more efficient and accurate methods of routine inspection for long-term monitoring of the infrastructure to make smarter decision on maintenance and rehabilitation. Although some recent technologies (i.e., robotic techniques) that are currently in practice can collect objective, quantified data, the inspector’s own expertise is still critical in many instances. Yet, these technologies are designed to replace human expertise, or are ineffective in terms of saving time and labor. This chapter investigates a new methodology for structural inspections with the help of mixed reality technology and real-time machine learning to accelerate certain tasks of the inspector such as detection, measurement, and assessment of defects, and easy accessibility to defect locations. A functional, real-time machine learning system that can be ideally deployed in mixed reality devices and headsets which can be used by inspectors during their routine concrete infrastructure inspection is introduced. The deep learning models to be employed in the AI system can localize a concrete defect in real time and further analyze it by performing pixel wise segmentation while running on a mobile device architecture. First, a sufficiently large database of concrete defect images is gathered from various sources including publicly available crack and spalling datasets, real-world images taken during bridge inspections, and the public images from the internet search results. For defect localization, various state-of-the-art deep learning model architectures are investigated based on their memory allocation, inference speed, and flexibility to deploy different deep learning platforms. YoloV5s model was found to be the optimal model architecture for concrete defect localization to be deployed in the mixed reality system. For defect quantification, several segmentation architectures with three different classification backbones are trained on the collected image dataset with segmentation labels. Based on the model evaluation results, the PSPNet with EfficientNet-b0 backbone is found to be the best performing model in terms of inference speed and accuracy. The selected models for defect localization and quantification are deployed to the mixed reality platform and image tracking libraries are configured in the platform environment, and accurate distance estimation is accomplished using a calibration process. Lastly, a methodology for condition assessment of concrete defects using the mixed reality system is discussed. The proposed methodology can locate and track the defects using the mixed reality platform, which can eventually be transferred to cloud data and potentially used for remote assessments or updating a digital twins or BIMs.}
}
@article{SEPAHVAND202264,
title = {A novel method for reducing arrhythmia classification from 12-lead ECG signals to single-lead ECG with minimal loss of accuracy through teacher-student knowledge distillation},
journal = {Information Sciences},
volume = {593},
pages = {64-77},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522000457},
author = {Majid Sepahvand and Fardin Abdali-Mohammadi},
keywords = {arrhythmia classification, electrocardiogram lead reduction, deep learning, knowledge distillation, lightweight classification},
abstract = {Deep learning models developed through multi-lead electrocardiogram (ECG) signals are considered the leading methods for the automated detection of arrhythmia on computer systems. However, due to the amplitudes of input signals, these models generate too many parameters for practical use. Therefore, they are rarely used on devices with limited computational resources in the newly-emerged technology of the Internet of medical things (IoMT). Knowledge distillation was utilized in this paper to propose a method for bridging the gap between the arrhythmia classification model with multi-lead ECG signals and the arrhythmia classification model with single-lead ECG signals by minimizing the performance decline. The proposed method consists of a teacher model with advanced architecture and a student model with simple architecture. The teacher model was already developed through multi-lead ECG signals, whereas the student model was developed through single-lead signals under the supervision of the teacher. Despite its simplicity, the student model receives the dark knowledge of multi-lead ECG signals from the teacher by imitating the teacher’s behavior in the development process. According to the results, the student model was nearly 262.18 times more compressed than its teacher. Moreover, the student experienced approximately 0.81% of accuracy decline in Chapman ECG with 10646 patients.}
}
@incollection{AGOSTINELLI2022149,
title = {Chapter six - Digital twin predictive maintenance strategy based on machine learning improving facility management in built environment},
editor = {Ammar H. Elsheikh and Mohamed Elasyed {Abd Elaziz}},
booktitle = {Artificial Neural Networks for Renewable Energy Systems and Real-World Applications},
publisher = {Academic Press},
pages = {149-158},
year = {2022},
isbn = {978-0-12-820793-2},
doi = {https://doi.org/10.1016/B978-0-12-820793-2.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207932000070},
author = {S. Agostinelli and A. Heydari},
keywords = {Machine learning, predictive maintenance, digital twin},
abstract = {Predictive maintenance is a concept linked to Industry 4.0, the fourth industrial revolution, which monitors the performance and condition of equipment during normal operation to reduce failure rates. This chapter deals with a predictive maintenance strategy to reduce mechanical and electrical plants malfunctioning for residential technical plant systems. The developed strategy can guarantee a tailored maintenance service based on machine learning systems, drastically reducing breakdowns after a maximum period of 3 years. The developed strategy evaluates an acceptable components failure rate based on statistical data and combining the average labor costs with the duration of each maintenance operation. The predictive strategies are elaborated on the minimum cost increase necessary to achieve the abovementioned objectives. A case study based on a 3-year period has been developed on a modern residential district in Rome comprised of 16 buildings and 911 apartments. In particular, the analysis has been performed considering mechanical, electrical, and lighting systems supplying the external and common areas, excluding the apartments to avoid data perturbation due to differenced users’ behaviors. The overall benefits of predictive maintenance management through Big Data analysis have proven to substantially improve the overall operation of different plants such as mechanical and electrical plants of residential systems.}
}
@article{ZHANG2022102804,
title = {A survey on QoT prediction using machine learning in optical networks},
journal = {Optical Fiber Technology},
volume = {68},
pages = {102804},
year = {2022},
issn = {1068-5200},
doi = {https://doi.org/10.1016/j.yofte.2021.102804},
url = {https://www.sciencedirect.com/science/article/pii/S1068520021003540},
author = {Lu Zhang and Xin Li and Ying Tang and Jingjie Xin and Shanguo Huang},
keywords = {Machine learning, Optical networks, QoT prediction},
abstract = {In optical networks, a connection (e.g., light-path and light-tree) is set up to carry data from its source to destination(s). When the optical signal transmits through the fiber links and optical devices, the quality of transmission (QoT) degrades due to various physical layer impairments (PLIs), including linear and nonlinear impairments. QoT is an important metric that determines the availability of a connection. Therefore, the QoT guarantee is the premise of successful connection establishment in optical networks. QoT prediction before connections establishment can provide guidance for the routing and resources allocation of connections. In order to receive the correct signal at the receiving end, during network planning design margins are introduced to compensate the inaccuracy of the QoT prediction model itself and its inputs. Improving the accuracy of prediction can make better use of network resources and reduce margins. With the help of strong computing power and data acquisition based on software defined optical network (SDON), machine learning (ML) based models are more suitable for QoT prediction than analytical models that are difficult to derive and computationally heavy. This paper provides an overview on the applications of ML technologies in QoT prediction. Firstly, we elaborate the QoT problem in optical networks, including main QoT influence factors, QoT metrics, and QoT prediction strategies. Then, suitable ML algorithms, the generation of sample data, ML frameworks and the construction of QoT prediction model, are briefly introduced. Next, three solutions of QoT prediction using various ML technologies in recent studies and their practical feasibility are reviewed and discussed in detail. Finally, based on the existing researches, we present some future research directions about the improvement of QoT prediction.}
}
@article{LIN2022108715,
title = {Digital-twin-based improvements to diagnosis, prognosis, strategy assessment, and discrepancy checking in a nearly autonomous management and control system},
journal = {Annals of Nuclear Energy},
volume = {166},
pages = {108715},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2021.108715},
url = {https://www.sciencedirect.com/science/article/pii/S0306454921005910},
author = {Linyu Lin and Paridhi Athe and Pascal Rouxelin and Maria Avramova and Abhinav Gupta and Robert Youngblood and Jeffrey Lane and Nam Dinh},
keywords = {autonomous control, digital twin, diagnosis, prognosis},
abstract = {The Nearly Autonomous Management and Control System (NAMAC) is a comprehensive control system that assists plant operations by furnishing control recommendations to operators in a broad class of situations. This study refines a NAMAC system for making reasonable recommendations during complex loss-of-flow scenarios with a validated Experimental Breeder Reactor II simulator, digital twins improved by machine-learning algorithms, a multi-attribute decision-making scheme, and a discrepancy checker for identifying unexpected recommendation effects. We assess the performance of each NAMAC component, while we demonstrate and evaluated the capability of NAMAC in a class of loss-of-flow scenarios.}
}
@article{MA2021220,
title = {Progressive Mimic Learning: A new perspective to train lightweight CNN models},
journal = {Neurocomputing},
volume = {456},
pages = {220-231},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.086},
url = {https://www.sciencedirect.com/science/article/pii/S092523122100638X},
author = {Hongbin Ma and Shuyuan Yang and Dongzhu Feng and Licheng Jiao and Luping Zhang},
keywords = {Lightweight CNN models, Progressive Mimic Learning, Knowledge distillation},
abstract = {Knowledge distillation (KD) builds a lightweight Student Model (SM) and trains it to approximate a large Teacher Model (TM) by exploring knowledge learned by the TM, which shows effectiveness to train lightweight CNN models. However, training a small SM to achieve better performance remains a challenging problem. Recent researches on human learning behaviors show that both the knowledge from teachers and the knowledge learning processes of teachers are significant for students. Inspired by this characteristic, in this paper, we propose a new perspective, called Progressive Mimic Learning (PML), to train lightweight CNN models by mimicking the learning trajectory of the TM. In order to obtain a more powerful SM, the useful hints in the learning process of the TM are explored. To start with, the TM learning process is divided into multiple stages, and the last state of the TM in each stage is recorded as a landmark. The learning trajectory of the TM is composed of these landmarks. Then, a landmark loss is defined to constrain the SM to progressively mimic the learning process of the TM, by employing landmarks in the learning trajectory as a training hint of the SM. Several experiments are conducted on four benchmark data sets, CIFAR-10, CIFAR-100, Fashion-MNIST, and ImageNet-10, to investigate the performance of the PML. The results show that the PML can make SMs generate more accurate predictions than SMs trained by its counterparts.}
}
@article{HALLAJI2022104049,
title = {Predictive maintenance of pumps in civil infrastructure: State-of-the-art, challenges and future directions},
journal = {Automation in Construction},
volume = {134},
pages = {104049},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104049},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005008},
author = {Seyed Mostafa Hallaji and Yihai Fang and Brandon K. Winfrey},
keywords = {Pump, Predictive maintenance, Digital twin, Building information modelling, Machine learning},
abstract = {Predictive maintenance (PdM) is a technique that employs data-driven analysis to detect anomalous working conditions and predict future failure risks of assets. Despite wide applications in the manufacturing and oil and gas industries, the application of PdM in infrastructure facilities, such as wastewater treatment plants, is scarce. Recent advent of information and communication technologies and artificial intelligence presents a great opportunity to enhance the practice in infrastructure maintenance by integrating PdM techniques. This study aims to investigate the potentials and challenges of integrating emerging technologies in the PdM of pumps. A quantitative review of the literature was conducted to identify primary research themes and knowledge domains. A qualitative review was conducted to assess their potentials for realizing PdM in pump maintenance. Findings from this research are expected to point out key technical and practical challenges and future research directions.}
}
@article{LI2021529,
title = {A data-based soft-sensor approach to estimating raceway depth in ironmaking blast furnaces},
journal = {Powder Technology},
volume = {390},
pages = {529-538},
year = {2021},
issn = {0032-5910},
doi = {https://doi.org/10.1016/j.powtec.2021.05.072},
url = {https://www.sciencedirect.com/science/article/pii/S0032591021004812},
author = {Wangyan Li and Yuting Zhuo and Jie Bao and Yansong Shen},
keywords = {Raceway, Soft-sensor, Ironmaking blast furnace, Principal component analysis (PCA), Support vector machine (SVM), Thermal image},
abstract = {Raceway is a key region in ironmaking blast furnace (BF). While the raceway depth is extremely difficult to measure, thermal images near tuyeres may be available. In this study, inspired by the concept of digital-twin, a soft-sensor approach is proposed to estimate the raceway depth from thermal images. This approach includes (1) The representative thermal images are generated through a raceway CFD model under industry-scale conditions of a specific BF; (2) A principal component analysis (PCA) method is used to reduce data dimension and extract key features from the thermal images of high dimension; (3) A model-learning tool, support vector machine (SVM) is developed to learn the underlying data-driven soft-sensor model between extracted features from PCA and raceway depth from CFD simulations. The result shows that the soft-sensor model can effectively capture the latent relationship between thermal images and the raceway depth, which can be used to estimate raceway depth in real-time in practice.}
}
@article{WERNECK2021114775,
title = {Effective and diverse POI recommendations through complementary diversification models},
journal = {Expert Systems with Applications},
volume = {175},
pages = {114775},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114775},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002165},
author = {Heitor Werneck and Rodrigo Santos and Nícollas Silva and Adriano C.M. Pereira and Fernando Mourão and Leonardo Rocha},
keywords = {Recommender systems and algorithms, Knowledge management, Point-of-interest, Location-based social networks, Diversity},
abstract = {Nowadays, recommender systems play an important role in several Location-Based Social Networks (LBSNs). The current advances have considered the trade-off between accuracy and diversity to help users to discover and explore new points-of-interest (POI). However, differently from traditional recommendation scenarios, other equally relevant dimensions (e.g., social and geographical user information) have to be considered to understand how the characteristics of services offered by each POI fit the user needs. Specifically, this work sheds light upon naive failures introduced by traditional recommendation methods while they handle this trade-off between diversity and accuracy in POI recommendations. We hypothesize that some efforts on POI recommendations somehow are deviating from basic learnings from the area. In this context, this work addresses four characteristics inherent to the POI domain that previous efforts have failed to recognize: (1) POI categories and locations are complementary dimensions of diversification that should be simultaneously addressed; (2) Diversity is a complex concept that should be modeled by distinct and non-orthogonal models; (3) Distinct users have different biases and willingness to move to fulfill their needs; (4) POI recommendation is a multi-objective task. In order to demonstrate the gains of properly addressing these aspects, we also propose DisCovER, a straightforward re-ordering method that linearly combines geographical and categorical diversification. DisCovER results demonstrate that even simple strategies to exploit simultaneously these complementary dimensions can increase diversification while keeping accuracy high. Differently from state-of-the-art diversification methods, DisCovER does not penalize any quality dimension in favor of others. It allows us to discuss future directions towards more robust user modeling and preference elicitation in POI domains.}
}
@article{CHAKRABARTY2021111460,
title = {Scalable Bayesian optimization for model calibration: Case study on coupled building and HVAC dynamics},
journal = {Energy and Buildings},
volume = {253},
pages = {111460},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111460},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821007441},
author = {Ankush Chakrabarty and Emilio Maddalena and Hongtao Qiao and Christopher Laughman},
keywords = {Parameter estimation, Gaussian processes, Bayesian methods, Sensitivity analysis, Digital twins, Building energy systems, Probabilistic machine learning},
abstract = {Model calibration for building systems is a key step to achieving accurate and reliable predictions that reflect the dynamics of real systems under study. Calibration becomes particularly challenging when integrating building and HVAC dynamics, due to large-scale, nonlinear, and stiff underlying differential algebraic equations. In this paper, we describe a framework for calibrating multiple parameters of coupled building/HVAC models using scalable Bayesian optimization (BO), whose advantages include global optimization without requiring gradient information, and data-efficiency. The proposed methodology is improved online via two additional steps: domain tightening and domain slicing, both of which leverage the learned calibration cost function to reduce the search space volume and dimension, respectively. We demonstrate effectiveness of the proposed algorithm by simultaneously calibrating 17 parameters (including emissivities, heat transfer coefficients, and thickness of walls/floors) of a Modelica model of joint building and HVAC dynamics, with 2 weeks worth of building data. This high-dimensional calibration task is solved via our proposed scalable BO calibration method, and yields parameters that are >90% accurate with <1000 model simulations; additionally, the outputs of the final calibrated model on unseen testing data complies with standard ASHRAE calibration guidelines.}
}
@article{CHOI2022102258,
title = {An integrated mixed reality system for safety-aware human-robot collaboration using deep learning and digital twin generation},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {73},
pages = {102258},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102258},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521001381},
author = {Sung Ho Choi and Kyeong-Beom Park and Dong Hyeon Roh and Jae Yeol Lee and Mustafa Mohammed and Yalda Ghasemi and Heejin Jeong},
keywords = {Human-robot collaboration, Mixed reality, Deep learning, Safety distance calculation, Digital twin, Cyber-physical system},
abstract = {For human-robot collaboration (HRC), one of the most practical methods to ensure human safety with a vision-based system is establishing a minimum safe distance. This study proposes a novel integrated mixed reality (MR) system for safety-aware HRC using deep learning and digital twin generation. The proposed approach can accurately measure the minimum safe distance in real-time and provide MR-based task assistance to the human operator. The approach integrates MR with safety-related monitoring by tracking the shared workplace and providing user-centric visualization through smart MR glasses for safe and effective HRC. Two RGB-D sensors are used to reconstruct and track the working environment. One sensor scans one area of the physical environment through 3D point cloud data. The other also scans another area of the environment and tracks the user's 3D skeletal information. In addition, the two partially scanned environments are registered together by applying a fast global registration method to two sets of the 3D point cloud. Furthermore, deep learning-based instance segmentation is applied to the target object's 3D point cloud to increase the registration between the real robot and its virtual robot, the digital twin of the real robot. While only 3D point cloud data are widely used in previous studies, this study proposes a simple yet effective 3D offset-based safety distance calculation method based on the robot's digital twin and the human skeleton. The 3D offset-based method allows for real-time applicability without sacrificing the accuracy of safety distance calculation for HRI. In addition, two comparative evaluations were conducted to confirm the originality and advantage of the proposed MR-based HRC.}
}
@article{WANG2021103786,
title = {Semi-supervised semantic segmentation network for surface crack detection},
journal = {Automation in Construction},
volume = {128},
pages = {103786},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103786},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002375},
author = {Wenjun Wang and Chao Su},
keywords = {Deep learning, Convolutional neural network, Semi-supervised network, Crack detection, Pixel-wise segmentation},
abstract = {The detection of surface crack is essential to ensure the safety and the serviceability of civil infrastructure. The automatic method is highly efficient and the test results are objective, which makes it gradually replace conventical manual inspection. Recently, semantic segmentation algorithms based on deep learning have shown excellent performance in crack detection tasks. However, the commonly used fully supervised segmentation method requires manual annotation of large amounts of data, which is time-consuming. In order to solve this problem, we propose a semi-supervised semantic segmentation network for crack detection. The proposed method consists of student model and teacher model. The two models have the same network structure and use the EfficientUNet to extract multi-scale crack feature information, reducing the loss of image information. The student model updates weights through the gradient descent of loss function, and the teacher model uses the exponential moving average weights of the student model. During training, the robustness of the model is improved by adding noise to the input data. When using only 60% of the annotated data, our method achieves an F1 score of 0.6540 on the concrete crack dataset and 0.8321 on the Crack500 dataset. The results show that our method can greatly reduce the workload of annotation while maintaining high accuracy.}
}
@article{LEUNG2022108353,
title = {From traditional warehouses to Physical Internet hubs: A digital twin-based inbound synchronization framework for PI-order management},
journal = {International Journal of Production Economics},
volume = {244},
pages = {108353},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108353},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321003297},
author = {Eric K.H. Leung and Carmen Kar Hang Lee and Zhiyuan Ouyang},
keywords = {Hyperconnected city logistics, Physical internet, Synchronization, Joint order fulfillment and replenishment, Digital twins, Internet of things},
abstract = {Physical Internet (PI) is a new concept to ensure global mobility of physical objects. Conventionally, logistics networks are closed and independent. Under the concept of PI, they are transformed into an open logistics network, providing an efficient way to relocate physical goods to a given place in a short period of time. A hyperconnected city logistics system is conceptualized as the final segment of a PI-network. It uses regional and city hubs as the final leg of last-mile delivery. Inventory at PI-hubs has to be managed efficiently so as to maximize the benefits of PI. This paper proposes a digital twin-based inbound synchronization framework to streamline the operations of a PI-hub in a hyperconnected city logistics system. Digital twins and Internet of Things technologies are proposed for data acquisition and virtualization of real conditions of physical objects, followed by machine learning-integrated models to optimize a joint order fulfillment and replenishment operation in the PI-hubs. Adopting the proposed framework can formulate a Total Inbound Synchronization at three levels: order synchronization, process synchronization and information synchronization. Simulation results show a significant reduction of traveling distance in PI-hubs if the interdependent order fulfillment and replenishment operations are considered as a joint operation. In addition, this paper provides practical implications for logistics service providers to manage information flows within a PI-network driven by digital twins.}
}
@article{OVERENG2021109433,
title = {Dynamic Positioning using Deep Reinforcement Learning},
journal = {Ocean Engineering},
volume = {235},
pages = {109433},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109433},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821008398},
author = {Simen Sem Øvereng and Dong Trong Nguyen and Geir Hamre},
keywords = {Dynamic Positioning, Deep Reinforcement Learning, Proximal policy optimization, Reward shaping},
abstract = {This paper demonstrates the implementation and performance testing of a Deep Reinforcement Learning based control scheme used for Dynamic Positioning of a marine surface vessel. The control scheme encapsulated motion control and control allocation by using a neural network, which was trained on a digital twin without having any prior knowledge of the system dynamics, using the Proximal Policy Optimization learning algorithm. By using a multivariate Gaussian reward function for rewarding small errors between the vessel and the various setpoints, while encouraging small actuator outputs, the proposed Deep Reinforcement Learning based control scheme showed good positioning performance while being energy efficient. Both simulations and model scale sea trials were carried out to demonstrate performance compared to traditional methods, and to evaluate the ability of neural networks trained in simulation to perform on real life systems.}
}
@article{CHEN2021118568,
title = {MTANS: Multi-Scale Mean Teacher Combined Adversarial Network with Shape-Aware Embedding for Semi-Supervised Brain Lesion Segmentation},
journal = {NeuroImage},
volume = {244},
pages = {118568},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118568},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921008417},
author = {Gaoxiang Chen and Jintao Ru and Yilin Zhou and Islem Rekik and Zhifang Pan and Xiaoming Liu and Yezhi Lin and Beichen Lu and Jialin Shi},
keywords = {Brain Lesion Segmentation, Semi-Supervised Learning, MRI, Deep Learning},
abstract = {The annotation of brain lesion images is a key step in clinical diagnosis and treatment of a wide spectrum of brain diseases. In recent years, segmentation methods based on deep learning have gained unprecedented popularity, leveraging a large amount of data with high-quality voxel-level annotations. However, due to the limited time clinicians can provide for the cumbersome task of manual image segmentation, semi-supervised medical image segmentation methods present an alternative solution as they require only a few labeled samples for training. In this paper, we propose a novel semi-supervised segmentation framework that combines improved mean teacher and adversarial network. Specifically, our framework consists of (i) a student model and a teacher model for segmenting the target and generating the signed distance maps of object surfaces, and (ii) a discriminator network for extracting hierarchical features and distinguishing the signed distance maps of labeled and unlabeled data. Besides, based on two different adversarial learning processes, a multi-scale feature consistency loss derived from the student and teacher models is proposed, and a shape-aware embedding scheme is integrated into our framework. We evaluated the proposed method on the public brain lesion datasets from ISBI 2015, ISLES 2015, and BRATS 2018 for the multiple sclerosis lesion, ischemic stroke lesion, and brain tumor segmentation respectively. Experiments demonstrate that our method can effectively leverage unlabeled data while outperforming the supervised baseline and other state-of-the-art semi-supervised methods trained with the same labeled data. The proposed framework is suitable for joint training of limited labeled data and additional unlabeled data, which is expected to reduce the effort of obtaining annotated images.}
}
@article{CENTORRINO2021101357,
title = {Managing crowded museums: Visitors flow measurement, analysis, modeling, and optimization},
journal = {Journal of Computational Science},
volume = {53},
pages = {101357},
year = {2021},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2021.101357},
url = {https://www.sciencedirect.com/science/article/pii/S1877750321000521},
author = {P. Centorrino and A. Corbetta and E. Cristiani and E. Onofri},
keywords = {IoT, Machine learning, Clustering, Tracking system, Museum simulator, Museum optimization},
abstract = {We present an all-around study of the visitors flow in crowded museums: a combination of Lagrangian field measurements and statistical analyses enable us to create stochastic digital-twins of the guest dynamics, unlocking comfort- and safety-driven optimizations. Our case study is the Galleria Borghese museum in Rome (Italy), in which we performed a real-life data acquisition campaign. We specifically employ a Lagrangian IoT-based visitor tracking system based on Raspberry Pi receivers, displaced in fixed positions throughout the museum rooms, and on portable Bluetooth Low Energy beacons handed over to the visitors. Thanks to two algorithms: a sliding window-based statistical analysis and an MLP neural network, we filter the beacons RSSI and accurately reconstruct visitor trajectories at room-scale. Via a clustering analysis, hinged on an original Wasserstein-like trajectory-space metric, we analyze the visitors paths to get behavioral insights, including the most common flow patterns. On these bases, we build the transition matrix describing, in probability, the room-scale visitor flows. Such a matrix is the cornerstone of a stochastic model capable of generating visitor trajectories in silico. We conclude by employing the simulator to enhance the museum fruition while respecting numerous logistic and safety constraints. This is possible thanks to optimized ticketing and new entrance/exit management.}
}
@article{PARK2021100702,
title = {Bioprocess digital twins of mammalian cell culture for advanced biomanufacturing},
journal = {Current Opinion in Chemical Engineering},
volume = {33},
pages = {100702},
year = {2021},
issn = {2211-3398},
doi = {https://doi.org/10.1016/j.coche.2021.100702},
url = {https://www.sciencedirect.com/science/article/pii/S2211339821000344},
author = {Seo-Young Park and Cheol-Hwan Park and Dong-Hyuk Choi and Jong Kwang Hong and Dong-Yup Lee},
abstract = {Today’s biomanufacturing processes are still operated based on experience, and thus can hardly cope with increasing bioprocess complexity. Recently, there is a growing interest in industrial applications of the digital twins (DT) which integrate physical and virtual systems via real-time data monitoring, thus enabling their interactive communications for the enhanced operational efficiency towards advanced biomanufacturing. We suggest that bioprocess DT of cell cultures can be developed by incorporating in-line monitoring, advanced data analytics with machine and deep learning, and mechanistic models representing the mammalian cells and bioreactor for virtually mirroring their behaviors under adjustable process conditions. In this review, we summarize and highlight recent advances in the key components within bioprocess DT platform and discuss the current challenges with future research direction.}
}
@article{CHABANET2021103529,
title = {Coupling digital simulation and machine learning metamodel through an active learning approach in Industry 4.0 context},
journal = {Computers in Industry},
volume = {133},
pages = {103529},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103529},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001366},
author = {Sylvain Chabanet and Hind {Bril El-Haouzi} and Philippe Thomas},
keywords = {Stream based active learning, K-nearest neighbors, Simulation metamodel, Sawmill, Smart data, Artificial intelligence},
abstract = {Although digital simulations are becoming increasingly important in the industrial world owing to the transition toward Industry 4.0, as well as the development of digital twin technologies, they have become increasingly computationally intensive. Many authors have proposed the use of machine learning (ML) metamodels to alleviate this cost and take advantage of the enormous amount of data that are currently available in industry. In an industrial context, it is necessary to continuously train predictive models integrated into decision support systems to ensure the consistency of their prediction quality over time. This led the authors to investigate active learning (AL) concepts in the particular context of the sawmilling industry. In this paper, a method based on AL is proposed to combine simulation and an ML metamodel that is trained incrementally using only selected data (smart data). A case study based on the sawmilling industry and experiments are shown, the results of which prove the possible advantages of this approach.}
}
@article{LI2022122178,
title = {Data-driven hybrid petri-net based energy consumption behaviour modelling for digital twin of energy-efficient manufacturing system},
journal = {Energy},
volume = {239},
pages = {122178},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122178},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221024269},
author = {Hongcheng Li and Dan Yang and Huajun Cao and Weiwei Ge and Erheng Chen and Xuanhao Wen and Chongbo Li},
keywords = {Energy management, Energy behaviour modelling, Digital twin, Data-driven hybrid petri-net, Gaussian kernel extreme learning machine},
abstract = {Advances in energy-saving technology is main way to achieve carbon neutrality. With the development of digital twin, building the physical-virtual data space for improving energy management capacity of enterprises has received tremendous attention. The energy behaviour model implementing accurate simulation and prediction of energy state is the core meta-model of energy-efficient manufacturing digital twin (EMDT). The widely used state-based energy modelling assumes constant power in operation state and approximately fits the energy behaviour without considering uncertain operation environment, resulting in energy behaviour distortion. A data-driven hybrid petri-net (DDHPN) inspired by both the state-based energy modelling and machine learning was developed for establishing the energy behaviour meta-model. Gaussian kernel extreme learning machine is proposed to fit the instantaneous firing speed of energy consumption continuous transitions in DDHPN. DDHPN-based energy behaviour model is driven by physical data under real-time working conditions, operating parameters, and production load for generating a virtual data space of energy management. Finally, DDHPN was integrated into the EMDT model using unified modelling language. The application in extrusion process and die casting process show that the presented model has higher accuracy in energy behaviour prediction. Furthermore, a digital-twin-based energy management prototype system for extrusion workshop demonstrates its potential.}
}
@article{TIAN2022108284,
title = {Real-time model calibration with deep reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {165},
pages = {108284},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108284},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021006506},
author = {Yuan Tian and Manuel Arias Chao and Chetan Kulkarni and Kai Goebel and Olga Fink},
keywords = {Model calibration, Reinforcement learning, Model-based diagnostics, Deep learning},
abstract = {The real-time, and accurate inference of model parameters is of great importance in many scientific and engineering disciplines that use computational models (such as a digital twin) for the analysis and prediction of complex physical processes. However, fast and accurate inference for processes of complex systems cannot easily be achieved in real-time with state-of-the-art methods under noisy real-world conditions with the requirement of a real-time response. The primary reason is that the inference of model parameters with traditional techniques based on optimization or sampling often suffers from computational and statistical challenges, resulting in a trade-off between accuracy and deployment time. In this paper, we propose a novel framework for inference of model parameters based on reinforcement learning. The proposed methodology is demonstrated and evaluated on two different physics-based models of turbofan engines. The experimental results demonstrate that the proposed methodology outperforms all other tested methods in terms of speed and robustness, with high inference accuracy.}
}
@article{SADEGHIAN2021101389,
title = {Happiness recognition from smartphone usage data considering users’ estimated personality traits},
journal = {Pervasive and Mobile Computing},
volume = {73},
pages = {101389},
year = {2021},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2021.101389},
url = {https://www.sciencedirect.com/science/article/pii/S157411922100050X},
author = {Alireza Sadeghian and Marjan Kaedi},
keywords = {User modeling, Happiness, Smartphone usage, Personality traits, Machine learning},
abstract = {The daily and routine interactions of people with their smartphones in various situations make these devices valuable data sources to understand user behaviors. Passive users’ emotion recognition is one of the most essential user modeling areas and has been studied for various purposes so far. Psychological studies, on the other hand, show that the personality of users can influence their behavior when they experience different emotions. Individuals with varying types of personality exhibit different reactions in the same emotional situation. It is concluded that if we consider the user’s personality in passive recognition of his/her emotion, the emotion can be identified more accurately. However, researchers have not paid enough attention to the users’ personality traits when identifying the users’ emotions based on their interaction with cell phones. In the present study, we strive to address this research gap. Among the various emotions, our focus is on happiness recognition. In our proposed method, the user’s personality traits are first estimated based on his/her interactions with the smartphone. Then the estimated personality of the user, along with the data of his/her interactions with the smartphone, is taken into account to recognize his/her happiness. Evaluations showed that taking into account the users’ personality traits reduces the happiness recognition error.}
}
@article{AHMED2021104895,
title = {A nudged hybrid analysis and modeling approach for realtime wake-vortex transport and decay prediction},
journal = {Computers & Fluids},
volume = {221},
pages = {104895},
year = {2021},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2021.104895},
url = {https://www.sciencedirect.com/science/article/pii/S004579302100061X},
author = {Shady E. Ahmed and Suraj Pawar and Omer San and Adil Rasheed and Mandar Tabib},
keywords = {Wake vortex, Digital twins, Nudging, Data fusion, Nonlinear filtering, Galerkin projection, Model order reduction, Proper orthogonal decomposition, Sparse reconstruction, Closure modeling},
abstract = {We put forth a long short-term memory (LSTM) nudging framework for the enhancement of reduced order models (ROMs) of fluid flows utilizing noisy measurements for air traffic improvements. Toward emerging applications of digital twins in aviation, the proposed approach allows for constructing a realtime predictive tool for wake-vortex transport and decay systems. We build on the fact that in realistic application, there are uncertainties in initial and boundary conditions, model parameters, as well as measurements. Moreover, conventional nonlinear ROMs based on Galerkin projection (GROMs) suffer from imperfection and solution instabilities, especially for advection-dominated flows with slow decay in the Kolmogorov n-width. In the presented LSTM nudging (LSTM-N) approach, we fuse forecasts from a combination of imperfect GROM and uncertain state estimates, with sparse Eulerian sensor measurements to provide more reliable predictions in a dynamical data assimilation framework. We illustrate our concept by solving the two-dimensional vorticity transport equation. We investigate the effects of measurements noise and state estimate uncertainty on the performance of the LSTM-N behavior. We also demonstrate that it can sufficiently handle different levels of temporal and spatial measurement sparsity, and offer a huge potential in developing next-generation digital twin technologies for aerospace applications.}
}
@article{ALANNE2022103445,
title = {An overview of machine learning applications for smart buildings},
journal = {Sustainable Cities and Society},
volume = {76},
pages = {103445},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103445},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721007186},
author = {Kari Alanne and Seppo Sierla},
keywords = {Smart building, Intelligent building, Learning, HVAC, Reinforcement learning, Energy efficiency},
abstract = {The efficiency, flexibility, and resilience of building-integrated energy systems are challenged by unpredicted changes in operational environments due to climate change and its consequences. On the other hand, the rapid evolution of artificial intelligence (AI) and machine learning (ML) has equipped buildings with an ability to learn. A lot of research has been dedicated to specific machine learning applications for specific phases of a building's life-cycle. The reviews commonly take a specific, technological perspective without a vision for the integration of smart technologies at the level of the whole system. Especially, there is a lack of discussion on the roles of autonomous AI agents and training environments for boosting the learning process in complex and abruptly changing operational environments. This review article discusses the learning ability of buildings with a system-level perspective and presents an overview of autonomous machine learning applications that make independent decisions for building energy management. We conclude that the buildings’ adaptability to unpredicted changes can be enhanced at the system level through AI-initiated learning processes and by using digital twins as training environments. The greatest potential for energy efficiency improvement is achieved by integrating adaptability solutions at the timescales of HVAC control and electricity market participation.}
}
@article{LIU2021108140,
title = {Certainty driven consistency loss on multi-teacher networks for semi-supervised learning},
journal = {Pattern Recognition},
volume = {120},
pages = {108140},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108140},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003277},
author = {Lu Liu and Robby T. Tan},
keywords = {Semi-supervised learning, Certainty-driven consistency loss, Uncertainty estimation, Decoupled student-teacher, Reliable targets, Noisy labels},
abstract = {One of the successful approaches in semi-supervised learning is based on the consistency regularization. Typically, a student model is trained to be consistent with teacher prediction for the inputs under different perturbations. To be successful, the prediction targets given by teacher should have good quality, otherwise the student can be misled by teacher. Unfortunately, existing methods do not assess the quality of the teacher targets. In this paper, we propose a novel Certainty-driven Consistency Loss (CCL) that exploits the predictive uncertainty in the consistency loss to let the student dynamically learn from reliable targets. Specifically, we propose two approaches, i.e. Filtering CCL and Temperature CCL to either filter out uncertain predictions or pay less attention on them in the consistency regularization. We further introduce a novel decoupled framework to encourage model difference. Experimental results on SVHN, CIFAR-10, and CIFAR-100 demonstrate the advantages of our method over a few existing methods.}
}
@article{BI2022100316,
title = {New digital triad (DT-II) concept for lifecycle information integration of sustainable manufacturing systems},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100316},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100316},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21001096},
author = {Zhuming Bi and Chris W.J. Zhang and Chong Wu and Ling Li},
keywords = {Information integration (II), Digital manufacturing (DM), Digital twin (DT-I), Digital triad (DT-II), Internet of Things (IoT), Internet of digital triad things (IoDTT), Enterprise architecture (EA), Cyber-physical system (CPS), Cloud computing (CC), Big data analytics (BDA), Machine learning (ML), Blockchain technology (BCT)},
abstract = {A system paradigm is a typical pattern or model of enterprise architectures (EAs) that describes constitutive system elements and their relations in achieving missions and goals of enterprises. A well-defined system paradigm and EA help enterprises in adopting appropriate manufacturing resources and technologies, optimizing plans, schedules, and controls of production lines, and coping with complexity, changes, and uncertainties of business processes cost-effectively. It is understandable that a system paradigm should be evolved along with the availability and advancement of advanced manufacturing technologies; on the other hand, new system paradigms are mostly inspired by some successful manufacturing applications of new information technologies (ITs). One of the recently developed system paradigms is Digital Manufacturing (DM) where Digital Twin (DT-I) is used to describe the interactions of virtual and physical entities. This paper discusses the concepts of DM and DT-I and rationalizes the relations of DM and DT-I; in particular, the origin and evolution of DT-I are explored in details to identify its limitations to be used as an EA for DM. It is our finding that existing IT concepts show their limitations in supporting smooth transitions when systems have to be reconfigured in dealing with long-term changes in Sustainable Manufacturing, and this is evidenced by the fact of the trend of the shortened lifespans of modern enterprises even with the aids of rapidly developed digital technologies in decades. To overcome these limitations, a new concept so called Digital Triad (DT-II) is coined and the Internet of Digital Triad Things (IoDTT) is proposed as an information integration (II) solution for digital manufacturing enterprises, and their application is illustrated through a reconfigurable robotic system example. The rationales and significances of DT-II and IoDTT as well as future research directions of DM are summarized as a conclusion.}
}
@article{LIU2022108341,
title = {Weakly Supervised Segmentation of COVID19 Infection with Scribble Annotation on CT Images},
journal = {Pattern Recognition},
volume = {122},
pages = {108341},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108341},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005215},
author = {Xiaoming Liu and Quan Yuan and Yaozong Gao and Kelei He and Shuo Wang and Xiao Tang and Jinshan Tang and Dinggang Shen},
keywords = {COVID-19, infection segmentation, weakly supervised learning, transformation consistency, uncertainty},
abstract = {Segmentation of infections from CT scans is important for accurate diagnosis and follow-up in tackling the COVID-19. Although the convolutional neural network has great potential to automate the segmentation task, most existing deep learning-based infection segmentation methods require fully annotated ground-truth labels for training, which is time-consuming and labor-intensive. This paper proposed a novel weakly supervised segmentation method for COVID-19 infections in CT slices, which only requires scribble supervision and is enhanced with the uncertainty-aware self-ensembling and transformation-consistent techniques. Specifically, to deal with the difficulty caused by the shortage of supervision, an uncertainty-aware mean teacher is incorporated into the scribble-based segmentation method, encouraging the segmentation predictions to be consistent under different perturbations for an input image. This mean teacher model can guide the student model to be trained using information in images without requiring manual annotations. On the other hand, considering the output of the mean teacher contains both correct and unreliable predictions, equally treating each prediction in the teacher model may degrade the performance of the student network. To alleviate this problem, the pixel level uncertainty measure on the predictions of the teacher model is calculated, and then the student model is only guided by reliable predictions from the teacher model. To further regularize the network, a transformation-consistent strategy is also incorporated, which requires the prediction to follow the same transformation if a transform is performed on an input image of the network. The proposed method has been evaluated on two public datasets and one local dataset. The experimental results demonstrate that the proposed method is more effective than other weakly supervised methods and achieves similar performance as those fully supervised.}
}
@article{ZHOU20211274,
title = {Intelligent Ironmaking Optimization Service on a Cloud Computing Platform by Digital Twin},
journal = {Engineering},
volume = {7},
number = {9},
pages = {1274-1281},
year = {2021},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2021.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S209580992100299X},
author = {Heng Zhou and Chunjie Yang and Youxian Sun},
keywords = {Cloud factory, Blast furnace, Multi-objective optimization, Distributed computation},
abstract = {The shortage of computation methods and storage devices has largely limited the development of multi-objective optimization in industrial processes. To improve the operational levels of the process industries, we propose a multi-objective optimization framework based on cloud services and a cloud distribution system. Real-time data from manufacturing procedures are first temporarily stored in a local database, and then transferred to the relational database in the cloud. Next, a distribution system with elastic compute power is set up for the optimization framework. Finally, a multi-objective optimization model based on deep learning and an evolutionary algorithm is proposed to optimize several conflicting goals of the blast furnace ironmaking process. With the application of this optimization service in a cloud factory, iron production was found to increase by 83.91 t∙d−1, the coke ratio decreased 13.50 kg∙t−1, and the silicon content decreased by an average of 0.047%.}
}
@article{DEGIORGIO202122,
title = {Towards online reinforced learning of assembly sequence planning with interactive guidance systems for industry 4.0 adaptive manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {22-34},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521000984},
author = {Andrea {de Giorgio} and Antonio Maffei and Mauro Onori and Lihui Wang},
keywords = {Reinforcement learning, Adaptive assembly, Assembly sequence planning, Assembly guidance system, Manufacturing, Industry 4.0, Optimization, Knowledge retrieval},
abstract = {Literature shows that reinforcement learning (RL) and the well-known optimization algorithms derived from it have been applied to assembly sequence planning (ASP); however, the way this is done, as an offline process, ends up generating optimization methods that are not exploiting the full potential of RL. Today’s assembly lines need to be adaptive to changes, resilient to errors and attentive to the operators’ skills and needs. If all of these aspects need to evolve towards a new paradigm, called Industry 4.0, the way RL is applied to ASP needs to change as well: the RL phase has to be part of the assembly execution phase and be optimized with time and several repetitions of the process. This article presents an agile exploratory experiment in ASP to prove the effectiveness of RL techniques to execute ASP as an adaptive, online and experience-driven optimization process, directly at assembly time. The human-assembly interaction is modelled through the input-outputs of an assembly guidance system built as an assembly digital twin. Experimental assemblies are executed without pre-established assembly sequence plans and adapted to the operators’ needs. The experiments show that precedence and transition matrices for an assembly can be generated from the statistical knowledge of several different assembly executions. When the frequency of a given subassembly reinforces its importance, statistical results obtained from the experiments prove that online RL applications are not only possible but also effective for learning, teaching, executing and improving assembly tasks at the same time. This article paves the way towards the application of online RL algorithms to ASP.}
}
@article{LI2022167,
title = {Big data analysis of the Internet of Things in the digital twins of smart city based on deep learning},
journal = {Future Generation Computer Systems},
volume = {128},
pages = {167-177},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003964},
author = {Xiaoming Li and Hao Liu and Weixi Wang and Ye Zheng and Haibin Lv and Zhihan Lv},
keywords = {Deep learning, Smart city, Digital twins, Internet of Things, Big data analysis},
abstract = {The study aims to conduct big data analysis (BDA) on the massive data generated in the smart city Internet of things (IoT), make the smart city change to the direction of fine governance and efficient and safe data processing. Aiming at the multi-source data collected in the smart city, the study introduces the deep learning (DL) algorithm while using BDA, and puts forward the distributed parallelism strategy of convolutional neural network (CNN). Meantime, the digital twins (DTs) and multi-hop transmission technology are introduced to construct the smart city DTs multi-hop transmission IoT-BDA system based on DL, and further simulate and analyze the performance of the system. The results reveal that in the energy efficiency analysis of model data transmission, the energy efficiency first increases and then decrease as the minimum energy collected α0 increases. But a more suitable power diversion factor ρ is crucial to the signal transmission energy efficiency of the IoT-BDA system. The prediction accuracy of the model is analyzed and it suggests that the accuracy of the constructed system reaches 97.80%, which is at least 2.24% higher than the DL algorithm adopted by other scholars. Regarding the data transmission performance of the constructed system, it is found that when the successful transmission probability is 100% and the exponential distribution parameters λ is valued 0.01∼0.05, it is the closest to the actual result, and the data delay is the smallest, which is maintained at the ms level. To sum up, improving the smart city’s IoT-BDA system using the DL approach can reduce data transmission delay, improve data forecasting accuracy, and offer actual efficacy, providing experimental references for the digital development of smart cities in the future.}
}
@article{SVABENSKY2021107398,
title = {Dataset of shell commands used by participants of hands-on cybersecurity training},
journal = {Data in Brief},
volume = {38},
pages = {107398},
year = {2021},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2021.107398},
url = {https://www.sciencedirect.com/science/article/pii/S2352340921006806},
author = {Valdemar Švábenský and Jan Vykopal and Pavel Seda and Pavel Čeleda},
keywords = {Cybersecurity education, Cybersecurity exercise, Linux shell, Metasploit, Command-line history, Host-based data collection, Educational data mining, Learning analytics},
abstract = {We present a dataset of 13446 shell commands from 175 participants who attended cybersecurity training and solved assignments in the Linux terminal. Each acquired data record contains a command with its arguments and metadata, such as a timestamp, working directory, and host identification in the emulated training infrastructure. The commands were captured in Bash, ZSH, and Metasploit shells. The data are stored as JSON records, enabling vast possibilities for their further use in research and development. These include educational data mining, learning analytics, student modeling, and evaluating machine learning models for intrusion detection. The data were collected from 27 cybersecurity training sessions using an open-source logging toolset and two open-source interactive learning environments. Researchers and developers may use the dataset or deploy the learning environments with the logging toolset to generate their own data in the same format. Moreover, we provide a set of common analytical queries to facilitate the exploratory analysis of the dataset.}
}
@article{WU2022104301,
title = {An intelligent tunnel firefighting system and small-scale demonstration},
journal = {Tunnelling and Underground Space Technology},
volume = {120},
pages = {104301},
year = {2022},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2021.104301},
url = {https://www.sciencedirect.com/science/article/pii/S0886779821004922},
author = {Xiqiang Wu and Xiaoning Zhang and Yishuo Jiang and Xinyan Huang and George G.Q. Huang and Asif Usmani},
keywords = {Smart firefighting, IoT system, Artificial intelligence, Tunnel fire prediction, Fire modelling},
abstract = {Disastrous fire event in the confined tunnel is a fatal hazard, threatening the lives of trapped people and firefighters. Considering the rapid development of fire and the complex environment of tunnels, an accurate and timely fire identification system is in urgent need for guiding the evacuation, rescue, and firefighting actions. This study proposes an intelligent system and digital twin composed of four main components to collect, manage, process and visualize the tunnel fire information. As demonstrated in a laboratory-scale tunnel model, the AI model is trained with a large numerical database to successfully identify the fire size and location. The whole system is assessed in terms of accuracy, timeliness and robustness. The AI model attained an overall accuracy of 98% in predicting the tunnel fire scenarios. The total time delay is around 1 s from the on-site measurement of temperature to the final display of the tunnel fire scenario on a remote user interface. Moreover, the system is robust enough to predict fire, even if part of the temperature sensors is failed or destroyed by fire. The proposed intelligent system will be a valuable step for smart firefighting from the concept to practice.}
}
@article{LIAO2022102797,
title = {Group event recommendation based on graph multi-head attention network combining explicit and implicit information},
journal = {Information Processing & Management},
volume = {59},
number = {2},
pages = {102797},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102797},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002752},
author = {Guoqiong Liao and Xiaobin Deng and Changxuan Wan and Xiping Liu},
keywords = {Explicit information, Implicit information, Group recommendation, Graph attention network},
abstract = {In event-based social networks (EBSN), group event recommendation has become an important task for groups to quickly find events that they are interested in. Existing methods on group event recommendation either consider just one type of information, explicit or implicit, or separately model the explicit and implicit information. However, these methods often generate a problem of data sparsity or of model vector redundancy. In this paper, we present a Graph Multi-head Attention Network (GMAN) model for group event recommendation that integrates the explicit and implicit information in EBSN. Specifically, we first construct a user-explicit graph based on the user's explicit information, such as gender, age, occupation and the interactions between users and events. Then we build a user-implicit graph based on the user's implicit information, such as friend relationships. The incorporated both explicit and implicit information can effectively describe the user's interests and alleviate the data sparsity problem. Considering that there may be a correlation between the user's explicit and implicit information in EBSN, we take the user's explicit vector representation as the input of the implicit information aggregation when modeling with graph neural networks. This unified user modeling can solve the aforementioned problem of user model vector redundancy and is also suitable for event modeling. Furthermore, we utilize a multi-head attention network to learn richer implicit information vectors of users and events from multiple perspectives. Finally, in order to get a higher level of group vector representation, we use a vanilla attention mechanism to fuse different user vectors in the group. Through experimenting on two real-world Meetup datasets, we demonstrate that GMAN model consistently outperforms state-of-the-art methods on group event recommendation.}
}
@article{MARINI2021102165,
title = {Semi-supervised training of deep convolutional neural networks with heterogeneous data and few local annotations: An experiment on prostate histopathology image classification},
journal = {Medical Image Analysis},
volume = {73},
pages = {102165},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.102165},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521002115},
author = {Niccolò Marini and Sebastian Otálora and Henning Müller and Manfredo Atzori},
keywords = {Computational pathology, Deep learning, Semi-supervision, Prostate cancer},
abstract = {Convolutional neural networks (CNNs) are state-of-the-art computer vision techniques for various tasks, particularly for image classification. However, there are domains where the training of classification models that generalize on several datasets is still an open challenge because of the highly heterogeneous data and the lack of large datasets with local annotations of the regions of interest, such as histopathology image analysis. Histopathology concerns the microscopic analysis of tissue specimens processed in glass slides to identify diseases such as cancer. Digital pathology concerns the acquisition, management and automatic analysis of digitized histopathology images that are large, having in the order of 100′0002 pixels per image. Digital histopathology images are highly heterogeneous due to the variability of the image acquisition procedures. Creating locally labeled regions (required for the training) is time-consuming and often expensive in the medical field, as physicians usually have to annotate the data. Despite the advances in deep learning, leveraging strongly and weakly annotated datasets to train classification models is still an unsolved problem, mainly when data are very heterogeneous. Large amounts of data are needed to create models that generalize well. This paper presents a novel approach to train CNNs that generalize to heterogeneous datasets originating from various sources and without local annotations. The data analysis pipeline targets Gleason grading on prostate images and includes two models in sequence, following a teacher/student training paradigm. The teacher model (a high-capacity neural network) automatically annotates a set of pseudo-labeled patches used to train the student model (a smaller network). The two models are trained with two different teacher/student approaches: semi-supervised learning and semi-weekly supervised learning. For each of the two approaches, three student training variants are presented. The baseline is provided by training the student model only with the strongly annotated data. Classification performance is evaluated on the student model at the patch level (using the local annotations of the Tissue Micro-Arrays Zurich dataset) and at the global level (using the TCGA-PRAD, The Cancer Genome Atlas-PRostate ADenocarcinoma, whole slide image Gleason score). The teacher/student paradigm allows the models to better generalize on both datasets, despite the inter-dataset heterogeneity and the small number of local annotations used. The classification performance is improved both at the patch-level (up to κ=0.6127±0.0133 from κ=0.5667±0.0285), at the TMA core-level (Gleason score) (up to κ=0.7645±0.0231 from κ=0.7186±0.0306) and at the WSI-level (Gleason score) (up to κ=0.4529±0.0512 from κ=0.2293±0.1350). The results show that with the teacher/student paradigm, it is possible to train models that generalize on datasets from entirely different sources, despite the inter-dataset heterogeneity and the lack of large datasets with local annotations.}
}
@article{WANG2022110523,
title = {The replacement of dysfunctional sensors based on the digital twin method during the cutter suction dredger construction process},
journal = {Measurement},
volume = {189},
pages = {110523},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.110523},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121014020},
author = {Bin Wang and Shidong Fan and Yong Chen and Liangyan Zheng and Hanhua Zhu and Zhenlong Fang and Min Zhang},
keywords = {Cutter suction dredger, Sensor failure, Digital twin, Regression prediction, Stacking generalization},
abstract = {In practice, the construction environment of the cutter suction dredger (CSD) is inclement, which results in a high frequency of the key parameter sensor failure. If the key parameter data is missing or false, it will affect the continuity of the construction. This paper proposed a sensor network regression prediction method based on the “Digital Twin” to establish a correlated model between the key sensor and other highly reliable sensors in the CSD construction. The stacking model is trained by learning the CSD construction data that can synchronously calculate any key parameters when the dredger is running. The proposed method is validated on the “Changshi 12” CSD construction case. The results indicate that the method has high prediction accuracy and computes less expensively. Thus, the proposed method could better solve the problem of construction discontinuity caused by key sensor failure.}
}
@article{KAMARI2022104091,
title = {AI-based risk assessment for construction site disaster preparedness through deep learning-based digital twinning},
journal = {Automation in Construction},
volume = {134},
pages = {104091},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104091},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005422},
author = {Mirsalar Kamari and Youngjib Ham},
keywords = {Visual sensing and analytics, Construction site disaster preparedness, Scene understanding, Artificial intelligence},
abstract = {Hurricanes are among the most devastating natural disasters in the United States, causing billions of dollars of property damage and insured losses. During extreme wind events, unsecured objects in jobsites can easily become airborne debris, which results in substantial loss to construction projects and neighboring communities. Towards a systematic disaster preparedness in construction jobsites, this paper presents a novel vision-based digital twinning and threat assessment framework. We encode the context of disaster risk into deep-learning architectures to identify and analyze the characteristics and impacts of potential wind-borne debris in construction site digital twin models. Case studies on nine piles of construction materials are presented to demonstrate and discuss the fidelity of the proposed computational modules. The proposed methods are expected to help provide heads up for practitioners to quickly recognize, localize, and assess potential wind-borne derbies in construction jobsites, and thereby implementing hurricane preparedness in an effective and timely manner.}
}
@article{GUNASEGARAM2021102089,
title = {Towards developing multiscale-multiphysics models and their surrogates for digital twins of metal additive manufacturing},
journal = {Additive Manufacturing},
volume = {46},
pages = {102089},
year = {2021},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.102089},
url = {https://www.sciencedirect.com/science/article/pii/S2214860421002542},
author = {D.R. Gunasegaram and A.B. Murphy and A. Barnard and T. DebRoy and M.J. Matthews and L. Ladani and D. Gu},
keywords = {Additive manufacturing, Artificial intelligence, Digital twins, Machine learning, Multiscale modeling, Multiphysics modeling, Industry 4.0},
abstract = {Artificial intelligence (AI) embedded within digital models of manufacturing processes can be used to improve process productivity and product quality significantly. The application of such advanced capabilities particularly to highly digitalized processes such as metal additive manufacturing (AM) is likely to make those processes commercially more attractive. AI capabilities will reside within Digital Twins (DTs) which are living virtual replicas of the physical processes. DTs will be empowered to operate autonomously in a diagnostic control capacity to supervise processes and can be interrogated by the practitioner to inform the optimal processing route for any given product. The utility of the information gained from the DTs would depend on the quality of the digital models and, more importantly, their faster-solving surrogates which dwell within DTs for consultation during rapid decision-making. In this article, we point out the exceptional value of DTs in AM and focus on the need to create high-fidelity multiscale-multiphysics models for AM processes to feed the AI capabilities. We identify technical hurdles for their development, including those arising from the multiscale and multiphysics characteristics of the models, the difficulties in linking models of the subprocesses across scales and physics, and the scarcity of experimental data. We discuss the need for creating surrogate models using machine learning approaches for real-time problem-solving. We further identify non-technical barriers, such as the need for standardization and difficulties in collaborating across different types of institutions. We offer potential solutions for all these challenges, after reflecting on and researching discussions held at an international symposium on the subject in 2019. We argue that a collaborative approach can not only help accelerate their development compared with disparate efforts, but also enhance the quality of the models by allowing modular development and linkages that account for interactions between the various sub-processes in AM. A high-level roadmap is suggested for starting such a collaboration.}
}
@article{ATTENBOROUGH20211,
title = {Clinical assessment during a global pandemic – Transitioning to a COVID safe hybrid OSCE},
journal = {International Journal of Osteopathic Medicine},
volume = {42},
pages = {1-4},
year = {2021},
issn = {1746-0689},
doi = {https://doi.org/10.1016/j.ijosm.2021.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1746068921000808},
author = {Paul Attenborough and Jacquelyn Towns and Azharuddin Fazalbhoy and Kylie Fitzgerald},
keywords = {COVID-19, Educational measurement, Educational technology, Learning methods, Osteopathic medicine},
abstract = {Objective structured clinical examinations (OSCEs) are often used to assess the clinical competence of students in preprofessional osteopathy training. During the COVID-19 global pandemic, the final year OSCE in the RMIT University osteopathy program was redeveloped leveraging online technologies within COVID-19 guidelines such as hygiene and occupancy limitations. Final year osteopathy students were assessed using a hybrid ten station OSCE, comprising both online and face-to-face components. The examination was led by a pre-recorded narrated PowerPoint video. The video contained instructions, case information for five cases and prompts for the practical stations. A student model stepped into the room as needed for practical stations. The examiner assessed students from another room via video streaming, with limited interaction with students. The hybrid OSCE was conducted safely during Stage 4 restrictions adhering to COVID Safe guidelines, allowing robust competency assessment of final year students, enabling timely graduation and transition to practice. Institutional support, technology infrastructure, clear communication and stakeholder collaboration are key to successful implementation. The hybrid OSCE format offers a potential solution for institutions delivering high-stakes assessment in the continuing challenges of clinical assessment in the post COVID landscape. Adopting hybrid assessment formats may facilitate remote assessment of students in clinical placements.}
}
@article{BANG2021743,
title = {Distilling from professors: Enhancing the knowledge distillation of teachers},
journal = {Information Sciences},
volume = {576},
pages = {743-755},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521008203},
author = {Duhyeon Bang and Jongwuk Lee and Hyunjung Shim},
keywords = {Knowledge distillation, Professor model, Conditional adversarial autoencoder},
abstract = {Knowledge distillation (KD) is a successful technique for transferring knowledge from one machine learning model to another model. Specifically, the idea of KD has been widely used for various tasks such as model compression and knowledge transfer between different models. However, existing studies in KD have overlooked the possibility that dark knowledge (i.e., soft targets) obtained from a complex and large model (a.k.a., a teacher model) may be either incorrect or insufficient. Such knowledge can hinder the effective learning of another small model (a.k.a., a student model). In this paper, we propose the professor model, which refines the soft target from the teacher model to improve KD. The professor model aims to achieve two goals; 1) improving the prediction accuracy and 2) capturing the inter-class correlation of the soft target from the teacher model. We first design the professor model by reformulating a conditional adversarial autoencoder (CAAE). Then, we devise two KD strategies using both teacher and professor models. Our empirical study demonstrates that the professor model effectively improves KD in three benchmark datasets: CIFAR100, TinyImagenet, and ILSVRC2015. Moreover, our comprehensive analysis shows that the professor model is much more effective than employing the stronger teacher model, in which parameters are greater than the sum of the teacher’s and professor’s parameters. Since the proposed model is model-agnostic, our model can be combined with any KD algorithm and consistently improves various KD techniques.}
}
@article{MOHAMMED2021106912,
title = {FASTory digital twin data},
journal = {Data in Brief},
volume = {35},
pages = {106912},
year = {2021},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2021.106912},
url = {https://www.sciencedirect.com/science/article/pii/S2352340921001967},
author = {Wael M. Mohammed and Jose L. {Martinez Lastra}},
keywords = {Digital twin, Data engineering, Linked data, Discrete manufacturing process, Assembly process},
abstract = {The vast adoption of machine learning techniques in developing smart solutions increases the need of training and testing data. This data can be either collected from physical systems or created using simulation tools. In this regard, this paper presents a set of data collected using a digital twin known as the FASTory Simulator. The data contains more than 100 K events which are collected during a simulated assembly process. The FASTory simulator is a replica of a real assembly line with web-based industrial controllers. The data have been collected using specific-developed orchestrator. During the simulated process, the orchestrator was able to record all the events that occurred in the system. The provided data contains raw JavaScript Object Notation (JSON) formatted data and filtered Comma Separated Values (CSV) formatted data. This data can be exploited in machine learning for modelling the behaviour of the production systems or as testing data for optimization solution for the production system. Finally, this data has been utilized in a research for comparing different data analysis approaches including Knowledge-based systems and data-based systems.}
}
@article{QUILODRANCASAS202211,
title = {Digital twins based on bidirectional LSTM and GAN for modelling the COVID-19 pandemic},
journal = {Neurocomputing},
volume = {470},
pages = {11-28},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.10.043},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221015290},
author = {César Quilodrán-Casas and Vinicius L.S. Silva and Rossella Arcucci and Claire E. Heaney and YiKe Guo and Christopher C. Pain},
keywords = {Reduced order models, Digital twins, Deep learning, Long short-term memory networks, Generative adversarial networks},
abstract = {The outbreak of the coronavirus disease 2019 (COVID-19) has now spread throughout the globe infecting over 150 million people and causing the death of over 3.2 million people. Thus, there is an urgent need to study the dynamics of epidemiological models to gain a better understanding of how such diseases spread. While epidemiological models can be computationally expensive, recent advances in machine learning techniques have given rise to neural networks with the ability to learn and predict complex dynamics at reduced computational costs. Here we introduce two digital twins of a SEIRS model applied to an idealised town. The SEIRS model has been modified to take account of spatial variation and, where possible, the model parameters are based on official virus spreading data from the UK. We compare predictions from one digital twin based on a data-corrected Bidirectional Long Short-Term Memory network with predictions from another digital twin based on a predictive Generative Adversarial Network. The predictions given by these two frameworks are accurate when compared to the original SEIRS model data. Additionally, these frameworks are data-agnostic and could be applied to towns, idealised or real, in the UK or in other countries. Also, more compartments could be included in the SEIRS model, in order to study more realistic epidemiological behaviour.}
}
@article{WEN202125,
title = {Preparing lessons: Improve knowledge distillation with better supervision},
journal = {Neurocomputing},
volume = {454},
pages = {25-33},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.102},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221006603},
author = {Tiancheng Wen and Shenqi Lai and Xueming Qian},
keywords = {Knowledge distillation, Label regularization, Hard example mining},
abstract = {Knowledge distillation (KD) is widely applied in the training of efficient neural network. A compact model, which is trained to mimic the representation of a cumbersome model for the same task, generally obtains a better performance compared with being trained with the ground truth label. Previous KD-based works mainly focus on two aspects: (1) designing various feature representation for knowledge transfer; (2) introducing different training mechanism such as progressive learning or adversarial learning. In this paper, we revisit the standard KD and observe that training with teacher’s logits might suffer from incorrect and uncertain supervision. To tackle these problems, we propose two novel approaches to deal with incorrect logits and uncertain logits respectively, which are called Logits Adjustment (LA) and Dynamic Temperature Distillation (DTD). To be specific, LA rectifies the incorrect logits according to ground truth label and certain rules. While DTD treats the temperature of KD as a dynamic sample wise parameter rather than a static and global hyper-parameter, which actually notes the uncertainty for each sample’s logits. With iteratively updating the sample wise temperature, the student model could pay more attention on the samples that confuse the teacher model. Experiments on CIFAR-10/100, CINIC-10 and Tiny ImageNet verify that the proposed methods yield encouraging improvement compared with the standard KD. Furthermore, considering the simple implementations, LA and DTD can be easily attached to many KD-based frameworks and bring improvements without extra cost of training time and computing resources.}
}
@article{GARG2021103173,
title = {Machine learning based digital twin for stochastic nonlinear multi-degree of freedom dynamical system},
journal = {Probabilistic Engineering Mechanics},
volume = {66},
pages = {103173},
year = {2021},
issn = {0266-8920},
doi = {https://doi.org/10.1016/j.probengmech.2021.103173},
url = {https://www.sciencedirect.com/science/article/pii/S0266892021000576},
author = {Shailesh Garg and Ankush Gogoi and Souvik Chakraborty and Budhaditya Hazra},
keywords = {Digital twin, Bayesian filters, Gaussian process, Stochastic, Non-linear MDOF systems},
abstract = {The potential of digital twin technology is immense, specifically in the infrastructure, aerospace, and automotive sector. However, practical implementation of this technology is not at an expected speed, specifically because of lack of application-specific details. In this paper, we propose a novel digital twin framework for stochastic nonlinear multi-degree of freedom (MDOF) dynamical systems. The proposed digital twin has four modules — (a) a physics-based nominal model, (b) a data collection module, (c) algorithm for real-time update of the digital twin and (d) module for predicting future state. The modules for real-time update and prediction are based on the so-called gray-box modeling approach, and utilizes both physics based and data driven frameworks; this enables the proposed digital twin to generalize and predict future responses. The gray box modeling framework used within the digital twin is developed by coupling Bayesian filtering and machine learning algorithm. Although, the proposed digital twin can be used with any machine learning regression algorithm, we have used Gaussian process in this study. Performance of the proposed approach is illustrated using two examples. Results obtained indicate the applicability and excellent performance of the proposed digital twin framework.}
}
@article{TONG2021113871,
title = {Experiment analysis and computational optimization of the Atkinson cycle gasoline engine through NSGA Ⅱ algorithm using machine learning},
journal = {Energy Conversion and Management},
volume = {238},
pages = {113871},
year = {2021},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2021.113871},
url = {https://www.sciencedirect.com/science/article/pii/S0196890421000480},
author = {Ji Tong and Yangyang Li and Jingping Liu and Ran Cheng and Jinhuan Guan and Shuqian Wang and Shujing Liu and Song Hu and Tao Guo},
keywords = {Atkinson cycle gasoline engine, NSGA Ⅱ algorithm, Support vector machine, Machine learning, Digital twins},
abstract = {This paper is pioneered in developing digital twins by GT-Power software and multi-objective evolutionary optimization (MOEO) using NSGA II algorithm for an Atkinson cycle gasoline engine under China VI emissions standards. Firstly, an experimental investigation is conducted and relevant experimental data is obtained. Based on this, the corresponding 1D GT-Power simulation model is established and calibrated according to the obtained test data. Secondly, the four decision variables including the spark advance angle (SA), exhaust gas recirculation (EGR) rate, exhaust variable valve timing (VVT-E) and intake variable valve timing (VVT-I) are input into the simulation model, the optimal values of the decision variables will be determined via MOEO to minimize NOx emissions and the brake specific fuel consumption (BSFC). Thirdly, based on the data obtained from the scanning test, a machine learning method is used to build an engine performance prediction model through the support vector machine (SVM) regression algorithm. The inputs (control parameters obtained from the optimization process) including SA, EGR, VVT-I and VVT-E are imported to predict the performance output of the engine. The results show that under the engine control parameters obtained by the NSGA II algorithm, the simulation values of engine performance parameters have been greatly optimized, the decreasing extent of fuel consumption is about 5.0%, besides, the decreasing extent of NOx is about 70%. What is more, the increased EER and EEE is up to 6.21% and 2.26%, respectively. And then most of the predicted values obtained by machine learning have been optimized. For BSFC, in general, the simulation value and the predicted value are in good agreement at the smaller value, indicating that the simulation model and the regression prediction model basically achieve the same value at the lower BSFC of the engine. For NOx, the simulated and predicted values have all been optimized. Furthermore, the method and platform developed in this paper will help to carry out a series of related work in the field of vehicle energy flow distribution and optimization when changing different control strategies and optimization methods in the future. Besides, the above work provides a reliable theoretical basis and digital model support for the development of energy-saving and efficient Atkinson cycle engines, which further drives the application of Atkinson cycle engines in new energy vehicles.}
}
@article{SOLOMON2021101596,
title = {Analyzing movement predictability using human attributes and behavioral patterns},
journal = {Computers, Environment and Urban Systems},
volume = {87},
pages = {101596},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101596},
url = {https://www.sciencedirect.com/science/article/pii/S019897152100003X},
author = {Adir Solomon and Amit Livne and Gilad Katz and Bracha Shapira and Lior Rokach},
keywords = {Location prediction, User modeling, Spatial information, Deep learning},
abstract = {The ability to predict human mobility, i.e., transitions between a user's significant locations (the home, workplace, etc.) can be helpful in a wide range of applications, including targeted advertising, personalized mobile services, and transportation planning. Most studies on human mobility prediction have focused on the algorithmic perspective rather than on investigating human predictability. Human predictability has great significance, because it enables the creation of more robust mobility prediction models and the assignment of more accurate confidence scores to location predictions. In this study, we propose a novel method for detecting a user's stay points from millions of GPS samples. Then, after detecting these stay points, a long short-term memory (LSTM) neural network is used to predict future stay points. We explore the use of two types of stay point prediction models (a general model that is trained in advance and a personal model that is trained over time) and analyze the number of previous locations needed for accurate prediction. Our evaluation on two real-world datasets shows that by using our preprocessing approach, we can detect stay points from routine trajectories with higher accuracy than the methods commonly used in this domain, and that by utilizing various LSTM architectures instead of the traditional Markov models and advanced deep learning models, our method can predict human movement with high accuracy of more than 40% when using the Acc@1 measure and more than 59% when using the Acc@3 measure. We also demonstrate that the movement prediction accuracy varies for different user populations based on their trajectory characteristics and demographic attributes.}
}
@article{XIE2022108550,
title = {KD-CLDNN: Lightweight automatic recognition model based on bird vocalization},
journal = {Applied Acoustics},
volume = {188},
pages = {108550},
year = {2022},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2021.108550},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X21006447},
author = {Jiangjian Xie and Sibo Zhao and Xingguang Li and Dongming Ni and Junguo Zhang},
keywords = {Bird vocalization, Lightweight, CLDNN, Recognition, Knowledge distillation},
abstract = {Passive acoustic monitoring (PAM) equipment embedded automatic bird recognition is conducive to the real-time monitoring of birds. In this paper, the lightweight bird recognition model KD-CLDNN is proposed to adapt to the monitoring equipment with limited computing power. The KD-CLDNN model is obtained by knowledge distillation. The teacher model CS-CLDNN (CBAM-Switch-CLDNN) is built by introducing Convolutional Block Attention Module (CBAM) and Swish activation function to CLDNN model. Furthermore, the student model Net-S is constructed by simplifying the CS-CLDNN model. The recordings of 20 bird species are used as the dataset in this study, and the performances of six complex models are evaluated with this dataset. The results show that the CS-CLDNN model outperforms other complex models. Compared with the CS-CLDNN model, the performance of the KD-CLDNN model in accuracy, recall, precision, and F1 score decrease slightly by 0.028, 0.025, 0.01, and 0.023, respectively. However, the training GPU time of KD-CLDNN is reduced about seven times, and the total number of parameters is reduced about three times. On the edge computing platform NVIDIA Jetson TX2, the inference speed of KD-CLDNN is 3.2 times faster than that of CS-CLDNN. Therefore, the lightweight KD-CLDNN model can significantly reduce calculation requirement on the premise of ensuring recognition accuracy, which can be helpful for the development of intelligent bird monitoring equipment based on deep learning.}
}
@article{LEE2021126681,
title = {A stacking ensemble model for hydrological post-processing to improve streamflow forecasts at medium-range timescales over South Korea},
journal = {Journal of Hydrology},
volume = {600},
pages = {126681},
year = {2021},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2021.126681},
url = {https://www.sciencedirect.com/science/article/pii/S0022169421007290},
author = {Dong-Gi Lee and Kuk-Hyun Ahn},
keywords = {Stacking generalization, Hydrological forecast, South Korea, Medium-range forecast, Hydrological post-processing},
abstract = {This study presents the potential of hydrological ensemble forecasts over South Korea for medium-range forecast lead times (1–7 days). To generate hydrological forecasts, this study utilizes a framework based on stacking ensemble learning, an emerging machine learning technique that includes a two-level structure: base-learner and meta-learner models. In particular, the present research contributes to hydrological post-processing techniques by: (1) introducing a penalized quantile regression-based meta-learner to generate probabilistic predictions, (2) considering modeled climate predictions and antecedent hydrologic conditions simultaneously for regional hydrological forecast development, and (3) quantifying the skill enhancements from the multi-model forecasts under the stacking generalization. The proposed model is evaluated in massive 473 grid cells along with nine additional simpler models to test the specific hypotheses introduced in this study. Results indicate that our proposed forecasts can be used for relatively short lead times. In addition, results demonstrate that utilizing a penalized probabilistic meta-learner and antecedent conditions contributes to the forecast skill improvements. Lastly, we find that base-model diversity outperforms increased ensemble size alone in enhancing the forecast abilities under the stacking ensemble generalization. We conclude this paper with a discussion of possible forecast model improvements from an adaptation of additional information from input and model structures under the stacking generalization.}
}
@article{PAN2022117271,
title = {Data-centric Engineering: integrating simulation, machine learning and statistics. Challenges and opportunities},
journal = {Chemical Engineering Science},
volume = {249},
pages = {117271},
year = {2022},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2021.117271},
url = {https://www.sciencedirect.com/science/article/pii/S0009250921008368},
author = {Indranil Pan and Lachlan R. Mason and Omar K. Matar},
keywords = {Digital twins, Artificial Intelligence, CFD, FEM, Data-centric Engineering, SimOps},
abstract = {Recent advances in machine learning, coupled with low-cost computation, availability of cheap streaming sensors, data storage and cloud technologies, has led to widespread multi-disciplinary research activity with significant interest and investment from commercial stakeholders. Mechanistic models, based on physical equations, and purely data-driven statistical approaches represent two ends of the modelling spectrum. New hybrid, data-centric engineering approaches, leveraging the best of both worlds and integrating both simulations and data, are emerging as a powerful tool with a transformative impact on the physical disciplines. We review the key research trends and application scenarios in the emerging field of integrating simulations, machine learning, and statistics. We highlight the opportunities that such an integrated vision can unlock and outline the key challenges holding back its realisation. We also discuss the bottlenecks in the translational aspects of the field and the long-term upskilling requirements for the existing workforce and future university graduates.}
}
@article{MOWBRAY2021108054,
title = {Machine learning for biochemical engineering: A review},
journal = {Biochemical Engineering Journal},
volume = {172},
pages = {108054},
year = {2021},
issn = {1369-703X},
doi = {https://doi.org/10.1016/j.bej.2021.108054},
url = {https://www.sciencedirect.com/science/article/pii/S1369703X21001303},
author = {Max Mowbray and Thomas Savage and Chufan Wu and Ziqi Song and Bovinille Anye Cho and Ehecatl A. {Del Rio-Chanona} and Dongda Zhang},
keywords = {Machine learning, Data-driven modelling, Biochemical engineering, Industrial biotechnology, Digitalisation, Digital twin},
abstract = {The field of machine learning is comprised of techniques, which have proven powerful approaches to knowledge discovery and construction of ‘digital twins’ in the highly dimensional, nonlinear and stochastic domains common to biochemical engineering. We review the use of machine learning within biochemical engineering over the last 20 years. The most prevalent machine learning methods are demystified, and their impact across individual biochemical engineering subfields is outlined. In doing so we provide insights into the true benefits of each technique, and obstacles for their wider deployment. Finally, core challenges into the application of machine learning in biochemical engineering are thoroughly discussed, and further insight into adoption of innovative hybrid modelling and transfer learning strategies for development of new digital biotechnologies is provided.}
}
@article{LEE2021108443,
title = {Development of the Machine Learning-based Safety Significant Factor Inference Model for Diagnosis in Autonomous Control System},
journal = {Annals of Nuclear Energy},
volume = {162},
pages = {108443},
year = {2021},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2021.108443},
url = {https://www.sciencedirect.com/science/article/pii/S0306454921003194},
author = {Joomyung Lee and Linyu Lin and Paridhi Athe and Nam Dinh},
keywords = {Diagnosis, Digital twin, Recurrent Neural Network, Safety significant factor, Machine Learning},
abstract = {As a critical component to the autonomous control system, Digital Twin for Diagnosis (DT-D) is a virtual replica of physical systems for an accurate understanding of reactor states. Since the physical damage state cannot be measured directly in transient or accident conditions, safety significant factor (SSF) is introduced as a surrogate index for physical damage states to support safety-related decision making. This study develops a machine learning (ML) based SSF inference model (SSFIM) using the Recurrent Neural Network (RNN) with acceptable accuracy, generalization capability, effectiveness, and robustness against sensor errors. To demonstrate the capability of the ML-based SSFIM, case studies are implemented on a plant simulator for Experimental Breeder Reactor – II. For partial loss of flow accident scenarios, the SSFIM is able to infer the peak fuel centerline temperature with minimally one sensor. Meanwhile the SSFIM is also found to be robust against manipulated sensor drifts and/or random noises.}
}
@article{CHEN202231,
title = {Artificial intelligence enabled Digital Twins for training autonomous cars},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {2},
pages = {31-41},
year = {2022},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2022.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667345222000116},
author = {Dongliang Chen and Zhihan Lv},
keywords = {Deep learning, Autonomous cars, Digital twins, Information security, Artificial intelligence},
abstract = {This exploration is aimed at the system prediction and safety performance of the Digital Twins (DTs) of autonomous cars based on artificial intelligence technology, and the intelligent development of transportation in the smart city. On the one hand, considering the problem of safe driving of autonomous cars in intelligent transportation systems, it is essential to ensure the transmission safety of vehicle data and realize the load balancing scheduling of data transmission resources. On the other hand, convolution neural network (CNN) of the deep learning algorithm is adopted and improved, and then, the DTs technology is introduced. Finally, an autonomous cars DTs prediction model based on network load balancing and spatial-temporal graph convolution network is constructed. Moreover, through simulation, the performance of this model is analyzed from perspectives of Accuracy, Precision, Recall, and F1-score. The experimental results demonstrate that in comparative analysis, the accuracy of road network prediction of the model reported here is 92.70%, which is at least 2.92% higher than that of the models proposed by other scholars. Through the analysis of the security performance of network data transmission, it is found that this model achieves a lower average delay time than other comparative models. Besides, the message delivery rate is basically stable at 80%, and the message leakage rate is basically stable at about 10%. Therefore, the prediction model for autonomous cars constructed here not only ensures low delay but also has excellent network security performance, so that information can interact more efficiently. The research outcome can provide an experimental basis for intelligent development and safety performance improvement in the transportation field of smart cities.}
}
@article{XU2022242,
title = {Contrastive adversarial knowledge distillation for deep model compression in time-series regression tasks},
journal = {Neurocomputing},
volume = {485},
pages = {242-251},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.139},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016374},
author = {Qing Xu and Zhenghua Chen and Mohamed Ragab and Chao Wang and Min Wu and Xiaoli Li},
keywords = {Knowledge distillation, Contrastive learning, Adversarial learning, Time series regression},
abstract = {Knowledge distillation (KD) attempts to compress a deep teacher model into a shallow student model by letting the student mimic the teacher’s outputs. However, conventional KD approaches can have the following shortcomings. First, existing KD approaches align the global distribution between teacher and student models and overlook the fine-grained features. Second, most of existing approaches focus on classification tasks and require the architecture of teacher and student models to be similar. To address these limitations, we propose a contrastive adversarial knowledge distillation called CAKD for time series regression tasks where the student and teacher are using different architectures. Specifically, we first propose adversarial adaptation to automatically align the feature distribution between student and teacher networks respectively. Yet, adversarial adaptation can only align the global feature distribution without considering the fine-grained features. To mitigate this issue, we employ a novel contrastive loss for instance-wise alignment between the student and teacher. Particularly, we maximize similarity between teacher and student features that originate from the same sample. Lastly, a KD loss is used to for the knowledge distillation where the teacher and student have two different architectures. We used a turbofan engine dataset that consists of four sub-datasets to evaluate the model performance. The results show that the proposed CAKD method consistently outperforms state-of-the-art methods in terms of two different metrics.}
}
@article{BHARDWAJ2021172,
title = {Empowering Knowledge Distillation via Open Set Recognition for Robust 3D Point Cloud Classification},
journal = {Pattern Recognition Letters},
volume = {151},
pages = {172-179},
year = {2021},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2021.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167865521002701},
author = {Ayush Bhardwaj and Sakshee Pimpale and Saurabh Kumar and Biplab Banerjee},
keywords = {Knowledge Distillation, Open Set Recognition, 3D Object Recognition, Point Cloud Classification},
abstract = {Real-world scenarios pose several challenges to deep learning based computer vision techniques despite their tremendous success in research. Deeper models provide better performance, but are challenging to deploy and knowledge distillation allows us to train smaller models with minimal loss in performance. A model also has to deal with open set samples from classes outside the ones it was trained on and should be able to identify them as unknown samples while classifying the known ones correctly. Finally, most existing image recognition research focuses only on using two-dimensional snapshots of the three-dimensional real world objects. In this work, we attempt to bridge these three research fields, which have been developed independently until now, despite being deeply interrelated in practice. We propose a joint knowledge distillation and open set recognition training methodology for three-dimensional object recognition. We demonstrate the effectiveness of the proposed method via various experiments on how it allows us to obtain a much smaller model, which takes a minimal hit in performance while being capable of open set recognition for 3D point cloud data.}
}
@article{SINGH2021105407,
title = {Highway 4.0: Digitalization of highways for vulnerable road safety development with intelligent IoT sensors and machine learning},
journal = {Safety Science},
volume = {143},
pages = {105407},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105407},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521002514},
author = {Rajesh Singh and Rohit Sharma and Shaik {Vaseem Akram} and Anita Gehlot and Dharam Buddhi and Praveen Kumar Malik and Rajeev Arya},
keywords = {Highway, DL, Vulnerable Road Safety, Smart city, IoT, Renewable energy, And vision node},
abstract = {According to United Nations (UN) 2030 agenda, the transportation system needs to be enhanced for the establishment of access to safe, affordable, accessible, and sustainable transport systems along with enhanced road safety. The highway road transport system is one of the transport systems that enables to transits goods and humans from one location to another location. The agenda of UN 2030 for the transport system will be accomplished with the assistance of digital technologies like the internet of things (IoT) and artificial intelligence (AI). The implementation of these digital technologies on highways empowers to provide reliable, smarter, intelligent, and renewable energy sources experience to the users travelling along the highways. This study discusses the significance of the digitalization of highways that supporting and realizing a sustainable environment on the highways. To discuss the significance of digitalization, the study has categorized digitalization into five subcomponents namely smart highway lighting system, smart traffic and emergency management system, renewable energy sources on highways, smart display and AI in highways. An architecture-for smart highway lighting, smart traffic, and emergency management are proposed and discussed in the study. The significance of implementing smart display boards and renewable sources with real-time applications is also addressed in this study. Moreover, the integration of AI in highways is addressed with the perspective of enhancing road safety. The integration of deep learning (DL) in the edge-based vision node for predicting the patterns of traffic flow, highway road safety, and maintenance of quality roads have been addressed in the discussion section. Embedding the deep learning techniques in the vison node at the traffic junction and the highway lighting controller is able to deliver an intelligent system that provides sustained experience and management of the highways. Smart reflectors, adoption of renewable energy, developing vehicle-to-vehicle communication in vehicles, and smart lamppost are the few recommendations for the implementation of digitalizing highways.}
}
@article{LIN2021108362,
title = {Uncertainty quantification and software risk analysis for digital twins in the nearly autonomous management and control systems: A review},
journal = {Annals of Nuclear Energy},
volume = {160},
pages = {108362},
year = {2021},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2021.108362},
url = {https://www.sciencedirect.com/science/article/pii/S0306454921002383},
author = {Linyu Lin and Han Bao and Nam Dinh},
keywords = {Digital twin, Autonomous control, Uncertainty quantification, Software risk analysis},
abstract = {A nearly autonomous management and control (NAMAC) system is designed to furnish recommendations to operators for achieving particular goals based on NAMAC’s knowledge base. As a critical component in a NAMAC system, digital twins (DTs) are used to extract information from the knowledge base to support decision-making in reactor control and management during all modes of plant operations. With the advancement of artificial intelligence and data-driven methods, machine learning algorithms are used to build DTs of various functions in the NAMAC system. To evaluate the uncertainty of DTs and its impacts on the reactor digital instrumentation and control systems, uncertainty quantification (UQ) and software risk analysis is needed. As a comprehensive overview of prior research and a starting point for new investigations, this study selects and reviews relevant UQ techniques and software hazard and software risk analysis methods that may be suitable for DTs in the NAMAC system.}
}
@article{KLYMKOWSKY2021308,
title = {Making mechanistic sense: are we teaching students what they need to know?},
journal = {Developmental Biology},
volume = {476},
pages = {308-313},
year = {2021},
issn = {0012-1606},
doi = {https://doi.org/10.1016/j.ydbio.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0012160621000993},
author = {Michael W. Klymkowsky},
abstract = {Evaluating learning outcomes depends upon objective and actionable measures of what students know – that is, what can they do with what they have learned. In the context of a developmental biology course, a capstone of many molecular biology degree programs, I asked students to predict the behaviors of temporal and spatial signaling gradients. Their responses led me to consider an alternative to conventional assessments, namely a process in which students are asked to build and apply plausible explanatory mechanistic models (“PEMMs”). A salient point is not whether students' models are correct, but whether they “work” in a manner consistent with underlying scientific principles. Analyzing such models can reveal the extent to which students recognize and accurately apply relevant ideas. An emphasis on model building, analysis and revision, an authentic scientific practice, can be expected to have transformative effects on course and curricular design as well as on student engagement and learning outcomes.}
}
@article{WANG2022116036,
title = {Attention-based dynamic user modeling and Deep Collaborative filtering recommendation},
journal = {Expert Systems with Applications},
volume = {188},
pages = {116036},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116036},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421013816},
author = {Ruiqin Wang and Zongda Wu and Jungang Lou and Yunliang Jiang},
keywords = {Short-term preferences, Long-term preferences, Dynamic preference modeling, Matching score prediction, Time-aware attention},
abstract = {Deep learning (DL) techniques have been widely used in recommender systems for user modeling and matching function learning based on historical interaction matrix. However, existing DL-based recommendation methods usually perform static user preference modeling by using historical interacted items of the user. In this article, we present a time-aware deep CF framework which contains two stages: dynamic user preference modeling based on attention mechanism and matching score prediction based on DL. In the first stage, short-term user preferences are modeled by the time-aware attention mechanism that fully considered the predicted item, the recent interacted items and their interaction time. The resulting short-term preferences are combined with long-term preferences for dynamic user preference modeling. In the second stage, high-order user-item feature interactions are learned by two types of DL models, Deep Matrix Factorization (DMF) and Multiple-Layer Perception (MLP), and the feature interaction vectors of the two models are fused in the last layer of the model to predict the matching score. Extensive experiments on five datasets indicate that our method is superior to the existing time-aware and DL-based recommendation methods in top-k recommendations significantly and consistently.}
}
@article{DIPALMA2021102136,
title = {Resolution-based distillation for efficient histology image classification},
journal = {Artificial Intelligence in Medicine},
volume = {119},
pages = {102136},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102136},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721001299},
author = {Joseph DiPalma and Arief A. Suriawinata and Laura J. Tafe and Lorenzo Torresani and Saeed Hassanpour},
keywords = {Deep neural networks, Digital pathology, Knowledge distillation, Self-supervised learning},
abstract = {Developing deep learning models to analyze histology images has been computationally challenging, as the massive size of the images causes excessive strain on all parts of the computing pipeline. This paper proposes a novel deep learning-based methodology for improving the computational efficiency of histology image classification. The proposed approach is robust when used with images that have reduced input resolution, and it can be trained effectively with limited labeled data. Moreover, our approach operates at either the tissue- or slide-level, removing the need for laborious patch-level labeling. Our method uses knowledge distillation to transfer knowledge from a teacher model pre-trained at high resolution to a student model trained on the same images at a considerably lower resolution. Also, to address the lack of large-scale labeled histology image datasets, we perform the knowledge distillation in a self-supervised fashion. We evaluate our approach on three distinct histology image datasets associated with celiac disease, lung adenocarcinoma, and renal cell carcinoma. Our results on these datasets demonstrate that a combination of knowledge distillation and self-supervision allows the student model to approach and, in some cases, surpass the teacher model's classification accuracy while being much more computationally efficient. Additionally, we observe an increase in student classification performance as the size of the unlabeled dataset increases, indicating that there is potential for this method to scale further with additional unlabeled data. Our model outperforms the high-resolution teacher model for celiac disease in accuracy, F1-score, precision, and recall while requiring 4 times fewer computations. For lung adenocarcinoma, our results at 1.25× magnification are within 1.5% of the results for the teacher model at 10× magnification, with a reduction in computational cost by a factor of 64. Our model on renal cell carcinoma at 1.25× magnification performs within 1% of the teacher model at 5× magnification while requiring 16 times fewer computations. Furthermore, our celiac disease outcomes benefit from additional performance scaling with the use of more unlabeled data. In the case of 0.625× magnification, using unlabeled data improves accuracy by 4% over the tissue-level baseline. Therefore, our approach can improve the feasibility of deep learning solutions for digital pathology on standard computational hardware and infrastructures.}
}
@article{DING2021104790,
title = {Control performance monitoring and degradation recovery in automatic control systems: A review, some new results, and future perspectives},
journal = {Control Engineering Practice},
volume = {111},
pages = {104790},
year = {2021},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.104790},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121000678},
author = {Steven X. Ding and Linlin Li},
keywords = {Control performance monitoring, Control performance degradation recovery, Loop performance degradation index},
abstract = {This paper addresses control performance monitoring (CPM) and degradation recovering in automatic control systems. It begins with a re-visit of CPM techniques and a summary of the major limitations of the existing CPM methods. They are (i) deficit in assessing control performance degradation caused by different types of disturbances and environment uncertainties, (ii) incapability for predicting performance degradation, and (iii) deficiency of efficient performance degradation recovering methods. In order to meet increasing demands of next generation automatic control systems for higher system performance, novel CPM methods have been developed in recent years, including performance assessment of control systems with deterministic disturbances and uncertainties, prediction of control performance degradation, and recovery of control performance degradation. Some of these methods and algorithms are introduced in the second part of this paper. The basis of these methods is a so-called residual centred model of feedback control systems, which allows a unified handling of control, monitoring and diagnosis in feedback control systems corrupted by disturbances and uncertainties. The focuses of these methods are on (i) introduction of the loop performance degradation index for the assessment and prediction of performance degradation in automatic control systems, (ii) predictive detection and estimation of loop performance degradation, and (iii) a data-driven performance degradation recovering scheme. The paper is concluded by a short summary of three future perspective topics, (i) prediction of economic system performance monitoring and estimation, (ii) reinforcement learning aided system performance recovery, and (iii) CPM digital twin.}
}
@article{MIRAZ2021100363,
title = {Adaptive user interfaces and universal usability through plasticity of user interface design},
journal = {Computer Science Review},
volume = {40},
pages = {100363},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100363},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000034},
author = {Mahdi H. Miraz and Maaruf Ali and Peter S. Excell},
keywords = {User interface (UI), Dynamic UI Design, Plasticity, Adaptive user interface (AUI), Universal usability, Interaction design, Inclusive Design, Design for All},
abstract = {A review of research on universal usability, plasticity of user interface design and facilitation of interface development with universal usability is presented. The survey was based on 165 research papers spanning over fifty-five years. The foundations of adaptive or intelligent user interfaces (AUI or IUI) are presented, three core domains being focused upon: Artificial Intelligence (AI), User Modelling (UM) and Human–Computer Interaction (HCI). For comparison of the various AUIs, a proposed taxonomy is given. One conclusion is that an efficient training vector for fast optimal convergence of the machine-learning algorithm is a necessity, but key to this is the bounding of the dataset, the goal being to achieve an accurate user preference model, which has to be built from a limited number of datasets obtained from the human interaction. More research also needs to be conducted to ascertain the usefulness and effectiveness of IUIs compared against AUIs. With the global mobility of users, interface design must take account of the abilities and cultures of users, derived from actual user behaviour and not on their feedback. A key question is whether the interface should be adaptive under system control or be made adaptable under user control. A need is identified for an “afferential component” that stores a priori information about the end user, an “inferential component” that determines to what extent the user interface actually needs to be adapted, and the “efferential component” that actually determines how the adaptivity is applied seamlessly to the system. Application to e-learning is a priority: the use of machine intelligence to achieve appropriate learnability, ideally enhanced by “Playful interaction”, was found to be desirable. Universal application of adaptation lies in the future, but AUI properties cannot be ascertained while disregarding the other parameters of the system in which it will be used. A more complete understanding of the human mental model is necessary, requiring a highly multidisciplinary approach and cooperation between diverse researchers. Finally, a performance evaluation of plasticity of user interface was conducted: it is concluded that the use of dynamic techniques can enhance the user experience to a much greater extent than more basic approaches, although optimisation of usability parameter trade-offs needs further attention. It is noted that most of the work reviewed originated from a limited range of cultural perspectives. To make an interface simultaneously usable for users from a diverse range of cultural backgrounds will require a very large amount of adaptation, but the powerful principles of plasticity of user interface design hold the future promise of an optimum tool to achieve cross-cultural usability.}
}
@article{GAWADE2022875,
title = {Leveraging simulated and empirical data-driven insight to supervised-learning for porosity prediction in laser metal deposition},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {875-885},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001503},
author = {Vidita Gawade and Vani Singh and Weihong “Grace” Guo},
keywords = {Additive manufacturing, Machine learning, Porosity prediction},
abstract = {The advent of digital-twin manufacturing in additive manufacturing (AM) is to integrate the physical world of real-time 3D printing with the digital world of a simulated print. This paper contributes to digital-twin manufacturing in laser-based additive manufacturing by combining melt pools’ simulated thermal behavior via finite element analysis (FEA) and melt pools’ empirical thermal behavior via pyrometry-based sensors. Studying the thermal behavior of melt pools based on heat transfer characteristics determines melt pools’ porosity and part quality. FEA uses Godak’s moving heat flux to capture the melt pools’ physically bound temperature profile in three dimensions. Simulated data helps to mitigate the influence of measuring errors from real-world data and provides non-observable data such as gradient changes of thermal behavior at the curvature of the 3D melt pool. The pyrometer captures empirical temperature behavior, including uncertainty and randomness introduced to the process. A significant knowledge gap exists when predicting melt pool porosity accurately with theoretical FEA and empirical in situ evidence alone. The gap is bridged by combining the data sources, specifically, feature engineering via functional principal component analysis (empirical data source) and capturing the melt pool's 3-D temperature shape profile via FEA (simulated data source). A hybrid model predicts melt pool porosity by capturing the strengths of prior simulated and posterior in situ empirical data by matching simulated melt pools to real-world empirical melt pools. Moreover, comparing predicted porosity labels with true porosity labels of Ti–6Al–4V thin-wall structure from laser metal deposition verified the proposed interpretable and robust supervised-learning model's validity. This methodology can apply to other materials and part shapes printed under various additive-manufactured printers.}
}
@article{LI2021107862,
title = {Hierarchical distillation learning for scalable person search},
journal = {Pattern Recognition},
volume = {114},
pages = {107862},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.107862},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321000492},
author = {Wei Li and Shaogang Gong and Xiatian Zhu},
keywords = {Person search, Person re-identification, Person detection, Knowledge distillation, Scalability, Model inference efficiency},
abstract = {Existing person search methods typically focus on improving person detection accuracy. This ignores the model inference efficiency, which however is fundamentally significant for real-world applications. In this work, we address this limitation by investigating the scalability problem of person search involving both model accuracy and inference efficiency simultaneously. Specifically, we formulate a Hierarchical Distillation Learning (HDL) approach. With HDL, we aim to comprehensively distil the knowledge of a strong teacher model with strong learning capability to a lightweight student model with weak learning capability. To facilitate the HDL process, we design a simple and powerful teacher model for joint learning of person detection and person re-identification matching in unconstrained scene images. Extensive experiments show the modelling advantages and cost-effectiveness superiority of HDL over the state-of-the-art person search methods on three large person search benchmarks: CUHK-SYSU, PRW, and DukeMTMC-PS.}
}
@article{LI2022104957,
title = {A flexible manufacturing assembly system with deep reinforcement learning},
journal = {Control Engineering Practice},
volume = {118},
pages = {104957},
year = {2022},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.104957},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121002343},
author = {Junzheng Li and Dong Pang and Yu Zheng and Xinping Guan and Xinyi Le},
keywords = {Reinforcement learning, Digital twin, Flexible manufacture, Assembly line},
abstract = {Traditional assembly line requires a significant amount of designs from engineers, especially in the case of multi-species and small-lot production. Recently, intelligent algorithms based on reinforcement learning are proposed to address this issue. However, the lower success rate and safety reasons limit their industrial applications. In this article, we proposed a systematic solution, including the automatic planning of assembly motions and the monitoring system of the production lines. In the planning stage, we built the digital twin model of the assembly line, then trained a deep reinforcement learning agent to assembly the workpieces. In the production stage, the digital twin model is used to monitor the assembly lines and predict failures. To validate the system we proposed, we conducted a peg-in-hole assembly experiment, and reached a 90% success rate for a single assembly attempt. During the whole experiment, no collision happens in the real world.}
}
@article{MATULIS2021106,
title = {A robot arm digital twin utilising reinforcement learning},
journal = {Computers & Graphics},
volume = {95},
pages = {106-114},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S009784932100011X},
author = {Marius Matulis and Carlo Harvey},
keywords = {Robot arm, Reinforcement learning, Artificial intelligence, Digital twin},
abstract = {For many industry contexts, the implementation of Artificial Intelligence (AI) has contributed to what has become known as the fourth industrial revolution or “Industry 4.0” and creates an opportunity to deliver significant benefit to both businesses and their stakeholders. Robot arms are one of the most common devices utilised in manufacturing and industrial processes, used for a wide variety of automation tasks on, for example, a factory floor but the effective use of these devices requires AI to be appropriately trained. One approach to support AI training of these devices is the use of a “Digital Twin”. There are, however, a number of challenges that exist within this domain, in particular, success depends upon the ability to collect data of what are considered as observations within the environment and the application of one or many trained AI policies to the task that is to be completed. This project presents a case-study of creating and training a Robot Arm Digital Twin as an approach for AI training in a virtual space and applying this simulation learning within physical space. A virtual space, created using Unity (a contemporary Game Engine), incorporating a virtual robot arm was linked to a physical space, being a 3D printed replica of the virtual space and robot arm. These linked environments were applied to solve a task and provide training for an AI model. The contribution of this work is to provide guidance on training protocols for a digital twin together with details of the necessary architecture to support effective simulation in a virtual space through the use of Tensorflow and hyperparameter tuning. It provides an approach to addressing the mapping of learning in the virtual domain to the physical robot twin.}
}
@article{GHOSH2021100242,
title = {Developing sensor signal-based digital twins for intelligent machine tools},
journal = {Journal of Industrial Information Integration},
volume = {24},
pages = {100242},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100242},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000418},
author = {Angkush Kumar Ghosh and AMM Sharif Ullah and Roberto Teti and Akihiko Kubo},
keywords = {Digital twin, Sensor signal, Cyber-physical systems, Machine tool, Monitoring},
abstract = {Digital twins can assist machine tools in performing their monitoring and troubleshooting tasks autonomously from the context of smart manufacturing. For this, a special type of twin denoted as sensor signal-based twin must be constructed and adapted into the cyber-physical systems. The twin must (1) machine-learn the required knowledge from the historical sensor signal datasets, (2) seamlessly interact with the real-time sensor signals, (3) handle the semantically annotated datasets stored in clouds, and (4) accommodate the data transmission delay. The development of such twins has not yet been studied in detail. This study fills this gap by addressing sensor signal-based digital twin development for intelligent machine tools. Two computerized systems denoted as Digital Twin Construction System (DTCS) and Digital Twin Adaptation System (DTAS) are proposed to construct and adapt the twin, respectively. The modular architectures of the proposed DTCS and DTAS are presented in detail. The real-time responses and delay-related computational arrangements are also elucidated for both systems. The systems are also developed using a Java™-based platform. Milling torque signals are used as an example to demonstrate the efficacy of DTCS and DTAS. This study thus contributes toward the advancement of intelligent machine tools from the context of smart manufacturing.}
}
@article{YANG2021100088,
title = {Implementation for a cloud battery management system based on the CHAIN framework},
journal = {Energy and AI},
volume = {5},
pages = {100088},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000422},
author = {Shichun Yang and Zhengjie Zhang and Rui Cao and Mingyue Wang and Hanchao Cheng and Lisheng Zhang and Yinan Jiang and Yonglin Li and Binbin Chen and Heping Ling and Yubo Lian and Billy Wu and Xinhua Liu},
keywords = {Battery, CHAIN, Cloud, Battery management system, SOX estimation, end-edge-cloud architecture},
abstract = {Summary
An intelligent battery management system is a crucial enabler for energy storage systems with high power output, increased safety and long lifetimes. With recent developments in cloud computing and the proliferation of big data, machine learning approaches have begun to deliver invaluable insights, which drives adaptive control of battery management systems (BMS) with improved performance. In this paper, a general framework utilizing an end-edge-cloud architecture for a cloud-based BMS is proposed, with the composition and function of each link described. Cloud-based BMS leverages from the Cyber Hierarchy and Interactional Network (CHAIN) framework to provide multi-scale insights, more advanced and efficient algorithms can be used to realize the state-of-X estimation, thermal management, cell balancing, fault diagnosis and other functions of traditional BMS system. The battery intelligent monitoring and management platform can visually present battery performance, store working-data to help in-depth understanding of the microscopic evolutionary law, and provide support for the development of control strategies. Currently, the cloud-based BMS requires more effects on the multi-scale integrated modeling methods and remote upgrading capability of the controller, these two aspects are very important for the precise management and online upgrade of the system. The utility of this approach is highlighted not only for automotive applications, but for any battery energy storage system, providing a holistic framework for future intelligent and connected battery management.}
}
@article{YOU2022117899,
title = {Digital twins based day-ahead integrated energy system scheduling under load and renewable energy uncertainties},
journal = {Applied Energy},
volume = {305},
pages = {117899},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117899},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921012125},
author = {Minglei You and Qian Wang and Hongjian Sun and Iván Castro and Jing Jiang},
keywords = {Digital twins, Multi-vector energy system, Integrated energy system, Machine learning},
abstract = {By constructing digital twins (DT) of an integrated energy system (IES), one can benefit from DT’s predictive capabilities to improve coordinations among various energy converters, hence enhancing energy efficiency, cost savings and carbon emission reduction. This paper is motivated by the fact that practical IESs suffer from multiple uncertainty sources, and complicated surrounding environment. To address this problem, a novel DT-based day-ahead scheduling method is proposed. The physical IES is modelled as a multi-vector energy system in its virtual space that interacts with the physical IES to manipulate its operations. A deep neural network is trained to make statistical cost-saving scheduling by learning from both historical forecasting errors and day-ahead forecasts. Case studies of IESs show that the proposed DT-based method is able to reduce the operating cost of IES by 63.5%, comparing to the existing forecast-based scheduling methods. It is also found that both electric vehicles and thermal energy storages play proactive roles in the proposed method, highlighting their importance in future energy system integration and decarbonisation.}
}
@article{SHI2022108316,
title = {Explainable scale distillation for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {122},
pages = {108316},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108316},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004969},
author = {Cheng Shi and Li Fang and Zhiyong Lv and Minghua Zhao},
keywords = {Hyperspectral image classification, Knowledge distillation, Scale distillation, Explainable scale network},
abstract = {The land-covers within an observed remote sensing scene are usually of different scales; therefore, the ensemble of multi-scale information is a commonly used strategy to achieve more accurate scene interpretation; however, this process suffers from being time-consuming. In terms of this issue, this paper proposes a scale distillation network to explore the possibility that single-scale classification network can achieve the same (or even better) classification performance compared with multi-scale one. The proposed scale distillation network consists of a cumbersome multi-scale teacher network and a lightweight single-scale student network. The former is trained for multi-scale information learning, and the latter improves the classification accuracy by accepting the knowledge from the multi-scale teacher network and its true label. The experimental results show the advantages of scale distillation on hyperspectral image classification. The single-scale student network can even achieve higher evaluation accuracy than the multi-scale teacher network. In addition, a faithful explainable scale network is designed to visually explain the trained scale distillation network. The traditional deep neural network is a black-box and lacks interpretability. The explanation of the trained network can explore more hidden information from the predictions. We visually explain the prediction results of scale distillation network, and the results show that the explainable scale network can more precisely analyze the relationship between the learned scale features and the land-cover categories. Moreover, the possible application of the explainable scale network on classification is further discussed in this study.}
}
@article{BERMEOAYERBE2022121691,
title = {Data-driven energy prediction modeling for both energy efficiency and maintenance in smart manufacturing systems},
journal = {Energy},
volume = {238},
pages = {121691},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121691},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221019393},
author = {Miguel Angel Bermeo-Ayerbe and Carlos Ocampo-Martinez and Javier Diaz-Rozo},
keywords = {Non-intrusive load monitoring, Data-driven model, Subspace identification, Energy models, Concept drift, Digital twin, Gaussian mixture models, Energy efficiency, Machine fault diagnosis},
abstract = {The optimization and monitoring of the energy consumption of machinery lead to a sustainable and efficient industry. For this reason and following a digital twin strategy, an online data-driven energy modeling approach with adaptive capabilities has been proposed and described throughout this paper. This approach is useful in developing robust energy management systems that enhance the energy efficiency of industrial machinery. In this way, the dynamic behavior of their energy consumption is modeled without using phenomenological laws. In contrast, traditional methodologies hardly consider such dynamic behavior or use an exhaustive modeling process. The proposed approach includes an adaptive mechanism to consider the natural degradation of machinery. This mechanism is based on a concept drift detector, which detects when the current consumption of the machine is not correctly represented by the model estimation and adapts the model to account for these new behaviors. The concept drift detector has broad applicability in the face of reducing maintenance costs, measuring the impact and evolution of either abnormal behaviors (e.g., failures) or degradation, and identify which elements change. The proposed methodology has been validated in an industrial testbed. An experiment with three emulated concept drifts was carried out in the testbed. As a result, the proposed adaptive approach obtained more than doubled the fit rate of the energy prediction/estimation compared to the non-adaptive model and successfully detected these changes in energy consumption.}
}
@article{WANG2022124,
title = {Digital twin and cloud-side-end collaboration for intelligent battery management system},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {124-134},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521002284},
author = {Yujie Wang and Ruilong Xu and Caijie Zhou and Xu Kang and Zonghai Chen},
keywords = {Digital twin, Battery management system, Cloud computing, Internet of Things, Distributed computing},
abstract = {Nowadays the wave of digital economy has swept the world, and the competition in the field of battery management has become increasingly vigorous. The application of digital twin technology gives a new concept of networked management and service of lithium-ion batteries. In this paper, the digital twin technology and cloud-side-end collaboration for the future battery management system is discussed. A four layer networked architecture of cloud-side-end collaboration for battery management system is presented which breaks through the computing capacity and storage space limitations of the conventional battery management and enables high performance algorithms. The digital twin model of the battery is established, which enables refined and safety management of the batteries in their entire life cycle. Furthermore, the digital twin model and key technologies such as state estimation and cloud assisted equalization of the batteries are introduced. The results indicate that digital twin models are helpful for battery management and the full life cycle data are useful to build the upgrade route of the battery.}
}
@article{PYLIANIDIS2022105274,
title = {Simulation-assisted machine learning for operational digital twins},
journal = {Environmental Modelling & Software},
volume = {148},
pages = {105274},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105274},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221003169},
author = {Christos Pylianidis and Val Snow and Hiske Overweg and Sjoukje Osinga and John Kean and Ioannis N. Athanasiadis},
keywords = {Machine learning, Digital twin, Data availability, Data resolution, APSIM, Metamodel},
abstract = {In the environmental sciences, there are ongoing efforts to combine multiple models to assist the analysis of complex systems. Combining process-based models, which have encoded domain knowledge, with machine learning models, which can flexibly adapt to input data, can improve modeling capabilities. However, both types of models have input data limitations. We propose a methodology to overcome these issues by using a process-based model to generate data, aggregating them to a lower resolution to mimic real situations, and developing machine learning models using a fraction of the process-based model inputs. We showcase this method with a case study of pasture nitrogen response rate prediction. We train models of different scales and test them in sampled and unsampled location experiments to assess their practicality in terms of accuracy and generalization. The resulting models provide accurate predictions and generalize well, showing the usefulness of the proposed method for tactical decision support.}
}
@article{MEIXEDO2021112189,
title = {Damage detection in railway bridges using traffic-induced dynamic responses},
journal = {Engineering Structures},
volume = {238},
pages = {112189},
year = {2021},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2021.112189},
url = {https://www.sciencedirect.com/science/article/pii/S0141029621003394},
author = {Andreia Meixedo and João Santos and Diogo Ribeiro and Rui Calçada and Michael Todd},
keywords = {Damage detection, Unsupervised learning, Structural Health Monitoring, Traffic-induced dynamic responses, Autoregressive models, PCA, Regression},
abstract = {This paper aims at detecting damage in railway bridges based on traffic-induced dynamic responses. To achieve this goal, an unsupervised automatic data-driven methodology is proposed, consisting of a combination of time series analysis methods and multivariate statistical techniques. Damage-sensitive features of train-induced responses are extracted and allow taking advantage, not only of the repeatability of the loading, but also, and more importantly, of its great magnitude, thus enhancing the sensitivity to small-magnitude structural changes. The efficiency of the proposed methodology is validated in a long-span steel-concrete composite bowstring-arch railway bridge with a permanent structural monitoring system installed. An experimentally validated finite element model was used, along with experimental values of temperature, noise, and train loadings and speeds, to realistically simulate baseline and damage scenarios. The proposed methodology proved to be highly sensitive in detecting early damage, even when it consists of small stiffness reductions that do not impair the safety or use of the structure, and highly robust to false detections. The analysis and validation allowed concluding that the ability to identify early damage, imperceptible in the original signals, while avoiding observable changes induced by variations in train speed or temperature, was achieved by carefully defining the modelling and fusion sequence of the information. A single-value damage indicator, proposed as a tool for real-time structural assessment of bridges without interfering with the normal service condition, proved capable of characterizing multi-sensor data while being sensitive to identify local changes.}
}
@article{LI2021104939,
title = {A numerical integrated approach for the estimation of the uniaxial compression strength of rock from point load tests},
journal = {International Journal of Rock Mechanics and Mining Sciences},
volume = {148},
pages = {104939},
year = {2021},
issn = {1365-1609},
doi = {https://doi.org/10.1016/j.ijrmms.2021.104939},
url = {https://www.sciencedirect.com/science/article/pii/S1365160921003233},
author = {Yi-Ming Li and Gao-Feng Zhao},
keywords = {Point load test, Uniaxial compressive strength, Digital twin, Numerical modelling, Lattice spring model},
abstract = {The point load test (PLT) has been considered a flexible approach to estimate the uniaxial compressive strength (UCS) of rocks. Previously, empirical equations were obtained by mathematical fitting or machine learning to predict the UCS of rocks. The acquisition of these equations usually required a large amount of experimental data, while the corresponding parameters often lacked clear physical meanings, and the applicability of these empirical equations was limited. In this work, we attempted to develop a new method to predict the UCS of rocks by using the concept of a digital twin (numerical modelling). First, an automatic calibration procedure was used to obtain the numerical parameters of a digital twin for the PLT. Next, the UCS was predicted numerically by using a digital twin of the UCS test with the calibrated parameters. We performed a comprehensive comparison of our proposed method with previously obtained empirical equations and showed the superiority of our approach in better predicting the UCS of rocks. In this paper, we also discuss the influence of particle size and heterogeneity of rock material to illustrate the possible merits of the proposed method. Our work also shows the possible benefits of integrating numerical modelling into physical experimental tests of rocks.}
}
@article{ABBASI2021102176,
title = {Classification of diabetic retinopathy using unlabeled data and knowledge distillation},
journal = {Artificial Intelligence in Medicine},
volume = {121},
pages = {102176},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102176},
url = {https://www.sciencedirect.com/science/article/pii/S093336572100169X},
author = {Sajjad Abbasi and Mohsen Hajabdollahi and Pejman Khadivi and Nader Karimi and Roshanak Roshandel and Shahram Shirani and Shadrokh Samavi},
keywords = {Convolutional neural networks (CNN), Transfer learning, Knowledge distillation, Teacher-student model, Unlabeled data, Diabetic retinopathy},
abstract = {Over the last decade, advances in Machine Learning and Artificial Intelligence have highlighted their potential as a diagnostic tool in the healthcare domain. Despite the widespread availability of medical images, their usefulness is severely hampered by a lack of access to labeled data. For example, while Convolutional Neural Networks (CNNs) have emerged as an essential analytical tool in image processing, their impact is curtailed by training limitations due to insufficient labeled data availability. Transfer Learning enables models developed for one task to be reused for a second task. Knowledge distillation enables transferring knowledge from a pre-trained model to another. However, it suffers from limitations, and the two models' constraints need to be architecturally similar. Knowledge distillation addresses some of the shortcomings of transfer learning by generalizing a complex model to a lighter model. However, some parts of the knowledge may not be distilled by knowledge distillation sufficiently. In this paper, a novel knowledge distillation approach using transfer learning is proposed. The proposed approach transfers the complete knowledge of a model to a new smaller one. Unlabeled data are used in an unsupervised manner to transfer the new smaller model's maximum amount of knowledge. The proposed method can be beneficial in medical image analysis, where labeled data are typically scarce. The proposed approach is evaluated in classifying images for diagnosing Diabetic Retinopathy on two publicly available datasets, including Messidor and EyePACS. Simulation results demonstrate that the approach effectively transfers knowledge from a complex model to a lighter one. Furthermore, experimental results illustrate that different small models' performance is improved significantly using unlabeled data and knowledge distillation.}
}
@article{LWANDE2021e07701,
title = {Identifying learning styles and cognitive traits in a learning management system},
journal = {Heliyon},
volume = {7},
number = {8},
pages = {e07701},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e07701},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021018041},
author = {Charles Lwande and Lawrence Muchemi and Robert Oboko},
keywords = {Learning style, Cognitive trait, Learner behavior, Learning management system, Learner modeling},
abstract = {Investigating learner behavior is an increasingly important research topic in online learning. Learning styles and cognitive traits have been the subjects of research in this area. Although learning institutions use Learning Management Systems such as Moodle, Claroline, and Blackboard to facilitate teaching, the platforms do not have features for analyzing data and identifying behavior such as learning styles and cognitive traits. Instead, they only produce certain statistical reports from the daily access records. Even though complex models have been proposed in the literature, most studies are based on a single behavior such as learning styles or cognitive traits but not both. Only a few have investigated a combination of cognition-based theories such as working memory capacity and psychology-based ones such as learning styles. Thus, this study sought to answer the research question of whether it was possible to establish a methodology for the estimation of learning styles and cognitive traits from a learning management system. The study combined the Felder-Silverman Learning Style Model and Cognitive Trait Model as theoretical frameworks to identify behavior in a Learning Management System. This study designed a model for extracting records from Learning Management Systems access records to estimate learning style and cognitive traits. From this, a prototype was developed to estimate the learning style and cognitive traits for each student. The model was evaluated by administering manual tools to students in a classroom environment then comparing the results gathered against those estimated by the model. The results analyzed using Kappa statistics demonstrated the interrater reliability results were moderately in agreement. Taken together, these results suggest that it is possible to estimate the learning styles and cognitive traits of a learner in a Learning Management System. The information generated by the model can be used by tutors to provide a conducive online learning environment where learners with similar behavior ask each other for help. This can reduce the teaching load for online tutors because learners themselves act as a teaching resource. Information on learning styles and cognitive styles can also facilitate online group formation by isolating the individual factors that contribute to team success.}
}
@article{PEI2021127,
title = {The digital twin of the quality monitoring and control in the series solar cell production line},
journal = {Journal of Manufacturing Systems},
volume = {59},
pages = {127-137},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S027861252100025X},
author = {Feng-Que Pei and Yi-Fei Tong and Ming-Hai Yuan and Kun Ding and Xi-Hui Chen},
keywords = {Digital twin, Quality monitoring & control, Series solar cell production line},
abstract = {With the development of intelligent manufacturing (IM), the Digital twin (DT) has become an important means to the evolution mechanism of the process. Many researchers pay attention on the realization of DT in different industries. Based on the DT and Digital Twin Shop Floor (DTS) model, a novel, high throughput metrology method is proposed in the process quality monitoring and control of the Series Solar Cell Production Line (SSCPL) for detailed performance analysis. The variance of individual loss parameters and their impact on quality performance are quantified and mapped into the virtual space. The nature of their distributions and correlations provide great insights about quality loss mechanisms in process monitoring, helping to prioritize efforts for optimizing the control of the SSCPL in the physical space. Additionally, the parameters can be tied back to the physical space, allowing the data to be used directly for the control in the manufacturing. The data-loop of “Autonomous perception of process parameters - Dynamic behaver mapping - Online monitoring - Online data analysis - Parameters configuration & control” can be obtained in the model. This paper provides an application paradigm for DT and IM.}
}
@article{SHRIVASTAVA2021107404,
title = {Addressing the challenges in remanufacturing by laser-based material deposition techniques},
journal = {Optics & Laser Technology},
volume = {144},
pages = {107404},
year = {2021},
issn = {0030-3992},
doi = {https://doi.org/10.1016/j.optlastec.2021.107404},
url = {https://www.sciencedirect.com/science/article/pii/S0030399221004928},
author = {Ankit Shrivastava and Sumanta Mukherjee and Shitanshu S. Chakraborty},
keywords = {Remanufacturing, Laser additive manufacturing, Process monitoring, Laser remelting, Laser cladding},
abstract = {Increased focus on reduction of impact on the environment has put the aspect of remanufacturing in the spotlight, and remanufacturing of high-value engineering components is gradually becoming a mainstream practice. Out of different alternatives, laser-based deposition has been the central choice for remanufacturing, thanks to its accuracy, and precision. However, considering the complex process physics involved in laser-based remanufacturing processes, it is essential to establish the reliability of the process so that certifiable remanufactured parts can be produced. This work provides a comprehensive analysis of the issues encountered during laser-based remanufacturing, and the different approaches to address them. Apart from covering the state-of-the-art of remanufacturing by laser-based deposition, this article also discusses tools like deep learning, and digital twin which are still in their early phases in terms of applications in the remanufacturing domain.}
}
@article{ELMGHOUCHI2022100157,
title = {On the prediction of daily global solar radiation using temperature as input. An application of hybrid machine learners to the six climatic Moroccan zones},
journal = {Energy Conversion and Management: X},
volume = {13},
pages = {100157},
year = {2022},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2021.100157},
url = {https://www.sciencedirect.com/science/article/pii/S2590174521000829},
author = {Y. {El Mghouchi}},
keywords = {Solar radiation forecasting, Temperature-based models, Least Square regression, Machines learning, Moroccan climatic zones},
abstract = {Forecasting of solar radiation intensity is a necessity for the establishment of solar energy projects and for decision-making in other related fields. Current prediction models/methods are site-dependent and their performance/accuracy outside the area of application is debatable. Temperature-based solar radiation models are highly recommended in areas where only air temperature data is available. Therefore, the purpose of this study is to evaluate the prediction accuracy of 42 existing temperature-based solar radiation models in forecasting the daily global solar radiation (DGSR) on horizontal surface for the six climatic zones of Morocco. In the first time, the models were assessed using only the least square method. Then, four Machine Learners models (SVM, Decision Tree, Gaussian Regression and Linear Regression) were employed as optimizers to improve the accuracy prediction of the models. The results differ from model to another based on their values of MBE, MSE, RMSE, σ and R2. Two methods were employed for ordering the studied models: the Performance score and the Taylor diagram. Long term meteorological data was used in the evaluation processes. The correlation R2 of the optimized models changes from 0.80 to 0.95 for all skies and from 0.95 to 0.98 for clear skies.}
}
@article{ABDELRAHMAN2022108532,
title = {Personal thermal comfort models using digital twins: Preference prediction with BIM-extracted spatial–temporal proximity data from Build2Vec},
journal = {Building and Environment},
volume = {207},
pages = {108532},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108532},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321009240},
author = {Mahmoud M. Abdelrahman and Adrian Chong and Clayton Miller},
keywords = {Spatial–temporal modeling, Building information models, Graph network structure, Personal thermal comfort model, Digital twin},
abstract = {Conventional thermal preference prediction in buildings has limitations due to the difficulty in capturing all environmental and personal factors. New model features can improve the ability of a machine learning model to classify a person’s thermal preference. The spatial context of a building can provide information to models about the windows, walls, heating and cooling sources, air diffusers, and other factors that create micro-environments that influence thermal comfort. Due to spatial heterogeneity, it is impractical to position sensors at a high enough resolution to capture all conditions. This research aims to build upon an existing vector-based spatial model, called Build2Vec, for predicting spatial–temporal occupants’ indoor environmental preferences. Build2Vec utilizes the spatial data from the Building Information Model (BIM) and indoor localization in a real-world setting. This framework uses longitudinal intensive thermal comfort subjective feedback from smart watch-based ecological momentary assessments (EMA). The aggregation of these data is combined into a graph network structure (i.e., objects and relations) and used as input for a classification model to predict occupant thermal preference. The results of a test implementation show 14%–28% accuracy improvement over a set of baselines that use conventional thermal preference prediction input variables.}
}
@article{ALSAIHATI2022109335,
title = {Rate of penetration prediction while drilling vertical complex lithology using an ensemble learning model},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109335},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109335},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521009852},
author = {Ahmed Alsaihati and Salaheldin Elkatatny and Hany Gamal},
keywords = {Rate of penetration, Complex lithology, Principal component analysis, Artificial intelligence, Ensemble learning},
abstract = {The rate of penetration (ROP) accounts for a substantial portion of the overall drilling cost. The drilling optimization process, which mostly involves the adjustment of the mechanical drilling parameters, is therefore of prime importance in ensuring efficient drilling. However, drilling formations with assorted types of lithology necessitate the involvement of more parameters to reduce uncertainty and enhance confidence when predicting the ROP. The objective of this paper is to introduce an ensemble model based on random forest (RF), in which artificial neural network (ANN), and adaptive neuro-fuzzy inference system (ANFIS) are the base learner models, to predict the ROP across different lithological formations In this study, two types of actual field data of Well-1 were employed to build the model: (i) mechanical drilling parameters collected from real-time sensors allocated at the rig site, and (ii) petrophysical properties obtained from conventional well logs. Well-2 with more than 2300 unseen data points was used to compare the capability of the base learners (i.e., ANN and ANFIS), standalone RF, and RF-meta model in predicting the ROP with two of the earliest published ROP empirical models (Maurer's and Bingham's models). The results showed that the RF-meta model outperformed the base learners and Maurer's and Bingham's empirical models in predicting the ROP in Well-2 with a low absolute average percentage error (AAPE) of 7.8 % and a high coefficient of determination (R2) of 0.94.}
}
@article{DAVILADELGADO2021101332,
title = {Digital Twins for the built environment: learning from conceptual and process models in manufacturing},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101332},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101332},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000859},
author = {Juan Manuel {Davila Delgado} and Lukumon Oyedele},
keywords = {Digital Twin, BIM, Cyber-Physical Systems, Conceptual Models, Process Models, Maturity Models},
abstract = {The overall aim of this paper is to contribute to a better understanding of the Digital Twin (DT) paradigm in the built environment by drawing inspiration from existing DT research in manufacturing. The DT is a Product Life Management information construct that has migrated to the built environment while research on the subject has grown intensely in recent years. Common to early research phases, DT research in the built environment has developed organically, setting the basis for mature definitions and robust research frameworks. As DT research in manufacturing is the most developed, this paper seeks to advance the understanding of DTs in the built environment by analysing how the DT systems reported in manufacturing literature are structured and how they function. Firstly, this paper presents a thorough review and a comparison of DT, cyber-physical systems (CPS), and building information modelling (BIM). Then, the results of the review and categorisation of DT structural and functional descriptions are presented. Fifty-four academic publications and industry reports were reviewed, and their structural and functional descriptions were analysed in detail. Three types of structural models (i.e. conceptual models, system architectures, and data models) and three types of functional models (process and communication models) were identified. DT maturity models were reviewed as well. From the reviewed descriptions, four categories of DT conceptual models (prototypical, model-based, interface-oriented, and service-based) and six categories of DT process models (DT creation, DT synchronisation, asset monitoring, prognosis and simulation, optimal operations, and optimised design) were defined and its applicability to the AECO assessed. While model-based and service-based models are the most applicable to the built environment, amendments are still required. Prognosis and simulation process models are the most widely applicable for AECO use-cases. The main contribution to knowledge of this study is that it compiles the DT’s structural and functional descriptions used in manufacturing and it provides the basis to develop DT conceptual and process models specific to requirements of the built environment sectors.}
}
@article{FENG2022108165,
title = {An adaptive learning approach to determine and update crack sizes from strain relaxation data for welded plate joints},
journal = {Engineering Fracture Mechanics},
volume = {259},
pages = {108165},
year = {2022},
issn = {0013-7944},
doi = {https://doi.org/10.1016/j.engfracmech.2021.108165},
url = {https://www.sciencedirect.com/science/article/pii/S0013794421005701},
author = {Liuyang Feng and Xudong Qian},
keywords = {Crack sizing, Neural network, Modified bootstrap particle filtering, Strain relaxation, Welded plate joints, Digital twin},
abstract = {This paper proposes the framework to determine and update the crack-front profile at the toe of welded plate joints based on the strain relaxation data. This study determines the crack depth by classifying the nodes in the thickness direction as open nodes on the crack surface and closed nodes on the intact ligament. To update the crack size during the loading history, this research employs the modified bootstrap particle filtering approach, which entails enhanced adjustment capabilities by imposing additional uncertainty distributions. This approach improves the crack size prediction by absorbing limited measurement data on the strain values or crack sizes.}
}
@article{WU2021114292,
title = {Similarity based telemetry data recovery for enhancing operating reliability of satellite},
journal = {Microelectronics Reliability},
volume = {126},
pages = {114292},
year = {2021},
note = {Proceedings of ESREF 2021, 32nd European Symposium on Reliability of Electron Devices, Failure Physics and Analysis},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2021.114292},
url = {https://www.sciencedirect.com/science/article/pii/S0026271421002584},
author = {Y. Wu and J. Liang and Y. Peng},
keywords = {Similarity searching, Data recovery, LSTM},
abstract = {Recently, the satellite launching grows rapidly, especially in the commercial aerospace, which produces thousands of in-orbit spacecrafts. The operation and ground management of these satellites is a hotspot issue to ensure the satellites' in-orbit operation reliability. The telemetry data transmission may be delayed by the unstable telemetry link, insufficient ground station sources, interface failures and so on, which exerts a negative influence on the ground management capability and increases the failure risk. Meanwhile, the deep space spacecrafts, space station and Mars exploration expand this transmission delay. Thus, management and intelligent analysis of the telemetry data from Telemetry Ground Station (TGS) gradually become critical for in-orbit satellite operation. To solve above issues, a similarity-based deep learning time series prediction method is proposed to achieve telemetry data recovery. Considering the pseudo-periodic characteristics in telemetry data, this approach extracts similar segments to construct training samples instead of feeding all the observation samples to train the prediction model. As the predicted data is obtained ahead of the actual time-delayed telemetry data, which can provide sufficient analysis time for ground operator to optimize the maintenance strategy. Furthermore, the telemetry prediction data can also guarantee high-complexity digital twin modelling, virtual maintenance and so on. Experimental results with actual satellite telemetry data verify the effectiveness of the proposed method.}
}
@article{ZHANG2022101481,
title = {Implementation path and reference framework for Industrial Internet Platform (IIP) in product service system using industrial practice investigation method},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101481},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101481},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002317},
author = {Xianyu Zhang and Xinguo Ming},
keywords = {Industrial internet, Industrial internet platform, industrial internet of things (IIoT), internet of things (IoT), Smart manufacturing, Intelligent manufacturing},
abstract = {With the development of intelligent sensing, edge computing, fog computing, cloud computing, parallel computing, smart grid, big data, block chain, 5G, cyber-physical systems, digital twins, machine learning and other technologies, the industrial internet has undergone control network stage, sensor network stage, internet stage, Internet of Things (IoT) stage, Industrial Internet of Things (IIoT) stage, and Industrial Internet (II) stage, etc. In the existing research, scholars focus on a local dot, such as: technology, function, elements and application based on industrial internet. However, there is a lack of an overall framework to study the top-level planning of Industrial Internet Platform (IIP) from a systematic perspective. On the other hand, there are few studies on the detailed path and steps for implementing IIP in a specific enterprise in a specific industry. The objective of this paper is to study a reference framework and industrial implementation path for IIP in product service system using industrial practice investigation method, which meets the needs of industry on the basis of existing theory and industrial practice, and to provide reference for government and industry planning, design, implementation and promotion of IIP. In addition, the proposed reference framework and industrial implementation for IIP in product service system can enhance the core value of the enterprise and increase benefits.}
}
@article{WEI2022106977,
title = {Super-learner model realizes the transient prediction of CO2 and NOx of diesel trucks: Model development, evaluation and interpretation},
journal = {Environment International},
volume = {158},
pages = {106977},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2021.106977},
url = {https://www.sciencedirect.com/science/article/pii/S0160412021006024},
author = {Ning Wei and Qijun Zhang and Yanjie Zhang and Jiaxin Jin and Junyu Chang and Zhiwen Yang and Chao Ma and Zhenyu Jia and Chunzhe Ren and Lin Wu and Jianfei Peng and Hongjun Mao},
keywords = {Vehicle transient emission model, Machine learning, Super-learner model, China VI N vehicle},
abstract = {The transient simulation of CO2 and NOX from motor vehicles has essential applications in evaluating vehicular greenhouse gas emissions and pollutant emissions. However, accurately estimating vehicular transient emissions is challenging due to the heterogeneity between different vehicles and the continuous upgrading of vehicle exhaust purification technology. To accurately characterize the transient emissions of motor vehicles, a Super-learner model is used to build CO2 and NOx transient emission models. The actual onboard test data of 9 China VI N2 vehicles were used to train the model, and the test data of another China VI N2 vehicle were selected for further robustness verification. There were significant differences in the emissions between the vehicles, but the constructed transient model could capture the common law of transient emissions from China VI N2 vehicles. The R2 values of CO2 and NOx emission in the test data of the validation vehicle were 0.71 and 0.82, respectively. In addition, to further prove the model's robustness, the training data were synchronously modelled based on the Moves-method. The Super-learner model has a smaller RMSE on the validation set than the model based on the Moves-method, indicating that the Super-learner model has more transient simulation advantages. The marginal contributions of the model characteristics to the model results were analysed by SHapley Additive exPlanation (SHAP) value interpretation, and the marginal contributions of different pollutant characteristic parameters varied. Therefore, when establishing transient models of different pollutants, the selection of the model parameters demands considering the generation and purification process of different pollutants. The present work provides novel insights into the parameter selection, construction, and interpretation of the transient vehicle emission model.}
}
@article{FRIEDERICH2022103586,
title = {A framework for data-driven digital twins of smart manufacturing systems},
journal = {Computers in Industry},
volume = {136},
pages = {103586},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103586},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001937},
author = {Jonas Friederich and Deena P. Francis and Sanja Lazarova-Molnar and Nader Mohamed},
keywords = {Data-driven, Digital twin, Machine learning, Process mining, Reconfigurable manufacturing, Smart factory},
abstract = {Adoption of digital twins in smart factories, that model real statuses of manufacturing systems through simulation with real time actualization, are manifested in the form of increased productivity, as well as reduction in costs and energy consumption. The sharp increase in changing customer demands has resulted in factories transitioning rapidly and yielding shorter product life cycles. Traditional modeling and simulation approaches are not suited to handle such scenarios. As a possible solution, we propose a generic data-driven framework for automated generation of simulation models as basis for digital twins for smart factories. The novelty of our proposed framework is in the data-driven approach that exploits advancements in machine learning and process mining techniques, as well as continuous model improvement and validation. The goal of the framework is to minimize and fully define, or even eliminate, the need for expert knowledge in the extraction of the corresponding simulation models. We illustrate our framework through a case study.}
}
@article{LI2021103961,
title = {Digital twin-driven virtual sensor approach for safe construction operations of trailing suction hopper dredger},
journal = {Automation in Construction},
volume = {132},
pages = {103961},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103961},
url = {https://www.sciencedirect.com/science/article/pii/S092658052100412X},
author = {Mingchao Li and Qiaorong Lu and Shuo Bai and Mengxi Zhang and Huijing Tian and Liang Qin},
keywords = {Trailing suction hopper dredger, Digital twin, Virtual sensor, Machine learning, Data mining, Construction safety},
abstract = {The stable and safe operation of Trailing suction hopper dredger (TSHD) is one of the most crucial considerations for ensuring its high dredging productivity. However, the instability and sudden failure of physical sensors pose challenges to the monitoring of dredging process. To address these issues, we propose a structure of digital twin-driven virtual sensor (DTDVS) for the construction safety of TSHD. Considering the potential internal relations among construction data, we compare the performance of four machine learning algorithms in predicting the torsional vibration in mechanical failure. The results showed that these algorithms provide high prediction accuracy (R2 > 0.9). Then the DBN model with the best performance was selected as a part of the virtual sensors to predict and analyze the status of TSHD. The digital twin technology provides a more stable and environmentally friendly scheme for TSHD construction safety control. On the one hand, the DTDVS assists physical sensors to monitor the construction state, overcoming the limitation of the sensors on detection targets which are difficult or costly to measure directly. On the other hand, by analyzing the residual between the physical sensor and the virtual sensor, the construction behavior can be diagnosed, and the fault situation can be pre-warned accurately. This improves the time utilization of TSHD and provides an important guarantee for the construction safety.}
}
@article{GAO2022252,
title = {Deep cognitive diagnosis model for predicting students’ performance},
journal = {Future Generation Computer Systems},
volume = {126},
pages = {252-262},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003277},
author = {Lina Gao and Zhongying Zhao and Chao Li and Jianli Zhao and Qingtian Zeng},
keywords = {Cognitive diagnosis, Deep learning, Student modeling, Educational data mining, Learning analysis},
abstract = {Cognitive model is playing very important role in predicting students’ performance and recommending learning resources. Thus, it has received a great deal of attention from researchers. However, most of the existing work design models from the aspect of students, ignoring the internal relation between problems and skills. To address this problem, we propose a deep cognitive diagnosis framework to obtain students’ mastery of skills and problems by enhancing traditional cognitive diagnosis methods with deep learning. First, we model the skill proficiency of students according to their responses to objective and subjective problems. Second, students’ mastery on problems is modeled based on attention mechanism and neural network, considering both the importance and the interactions of skills. Finally, considering the facts that students may carelessly select or simply guess the answer, we predict students’ performance via the proposed model. Extensive experiments are carried out on two real-world data sets, and the results have proved the effectiveness and interpretability of this work.}
}
@article{IBRION2021105355,
title = {Learning from failures in cruise ship industry: The blackout of Viking Sky in Hustadvika, Norway},
journal = {Engineering Failure Analysis},
volume = {125},
pages = {105355},
year = {2021},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2021.105355},
url = {https://www.sciencedirect.com/science/article/pii/S1350630721002156},
author = {Michaela Ibrion and Nicola Paltrinieri and Amir R. Nejad},
keywords = {Engine failure, Cruise ship, Blackout, Viking Sky, Hustadvika-Norway, Learning, Marine accidents and Digital Twin},
abstract = {This article brings to attention learning from the failure - blackout, loss of propulsion and near grounding - of Viking Sky cruise ship which occurred in Hustadvika, Norway, in March 2019. Failures and accidents in the cruise ship industry attract the global media and can severely impact reputation and business performance of companies and authorities involved. A system approach investigation and analysis - CAST - was employed with the aim to maximize learning from the Viking Sky’s failure through a systematic approach and to contribute to failure reduction in the cruise ship industry. Three main recommendations emerged from this study: an overview of the accident or failure precursors and resilience indicators; safety recommendations for other cruise ships; lessons and strategies of actions for the increased cruise operations in the Arctic and Antarctic areas. It was found that several accident or failure precursors, for example, a low level of lubricating oil, the failure of a turbocharger, an inoperative large diesel generator, lack of functionality for safety equipment due to bad weather, and others precursors contributed to failure and highly critical situation encountered by Viking Sky in Hustadvika. Resilience indicators such as the master’s immediate decision to launch mayday, the crew preparedness, and the way how the emergency situation was handled were found to have positive impacts on critical situation of Viking Sky. This article highlights also that adaptations and improvement of standards and regulations for harsh environmental conditions can play an important role in prevention of marine accidents. Furthermore, for a better understanding of correlation between environmental loads and their effects on machinery systems, digital solutions such as digital twin for condition monitoring of cruise ships in the Polar areas are seen as possible innovative solutions yet to be fully implemented in the marine industry.}
}
@article{WANG2022101897,
title = {Deep learning for assessment of environmental satisfaction using BIM big data in energy efficient building digital twins},
journal = {Sustainable Energy Technologies and Assessments},
volume = {50},
pages = {101897},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101897},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821009115},
author = {Weixi Wang and Han Guo and Xiaoming Li and Shengjun Tang and Jizhe Xia and Zhihan Lv},
keywords = {Building digital twins, BIM big data, Deep learning, Assessment of environmental satisfaction, Energy efficient building},
abstract = {Energy efficient Building Digital Twins (BDTs) are researched using Building Information Model (BIM) to explore the key techniques of Digital Twins (DTs). DTs in buildings can be regarded as an expression of “BIM+,” born to digital descriptions. Comprehensive perception of physical systems is the preconditions for DTs implementation. BIM’s energy-saving design includes the selection of building orientation and building shape. BIM energy consumption analysis can compare different materials, examine the performance of various materials, and select the most suitable and most energy-efficient materials for building structure maintenance. Data Fusion Algorithm (DFA) in Wireless Sensor Networks (WSNs) is improved. A novel DFA is constructed by combining Backpropagation Neural Network (BPNN) with Dynamic Host Configuration Protocol (DCHP), recorded as BP-DCHP. Simulation experiment proves that BP-DCHP can prolong sensor nodes’ survival time and provide the highest data fusion quality. BP-DCHP runs for about 310 s, 500 s, and 705 s in WSNs consisting of 20, 50, and 100 WSNs, respectively. Moreover, BP-DCHP can provide higher quality given insufficient data fusion degree. Once the WSNs consume 50% of the total initial energy, BP-DCHP presents a shorter network delay, only 0.6 s on average in the 100-sensor-node-WSN. To validate BDTs’ effectiveness, the environmental satisfaction of residents from two Beijing intelligent communities is assessed using Deep Learning (DL) approach. Taking the data as the clue, the study establishes DTs serving the application of urban scene, which plays a certain role in promoting the technological innovation of BDTs, better optimizing the city and managing the city.}
}
@article{PHANDEN202288,
title = {A state-of-the-art review on implementation of digital twin in additive manufacturing to monitor and control parts quality},
journal = {Materials Today: Proceedings},
volume = {56},
pages = {88-93},
year = {2022},
note = {International Conference on Materials, Machines and Information Technology-2022},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.12.217},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321079360},
author = {Rakesh Kumar Phanden and S.V. Aditya and Aaryan Sheokand and Kapil Kumar Goyal and Pardeep Gahlot and Adam Jacso},
keywords = {Digital twin, Additive manufacturing, Review, Parts quality monitoring},
abstract = {Both, Additive Manufacturing (AM) and Digital Twin (DT) are emerging technologies. DT is helping AM in process simulation, monitoring and controlling as well as to develop insights on process parameters relation to achieve high parts quality. Therefore, the implementation of DT technology in AM is highly desirable and fruitful. In the current state, DT application on AM has been explored by various researchers for education, manufacturing, maintenance and quality area from the theoretical and practical viewpoints. This paper presents the state-of-the-art literature review on the implementation of DT in AM to monitor and control the parts quality from theoretical and practical viewpoints. Based on the literature, a representation scheme has been extracted to implement DT in AM successfully, and various future research directions are given.}
}
@article{BLAIR2021100359,
title = {Digital twins of the natural environment},
journal = {Patterns},
volume = {2},
number = {10},
pages = {100359},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100359},
url = {https://www.sciencedirect.com/science/article/pii/S266638992100221X},
author = {Gordon S. Blair},
abstract = {Summary
Digital twins emerged in the field of engineering but are now being applied in many areas of study. This article reflects on the enormous potential of digital twins of the natural environment and proposes an approach that builds on the massive legacy of process model understanding in this area combined with new insights from data understanding, including from AI/machine learning.}
}
@article{LV2021108366,
title = {Beyond 5G for digital twins of UAVs},
journal = {Computer Networks},
volume = {197},
pages = {108366},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108366},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003534},
author = {Zhihan Lv and Dongliang Chen and Hailing Feng and Ranran Lou and Huihui Wang},
keywords = {Unmanned aerial vehicle, B5G, Deep learning, Coordinated multi-point transmission, Physical layer security, Digital twins},
abstract = {The purpose is to explore the application effects and limitations of Unmanned Aerial Vehicle (UAV) in 5G/B5G (Beyond 5G) mobile and wireless communication. Based on 5Gcommunication, the deep learning (DL) algorithm is introduced to construct the UAV Digital Twins (DTs) communication channel model based on DL. The Coordinated Multi-point Transmission (COMP) technology is adopted to study the interference suppression of UAVs. The key algorithm in the physical layer security is employed to ensure information communication security. Finally, the model constructed is simulated and analyzed. The transmission error rates and transmission estimation accuracy of several algorithms, including the proposed algorithm and ordinary Deep Neural Networks (DNNs), are compared under different Signal-to-Noise Ratios (SNRs). Results find that the convergence speed and convergence effect of the proposed algorithm has prominent advantages, presenting strong robustness; the proposed algorithm's estimation accuracy is about 150 times higher than the traditional algorithms. Further analysis reveals that the proposed algorithm's accuracy reaches 82.39%, which increases by at least 3.2% than other classic machine algorithms. The indicators of Precision, Recall, and F1 are compared as well. Apparently, the Precision, Recall, and F1 values of the proposed algorithm are the highest, while the transmission delay is the smallest. Therefore, the constructed UAV DTs wireless communication channel model has strong robustness and further reduces UAV limitations, providing a reference for improving UAV system performance in the later stage.}
}
@article{SCHILLINGER2021103866,
title = {Adaptive heterogeneous multi-robot collaboration from formal task specifications},
journal = {Robotics and Autonomous Systems},
volume = {145},
pages = {103866},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103866},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021001512},
author = {Philipp Schillinger and Sergio García and Alexandros Makris and Konstantinos Roditakis and Michalis Logothetis and Konstantinos Alevizos and Wei Ren and Pouria Tajvar and Patrizio Pelliccione and Antonis Argyros and Kostas J. Kyriakopoulos and Dimos V. Dimarogonas},
keywords = {Robotics, Multi-robot, Temporal logic, HRI, Heterogeneous robots, Task decomposition, Task allocation, Abstraction},
abstract = {Efficiently coordinating different types of robots is an important enabler for many commercial and industrial automation tasks. Here, we present a distributed framework that enables a team of heterogeneous robots to dynamically generate actions from a common, user-defined goal specification. In particular, we discuss the integration of various robotic capabilities into a common task allocation and planning formalism, as well as the specification of expressive, temporally-extended goals by non-expert users. Models for task allocation and execution both consider non-deterministic outcomes of actions and thus, are suitable for a wide range of real-world tasks including formally specified reactions to online observations. One main focus of our paper is to evaluate the framework and its integration of software modules through a number of experiments. These experiments comprise industry-inspired scenarios as motivated by future real-world applications. Finally, we discuss the results and learnings for motivating practically relevant, future research questions.}
}
@article{CHEN202270,
title = {Digital twins to fight against COVID-19 pandemic},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {2},
pages = {70-81},
year = {2022},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2022.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S266734522200013X},
author = {Dongliang Chen and Nojoom A. AlNajem and Mohammad Shorfuzzaman},
keywords = {Digital twins, Novel coronavirus disease, Blockchain, Deep learning, Information security},
abstract = {This study is aimed to explore the anti-epidemic effect of artificial intelligence (AI) algorithms such as digital twins on the COVID-2019 (novel coronavirus disease 2019), so that the information security and prediction accuracy of epidemic prevention and control (P & C) in smart cities can be further improved. It addresses the problems in the current public affairs governance strategy for the outbreak of the COVID-2019 epidemic, and uses digital twins technology to map the epidemic P & C situation in the real space to the virtual space. Then, the blockchain technology and deep learning algorithms are introduced to construct a digital twins model of the COVID-2019 epidemic (the COVID-DT model) based on blockchain combined with BiLSTM (Bi-directional Long Short-Term Memory). In addition, performance of the constructed COVID-DT model is analyzed through simulation. Analysis of network data security transmission performance reveals that the constructed COVID-DT model shows a lower average delay, its data message delivery rate (DMDR) is basically stable at 80%, and the data message disclosure rate (DMDCR) is basically stable at about 10%. The analysis on network communication cost suggests that the cost of this study does not exceed 700 bytes, and the prediction error does not exceed 10%. Therefore, the COVID-DT model constructed shows high network security performance while ensuring low latency performance, enabling more efficient and accurate interaction of information, which can provide experimental basis for information security and development trends of epidemic P & C in smart cities.}
}
@article{VIIRMAN2021100858,
title = {Running to keep up with the lecturer or gradual de-ritualization? Biology students’ engagement with construction and data interpretation graphing routines in mathematical modelling tasks},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100858},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100858},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000195},
author = {Olov Viirman and Elena Nardi},
keywords = {Mathematics in biology, Mathematical modelling, Commognition, Meta-level learning, Graph construction and interpretation routines, De-ritualization},
abstract = {Through a commognitive lens, we examine twelve first-semester biology students’. engagement with graphing routines as they work in groups, during four sessions of Mathematical Modelling (MM). We trace the students’ meta-level learning, particularly as they fluctuate between deploying graphs for mere illustration of data and as sense-making tools. We account for student activity in relation to precedent events in their experiences of graphing and as fluid, if not always productive, interplay between ritualised and exploratory engagement with graph construction and interpretation routines. The students’ construal of the task situations is marked by efforts to keep up with lecturer expectations which allow for changing degrees of student agency but do not factor in the influence of precedent events. Our analysis has pedagogical implications for the way MM problems are formulated and also foregrounds the capacity of the commognitive framework to trace de-ritualization and meta-level learning in students’ MM activity.}
}
@article{XIA2021107938,
title = {Intelligent fault diagnosis of machinery using digital twin-assisted deep transfer learning},
journal = {Reliability Engineering & System Safety},
volume = {215},
pages = {107938},
year = {2021},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2021.107938},
url = {https://www.sciencedirect.com/science/article/pii/S0951832021004531},
author = {Min Xia and Haidong Shao and Darren Williams and Siliang Lu and Lei Shu and Clarence W. {de Silva}},
keywords = {Digital twin, Fault diagnosis, Novel sparse de-noising auto-encoder, Deep transfer learning},
abstract = {Digital twin (DT) is emerging as a key technology for smart manufacturing. The high fidelity DT model of the physical assets can produce system performance data that is close to reality, which provides remarkable opportunities for machine fault diagnosis when the measured fault condition data are insufficient. This paper presents an intelligent fault diagnosis framework for machinery based on DT and deep transfer learning. First, the DT model of the machine is built by establishing the simulation model and with further updating through continuously measured data from the physical asset. Second, all important machine conditions can be simulated from the built DT. Third, a new-type deep structure based on novel sparse de-noising auto-encoder (NSDAE) is developed and pre-trained with condition data from the source domain, as generated from the DT. Then, to achieve accurate machine fault diagnosis with possible variations in working conditions and system characteristics, the pre-trained NSDAE is fine-tuned using parameter transfer with only one sample from the target domain. The presented method is validated through a case study of triplex pump fault diagnosis. The experimental results demonstrate that the proposed method achieves intelligent fault diagnosis with a limited amount of measured data and outperforms other state-of-the-art data-driven methods.}
}
@article{LUNACEK2021102061,
title = {A data-driven operational model for traffic at the Dallas Fort Worth International Airport},
journal = {Journal of Air Transport Management},
volume = {94},
pages = {102061},
year = {2021},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2021.102061},
url = {https://www.sciencedirect.com/science/article/pii/S0969699721000442},
author = {Monte Lunacek and Lindy Williams and Joseph Severino and Karen Ficenec and Juliette Ugirumurera and Matthew Eash and Yanbo Ge and Caleb Phillips},
keywords = {Microsimulation, Congestion, Digital twin, Machine learning, Airport, Traffic},
abstract = {Airports are on the front line of significant innovations, allowing the movement of more people and goods faster, cheaper, and with greater convenience. As air travel continues to grow, airports will face challenges in responding to increasing passenger vehicle traffic, which leads to lower operational efficiency, poor air quality, and security concerns. This paper evaluates methods for traffic demand forecasting combined with traffic microsimulation, which will allow airport operations staff to accurately predict traffic and congestion. Using two years of detailed data describing individual vehicle arrivals and departures, aircraft movements, and weather at Dallas-Fort Worth (DFW) International Airport, we evaluate multiple prediction methods including the Auto Regressive Integrated Moving Average (ARIMA) family of models, traditional machine learning models, and DeepAR, a modern recurrent neural network (RNN). We find that these algorithms are able to capture the diurnal trends in the surface traffic, and all do very well when predicting the next 30 minutes of demand. Longer forecast horizons are moderately effective, demonstrating the challenge of this problem and highlighting promising techniques as well as potential areas for improvement. Traffic demand is not the only factor that contributes to terminal congestion, because temporary changes to the road network, such as a lane closure, can make benign traffic demand highly congested. Combining a demand forecast with a traffic microsimulation framework provides a complete picture of traffic and its consequences. The result is an operational intelligence platform for exploring policy changes, as well as infrastructure expansion and disruption scenarios. To demonstrate the value of this approach, we present results from a case study at DFW Airport assessing the impact of a policy change for vehicle routing in high demand scenarios. This framework can assist airports like DFW as they tackle daily operational challenges, as well as explore the integration of emerging technology and expansion of their services into long term plans.}
}
@article{PRISACARU2021114134,
title = {Towards virtual twin for electronic packages in automotive applications},
journal = {Microelectronics Reliability},
volume = {122},
pages = {114134},
year = {2021},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2021.114134},
url = {https://www.sciencedirect.com/science/article/pii/S0026271421001001},
author = {Alexandru Prisacaru and Ernesto Oquelis Guerrero and Balakrishna Chimmineni and Przemyslaw Jakub Gromala and Yu-Hsiang Yang and Bongtae Han and Guo Qi Zhang},
keywords = {Digital twin, Machine learning, Finite element method, Surrogate modelling, Electronic package, Mechanical stress sensor},
abstract = {The piezoresistive silicon based stress sensor has the potential to be part of the Digital Twin implementation in automotive electronics. One solution to enforce reliability in digital twins is the use of Machine Learning (ML). One or more physical parameters are being monitored, while other parameters are projected with surrogate models, just like virtual sensors. Piezo-resistive stress sensors are employed to measure the internal stresses of electronic packages, an Acquisition Unit (AU) to read out sensor data and a Raspberry Pi to perform evaluation. Accelerated tests in air thermal chamber are performed to get time series data of the stress sensor signals, with which we can know better about how delamination develops inside the package. In this study stress measurements are performed in several electronic packages during the delamination. The delamination is detected by the stress sensor due to the continuous change of the stiffness and the local boundary conditions causing the stresses to change. Although, the stress change in multiple cells can give enough information if it is delaminated or not, its delamination area location is unknown. Surrogate models built upon Neural Networks (NN) and Finite Element Method (FEM) are developed to predict the out of plane stresses at the delaminated layer. FEM simulation models are calibrated with Moiré measurements and validated at the component and PCB level with stress difference measurements. Simulation delamination areas are constructed based on the Scanning Acoustic Microscope (SAM) images, and are also validated with the equivalent stress measurements. In the end the surrogate model is predicting the out of plane stress in the adhesive layer. The results show good correlation when compared to the SAM images.}
}
@article{CONATI2021103503,
title = {Toward personalized XAI: A case study in intelligent tutoring systems},
journal = {Artificial Intelligence},
volume = {298},
pages = {103503},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103503},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000540},
author = {Cristina Conati and Oswald Barral and Vanessa Putnam and Lea Rieger},
keywords = {Explainable artificial intelligence (XAI), Intelligent tutoring systems (ITS), User modeling, Personalization},
abstract = {Our research is a step toward ascertaining the need for personalization in XAI, and we do so in the context of investigating the value of explanations of AI-driven hints and feedback in Intelligent Tutoring Systems (ITS). We added an explanation functionality to the Adaptive CSP (ACSP) applet, an interactive simulation that helps students learn an algorithm for constraint satisfaction problems by providing AI-driven hints adapted to their predicted level of learning. We present the design of the explanation functionality and the results of a controlled study to evaluate its impact on students' learning and perception of the ACPS hints. The study includes an analysis of how these outcomes are modulated by several user characteristics such as personality traits and cognitive abilities, to asses if explanations should be personalized to these characteristics. Our results indicate that providing explanations increase students' trust in the ACPS hints, perceived usefulness of the hints, and intention to use them again. In addition, we show that students' access of the ACSP explanation and learning gains are modulated by three user characteristics, Need for Cognition, Contentiousness and Reading Proficiency, providing insights on how to personalize the ACSP explanations to these traits, as well as initial evidence on the potential value of personalized Explainable AI (XAI) for ITS.}
}
@article{MEIXEDO2022108268,
title = {Online unsupervised detection of structural changes using train–induced dynamic responses},
journal = {Mechanical Systems and Signal Processing},
volume = {165},
pages = {108268},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108268},
url = {https://www.sciencedirect.com/science/article/pii/S088832702100635X},
author = {Andreia Meixedo and João Santos and Diogo Ribeiro and Rui Calçada and Michael D. Todd},
keywords = {Online assessment, Unsupervised learning, Damage detection, Structural health monitoring, Traffic-induced dynamic responses, ARX model, PCA, Cluster analysis},
abstract = {This paper exploits unsupervised data-driven structural health monitoring (SHM) in order to propose a continuous online procedure for damage detection based on train-induced dynamic bridge responses, taking advantage of the large-magnitude loading for enhancing sensitivity to small-scale structural changes. While such large responses induced by trains might create more damage-sensitive information in the measured response, it also amplifies the effects on those measurements from the environment. Thus, one of the biggest contributions herein is a methodology that exploits the large bridge responses induced by train passage while rejecting the confounding influences of the environment in such a way that false positive detections are mitigated. Furthermore, this research work introduces an adaptable confidence decision threshold that further improves damage detection over time. To ensure an online continuous assessment, a hybrid combination of autoregressive exogenous input (ARX) models, principal components analysis (PCA), and clustering algorithms was sequentially applied to the monitoring data, in a moving window process. A comparison between the performance obtained from autoregressive (AR) and ARX models as feature extractors was conducted, and it was concluded that ARX models lead to increased sensitivity to damage due to their ability to capture cross information between the sensors. The PCA proved its importance and effectiveness in removing observable changes induced by variations in train speed or temperature without the need to measure them, and the clustering methods allowed for an automatic classification of the damage-sensitive features. Since it was not possible to introduce damage to the bridge, several structural conditions were simulated with a highly reliable digital twin of the Sado Bridge, tuned with experimental data acquired from a SHM system installed on site, in order to test and validate the efficiency of the proposed procedure. The strategy proved to be robust when detecting a comprehensive set of damage scenarios with a false detection incidence of 2%. Moreover, it showed sensitivity to smaller damage levels (earlier in life), even when it consists of small stiffness reductions that do not impair structural safety and are imperceptible in the original signals.}
}
@article{GUPTA2022105239,
title = {A hybrid partitioned deep learning methodology for moving interface and fluid–structure interaction},
journal = {Computers & Fluids},
volume = {233},
pages = {105239},
year = {2022},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2021.105239},
url = {https://www.sciencedirect.com/science/article/pii/S0045793021003479},
author = {Rachit Gupta and Rajeev Jaiman},
keywords = {Fluid–structure interaction, Deep learning-based reduced-order model, Proper orthogonal decomposition, Convolutional autoencoder, Long short-term memory network, Digital twin},
abstract = {In this work, we present a hybrid partitioned deep learning framework for the reduced-order modeling of moving interfaces and predicting fluid–structure interaction. Using the discretized Navier–Stokes in the arbitrary Lagrangian–Eulerian reference frame, we generate the full-order flow snapshots and point cloud displacements as target physical data for the learning and inference of coupled fluid–structure dynamics. The hybrid operation of this methodology comes by combining two separate data-driven models for fluid and solid subdomains via deep learning-based reduced-order models (DL-ROMs). The proposed multi-level framework comprises the partitioned data-driven drivers for unsteady flow and the moving point cloud displacements. At the fluid–structure interface, the force information is exchanged synchronously between the two partitioned subdomain solvers. The first component of our proposed framework relies on the proper orthogonal decomposition-based recurrent neural network (POD-RNN) as a DL-ROM procedure to infer the point cloud with a moving interface. This model utilizes the POD basis modes to reduce dimensionality and evolve them in time via long short-term memory-based recurrent neural networks (LSTM-RNNs). The second component employs the convolution-based recurrent autoencoder network (CRAN) as a self-supervised DL-ROM procedure to infer the nonlinear flow dynamics at static Eulerian probes. We introduce these probes as spatially structured query nodes in the moving point cloud to treat the Lagrangian-to-Eulerian conflict together with convenience in training the CRAN driver. To determine these Eulerian probes, we construct a novel snapshot-field transfer and load recovery algorithm. They are chosen in such a way that the two components (i.e., POD-RNN and CRAN) are constrained at the interface to recover the bulk force quantities. These DL-ROM-based data-driven drivers rely on the LSTM-RNNs to evolve the low-dimensional states. A popular prototypical fluid–structure interaction problem of flow past a freely oscillating cylinder is considered to assess the efficacy of the proposed methodology for a different set of reduced velocities that lead to vortex-induced vibrations. The proposed framework tracks the interface description with acceptable accuracy and predicts the nonlinear wake dynamics over the chosen test data range. The proposed framework aligns with the development of partitioned digital twin of engineering systems, especially those involving moving boundaries and fluid–structure interactions.}
}
@article{GHIASABADIFARAHANI2022101978,
title = {Adaptive personalized recommender system using learning automata and items clustering},
journal = {Information Systems},
volume = {106},
pages = {101978},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101978},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001654},
author = {Mansoureh {Ghiasabadi Farahani} and Javad {Akbari Torkestani} and Mohsen Rahmani},
keywords = {Recommender system, Personalization, Adaptive user profile, Clustering, Learning automata},
abstract = {The personalized recommender systems provide user-related services based on user preferences; these preferences are recorded in an individual profile. Therefore, the more complete and precise each user profile leads more successful the recommendation process. The people’s interests change over time though traditional researches do not follow these changes regularly. Under such circumstances, designing an efficient user model to track users’ interests is greatly important. In the current study, we suggest an algorithm to create the learning automata-based user profiling. Due to many items and the commonality of features between them, we clustered items. In this technique, a learning automaton is assigned to the active user. The learning automaton adjusts the amount of user interest in each cluster based on user feedback. As the user interactions with the system increase, the internal state of the learning automaton converges towards the user’s genuine interests in the item clusters. The experimental results demonstrate that our algorithm outperforms compared approaches in precision, recall, RMSE, and MAE. In addition, the proposed algorithm for new users has acceptable performance.}
}
@article{GAO2022108233,
title = {Discrepant multiple instance learning for weakly supervised object detection},
journal = {Pattern Recognition},
volume = {122},
pages = {108233},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108233},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004143},
author = {Wei Gao and Fang Wan and Jun Yue and Songcen Xu and Qixiang Ye},
keywords = {Weakly supervised detection, Multiple instance learning, Learner discrepancy, Collaborative learning},
abstract = {Multiple Instance Learning (MIL) is a fundamental method for weakly supervised object detection (WSOD), but experiences difficulty in excluding local optimal solutions and may miss objects or falsely localize object parts. In this paper, we introduce discrepantly collaborative modules into MIL and thereby create discrepant multiple instance learning (D-MIL), pursuing optimal solutions in a simple-yet-effective way. D-MIL adopts multiple MIL learners to pursue discrepant yet complementary solutions indicating object parts, which are fused with a collaboration module for precise object localization. D-MIL implements a new “teachers-students” model, where MIL learners act as “teachers” and object detectors as “students”. Multiple teachers provide rich yet complementary information, which are absorbed by students and transferred back to reinforce the performance of teachers. Experiments show that D-MIL significantly improves the baseline while achieves state-of-the-art performance on the challenging MS-COCO object detection benchmark.}
}
@article{PRIYANKA2022100272,
title = {Digital twin for oil pipeline risk estimation using prognostic and machine learning techniques},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100272},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100272},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000704},
author = {E.B. Priyanka and S. Thangavel and Xiao-Zhi Gao and N.S. Sivakumar},
keywords = {Oil pipeline, Risk probability rate, Digital twin, Prognostic-manifold learning},
abstract = {Digital Twin technology is emerging as the digitization platform to enhance the industrial information processing and management in concern with virtual and physical entities. It paves the path for integrated industrial data analysis by combining IoT and Artificial Intelligence for better data interpretation. At present in oil industry, pipelines prevail to be feasible mode, the risk probability rate is getting increased and maintenance system becomes difficult with attention to the earlier prediction of accidents risks by undertaking entire pipeline. This paper aims to provide the frame structure of Digital Twin based on machine learning and prognostics algorithms model to analyze and predict the risk probability rate of oil pipeline system. Prognostics focuses on the detection of a failure precursor by estimating risk condition with respect to the pressure data towards the evaluation of remaining useful life (RUL). The abnormality of pressure attribute is taken in prognostic analysis for risk probability estimation followed by Dirichlet Process Clustering and Canopy clustering to segregate the abnormal pressure drop and rise. Using multiple oil substation data integration platform, the features are extracted using manifold learning methods and the best feature probability rates are evaluated using kernel based SVM algorithm to provide on-time control action on the entire oil pipeline system through efficient wireless data communication between server and the oil substations. As a result, the proposed work creates Virtual Intelligent Integrated Automated Control System to predict the risk rate in oil industry by integrating entire transmission lines through enhanced wireless information networks in remote locations.}
}
@article{ZHANG2021107659,
title = {Adversarial co-distillation learning for image recognition},
journal = {Pattern Recognition},
volume = {111},
pages = {107659},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107659},
url = {https://www.sciencedirect.com/science/article/pii/S0031320320304623},
author = {Haoran Zhang and Zhenzhen Hu and Wei Qin and Mingliang Xu and Meng Wang},
keywords = {Knowledge distillation, Data augmentation, Generative adversarial nets, Divergent examples, Image classification},
abstract = {Knowledge distillation is an effective way to transfer the knowledge from a pre-trained teacher model to a student model. Co-distillation, as an online variant of distillation, further accelerates the training process and paves a new way to explore the “dark knowledge” by training n models in parallel. In this paper, we explore the “divergent examples”, which can make the classifiers have different predictions and thus induce the “dark knowledge”, and we propose a novel approach named Adversarial Co-distillation Networks (ACNs) to enhance the “dark knowledge” by generating extra divergent examples. Note that we do not involve any extra dataset, and we only utilize the standard training set to train the entire framework. ACNs are end-to-end frameworks composed of two parts: an adversarial phase consisting of Generative Adversarial Networks (GANs) to generate the divergent examples and a co-distillation phase consisting of multiple classifiers to learn the divergent examples. These two phases are learned in an iterative and adversarial way. To guarantee the quality of the divergent examples and the stability of ACNs, we further design “Weakly Residual Connection” module and “Restricted Adversarial Search” module to assist in the training process. Extensive experiments with various deep architectures on different datasets well demonstrate the effectiveness of our approach.}
}
@article{EDER2020107075,
title = {FASTIGUE: A computationally efficient approach for simulating discrete fatigue crack growth in large-scale structures},
journal = {Engineering Fracture Mechanics},
volume = {233},
pages = {107075},
year = {2020},
issn = {0013-7944},
doi = {https://doi.org/10.1016/j.engfracmech.2020.107075},
url = {https://www.sciencedirect.com/science/article/pii/S0013794419315565},
author = {Martin Alexander Eder and Xiao Chen},
keywords = {Digital twin, Fatigue crack growth, Large-scale structures, Computational efficiency, Stress intensity factor},
abstract = {The renaissance of digital twin technology heralded by recent advancements in machine learning raises the demand for structural analysis tools for real time predictions of the structural performance and ultimately the remaining lifetime. This paper proposes a novel approach – FASTIGUE - for computationally super-efficient discrete fatigue crack growth analysis of large structures particularly for bondlines with high aspect ratios. The computational speed is considerably increased by outsourcing the finite element analysis into a pre-processing step in which the numerical model is solved for a comparatively small number of crack stages; providing the Stress Intensity Factors (SIFs) for a set of auxiliary crack tip location permutations. 3D surface fitting of the auxiliary data provides the SIF ranges as continuous functions of the crack lengths. The fatigue crack growth simulation is performed independently from finite element analysis by utilising the SIF-functions within an explicit growth prediction scheme. The method is applied to a trailing edge crack in a 14.3 m wind turbine blade model and further validated against an analytical solution. It is demonstrated that the computation speed outperforms conventional fatigue analysis approaches relying on update-and-rerun schemes.}
}
@article{VALCKENAERS2020103226,
title = {Perspective on holonic manufacturing systems: PROSA becomes ARTI},
journal = {Computers in Industry},
volume = {120},
pages = {103226},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103226},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520302530},
author = {Paul Valckenaers},
keywords = {Holonic manufacturing systems, Manufacturing execution systems, Design for the unexpected, Obedient digital twins, PROSA, ARTI, Sciences of the artificial, Bounded rationality, Complex-adaptive systems, Autocatalytic sets},
abstract = {Looking back at 30 years of research into holonic manufacturing systems, these explorations made a lasting scientific contribution to the overall architecture of intelligent manufacturing systems. Most notably, holonic architectures are defined in terms of their world-of-interest (Van Brussel et al., 1998). They do not have an information layer, a communication layer, etc. Instead, they have components that relate to real-world assets (e.g. machine tools) and activities (e.g. assembly). And, they mirror and track the structure of their world-of-interest, which allows them to scale and adapt accordingly. This research has wandered around, at times learning from its mistakes, and progressively carved out an invariant structure while it translated and applied scientific insights from complex-adaptive systems theory (e.g. autocatalytic sets) and from bounded rationality (e.g. holons). This paper presents and discusses the outcome of these research efforts. At the top level, the holonic structure distinguishes intelligent beings (or digital twins) from intelligent agents. These digital twins inherit the consistency from reality, which they mirror. They are intelligent beings when they reflect what exists in the world without imposing artificial limitations in this reality. Consequently, a conflict with a digital twin is a conflict with reality. In contrast, intelligent agents typically transform NP-hard challenges into computations with low-polynomial complexity. Unavoidably, this involves arbitrariness (e.g. don’t care choices). Likewise, relying on case-specific properties, to ensure an outcome in polynomial time, usually renders the validity of an agent’s choices both short-lived and situation-dependent. Here, intelligent agents create conflicts by imposing limitations of their own making in their world-of-interest. Real-world smart systems are aggregates comprising both intelligent beings and intelligent agents. They are performers. Inside these performers, digital twins may constitute the foundations, supporting walls, support beams and pillars because these intelligent beings are protected by their real-world counterpart. Further refining the top-level of this architecture, a holonic structure enables these digital twins to shadow their real-world counterpart whenever it changes, adapts and evolves. In contrast, the artificial limitations, imposed by the intelligent agents, cannot be allowed to build up inertia, which would hamper the undoing of arbitrary or case-specific limitations. To this end, performers explicitly manage the rights over their assets. Revoking such rights from a limitation-imposing agent will free the assets. This will be at the cost of reduced services from the agent. When other service providers rely on this agent, their services may be affected as well; that’s how the inertia builds up and how harmful legacy is created. Thus, the services of digital twins are to be preferred over the services of an intelligent agent by developers of holonic manufacturing systems. Finally, digital twins corresponding to the decision making in the world-of-interest (a non-physical asset) allow to mirror the world-of-interest in a predictive mode (in addition to track and trace). It allows to generate short-term forecasts while preserving the benefits of intelligent beings. These twins are the intentions of the decision-making intelligent agents. Evidently, when intentions change, the forecasts needs to be regenerated (i.e. tracking the corresponding reality by the twin). This advanced feature can be deployed in a number of configurations (cf. annex).}
}
@article{WEI2021100703,
title = {Mechanistic models for additive manufacturing of metallic components},
journal = {Progress in Materials Science},
volume = {116},
pages = {100703},
year = {2021},
issn = {0079-6425},
doi = {https://doi.org/10.1016/j.pmatsci.2020.100703},
url = {https://www.sciencedirect.com/science/article/pii/S0079642520300670},
author = {H.L. Wei and T. Mukherjee and W. Zhang and J.S. Zuback and G.L. Knapp and A. De and T. DebRoy},
keywords = {Additive manufacturing, 3D printing, Modeling, Heat transfer and fluid flow, Microstructure, Defects},
abstract = {Additive manufacturing (AM), also known as 3D printing, is gaining wide acceptance in diverse industries for the manufacturing of metallic components. The microstructure and properties of the components vary widely depending on printing process and process parameters, and prediction of causative variables that affect structure, properties and defects is helpful for their control. Since models are most useful when they can correctly predict experimental observations, we focus on the available mechanistic models of AM that have been adequately validated. Specifically, the applications of transport phenomena models in the studies of solidification, residual stresses, distortion, formation of defects and the evolution of microstructure and properties are critically reviewed. The functionality of AM models in understanding of the printability of commonly used AM alloys and the fabrication of functionally graded alloys are also assessed. Opportunities for future research are identified considering the gaps in knowledge in modeling. The uniqueness of this review includes substantive discussions of the rapid certification of the AM components aided by scale models, bidirectional models, cloud based big data, machine learning and digital twins of AM hardware.}
}
@article{KEAVENEY20211674,
title = {Development and Implementation of a Digital Manufacturing Demonstrator for Engineering Education},
journal = {Procedia CIRP},
volume = {104},
pages = {1674-1679},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.282},
url = {https://www.sciencedirect.com/science/article/pii/S221282712101180X},
author = {Shane Keaveney and Lydia Athanasopoulou and Vasilis Siatras and Panagiotis Stavropoulos and Dimitris Mourtzis and Denis P. Dowling},
keywords = {IoT, Augment Reliaty, 3D printing, Engineering education},
abstract = {The fourth industrial revolution (Industry 4.0) offers enhanced processing efficiencies and reduced downtime, however there is a lack of manufacturing training equipment for those seeking practical training in the key underpinning digital technologies. This paper reports on the development of a digital manufacturing training demonstrator, called the PERFORM turbine demonstrator, which incorporates four aspects of Industry 4.0, that of the Internet of Things (IoT), Augmented Reality (AR), the digital twin and additive manufacturing (3D printing). The program involves the printing of a polymer turbine and its testing using a turbine demonstrator system. This incorporates the use of an Arduino microcontroller, and sensors to monitor the systems variable pump speed, turbine speed, temperature, and humidity. A cloud-based platform allows for data logging and passing to the AR application, visualization, and system control. The design and development of the system is presented, alongside the results of pilot training programme, which involved undergraduate engineering students and industry trainees, on the practical application of Industry 4.0. This paper demonstrated the effectiveness of the PERFORM demonstrator in successfully integrating different digital technologies in the same system.}
}
@article{WANG202116,
title = {A digital twin-based big data virtual and real fusion learning reference framework supported by industrial internet towards smart manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {16-32},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301990},
author = {Pei Wang and Ming Luo},
keywords = {Virtual and real fusion learning, Big data learning and analysis models, Digital twin, Industrial internet, Smart manufacturing},
abstract = {Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.}
}
@article{HU20201,
title = {Petri-net-based dynamic scheduling of flexible manufacturing system via deep reinforcement learning with graph convolutional network},
journal = {Journal of Manufacturing Systems},
volume = {55},
pages = {1-14},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300145},
author = {Liang Hu and Zhenyu Liu and Weifei Hu and Yueyang Wang and Jianrong Tan and Fei Wu},
keywords = {Dynamic scheduling, Petri nets, Deep reinforcement learning, Graph convolutional networks, Digital twin},
abstract = {To benefit from the accurate simulation and high-throughput data contributed by advanced digital twin technologies in modern smart plants, the deep reinforcement learning (DRL) method is an appropriate choice to generate a self-optimizing scheduling policy. This study employs the deep Q-network (DQN), which is a successful DRL method, to solve the dynamic scheduling problem of flexible manufacturing systems (FMSs) involving shared resources, route flexibility, and stochastic arrivals of raw products. To model the system in consideration of both manufacturing efficiency and deadlock avoidance, we use a class of Petri nets combining timed-place Petri nets and a system of simple sequential processes with resources (S3PR), which is named as the timed S3PR. The dynamic scheduling problem of the timed S3PR is defined as a Markov decision process (MDP) that can be solved by the DQN. For constructing deep neural networks to approximate the DQN action-value function that maps the timed S3PR states to scheduling rewards, we innovatively employ a graph convolutional network (GCN) as the timed S3PR state approximator by proposing a novel graph convolution layer called a Petri-net convolution (PNC) layer. The PNC layer uses the input and output matrices of the timed S3PR to compute the propagation of features from places to transitions and from transitions to places, thereby reducing the number of parameters to be trained and ensuring robust convergence of the learning process. Experimental results verify that the proposed DQN with a PNC network can provide better solutions for dynamic scheduling problems in terms of manufacturing performance, computational efficiency, and adaptability compared with heuristic methods and a DQN with basic multilayer perceptrons.}
}
@article{BOOYSE2020106612,
title = {Deep digital twins for detection, diagnostics and prognostics},
journal = {Mechanical Systems and Signal Processing},
volume = {140},
pages = {106612},
year = {2020},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2019.106612},
url = {https://www.sciencedirect.com/science/article/pii/S0888327019308337},
author = {Wihan Booyse and Daniel N. Wilke and Stephan Heyns},
keywords = {Artificial intelligence, Deep learning, System health management, Predictive maintenance, Deep generative models, Digital twins},
abstract = {A generic framework for prognostics and health monitoring (PHM) which is rapidly deployable to heterogeneous fleets of assets would allow for the automation of predictive maintenance scheduling directly from operational data. Deep learning based PHM implementations provide part of the solution, but their main benefits are lost when predictions still rely on historical failure data and case-by-case feature engineering. We propose a solution to these challenges in the form of a Deep Digital Twin (DDT). The DDT is constructed from deep generative models which learn the distribution of healthy data directly from operational data at the beginning of an asset’s life-cycle. As the DDT learns the distribution of healthy data it does not rely on historical failure data in order to produce an estimation of asset health. This article presents an overview of the DDT framework and investigates its performance on a number of datasets. Based on these investigations, it is demonstrated that the DDT is able to detect incipient faults, track asset degradation and differentiate between failure modes in both stationary and non-stationary operating conditions when trained on only healthy operating data.}
}
@article{GAO2021154,
title = {Residual error based knowledge distillation},
journal = {Neurocomputing},
volume = {433},
pages = {154-161},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.10.113},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220318117},
author = {Mengya Gao and Yujun Wang and Liang Wan},
keywords = {Model compression, Knowledge distillation, Residual learning},
abstract = {Knowledge distillation (KD) is one of the most popular ways for model compression. The key idea is to transfer the knowledge from a deep teacher model (T) to a shallower student (S). However, existing methods suffer from performance degradation due to the substantial gap between the learning capacities of S and T. To remedy this problem, this paper proposes Residual error based Knowledge Distillation (RKD), which further distills the knowledge by introducing an assistant model(A). Specifically, S is trained to mimic the feature maps of T, and A aids this process by learning the residual error between them. In this way, S and A complement with each other to get better knowledge from T. Furthermore, we devise an effective method to derive S and A from a given model without increasing the total computational cost. Extensive experiments show that our approach achieves appealing results on popular classification datasets, CIFAR-100 and ImageNet, surpassing state-of-the-art methods and keep strong robustness to adversarial samples.}
}
@article{RITTO2021107614,
title = {Digital twin, physics-based model, and machine learning applied to damage detection in structures},
journal = {Mechanical Systems and Signal Processing},
volume = {155},
pages = {107614},
year = {2021},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.107614},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021000091},
author = {T.G. Ritto and F.A. Rochinha},
keywords = {Digital twin, Physical based model, Machine learning classifier, Damage identification, Structural dynamics},
abstract = {This work is interested in digital twins, and the development of a simplified framework for them, in the context of dynamical systems. Digital twin is an ingenious concept that helps on organizing different areas of expertise aiming at supporting engineering decisions related to a specific asset; it articulates computational models, sensors, learning, real time analysis, diagnosis, prognosis, and so on. In this framework, and to leverage its capacity, we explore the integration of physics-based models with machine learning. A digital twin is constructed for a damaged structure, where a discrete physics-based computational model is employed to investigate several damage scenarios. A machine learning classifier, that serves as the digital twin, is trained with data taken from a stochastic computational model. This strategy allows the use of an interpretable model (physics-based) to build a fast digital twin (machine learning) that will be connected to the physical twin to support real time engineering decisions. Different classifiers (quadratic discriminant, support vector machines, etc) are tested, and different model parameters (number of sensors, level of noise, damage intensity, uncertainty, operational parameters, etc) are considered to construct datasets for the training. The accuracy of the digital twin depends on the scenario analyzed. Through the chosen application, we are able to emphasize each step of a digital twin construction, including the possibility of integrating physics-based models with machine learning. The different scenarios explored yield conclusions that might be helpful for a large range of applications.}
}
@article{PAPACHARALAMPOPOULOS2021490,
title = {Incorporating process physics phenomena in formation of digital twins: laser welding case},
journal = {Procedia CIRP},
volume = {99},
pages = {490-495},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.069},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121003528},
author = {Alexios Papacharalampopoulos and Kyriakos Sabatakakis and Panos Stavropoulos},
keywords = {Laser cutting, digital twin, physics, modelling, machine learning},
abstract = {Manufacturing process design and implementation can benefit from the adaptation of Digital Twins. However, the explanatory power coming by physics models presents a strong contradiction with the demand of rapid decision making required for their control and optimization. In the context of laser welding applications, this work investigates three physics-based modelling methods (namely direct Stefan method, apparent heat capacity method and Enthalpy method) along with sensorial data towards the formation of a knowledge database in order to aid the development of a Digital Twin. Also, the methodology for creating such a Digital Twin is discussed incorporating the best result(s) and method(s). This Digital Twin is proved useful in optimizing the process itself but also monitoring, through selection of sensors.}
}
@article{ZABALA2020109920,
title = {Virtual testbed for model predictive control development in district cooling systems},
journal = {Renewable and Sustainable Energy Reviews},
volume = {129},
pages = {109920},
year = {2020},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.109920},
url = {https://www.sciencedirect.com/science/article/pii/S1364032120302112},
author = {Laura Zabala and Jesus Febres and Raymond Sterling and Susana López and Marcus Keane},
keywords = {District cooling, MPC, Modelling, Modelica, Machine learning, Testbed},
abstract = {Recently, with increasing cooling demands, district cooling has assumed an important role as it is more efficient than stand-alone cooling systems. District cooling reduces the environmental impact and promotes the use of renewable sources. Earlier studies to optimise the production plants of district cooling systems were focused primarily on plants with compressor chillers and thermal energy storage devices. Although absorption chillers are crucial for integrating renewable sources into these systems, very few studies have considered them from the cooling perspective. In this regard, this paper presents the progress and results of the implementation of a virtual testbed based on a digital twin of a district cooling production plant with both compressor and absorption chillers. The aim of this study, carried out within the framework of INDIGO, a European Union-funded project, was (i) to develop a reliable model that can be used in a model predictive controller and (ii) to simulate the plant using this controller. The production plant components, which included absorption and compressor chillers, as well as cooling towers, were built using the equation-based Modelica programming language, and were calibrated using information from the manufacturer, together with real operation data. The remainder of the plant was modelled in Python. To integrate the Modelica models into the Python environment, a combination of machine learning techniques and state-space representation models was used. With these techniques, models with a high computational speed were obtained, which were suitable for real-time applications. These models were then used to build a model predictive control for the production plant to minimise the primary energy usage. The improvements in the control and the resultant energy savings achieved were compared with a baseline case working on a standard cascade control. Energy savings up to 50% were obtained in the simulation-based experiments.}
}
@article{ZOHDI2021113446,
title = {A digital twin framework for machine learning optimization of aerial fire fighting and pilot safety},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {373},
pages = {113446},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2020.113446},
url = {https://www.sciencedirect.com/science/article/pii/S0045782520306319},
author = {T.I. Zohdi},
keywords = {Aerial fire-fighting, Fire retardants, Optimization, Machine-learning},
abstract = {The objective of this work is to model and simulate aerial drops of fire retardants in dangerous fire environments. Specifically, the work develops a computational framework for a model problem combining: •[1.] A meshless discrete element component that tracks the trajectory of released airborne materials from a controlled aircraft, ranging from retardant powders to encapsulated packets, subjected to prevailing wind velocities and fire-driven updrafts.•[2.] A Machine Learning Algorithm (MLA) to rapidly ascertain the optimal aircraft (unmanned or manned) dynamics to maximize the fire-retardant release effectiveness (released material usage and target impact). The framework is designed to enable Digital Twin type technologies, i.e. digital replicas that run in real time with the physical system. However, it is also designed to run at much faster rates, in order to enable MLA’s to optimize the planning, by running quickly on laptops and mobile systems. The overall guiding motivation is to provide a useful tool to enable rapid flight-path planning for aerial first-responders in real-time and to train pilots. Numerical examples are provided to illustrate the process.}
}
@article{VRABIC2021349,
title = {An intelligent agent-based architecture for resilient digital twins in manufacturing},
journal = {CIRP Annals},
volume = {70},
number = {1},
pages = {349-352},
year = {2021},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2021.04.049},
url = {https://www.sciencedirect.com/science/article/pii/S0007850621000731},
author = {Rok Vrabič and John Ahmet Erkoyuncu and Maryam Farsi and Dedy Ariansyah},
keywords = {Manufacturing system, Digital twin, Machine learning},
abstract = {Digital twins (DTs) offer the potential for improved understanding of current and future manufacturing processes. This can only be achieved by DTs consistently and accurately representing the real processes. However, the robustness and resilience of the DT itself remain an issue. Accordingly, this paper offers an approach to deal with uncertainty and disruptions, as the DT detects these effectively and self-adapts as needed to maintain representativeness. The paper proposes an intelligent agent-based architecture to improve the robustness (including accuracy of representativeness) and resilience (including timely update) of the DT. The approach is demonstrated on a case of cryogenic secondary manufacturing.}
}
@article{SCHROEDER2021737,
title = {Digital Twin connectivity topologies},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {737-742},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.086},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008302},
author = {Greyce N. Schroeder and Charles Steinmetz and Ricardo N. Rodrigues and Achim Rettberg and Carlos E. Pereira},
keywords = {Digital Twin, Digitalization, Topology, Model},
abstract = {Digital Twin (DT) is one of the key concepts in the industry 4.0. Through the Internet of Things (IoT), it brings the ability of having a virtual representation of a real-world element which enables many possibilities such as saving and tracking its life-cycle, perform simulations, learning based on all this data and so on. However, it is still an ongoing concept that is continuously being evolved. Therefore, a well defined topology of how a DT can be build is still needed. In this context, this paper specifies six ways of how a DT can be build in different kind of applications. Examples of models for each proposed topology are given in AutomationML for better understanding of the proposal.}
}
@article{MOHANTY2021102032,
title = {A multi-modal approach towards mining social media data during natural disasters - A case study of Hurricane Irma},
journal = {International Journal of Disaster Risk Reduction},
volume = {54},
pages = {102032},
year = {2021},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2020.102032},
url = {https://www.sciencedirect.com/science/article/pii/S221242092031534X},
author = {Somya D. Mohanty and Brown Biggers and Saed Sayedahmed and Nastaran Pourebrahim and Evan B. Goldstein and Rick Bunch and Guangqing Chi and Fereidoon Sadri and Tom P. McCoy and Arthur Cosby},
keywords = {Data mining, Social media, Natural disaster, Machine learning},
abstract = {Streaming social media provides a real-time glimpse of extreme weather impacts. However, the volume of streaming data makes mining information a challenge for emergency managers, policy makers, and disciplinary scientists. Here we explore the effectiveness of data learned approaches to mine and filter information from streaming social media data from Hurricane Irma's landfall in Florida, USA. We use 54,383 Twitter messages (out of 784 K geolocated messages) from 16,598 users from Sept. 10–12, 2017 to develop 4 independent models to filter data for relevance: 1) a geospatial model based on forcing conditions at the place and time of each tweet, 2) an image classification model for tweets that include images, 3) a user model to predict the reliability of the tweeter, and 4) a text model to determine if the text is related to Hurricane Irma. All four models are independently tested, and can be combined to quickly filter and visualize tweets based on user-defined thresholds for each submodel. We envision that this type of filtering and visualization routine can be useful as a base model for data capture from noisy sources such as Twitter. The data can then be subsequently used by policy makers, environmental managers, emergency managers, and domain scientists interested in finding tweets with specific attributes to use during different stages of the disaster (e.g., preparedness, response, and recovery), or for detailed research.}
}
@article{WURSTER2021158,
title = {Towards planning and control in cognitive factories - A generic model including learning effects and knowledge transfer across system entities},
journal = {Procedia CIRP},
volume = {103},
pages = {158-163},
year = {2021},
note = {9th CIRP Global Web Conference – Sustainable, resilient, and agile manufacturing and service operations : Lessons from COVID-19},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121008660},
author = {Marco Wurster and Yannick Exner and Jan-Philipp Kaiser and Nicole Stricker and Gisela Lanza},
keywords = {Learning Effects, Cognitive Robots, Digital Twin, Hybrid Production Systems, Disassembly},
abstract = {Cognitive abilities allow robots to learn and reason from their environment. The gained knowledge can then be incorporated into the robot’s actions which in turn affect the environment. Therefore, a cognitive robot is no longer a static system that performs actions based on a pre-defined set of rules but a complex entity that dynamically adjusts over time. With this, challenges arise for production systems that need to observe and ideally anticipate the cognitive robot’s behavior. Often, digital twins are employed to test and optimize production control systems. This paper presents a generic approach to characterize, model and simulate learning processes and formalized knowledge in hybrid production systems assuming different station types with learning effects. Thereby, quantitative and qualitative learning processes are mapped including knowledge sharing and transfer across entities. A modular and parameterizable design enables the adjustment to different use cases. Eventually, the model is instantiated as a digital twin of a real production system for product disassembly employing cognitive-autonomous robots among human operators and rigidly automated machines. The model shows great potential to be integrated into test beds for planning and control systems of cognitive factories.}
}
@article{DAI2021107051,
title = {A tucker decomposition based knowledge distillation for intelligent edge applications},
journal = {Applied Soft Computing},
volume = {101},
pages = {107051},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.107051},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620309893},
author = {Cheng Dai and Xingang Liu and Zhuolin Li and Mu-Yen Chen},
keywords = {Knowledge distillation, Intelligent edge computing, Deep learning, Tensor decomposition},
abstract = {Knowledge distillation(KD) has been proven an effective method in intelligent edge computing and have achieved extensive study in recent deep learning research. However, when the teacher network is too stronger compared to the student network, the effect of knowledge distillation is not ideal. Aiming at resolving this problem, an improved method of knowledge distillation (TDKD) is proposed, which enables to transfer the complex mapping functions learned by cumbersome models to relatively simpler models. Firstly, the tucker-2 decomposition was performed on the convolutional layers of the original teacher model to reduce the capacity variance between the teacher network and student network. Then, the decomposed model will be used as a new teacher to participate in knowledge distillation for the student model. The experimental results show that the TDKD method can effectively solve the problem of poor distillation performance, which not only get better results if the KD method is effective, but also can reactivate the invalid KD method to some extents.}
}
@article{MERAGHNI20212555,
title = {A data-driven digital-twin prognostics method for proton exchange membrane fuel cell remaining useful life prediction},
journal = {International Journal of Hydrogen Energy},
volume = {46},
number = {2},
pages = {2555-2564},
year = {2021},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2020.10.108},
url = {https://www.sciencedirect.com/science/article/pii/S0360319920339252},
author = {Safa Meraghni and Labib Sadek Terrissa and Meiling Yue and Jian Ma and Samir Jemei and Noureddine Zerhouni},
keywords = {Deep learning, Digital twin, Prognostics, Proton exchange membrane fuel cell, Remaining useful life},
abstract = {Prognostics and health management of proton exchange membrane fuel cell (PEMFC) systems have driven increasing research attention in recent years as the durability of PEMFC stack remains as a technical barrier for its large-scale commercialization. To monitor the health state during PEMFC operation, digital twin (DT), as a smart manufacturing technique, is applied in this paper to establish an ensemble remaining useful life prediction system. A data-driven DT is constructed to integrate the physical knowledge of the system and a deep transfer learning model based on stacked denoising autoencoder is used to update the DT with online measurement. A case study with experimental PEMFC degradation data is presented where the proposed data-driven DT prognostics method has applied and reached a high prediction accuracy. Furthermore, the predicted results are proved to be less affected even with limited measurement data.}
}
@article{ROWE2020S52,
title = {Artificial Intelligence for Personalized Preventive Adolescent Healthcare},
journal = {Journal of Adolescent Health},
volume = {67},
number = {2, Supplement },
pages = {S52-S58},
year = {2020},
note = {Innovative Digital Technologies to Improve Adolescent and Young Adult Health},
issn = {1054-139X},
doi = {https://doi.org/10.1016/j.jadohealth.2020.02.021},
url = {https://www.sciencedirect.com/science/article/pii/S1054139X20300951},
author = {Jonathan P. Rowe and James C. Lester},
keywords = {Artificial intelligence, Prevention, Health information technology, Adaptive learning technologies, User modeling, Interactive narrative generation, Adolescents},
abstract = {Recent advances in artificial intelligence (AI) are creating new opportunities for personalizing technology-based health interventions to adolescents. This article provides a computer science perspective on how emerging AI technologies—intelligent learning environments, interactive narrative generation, user modeling, and adaptive coaching—can be utilized to model adolescent learning and engagement and deliver personalized support in adaptive health technologies. Many of these technologies have emerged from human-centered applications of AI in education, training, and entertainment. However, their application to improving healthcare, to date, has been comparatively limited. We illustrate the opportunities provided by AI-driven adaptive technologies for adolescent preventive healthcare by describing a vision of how future adolescent preventive health interventions might be delivered both inside and outside of the clinic. Key challenges posed by AI-driven health technologies are also presented, including issues of privacy, ethics, encoded bias, and integration into clinical workflows and adolescent lives. Examples of empirical findings about the effectiveness of AI technologies for user modeling and adaptive coaching are presented, which underscore their promise for application toward adolescent health. The article concludes with a brief discussion of future research directions for the field, which is well positioned to leverage AI to improve adolescent health and well-being.}
}
@article{ECHLIN2020100817,
title = {Serial sectioning in the SEM for three dimensional materials science},
journal = {Current Opinion in Solid State and Materials Science},
volume = {24},
number = {2},
pages = {100817},
year = {2020},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2020.100817},
url = {https://www.sciencedirect.com/science/article/pii/S1359028620300152},
author = {McLean P. Echlin and Timothy L. Burnett and Andrew T. Polonsky and Tresa M. Pollock and Philip J. Withers},
keywords = {Serial sectioning, Materials science, Life sciences, Biology, Scanning electron microscopy},
abstract = {Here we explore the range of serial sectioning techniques that have evolved over the past decade, providing a comprehensive toolkit for capturing rich 3D microstructures, chemistries and crystallographic information, with sub-micron resolution at volumes that extend out to mm3 or even cm3. In each case we consider the challenges associated with their application, the volumes they can analyze, the damage to the surface they impart, and their suitability for different materials. In certain cases these warrant hybrid methods, motivating workflows that leverage multiple sectioning modes within the same instrument. Finally, we provide a perspective on their future development, including advances in data collection, segmentation, registration, data fusion, and correlative microscopy. Furthermore, the exploitation of 3D techniques for a better understanding of existing materials, and the design of new ones, is discussed through their use in multiscale modelling, digital twinning, material informatics and machine learning frameworks.}
}
@article{EHWERHEMUEPHA2021100030,
title = {A super learner ensemble of 14 statistical learning models for predicting COVID-19 severity among patients with cardiovascular conditions},
journal = {Intelligence-Based Medicine},
volume = {5},
pages = {100030},
year = {2021},
issn = {2666-5212},
doi = {https://doi.org/10.1016/j.ibmed.2021.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2666521221000065},
author = {Louis Ehwerhemuepha and Sidy Danioko and Shiva Verma and Rachel Marano and William Feaster and Sharief Taraman and Tatiana Moreno and Jianwei Zheng and Ehsan Yaghmaei and Anthony Chang},
keywords = {Super learning, Ensemble learning, Cardiovascular conditions, COVID-19, COVID-19 severity, Predicting COVID-19 severity},
abstract = {Background
Cardiovascular and other circulatory system diseases have been implicated in the severity of COVID-19 in adults. This study provides a super learner ensemble of models for predicting COVID-19 severity among these patients.
Method
The COVID-19 Dataset of the Cerner Real-World Data was used for this study. Data on adult patients (18 years or older) with cardiovascular diseases between 2017 and 2019 were retrieved and a total of 13 of these conditions were identified. Among these patients, 33,042 admitted with positive diagnoses for COVID-19 between March 2020 and June 2020 (from 59 hospitals) were identified and selected for this study. A total of 14 statistical and machine learning models were developed and combined into a more powerful super learning model for predicting COVID-19 severity on admission to the hospital.
Result
LASSO regression, a full extreme gradient boosting model with tree depth of 2, and a full logistic regression model were the most predictive with cross-validated AUROCs of 0.7964, 0.7961, and 0.7958 respectively. The resulting super learner ensemble model had a cross validated AUROC of 0.8006 (range: 0.7814, 0.8163). The unbiased AUROC of the super learner model on an independent test set was 0.8057 (95% CI: 0.7954, 0.8159).
Conclusion
Highly predictive models can be built to predict COVID-19 severity of patients with cardiovascular and other circulatory conditions. Super learning ensembles will improve individual and classical ensemble models significantly.}
}
@article{LIANG2020103370,
title = {Teaching robots to perform quasi-repetitive construction tasks through human demonstration},
journal = {Automation in Construction},
volume = {120},
pages = {103370},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103370},
url = {https://www.sciencedirect.com/science/article/pii/S092658052030950X},
author = {Ci-Jyun Liang and Vineet R. Kamat and Carol C. Menassa},
keywords = {Robot learning from demonstration, Visual demonstration, Autoencoder, Reinforcement learning, Digital twin, Ceiling tile installation, Robot apprentice},
abstract = {Robots can assist workers in performing physically-demanding construction tasks, which are typically quasi-repetitive, wherein the geometry of the workspace is dissimilar despite similar tasks. As a result, robots must determine motion trajectories based on the encountered workspace geometry. Learning from Demonstration (LfD) methods have the potential to be used in teaching robots specific tasks through human demonstration, such that robots can then perform learned tasks under different conditions. In this paper, the LfD method is investigated to teach robots how to perform quasi-repetitive construction tasks. Considering ceiling tile installation as the experimental process, the tasks of maneuvering and positioning tiles in a ceiling grid are defined as the target knowledge to be learned. Using a set of human demonstration videos, the designed approach first translates the physical work context, e.g., the pose of the tile, to the target digital twin, i.e., the workspace as-perceived by the robot. The Reinforcement Learning method is then applied to generate the control policy for the robot to perform the subsequent tasks. The proposed method is evaluated in the Robot Operating System (ROS) Gazebo simulator using a KUKA mobile industrial robotic arm emulator and 60 different scenes as test cases. The results show a 78% success rate in installing ceiling tiles based on 3000 virtual and 85 real demonstration videos. The success rate tends to continually rise with an increase in the number of real demonstration videos, confirming the promise and applicability of the LfD method in teaching robot apprentices to perform quasi-repetitive tasks on construction sites.}
}
@article{ALGURI2021106338,
title = {Sim-to-Real: Employing ultrasonic guided wave digital surrogates and transfer learning for damage visualization},
journal = {Ultrasonics},
volume = {111},
pages = {106338},
year = {2021},
issn = {0041-624X},
doi = {https://doi.org/10.1016/j.ultras.2020.106338},
url = {https://www.sciencedirect.com/science/article/pii/S0041624X20302675},
author = {K. Supreet Alguri and Chen Ciang Chia and Joel B. Harley},
keywords = {Ultrasonic guided waves, Dictionary learning, Transfer learning, Digital twin, Baseline subtraction},
abstract = {Wavefield imaging is a powerful visualization tool in nondestructive evaluation for studying ultrasonic wave propagation and its interactions with damage. To isolate and study damage scattering, damage-free baseline data is often subtracted from a wavefield. This is often necessary because the damage wavefield can be orders of magnitude weaker than the incident waves. Yet, baselines are not always accessible. When the baselines are accessible, the experimental conditions for the baseline and test data must be extremely similar. Researchers have created several baseline-free approaches for isolating damage wavefields, but these often rely on specific experimental setups. In this paper, we discuss a flexible approach based on ultrasonic guided wave digital surrogates (i.e., numerical simulations of incident waves) and transfer learning. We demonstrate this approach with two setups. We first isolate reflections from a circular, 2 mm diameter half-thickness hole on a 10 × 10 cm steel plate. We then isolate 8 circular, half-thickness holes of various diameters from 1 mm to 40 mm on a 60 × 60 cm steel plate. The second plate has a non-square geometry and the data has multi-path reflections. With both data sets, we isolate damage reflections without explicit experimental baselines. We also briefly illustrate the comparison of our dictionary learning method with wavenumber filtering technique which is often used to enhance the defect wavefields.}
}
@article{JONES2021283,
title = {Towards integrated version control of virtual and physical artefacts in new product development: inspirations from software engineering and the digital twin paradigm},
journal = {Procedia CIRP},
volume = {100},
pages = {283-288},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.121},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121005953},
author = {David Jones and Aydin Nassehi and Chris Snider and James Gopsill and Peter Rosso and Ric Real and Mark Goudswaard and Ben Hicks},
keywords = {Digital Twin, New Product Development, Engineering Design, Version Control},
abstract = {Modern version control strategies are highly capable at supporting the management of virtual artefacts. The process of developing a new product, however, is not limited to virtual artefacts. Today’s fast-paced industrial processes require a diverse range of both virtual and physical artefacts to explore, refine, and evaluate designs. These virtual and physical artefacts are interrelated, and the information they embody, the knowledge they generate, and the transfer of learning between are fundamental to the design history. Consequently, there is a requirement to capture and curate both virtual and physical artefacts, iterations thereof, and the interrelationships between. The Digital Twin paradigm couples physical and virtual artefacts throughout the product life-cycle, providing a means to capture an evolving design irrespective of the medium in which the designer is working. Recent literature has, however, raised questions about the concept of a Digital Twin early in the product life-cycle when the design in question is conceptual (a cognitive model) rather than physical or virtual. This paper reflects on the challenges of implementing Digital Twin-based version control in the early-stage of new product development, moving towards integrated version control of virtual, physical and cognitive models/artefacts. Firstly, by presenting an argument that current design practices capture cognitive models through stakeholder creation and evaluation of physical and virtual boundary objects, the ambiguity surrounding conceptual design and the Digital Twin is addressed. Secondly, the principles of the Digital Twin and current version control strategies are reviewed to determine how one can maintain digital/physical synchronicity as a design evolves. Finally, this paper reflects on the implementation of such an approach and the proposed future work.}
}
@article{AGAPAKI2020101121,
title = {CLOI-NET: Class segmentation of industrial facilities’ point cloud datasets},
journal = {Advanced Engineering Informatics},
volume = {45},
pages = {101121},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101121},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300902},
author = {Eva Agapaki and Ioannis Brilakis},
keywords = {Class segmentation, Industrial facilities, Point cloud processing, CLOI},
abstract = {Shape segmentation from point cloud data is a core step of the digital twinning process for industrial facilities. However, it is also a very labor intensive step, which counteracts the perceived value of the resulting model. The state-of-the-art method for automating cylinder detection can detect cylinders with 62% precision and 70% recall, while other shapes must then be segmented manually and shape segmentation is not achieved. This performance is promising, but it is far from drastically eliminating the manual labor cost. We argue that the use of class segmentation deep learning algorithms has the theoretical potential to perform better in terms of per point accuracy and less manual segmentation time needed. However, such algorithms could not be used so far due to the lack of a pre-trained dataset of laser scanned industrial shapes as well as the lack of appropriate geometric features in order to learn these shapes. In this paper, we tackle both problems in three steps. First, we parse the industrial point cloud through a novel class segmentation solution (CLOI-NET) that consists of an optimized PointNET++ based deep learning network and post-processing algorithms that enforce stronger contextual relationships per point. We then allow the user to choose the optimal manual annotation of a test facility by means of active learning to further improve the results. We achieve the first step by clustering points in meaningful spatial 3D windows based on their location. Then, we apply a class segmentation deep network, and output a probability distribution of all label categories per point and improve the predicted labels by enforcing post-processing rules. We finally optimize the results by finding the optimal amount of data to be used for training experiments. We validate our method on the largest richly annotated dataset of the most important to model industrial shapes (CLOI) and yield 82% average accuracy per point, 95.6% average AUC among all classes and estimated 70% labor hour savings in class segmentation. This proves that it is the first to automatically segment industrial point cloud shapes with no prior knowledge at commercially viable performance and is the foundation for efficient industrial shape modeling in cluttered point clouds.}
}
@article{HURKAMP20211,
title = {Simulation-based digital twin for the manufacturing of thermoplastic composites},
journal = {Procedia CIRP},
volume = {100},
pages = {1-6},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004583},
author = {André Hürkamp and Ralf Lorenz and Tim Ossowski and Bernd-Arno Behrens and Klaus Dröder},
keywords = {Thermoplastic Composites, Thermoforming, Finite Element Method, Reduced Order Modelling, Digital Twin},
abstract = {The bond strength between a thermoformed fibre reinforced thermoplastic sheet and an injected polymer is the limiting factor for the structural integrity of overmoulded thermoplastic composites. In this contribution, a simulation based digital twin of the thermoforming process is presented. From numerical parametric studies a reduced order model based on Proper Orthogonal Decomposition (POD) is developed. The combination with machine learning methods enables the real-time computation of arbitrary physical reliable temperature fields with sufficient accuracy to be used for design purposes and as inline quality gates.}
}
@article{GROEN2020102876,
title = {FlexMM: A standard method for material descriptions in FEM},
journal = {Advances in Engineering Software},
volume = {148},
pages = {102876},
year = {2020},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2020.102876},
url = {https://www.sciencedirect.com/science/article/pii/S0965997820300855},
author = {Manso Groen and Soheil Solhjoo and Ruud Voncken and Jan Post and Antonis. I. Vakis},
keywords = {FEM analysis, Digital twin, Material modeling, Multi stage modeling, User subroutines, Zero-defect manufacturing, Industry 4.0},
abstract = {This article discusses a number of key issues concerning simulation-based digital twins in the domain of multistage processes. Almost all production processes are multistage in nature, and so most digital twins involve multiple physical phenomena, process steps and different solvers for the simulations. Good interoperability between model solvers and processes are key to achieving a functional digital twin. Passing information between steps can be challenging, complex and time consuming, especially for material data, because the constitutive model interacts with the full modeling environment: material behavior is interdependent with the history of the process, the solver subroutines and the boundary conditions. This work proposes a flexible yet robust standardization approach, called FlexMM, for dealing with material data, constitutive models, measurement data or mathematical models to overcome part of the abovementioned complexity. The implementation of FlexMM consists of a general rule structure in which constitutive behavior is described, as well as its interaction with the subroutines used by the finite element solver. The definition of the constitutive model is stored in a separate file, in which the material behavior can be described in a user selected format, such as look-up tables, standard statistical models, machine learning or analytical expressions. After a calculation step, the new local material properties are mapped to a file to facilitate the next history-dependent step. In this way, the interaction between the different fabrication steps and processes can be incorporated. A material/process case study is presented to demonstrate the flexibility and robustness of FlexMM.}
}
@article{PITKAAHO2021540,
title = {Indoor positioning, artificial intelligence and digital twins for enhanced robotics safety},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {540-545},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.062},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321007849},
author = {Tomi Pitkäaho and Tero Kaarlela and Sakari Pieskä and Sami Sarlin},
keywords = {Robotics technology, Perception, sensing, Positioning systems, Machine learning, Intelligent interfaces},
abstract = {Flexible robotics safety solutions allowing the implementation of fenceless robot cells are becoming a reality nowadays. Safety approved sensors such as light curtains, safety scanners, and safety cameras have been deployed already successfully in various industrial robotic solutions. Still, as these safety systems are installed in fixed locations, monitoring predefined regions, the systems can be rigid and inflexible. This paper introduces a novel hybrid safety solution. The solution comprises safety-approved sensors, additional sensors, and artificial intelligence analysis. The system increases flexibility, especially in cases where collaborating humans and robots need monitoring in larger areas. Typically, in such environments, work objects are large and heavy, introducing additional challenges. In addition, the proposed system includes a digital twin implementation that allows a connection between the real and virtual worlds. Already virtual models and robot simulation have been used for designing safe robot applications. However, the efficient use of digital twins in safety planning and safety monitoring is still uncommon.}
}
@article{GOVINDSAMY2021192,
title = {Leak Detection at Anglo Platinum Converting Process Using Digital Twins},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {21},
pages = {192-197},
year = {2021},
note = {Control Conference Africa CCA 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.12.033},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321023727},
author = {D. Govindsamy and G.A. Georgalli and A. Hoosen},
keywords = {ACP, Fault Detection, Digital Twins, Machine Learning, Data Processing},
abstract = {Leaks on the high-pressure cooling system supporting the Convertor at the Anglo Converting Process (ACP) poses safety risks and requires plant downtime to repair. The leaks are difficult to detect, as the accuracy of available sensors are insufficient at the operating conditions of the system. Fault detection using digital twins have been employed successfully in the process industry. This paper discusses the construction of digital twins for the leak detection problem at ACP and focuses on the practical considerations of data preparation and modelling. This analytic has been implemented in the control room Human Machine Interface and a web enabled dashboard.}
}
@article{RAMAPURAM2020381,
title = {Lifelong generative modeling},
journal = {Neurocomputing},
volume = {404},
pages = {381-400},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.02.115},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220303623},
author = {Jason Ramapuram and Magda Gregorova and Alexandros Kalousis},
keywords = {Lifelong learning, Continual learning, Variational inference, Generative modeling},
abstract = {Lifelong learning is the problem of learning multiple consecutive tasks in a sequential manner, where knowledge gained from previous tasks is retained and used to aid future learning over the lifetime of the learner. It is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on a lifelong learning approach to unsupervised generative modeling, where we continuously incorporate newly observed distributions into a learned model. We do so through a student-teacher Variational Autoencoder architecture which allows us to learn and preserve all the distributions seen so far, without the need to retain the past data nor the past models. Through the introduction of a novel cross-model regularizer, inspired by a Bayesian update rule, the student model leverages the information learned by the teacher, which acts as a probabilistic knowledge store. The regularizer reduces the effect of catastrophic interference that appears when we learn over sequences of distributions. We validate our model’s performance on sequential variants of MNIST, FashionMNIST, PermutedMNIST, SVHN and Celeb-A and demonstrate that our model mitigates the effects of catastrophic interference faced by neural networks in sequential learning scenarios.}
}
@article{HUANG2021106,
title = {Stochastic configuration network ensembles with selective base models},
journal = {Neural Networks},
volume = {137},
pages = {106-118},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000198},
author = {Changqin Huang and Ming Li and Dianhui Wang},
keywords = {Stochastic configuration networks, Randomized learner models, Neural network ensemble, Educational data analytics},
abstract = {Studies have demonstrated that stochastic configuration networks (SCNs) have good potential for rapid data modeling because of their sufficient adequate learning power, which is theoretically guaranteed. Empirical studies have verified that the learner models produced by SCNs can usually achieve favorable test performance in practice but more in-depth theoretical analysis of their generalization power would be useful for constructing SCN-based ensemble models with enhanced generalization capacities. In particular, given a collection of independently developed SCN-based learner models, it is useful to select certain base learners that can potentially obtain preferable test results rather than considering all of the base models together, before simply taking their average in order to build an effective ensemble model. In this study, we propose a novel framework for building SCN ensembles by exploring key factors that might potentially affect the generalization performance of the base model. Under a mild assumption, we provide a comprehensive theoretical framework for examining a learner model’s generalization error, as well as formulating a novel indicator that contains measurement information for the training errors, output weights, and a hidden layer output matrix, which can be used by our proposed algorithm to find a subset of appropriate base models from a pool of randomized learner models. A toy example of one-dimensional function approximation, a case study for developing a predictive model for forecasting student learning performance, and two large-scale data sets were used in our experiments. The experimental results indicate that our proposed method has some remarkable advantages for building ensemble models.}
}
@article{CHAKRABORTY2021106410,
title = {Machine learning based digital twin for dynamical systems with multiple time-scales},
journal = {Computers & Structures},
volume = {243},
pages = {106410},
year = {2021},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2020.106410},
url = {https://www.sciencedirect.com/science/article/pii/S0045794920302133},
author = {S. Chakraborty and S. Adhikari},
keywords = {Digital twin, Multi-timescale dynamics, Mixture of experts, Gaussian process, Frequency},
abstract = {Digital twin technology has a huge potential for widespread applications in different industrial sectors such as infrastructure, aerospace, and automotive. However, practical adoptions of this technology have been slower, mainly due to a lack of application-specific details. Here we focus on a digital twin framework for linear single-degree-of-freedom structural dynamic systems evolving in two different operational time scales in addition to its intrinsic dynamic time-scale. Our approach strategically separates into two components – (a) a physics-based nominal model for data processing and response predictions, and (b) a data-driven machine learning model for the time-evolution of the system parameters. The physics-based nominal model is system-specific and selected based on the problem under consideration. On the other hand, the data-driven machine learning model is generic. For tracking the multi-timescale evolution of the system parameters, we propose to exploit a mixture of experts as the data-driven model. Within the mixture of experts model, Gaussian Process (GP) is used as the expert model. The primary idea is to let each expert track the evolution of the system parameters at a single time-scale. For learning the hyperparameters of the ‘mixture of experts using GP’, an efficient framework that exploits expectation-maximization and sequential Monte Carlo sampler is used. Performance of the digital twin is illustrated on a multi-timescale dynamical system with stiffness and/or mass variations. The digital twin is found to be robust and yields reasonably accurate results. One exciting feature of the proposed digital twin is its capability to provide reasonable predictions at future time-steps. Aspects related to the data quality and data quantity are also investigated.}
}
@article{LADJ2021168,
title = {A knowledge-based Digital Shadow for machining industry in a Digital Twin perspective},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {168-179},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S027861252030128X},
author = {Asma Ladj and Zhiqiang Wang and Oussama Meski and Farouk Belkadi and Mathieu Ritou and Catherine {Da Cunha}},
keywords = {Digital shadow, Digital twin, Data and knowledge management, Machining},
abstract = {This paper addresses the problems of data management and analytics for decision-aid by proposing a new vision of Digital Shadow (DS) which would be considered as the core component of a future Digital Twin. Knowledge generated by experts and artificial intelligence, is transformed into formal business rules and integrated into the DS to enable the characterization of the real behavior of the physical system throughout its operation stage. This behavior model is continuously enriched by direct or derived learning, in order to improve the digital twin. The proposed DS relies on data analytics (based on unsupervised learning) and on a knowledge inference engine. It enables the incidents to be detected and it is also able to decipher its operational context. An example of this application in the aeronautic machining industry is provided to stress both the feasibility of the proposition and its potential impact on shop floor performance.}
}
@article{XIA2021210,
title = {A digital twin to train deep reinforcement learning agent for smart manufacturing plants: Environment, interfaces and intelligence},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {210-230},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301059},
author = {Kaishu Xia and Christopher Sacco and Max Kirkpatrick and Clint Saidy and Lam Nguyen and Anil Kircaliali and Ramy Harik},
keywords = {Smart manufacturing systems, Robotics, Artificial intelligence, Digital transformation, Virtual commissioning},
abstract = {Filling the gaps between virtual and physical systems will open new doors in Smart Manufacturing. This work proposes a data-driven approach to utilize digital transformation methods to automate smart manufacturing systems. This is fundamentally enabled by using a digital twin to represent manufacturing cells, simulate system behaviors, predict process faults, and adaptively control manipulated variables. First, the manufacturing cell is accommodated to environments such as computer-aided applications, industrial Product Lifecycle Management solutions, and control platforms for automation systems. Second, a network of interfaces between the environments is designed and implemented to enable communication between the digital world and physical manufacturing plant, so that near-synchronous controls can be achieved. Third, capabilities of some members in the family of Deep Reinforcement Learning (DRL) are discussed with manufacturing features within the context of Smart Manufacturing. Trained results for Deep Q Learning algorithms are finally presented in this work as a case study to incorporate DRL-based artificial intelligence to the industrial control process. As a result, developed control methodology, named Digital Engine, is expected to acquire process knowledges, schedule manufacturing tasks, identify optimal actions, and demonstrate control robustness. The authors show that integrating a smart agent into the industrial platforms further expands the usage of the system-level digital twin, where intelligent control algorithms are trained and verified upfront before deployed to the physical world for implementation. Moreover, DRL approach to automated manufacturing control problems under facile optimization environments will be a novel combination between data science and manufacturing industries.}
}
@article{VO202127,
title = {An integrated framework of learning and evidential reasoning for user profiling using short texts},
journal = {Information Fusion},
volume = {70},
pages = {27-42},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1566253520304292},
author = {Duc-Vinh Vo and Jessada Karnjana and Van-Nam Huynh},
keywords = {User profiling, Short texts, Mass functions, Information fusion, Dempster–Shafer theory},
abstract = {Inferring user profiles based on texts created by users on social networks has a variety of applications in recommender systems such as job offering, item recommendation, and targeted advertisement. The problem becomes more challenging when working with short texts like tweets on Twitter, or posts on Facebook. This work aims at proposing an integrated framework based on Dempster–Shafer theory of evidence, word embedding, and k-means clustering for user profiling problem, which is capable of not only working well with short texts but also dealing with uncertainty inherently in user texts. The proposed framework is essentially composed of three phases: (1) Learning abstract concepts at multiple levels of abstraction from user corpora; (2) Evidential inference and combination for user modeling; and (3) User profile extraction. Particularly, in the first phase, a word embedding technique is used to convert preprocessed texts into vectors which capture semantics of words in user corpus, and then k-means clustering is utilized for learning abstract concepts at multiple levels of abstraction, each of which reflects appropriate semantics of user profiles. In the second phase, by considering each document in user corpus as an evidential source that carries some partial information for inferring user profiles, we first infer a mass function associated with each user document by maximum a posterior estimation, and then apply Dempster’s rule of combination for fusing all documents’ mass functions into an overall one for the user corpus. Finally, in the third phase, we apply the so-called pignistic probability principle to extract top-n keywords from user’s overall mass function to define the user profile. Thanks to the ability of combining pieces of information from many documents, the proposed framework is flexible enough to be scaled when input data coming from not only multiple modes but different sources on web environments. Besides, the resulting profiles are interpretable, visualizable, and compatible in practical applications. The effectiveness of the proposed framework is validated by experimental studies conducted on datasets crawled from Twitter and Facebook.}
}
@article{WANG2021261,
title = {Digital twin improved via visual question answering for vision-language interactive mode in human–machine collaboration},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {261-269},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301217},
author = {Tian Wang and Jiakun Li and Zhaoning Kong and Xin Liu and Hichem Snoussi and Hongqiang Lv},
keywords = {Digital twin, Human–machine collaboration, Visual question answer, Deep learning},
abstract = {The human–machine collaboration system is a key means of manufacturing. Its surveillance, prognostic, and health management are related to safety and manufacturing persistence. This paper begins with the mission requirements of intelligent manufacturing. The study is based on the visual question answering (VQA) technology with a digital twin to increase efficiency. The research contents are as follows: (1) A method of modeling human–machine collaboration based on digital twins is proposed. (2) A VQA is adopted in the digital twin. The video and neural language are considered. (3) VQA technology is introduced into the modeling of the human–machine collaboration system for consistent integration. With VQA technology, humans and machines can collaborate. Human–machine interaction and product counting are implemented in a case study to provide a comprehensive perception.}
}
@article{SANTAMARIABONFIL2020103871,
title = {Learning analytics for student modeling in virtual reality training systems: Lineworkers case},
journal = {Computers & Education},
volume = {151},
pages = {103871},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103871},
url = {https://www.sciencedirect.com/science/article/pii/S0360131520300701},
author = {Guillermo Santamaría-Bonfil and María Blanca Ibáñez and Miguel Pérez-Ramírez and Gustavo Arroyo-Figueroa and Francisco Martínez-Álvarez},
keywords = {Learning analytics, Performance prediction, Feature importance analysis, Exploratory data analysis, Virtual reality},
abstract = {Live-line maintenance is a high risk activity. Hence, lineworkers require effective and safe training. Virtual Reality Training Systems (VRTS) provide an affordable and safe alternative for training in such high risk environments. However, their effectiveness relies mainly on having meaningful activities for supporting learning and on their ability to detect untrained students. This study builds a student model based on Learning Analytics (LA), using data collected from 1399 students that used a VRTS for the maintenance training of lineworkers in 329 courses carried out from 2008 to 2016. By employing several classifiers, the model allows discriminating between trained and untrained students in different maneuvers using three minimum evaluation proficiency scores. Using the best classifier, a Feature Importance Analysis is carried out to understand the impact of the variables regarding the trainees’ final performances. The model also involves the exploration of the trainees’ trace data through a visualization tool to pose non-observable behavioral variables related to displayed errors. The results show that the model can discriminate between trained and untrained students, the Random Forest algorithm standing out. The feature importance analysis revealed that the most relevant features regarding the trainees’ final performance were profile and course variables along with specific maneuver steps. Finally, using the visual tool, and with human expert aid, several error patterns in trace data associated with misconceptions and confusion were identified. In the light of these, LA enables disassembling the data jigsaw quandary from VRTS to enhance the human-in-the-loop evaluation.}
}
@article{UHLMANN20211430,
title = {Holistic Concept Towards a Reference Architecture Model for Predictive Maintenance},
journal = {Procedia CIRP},
volume = {104},
pages = {1430-1433},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.241},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011392},
author = {Eckart Uhlmann and Julian Polte and Nikolaos-Stefanos Koutrakis},
keywords = {Analytics, Reference Architecture, Digital Twin, Predictive Maintenance, IIoT, Machine Learning},
abstract = {In the era of digital transformation of factories, one of the most challenging applications of the Industrial Internet of Things (IIoT) is predictive maintenance. This paper presents a holistic concept for predictive maintenance together with a reference architecture that includes data acquisition on the sensor level, edge computing and digital twin applications. For that purpose, condition-based maintenance, lifecycle monitoring and digital assistance systems are integrated to develop application-specific digital twins based on the proposed architecture, integrating heterogenous data sources in order to enhance the accuracy of the machine learning models. The concept is illustrated through an experimental use case.}
}
@article{MOLINARO2021104759,
title = {Embedding data analytics and CFD into the digital twin concept},
journal = {Computers & Fluids},
volume = {214},
pages = {104759},
year = {2021},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2020.104759},
url = {https://www.sciencedirect.com/science/article/pii/S0045793020303297},
author = {Roberto Molinaro and Joel-Steven Singh and Sotiris Catsoulis and Chidambaram Narayanan and Djamel Lakehal},
keywords = {Fluid flow simulations, Data analytics, Machine learning, Data-driven models},
abstract = {Computer-Aided Engineering (CAE) has supported the industry in its transition from trial-and-error towards physics-based modelling, but our ways of treating and exploiting the simulation results have changed little during this period. Indeed, the business model of CAE centers almost exclusively around delivering base-case simulation results with a few additional operational conditions. In this contribution, we introduce a new paradigm for the exploitation of computational physics data, consisting in using machine learning to enlarge the simulation databases in order to cover a wider spectrum of operational conditions and provide quick response directly on field. The resulting product from this hybrid physics-informed and data-driven modelling is referred to as Simulation Digital Twin (SDT). While the paradigm can be equally used in different CAE applications, in this paper we address its implementation in the context of Computational Fluid Dynamics (CFD). We show that the generation of Simulation Digital Twins can be efficiently accomplished with the combination of the CFD tool TransAT and the data analytics platform eDAP.}
}
@article{ODWYER2020102412,
title = {Integration of an energy management tool and digital twin for coordination and control of multi-vector smart energy systems},
journal = {Sustainable Cities and Society},
volume = {62},
pages = {102412},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102412},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720306338},
author = {Edward O’Dwyer and Indranil Pan and Richard Charlesworth and Sarah Butler and Nilay Shah},
keywords = {Urban energy systems, Smart cities, Building energy, Transport energy, Machine learning},
abstract = {As Internet of Things (IoT) technologies enable greater communication between energy assets in smart cities, the operational coordination of various energy networks in a city or district becomes more viable. Suitable tools are needed that can harness advanced control and machine learning techniques to achieve environmental, economic and resilience objectives. In this paper, an energy management tool is presented that can offer optimal control, scheduling, forecasting and coordination services to energy assets across a district, enabling optimal decisions under user-defined objectives. The tool presented here can coordinate different sub-systems in a district to avoid the violation of high-level system constraints and is designed in a generic fashion to enable transferable use across different energy sectors. The work demonstrates the potential for a single open-source optimisation framework to be applied across multiple energy vectors, providing local government the opportunity to manage different assets in a coordinated fashion. This is shown through case studies that integrate low-carbon communal heating for social housing with electric vehicle charge-point management to achieve high-level system constraints and local government objectives in the borough of Greenwich, London. The paper illustrates the theoretical methodology, the software architecture and the digital twin-based testing environment underpinning the proposed approach.}
}
@article{GREGORIO2021108,
title = {A digital twin-based approach for the management of geometrical deviations during assembly processes},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {108-117},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300649},
author = {Jean-Loup Grégorio and Claire Lartigue and François Thiébaut and Régis Lebrun},
keywords = {Digital twin, Assembly, Geometry assurance, 3D acquisition},
abstract = {The recent transformation in the aeronautical industry gives new prospects in the field of product geometry assurance. These include, in particular the creation of sophisticated virtual models, or digital twins, which can reflect the as-built geometry of physical products and optimize the assembly operations consequently. One of the current obstacles to the implementation of such digital twins is linked to the difficult transition from a conceptual model to a usable virtual representation. In this article, we present the hybrid representation of a product which is capable of integrating the different states of the components at each step of the assembly process. We propose a method to update the virtual representation of already assembled components, in order to include the position and orientation deviations of their surfaces. The B-Rep model of each component is updated from data acquired during the assembly of the product. The various steps of this update, and its associated tools are discussed in the article. Based on the knowledge of the as-built component geometry, the geometry of the yet-to-be-assembled components is adapted so that the final product complies with the functional requirements. To this end, we also discuss a formalism to model the product's functional information and to translate it at a geometrical level thanks to an assembly skeleton.}
}
@article{MAY202127,
title = {Foresighted digital twin for situational agent selection in production control},
journal = {Procedia CIRP},
volume = {99},
pages = {27-32},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121002614},
author = {Marvin Carl May and Leonard Overbeck and Marco Wurster and Andreas Kuhnle and Gisela Lanza},
keywords = {Fluid Automation, Production Control, Digital Twin, Machine Learning, Human Behavior},
abstract = {As intelligent Data Acquisition and Analysis in Manufacturing nears its apex, a new era of Digital Twins is dawning. Foresighted Digital Twins enable short- to medium-term system behavior predictions to infer optimal production operation strategies. Creating up-to-the-minute Digital Twins requires both the availability of real-time data and its incorporation and serve as a stepping-stone into developing unprecedented forms of production control. Consequently, we regard a new concept of Digital Twins that includes foresight, thereby enabling situational selection of production control agents. One critical element for adequate system predictions is human behavior as it is neither rule-based nor deterministic, which we therefore model applying Reinforcement Learning. Owing to these ever-changing circumstances, rigid operation strategies crucially restrain reactions, as opposed to circumstantial control strategies that hence can outperform traditional approaches. Building on enhanced foresights we show the superiority of this approach and present strategies for improved situational agent selection.}
}
@article{CUNHA2021874,
title = {Designing the Digital Twins of Reconfigurable Manufacturing Systems: application on a smart factory},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {874-879},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.103},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008521},
author = {Catherine da Cunha and Olivier Cardin and Guillaume Gallot and Julien Viaud},
keywords = {Reconfigurable Manufacturing Systems, Modularity, Digital Twin, Manufacturing systems},
abstract = {The emergence of Digital Twins in manufacturing enables in-depth integration of the physical and informational worlds. Meanwhile, the concept of Reconfigurable Manufacturing Systems exhibits new opportunities and new constraints in the way the manufacturing systems are designed and managed. Among others, the notion of modularity is a prerequisite for both physical components and their informational counterparts. To provide the benefits of Digital Twin to Reconfigurable Manufacturing Systems, this paper introduces a modular design of the Digital Twin counterpart of the components. A prototype system is developed to demonstrate the modular design’s benefits, with an application on a reconfigurable learning factory.}
}
@article{LIN2021101209,
title = {Evolutionary digital twin: A new approach for intelligent industrial product development},
journal = {Advanced Engineering Informatics},
volume = {47},
pages = {101209},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101209},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620301786},
author = {Ting Yu Lin and Zhengxuan Jia and Chen Yang and Yingying Xiao and Shulin Lan and Guoqiang Shi and Bi Zeng and Heyu Li},
keywords = {Evolutionary digital twin, Intelligent industrial product, Collaborative evolution, Approximate world, Multiple cyber spaces, Simple evolution paradigm, Model evolution paradigm},
abstract = {To fulfill increasingly difficult and demanding tasks in the ever-changing complex world, intelligent industrial products are to be developed with higher flexibility and adaptability. Digital twin (DT) brings about a possible means, due to its ability to provide candidate behavior adjustments based on received “feedbacks” from its physical part. However, such candidate adjustments are deterministic, and thus lack of flexibility and adaptability. To address such problem, in this paper an extended concept – evolutionary digital twin (EDT) and an EDT-based new mode for intelligent industrial product development has been proposed. With our proposed EDT, a more precise approximated model of the physical world could be established through supervised learning, based on which the collaborative exploration for optimal policies via parallel simulation in multiple cyberspaces could be performed through reinforcement learning. Hence, more flexibility and adaptability could be brought to industrial products through machine learning (such as supervised learning and reinforcement learning) based self-evolution. As a primary verification of the effectiveness of our proposed approach, a case study has been carried out. The experimental results have well confirmed the effectiveness of our EDT based development mode.}
}
@article{CHRYSAFIADI2020113614,
title = {Combination of fuzzy and cognitive theories for adaptive e-assessment},
journal = {Expert Systems with Applications},
volume = {161},
pages = {113614},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113614},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420304383},
author = {Konstantina Chrysafiadi and Christos Troussas and Maria Virvou},
keywords = {Adaptivity, E-assessment, E-learning, Cognitive theories, Fuzzy rules},
abstract = {A crucial factor for successful educational results in computer-based educational systems and e-learning systems is the learner’s assessment. Assessment is more effective when it is tailored to each individual student’s learning needs and abilities. Therefore, a significant research challenge is to create tests that include exercises/questions/activities etc., which conform to each learner’s knowledge level and learning needs and abilities. This goal constitutes the need for creating adaptive tests. However, the area of adaptive e-assessment has not yet been explored sufficiently and thus there is scope for a lot of improvement. To this end, in this paper we present a novel solution for adaptive e-assessment. The novelty and significance lie in the blending of fuzzy logic and cognitive theories for further enhancing the personalization and adaptivity in e-assessment. Particularly, fuzzy sets are used to describe the knowledge level of students in a more realistic way. Furthermore, the cognitive theory of Revised Bloom Taxonomy is used to express the learning objectives that are required to be assessed through the created test. In addition, a fuzzy rule-based reasoner, which decides about the number and the difficulty level of the test items that have to be included into the created personalized test for each level of the Revised Bloom Taxonomy, is used. The fuzzy rules are applied to the fuzzy sets that describe the learners’ knowledge level. For the formation of the fuzzy sets and rules, the opinion of several tutors, holding experience in the educational process and instruction, was taken into consideration. The created adaptive test comprises distinct test items based on the individual learning needs of each student. The presented method has been used in two tutoring systems and has been fully evaluated. The evaluation results show great accuracy in the selection of test items for each individual student.}
}
@article{BIDEAULT20211540,
title = {Verification and Validation of Computational Models Used in Biopharmaceutical Manufacturing: Potential Application of the ASME Verification and Validation 40 Standard and FDA Proposed AI/ML Model Life Cycle Management Framework},
journal = {Journal of Pharmaceutical Sciences},
volume = {110},
number = {4},
pages = {1540-1544},
year = {2021},
issn = {0022-3549},
doi = {https://doi.org/10.1016/j.xphs.2021.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0022354921000319},
author = {Gautier Bideault and Anthony Scaccia and Thomas Zahel and Robert W. Landertinger and Chathuri Daluwatte},
keywords = {Verification and validation, Computational models, Machine learning, Digital twins, Biopharmaceutical manufacturing, GMLP},
abstract = {A wide variety of computational models covering statistical, mechanistic, and machine learning (locked and adaptive) methods are explored for use in biopharmaceutical manufacturing. Limited discussion exists on how to establish the credibility of a computational model for application in biopharmaceutical manufacturing. In this work, we tried to use the American Society of Mechanical Engineers (ASME) Verification and Validation 40 (V&V 40) standard and FDA proposed AI/ML model life cycle management framework for Software as a Medical Device (SaMD) in biopharmaceutical manufacturing use cases, by applying to a set of curated hypothetical examples. We discussed the need for standardized frameworks to facilitate consistent decision making to enable efficient adoption of computational models in biopharmaceutical manufacturing and alignment of existing good practices with existing frameworks. In the study of our examples, we anticipate existing frameworks like V&V 40 can be adopted.}
}
@article{SHI2021119572,
title = {Probabilistic real-time deep-water natural gas hydrate dispersion modeling by using a novel hybrid deep learning approach},
journal = {Energy},
volume = {219},
pages = {119572},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.119572},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220326797},
author = {Jihao Shi and Junjie Li and Asif Sohail Usmani and Yuan Zhu and Guoming Chen and Dongdong Yang},
keywords = {Marine natural hydrate gas, Probabilistic dispersion modeling, Convolution variational autoencoder, Variational Bayesian neural network, Uncertainty estimation of spatial features, Digital twin of emergency management},
abstract = {Computational Fluid Dynamic (CFD) has been widely used for the gas release and dispersion modeling, which however could not support real-time emergency response planning due to its high computation overhead. Surrogate models offer a potential alternative to rigorous computational approaches, however, as the point-estimation alternatives, the existing neural network-based surrogate models are not able to quantify the uncertainty of the released gas spatial concentration. This study aims to fill a gap by proposing an advanced hybrid probabilistic Convolutional-Variational Autoencoder-Variational Bayesian neural network (Conv-VAE-VBnn). Experimental study based on a benchmark simulation dataset was conducted. The results demonstrated the additional uncertainty information estimated by the proposed model contributes to reducing the harmful effect of too ‘confidence’ of the point-estimation models. In addition, the proposed model exhibits competitive accuracy with R2 = 0.94 compared and real-time capacity with inference time less than 1s. Latent size Nz = 2, noise σz=0.1 and Monte Carlo sampling number m = 500 to ensure the model’s real-time capacity, were also given. Overall, our proposed model could provide a reliable alternative for constructing a digital twin for emergency management during the exploration and exploitation of marine natural gas hydrate (NHG) in the near future.}
}
@article{WANG2020373,
title = {Intelligent welding system technologies: State-of-the-art review and perspectives},
journal = {Journal of Manufacturing Systems},
volume = {56},
pages = {373-391},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301102},
author = {Baicun Wang and S. Jack Hu and Lei Sun and Theodor Freiheit},
keywords = {Intelligent welding, Artificial intelligence (AI), Intelligent manufacturing, Robotic welding, Monitoring and control, Machine learning},
abstract = {Welding systems are being transformed by the advent of modern information technologies such as the internet of things, big data, artificial intelligence, cloud computing, and intelligent manufacturing. Intelligent welding systems (IWS), making use of these technologies, are drawing attention from academic and industrial communities. Intelligent welding is the use of computers to mimic, strengthen, and/or replace human operators in sensing, learning, decision-making, monitoring and control, etc. This is accomplished by integrating the advantages of humans and physical systems into intelligent cyber systems. While intelligent welding has found pilot applications in industry, a systematic analysis of its components, applications, and future directions will help provide a unified definition of intelligent welding systems. This paper examines fundamental components and techniques necessary to make welding systems intelligent, including sensing and signal processing, feature extraction and selection, modeling, decision-making, and learning. Emerging technologies and their application potential to IWS will also be surveyed, including Industry 4.0, cyber-physical system (CPS), digital twins, etc. Typical applications in IWS will be surveyed, including weld design, task sequencing, robot path planning, robot programming, process monitoring and diagnosis, prediction, process control, quality inspection and assessment, human-robot collaboration, and virtual welding. Finally, conclusions and suggestions for future development will be proposed. This review is intended to provide a reference of the state-of-the-art for those seeking to introduce intelligent welding capabilities as they modernize their traditional welding stations, systems, and factories.}
}
@article{KUMAR2021104111,
title = {Collaborative knowledge distillation for incomplete multi-view action prediction},
journal = {Image and Vision Computing},
volume = {107},
pages = {104111},
year = {2021},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2021.104111},
url = {https://www.sciencedirect.com/science/article/pii/S0262885621000160},
author = {Deepak Kumar and Chetan Kumar and Ming Shao},
keywords = {Multi-view, Action prediction, Knowledge distillation, Graph attention},
abstract = {Predicting future actions is a key in visual understanding, surveillance, and human behavior analysis. Current methods for video-based prediction are primarily using single-view data, while in the real world multiple cameras and produced videos are readily available, which may potentially benefit the action prediction tasks. However, it may bring up a new challenge: subjects in the videos are more likely to be occluded by objects when captured from different angles, or suffer from signal jittering in transmission. To that end, in this paper we propose a novel student network called Collaborative Knowledge Distillation (CKD) to predict human actions with missing information under a multi-view setting, i.e., incomplete multi-view action prediction. First, we create a graph attention based teacher model capable of fusing multi-view video features for prediction task. Second, we construct a corruption pattern bank (CPB) to simulate various missing segments in multi-view video, and each student model will manage one pattern through privileged information and knowledge distillation. Third, to account for arbitrary missing video segments in real-world, the ensemble of student models will be developed to make a joint prediction. The proposed framework has been extensively evaluated on popular multi-view visual action datasets, including PKU-MMD and NTU-RGB to validate the effectiveness of our approach and to the best of our knowledge action prediction has not yet been explored in the multi-view setting.}
}
@article{RAOPABOLU20211367,
title = {A dynamic job rotation scheduling conceptual framework by a human representing digital twin},
journal = {Procedia CIRP},
volume = {104},
pages = {1367-1372},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.230},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011288},
author = {Venkata Krishna {Rao Pabolu} and Divya Shrivastava},
keywords = {Worker Job rotation, Work fatigue, ALWABP, Fatigue classifier, IIoT},
abstract = {This work is an extension part of the Assembly Line Worker Assignment Balancing Problem(ALWABP), aims to provide a dynamic solution for an assembly line fatigue worker job rotation by using machine learning based digital twin. This framework explains, fatigue worker identification and work rotation possibilities for a reconfigurable assembly line. The fatigue causing parameters are sensed from the workers and classified with a fatigue classifier then send the fatigue worker details to a worker job rotation search algorithm. The job rotation search algorithm provides a suggestion to the production supervisor for a best possible worker job rotation/reallocation solution dynamically.}
}
@article{MANETTAS2021237,
title = {Synthetic datasets for Deep Learning in computer-vision assisted tasks in manufacturing},
journal = {Procedia CIRP},
volume = {103},
pages = {237-242},
year = {2021},
note = {9th CIRP Global Web Conference – Sustainable, resilient, and agile manufacturing and service operations : Lessons from COVID-19},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.10.038},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121008799},
author = {Christos Manettas and Nikolaos Nikolakis and Kosmas Alexopoulos},
keywords = {digital-twin, artificial intelligence, robotics, artificial datasets, machine learning},
abstract = {Artificial Intelligence applications based on Machine Learning methods are widely accepted as promising technologies in manufacturing. Deep Learning (DL) techniques, such as Convolutional Neural Networks (CNN), are successfully used in many computer-vision tasks in manufacturing. These state-of-the-art techniques are requiring large volumes of annotated datasets for training. However, such an approach is expensive, prone to errors and labor as well as time intensive, especially in highly complex and dynamic production environments. Synthetic datasets can be utilized for accelerating the training phase of DL by creating suitable training datasets. This work presents a framework for generating datasets through a chain of simulation tools. The framework is used for generating synthetic images of manufactured parts. States of the parts such as the rotation in different rotation axis need to be recognized by a computer-vision system that assists a manufacturing operation. A number of prior trained CNNs are retrained with the synthetically generated images. The CNNs are tested upon actual images of manufactured parts. The performance of different CNN models is presented, compared and discussed. The results indicate that CNNs trained on synthetically generated datasets may have acceptable performance when used in for assisting tasks in manufacturing.}
}
@article{HOOSHYAR2020103878,
title = {Open learner models in supporting self-regulated learning in higher education: A systematic literature review},
journal = {Computers & Education},
volume = {154},
pages = {103878},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103878},
url = {https://www.sciencedirect.com/science/article/pii/S0360131520300774},
author = {Danial Hooshyar and Margus Pedaste and Katrin Saks and Äli Leijen and Emanuele Bardone and Minhong Wang},
keywords = {Open learner model, Systematic review, Self-regulated learning, Higher education},
abstract = {The open learner model (OLM) represents the knowledge or skill levels of learners in various ways, encouraging learners to actively participate in thinking about and crafting their own learning. Despite the important roles that OLMs play in higher education to support the learning process and self-regulated learning (SRL) in particular, there are few studies systematically reviewing OLM technology in higher education, and investigating their potential to foster self-regulated learning. Therefore, we carried out a systematic review of a 30-year sample of OLM studies in higher education and identified 64 articles that study the use of OLMs in supporting SRL. Our findings show that OLMs have been mainly used to support learners' cognition and a bit less metacognition and motivation; however, emotional support has been rarely provided. The most supported ones are Appraisal and Performance phases; Preparation of learning is enhanced by OLMs not so often. Although learners can edit or negotiate with their learning model in advanced ways, a simple inspectable OLM is more preferred. Reliance on unobservable nodes is less favored in modeling techniques in OLMs because such methods are highly dependent on expert authoring, thereby time-intensive and costly. Comparison and color-coding are two most-used features in OLMs, where the comparison feature is often used for enhancing learners’ engagement and motivation.}
}
@article{REICHARDT2021968,
title = {Procedure model for the development and launch of intelligent assistance systems},
journal = {Procedia Computer Science},
volume = {180},
pages = {968-977},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.348},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921004026},
author = {Paul Reichardt and Sebastian Lang and Tobias Reggelin},
keywords = {Production planning, control, Machine learning, Planning assistance system, Practical implementation, Generic procedure model, Digital twin},
abstract = {The paper analyses the current state of knowledge on approaches for the practical implementation of machine learning based assistance systems for production planning and control. A concept of a procedure model for application-oriented projects in the field of industrial series production is proposed. It focusses on order sequencing and machine allocation in a real time production environment. As part of an application-oriented research project, a use case is referenced. In this paper, a first conceptual approach is presented, using the example of an industrial production of printed circuit boards. In the following steps, practical suitability is checked on the basis of the practical reference, conclusions are drawn and the methodology will be developed further. The aim is a generally valid procedure model for industrial series production.}
}
@article{NGUYENMEIDINE2021104096,
title = {Knowledge distillation methods for efficient unsupervised adaptation across multiple domains},
journal = {Image and Vision Computing},
volume = {108},
pages = {104096},
year = {2021},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2021.104096},
url = {https://www.sciencedirect.com/science/article/pii/S0262885621000019},
author = {Le Thanh Nguyen-Meidine and Atif Belal and Madhu Kiran and Jose Dolz and Louis-Antoine Blais-Morin and Eric Granger},
keywords = {Deep learning, Convolutional NNs, Knowledge distillation, Unsupervised domain adaptation, CNN acceleration and compression},
abstract = {Beyond the complexity of CNNs that require training on large annotated datasets, the domain shift between design and operational data has limited the adoption of CNNs in many real-world applications. For instance, in person re-identification, videos are captured over a distributed set of cameras with non-overlapping viewpoints. The shift between the source (e.g. lab setting) and target (e.g. cameras) domains may lead to a significant decline in recognition accuracy. Additionally, state-of-the-art CNNs may not be suitable for such real-time applications given their computational requirements. Although several techniques have recently been proposed to address domain shift problems through unsupervised domain adaptation (UDA), or to accelerate/compress CNNs through knowledge distillation (KD), we seek to simultaneously adapt and compress CNNs to generalize well across multiple target domains. In this paper, we propose a progressive KD approach for unsupervised single-target DA (STDA) and multi-target DA (MTDA) of CNNs. Our method for KD-STDA adapts a CNN to a single target domain by distilling from a larger teacher CNN, trained on both target and source domain data in order to maintain its consistency with a common representation. This method is extended to address MTDA problems, where multiple teachers are used to distill multiple target domain knowledge to a common student CNN. A different target domain is assigned to each teacher model for UDA, and they alternatively distill their knowledge to the student model to preserve specificity of each target, instead of directly combining the knowledge from each teacher using fusion methods. Our proposed approach is compared against state-of-the-art methods for compression and STDA of CNNs on the Office31 and ImageClef-DA image classification datasets. It is also compared against state-of-the-art methods for MTDA on Digits, Office31, and OfficeHome. In both settings – KD-STDA and KD-MTDA – results indicate that our approach can achieve the highest level of accuracy across target domains, while requiring a comparable or lower CNN complexity.}
}
@article{APOSTOLIDIS202155,
title = {An AI-based Digital Twin Case Study in the MRO Sector},
journal = {Transportation Research Procedia},
volume = {56},
pages = {55-62},
year = {2021},
note = {1st International Conference on Aviation Future: Challenge and Solution (AFCS 2020)},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2021.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352146521006347},
author = {Asteris Apostolidis and Konstantinos P. Stamoulis},
keywords = {Digital Twin, Machine Learning, Artificial Intelligence, Aviation MRO, Predictive Maintenance},
abstract = {In this work, the concept of an Artificial Intelligence-based (AI) Digital Twin (DT) of an aircraft system is introduced, with the goal to improve the corresponding MRO Operations. More specifically, the current study aims to obtaining knowledge on the optimal placement of sensors in an ideal Power Electronics Cooling System (PECS) of a modern airliner, aiming to improve input data as a basis for an AI-based DT. The three main fluid parameters to be measured directly or indirectly at various physical locations at the PECS are mass flow rate, temperature and static pressure. The physics-based model can then be combined with a Machine Learning (ML) model, such as a Random Forest (RF), with a multitude of decision trees. Following, the AI system determines whether the PECS operations is considered normal, aiming to optimize the performance of the system and to maximize the Useful Remaining Life (URL). The suggested AI-DT approach is based both on data-driven and physics-based models, an approach which results in increased reliability and availability, reducing possible Aircraft on Ground (AOG) events. Subsequently, the enhanced prediction capability results in the optimization of the maintenance processes and in reduced operational costs.}
}
@article{YU2021293,
title = {A Digital Twin approach based on nonparametric Bayesian network for complex system health monitoring},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {293-304},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301151},
author = {Jinsong Yu and Yue Song and Diyin Tang and Jing Dai},
keywords = {Digital Twin, Health monitoring, Nonparametric Bayesian networks, Dirichlet process mixture model},
abstract = {This paper proposes a Digital Twin approach for health monitoring. In this approach, a Digital Twin model based on nonparametric Bayesian network is constructed to denote the dynamic degradation process of health state and the propagation of epistemic uncertainty. Then, a real-time model updating strategy based on improved Gaussian particle filter (GPF) and Dirichlet process mixture model (DPMM) is presented to enhance the model adaptability. On one hand, for those parameters in the nonparametric Bayesian network with prior models, the improved GPF is used to update them in real time. On the other hand, for parameters lacking a prior model, DPMM is proposed to learn hidden variables, which adaptively update the model structure and greatly reduce uncertainty. Experiments on the electro-optical system are conducted to validate the feasibility of the Digital Twin approach and verify the effectiveness of the nonparametric Bayesian network. The results of comparative experiments prove that the Digital Twin approach based on nonparametric Bayesian Network has a good model self-learning ability, which improves the accuracy of health monitoring.}
}
@incollection{RAJ2021355,
title = {Chapter Eighteen - Industrial use cases at the cusp of the IoT and blockchain paradigms},
editor = {Shubhani Aggarwal and Neeraj Kumar and Pethuru Raj},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {121},
pages = {355-385},
year = {2021},
booktitle = {The Blockchain Technology for Secure and Smart Applications across Industry Verticals},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2020.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0065245820300735},
author = {Pethuru Raj},
keywords = {Blockchain, Smart contracts, Decentralized applications, Artificial intelligence, Digital twins, The Internet of Things, Consensus algorithms, Hashing, Machine learning, Cloud computing},
abstract = {There are fair amount of reasons and requirements for both the IoT and blockchain concepts to cooperate closely to solve bigger problems at hand. This deadly combination is to result in scores of fresh and fabulous opportunities and possibilities for the total human society. The Internet of Things (IoT) paradigm has made it possible to have digitized and connected things in plenty in our everyday environments. That is, all kinds of physical, mechanical, electrical, and electronics systems are systematically being digitized and connected through proven and potential edge and connectivity technologies. The leading market analysts and researchers have come out with forecasts that there will be billions of connected devices and trillions of digitized entities in the years ahead. The noteworthy point here is that all these empowered entities, on purposefully collaborating and correlating with one another, can generate massive amounts of multistructured data. The challenge is how to secure IoT devices and data. The arrival of the blockchain technology is being celebrated as the best thing toward convincingly meeting up the IoT security requirements. This chapter is to explore and expound how the cool linkage between IoT and blockchain is to substantially enhance the security and privacy needs of IoT devices and data.}
}
@article{OVERBECK2021170,
title = {Reinforcement Learning Based Production Control of Semi-automated Manufacturing Systems},
journal = {Procedia CIRP},
volume = {103},
pages = {170-175},
year = {2021},
note = {9th CIRP Global Web Conference – Sustainable, resilient, and agile manufacturing and service operations : Lessons from COVID-19},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121008684},
author = {Leonard Overbeck and Adrien Hugues and Marvin Carl May and Andreas Kuhnle and Gisela Lanza},
keywords = {Machine Learning, Reinforcement Learning, Digital Twin, Production Control, Task Allocation, Productivity},
abstract = {In an environment which is marked by an increasing speed of changes, industrial companies have to be able to quickly adapt to new market demands and innovative technologies. This leads to a need for continuous adaption of existing production systems and the optimization of their production control. To tackle this problem digitalization of production systems has become essential for new and existing systems. Digital twins based on simulations of real production systems allow the simplification of analysis processes and, thus, a better understanding of the systems, which leads to broad optimization possibilities. In parallel, machine learning methods can be integrated to process the numerical data and discover new production control strategies. In this work, these two methods are combined to derive a production control logic in a semi-automated production system based on the chaku-chaku principle. A reinforcement learning method is integrated into the digital twin to autonomously learn a superior production control logic for the distribution of tasks between the different workers on a production line. By analyzing the influence of different reward shaping and hyper-parameter optimization on the quality and stability of the results obtained, the use of a well-configured policy-based algorithm enables an efficient management of the workers and the deduction of an optimal production control logic for the production system. The algorithm manages to define a control logic that leads to an increase in productivity while having a stable task assignment so that a transfer to daily business is possible. The approach is validated in the digital twin of a real assembly line of an automotive supplier. The results obtained suggest a new approach to optimizing production control in production lines. Production control shall be centered directly on the workers’ routines and controlled by artificial intelligence infused with a global overview of the entire production system.}
}
@article{LIU2022857,
title = {Digital Twin-enabled Collaborative Data Management for Metal Additive Manufacturing Systems},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {857-874},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300741},
author = {Chao Liu and Léopold {Le Roux} and Carolin Körner and Olivier Tabaste and Franck Lacan and Samuel Bigot},
keywords = {Metal Additive Manufacturing, Digital Twin, data management, data model, machine learning, product lifecycle management},
abstract = {Metal Additive Manufacturing (AM) has been attracting a continuously increasing attention due to its great advantages compared to traditional subtractive manufacturing in terms of higher design flexibility, shorter development time, lower tooling cost, and fewer production wastes. However, the lack of process robustness, stability and repeatability caused by the unsolved complex relationships between material properties, product design, process parameters, process signatures, post AM processes and product quality has significantly impeded its broad acceptance in the industry. To facilitate efficient implementation of advanced data analytics in metal AM, which would support the development of intelligent process monitoring, control and optimisation, this paper proposes a novel Digital Twin (DT)-enabled collaborative data management framework for metal AM systems, where a Cloud DT communicates with distributed Edge DTs in different product lifecycle stages. A metal AM product data model that contains a comprehensive list of specific product lifecycle data is developed to support the collaborative data management. The feasibility and advantages of the proposed framework are validated through the practical implementation in a distributed metal AM system developed in the project MANUELA. A representative application scenario of cloud-based and deep learning-enabled metal AM layer defect analysis is also presented. The proposed DT-enabled collaborative data management has shown great potential in enhancing fundamental understanding of metal AM processes, developing simulation and prediction models, reducing development times and costs, and improving product quality and production efficiency.}
}
@article{WU2021139,
title = {A Framework of Dynamic Data Driven Digital Twin for Complex Engineering Products: the Example of Aircraft Engine Health Management},
journal = {Procedia Manufacturing},
volume = {55},
pages = {139-146},
year = {2021},
note = {FAIM 2021},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921002171},
author = {Zhenhua Wu and Jianzhi Li},
keywords = {Digital Twin, Dynamic Data Driven, Complex Engineering Products, Aircraft Engine, Health Management},
abstract = {Digital twin is a vital enabling technology for smart manufacturing in the era of Industry 4.0. Digital twin effectively replicates its physical asset enabling easy visualization, smart decision-making and cognitive capability in the system. In this paper, a framework of dynamic data driven digital twin for complex engineering products was proposed. To illustrate the proposed framework, an example of health management on aircraft engines was studied. This framework models the digital twin by extracting information from the various sensors and Industry Internet of Things (IIoT) monitoring the remaining useful life (RUL) of an engine in both cyber and physical domains. Then, with sensor measurements selected from linear degradation models, a long short-term memory (LSTM) neural network is proposed to dynamically update the digital twin, which can estimate the most up-to-date RUL of the physical aircraft engine. Through comparison with other machine learning algorithms, including similarity based linear regression and feed forward neural network, on RUL modelling, this LSTM based dynamical data driven digital twin provides a promising tool to accurately replicate the health status of aircraft engines. This digital twin based RUL technique can also be extended for health management and remote operation of manufacturing systems.}
}
@article{WANG2020429,
title = {Deep learning-empowered digital twin for visualized weld joint growth monitoring and penetration control},
journal = {Journal of Manufacturing Systems},
volume = {57},
pages = {429-439},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301710},
author = {Qiyue Wang and Wenhua Jiao and YuMing Zhang},
keywords = {Convolutional neural networks, (CNNs), Deep learning, Digital twin, Smart manufacturing, Welding quality},
abstract = {This paper presents an innovative digital twin to monitor and control complex manufacturing processes by integrating deep learning which offers strong feature extraction and analysis abilities. Taking welding manufacturing as a case study, a deep learning-empowered digital twin is developed as the visualized digital replica of the physical welding for joint growth monitoring and penetration control. In such a system, the information available directly from sensors including weld pool images, arc images, welding current and arc voltage is collected in pulsed gas tungsten arc welding (GTAW-P). Then, the undirect information charactering the weld joint geometry and determining the welding quality, including the weld joint top-side bead width (TSBW) and back-side bead width (BSBW), is computed/estimated by traditional image processing methods and deep convolutional neural networks (CNNs) respectively. Compared with single image source, weld pool image or arc image, the CNN model performs better when taking the 2-channel composite image combined by both as the input and the state-of-the-art accuracy in BSBW prediction with mean square error (MSE) as 0.047 mm2 is obtained. Then, a decision-making strategy is developed to control the welding penetration to meet the quality requirement and applied successfully in various welding conditions. By modeling the weld joint cross section as an ellipse, the developed digital twin is visualized to offer a graphical user interface (GUI) for users perceiving the weld joint growth intuitively and effectively.}
}
@article{OEDA2021804,
title = {Verification of Usefulness of Student Modeling with Real Educational Data using Convex Factorization Machines},
journal = {Procedia Computer Science},
volume = {192},
pages = {804-811},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.083},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015714},
author = {Shinichi Oeda and Daiki Shimizu},
keywords = {Educational Data Mining, Student Modeling, Convex Factorization Machines, Factorization Machines, Convex Optimization, Recommendation, Intelligent Tutoring Systems},
abstract = {Extracting useful information generated from educational settings involves the application of data mining, machine learning, and statistics to the large amount of electronic data collected by educational systems. To generate better higher learning outcomes using an intelligent tutoring system, such as an e-learning system, it is necessary to more accurately understand the state of student knowledge. The purpose of student modeling is to estimate the students’ skills from log data, such as examination results, and to predict whether or not a student will be able to solve a problem. In this study, we propose a student performance prediction method using convex factorization machines. Factorization machines offer a combination of the advantages of support vector machines and factorization models such as matrix factorization. The results of conventional methods, which predict student performance using factorization machines, exhibited better results than they have before. However, because factorization machines are not convex optimizations, they acquire local minima, which is a disadvantage. Therefore, we used convex factorization machines in order to improve the performance of student modeling predictions.}
}
@article{CALKA2021105786,
title = {Machine-Learning based model order reduction of a biomechanical model of the human tongue},
journal = {Computer Methods and Programs in Biomedicine},
volume = {198},
pages = {105786},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105786},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720316199},
author = {Maxime Calka and Pascal Perrier and Jacques Ohayon and Christelle Grivot-Boichon and Michel Rochette and Yohan Payan},
keywords = {Real-time simulation, Model Order Reduction, Digital Twins, Human tongue},
abstract = {Background and Objectives: This paper presents the results of a Machine-Learning based Model Order Reduction (MOR) method applied to a complex 3D Finite Element (FE) biomechanical model of the human tongue, in order to create a Digital Twin Model (DTM) that enables real-time simulations. The DTM is designed for future inclusion in a computer assisted protocol for tongue surgery planning. Methods: The proposed method uses an “a posteriori” MOR that allows, from a limited number of simulations with the FE model, to predict in real time mechanical responses of the human tongue to muscle activations. Results: The MOR method is evaluated for simulations associated with separate single tongue muscle activations. It is shown to be able to account with a sub-millimetric spatial accuracy for the non-linear dynamical behavior of the tongue model observed in these simulations. Conclusion: Further evaluations of the MOR method will include tongue movements induced by multiple muscle activations. At this stage our MOR method offers promising perspectives for the use of the tongue model in a clinical context to predict the impact of tongue surgery on tongue mobility. As a long term application, this DTM of the tongue could be used to predict the functional consequences of the surgery in terms of speech production and swallowing.}
}
@article{ASADI2021767,
title = {Machine-Learning Digital Twin of Overlay Metal Deposition for Distortion Control of Panel Structures},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {767-772},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.089},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008351},
author = {Mahyar Asadi and Michael Fernandez and Majid Tanbakuei Kashani and Mathew Smith},
keywords = {cyber-manufacturing, digital twin, machine learning, metal deposition, overlay pattern, distortion control, adaptive learning},
abstract = {Cyber-manufacturing relies on smart digital-twins of manufacturing processes that can quickly act for making a wise decision. However, the cognitive computing part of the digital-twin becomes time-intensive beyond the requirement of a smart system when it uses simulation tools that solve governing constitutive equations in the form of partial differential equations (PDE). On the other hand, many artificial intelligence (AI) and machine learning (ML) solutions rely on a large data set that does not exist in many manufacturing systems. We build a hybrid digital-twin that takes advantage of an ML-based digital-twin for quick response while gaining fidelity through adaptive learning with a PDE-based digital-twin. We use our hybrid digital-twin for active exploration of various overlay metal deposition patterns in real-time. This tool enables engineers to explore and compare many patterns they need to assess metal deposition scenarios with no delay for computational time.}
}
@article{SHENGLI2021100014,
title = {Is Human Digital Twin possible?},
journal = {Computer Methods and Programs in Biomedicine Update},
volume = {1},
pages = {100014},
year = {2021},
issn = {2666-9900},
doi = {https://doi.org/10.1016/j.cmpbup.2021.100014},
url = {https://www.sciencedirect.com/science/article/pii/S2666990021000136},
author = {Wei Shengli},
keywords = {Human digital twin, Augmented digital twin, Smart healthcare},
abstract = {While Digital Twin finds its applications in many fields, mainly in advanced manufacturing, PLM (Product Lifecycle Management), and smart healthcare, attempt of using Digital Twin in full lifecycle management of human being is discussed. The concept of Augmented Digital Twin is put forward as the basis of the concept of Human Digital Twin (HDT), which is the core idea of the paper. With the experiences of Digital Twin application in smart manufacturing, PLM and smart healthcare, and the development of other related technologies such as Data Mining, Data Fusion Analysis, Artificial Intelligence, especially Deep Learning and Human Computer Science, a conclusion can be drawn naturally, that HDT is an enabling way of full lifecycle health management and it is possible to construct Human Digital Twin, particularly from the technology view. Comparison between Digital Twin and Human Digital Twin demonstrates the possibility. The concept, conceptual model and characteristics of HDT are presented for preparation of its construction. How to construct Human Digital Twin is discussed by proposing Human Digital Twin System Architecture and Implementation Approach. Also, it is pointed out that there will be a long way to go because of not only its extreme complexity, but also so many aspects being involved, especially the security and social ethics problem.}
}
@incollection{RAJ20211,
title = {Chapter One - Demystifying the blockchain technology},
editor = {Shubhani Aggarwal and Neeraj Kumar and Pethuru Raj},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {121},
pages = {1-42},
year = {2021},
booktitle = {The Blockchain Technology for Secure and Smart Applications across Industry Verticals},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2020.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245820300565},
author = {Pethuru Raj},
keywords = {Blockchain, Smart contracts, Decentralized applications, Artificial intelligence, Digital twins, The Internet of things, Consensus algorithms, Hashing, Machine learning, Cloud computing},
abstract = {The blockchain paradigm is being widely touted by many as the innovative and disruptive one capable of bringing in a few exemplary and elegant transformations in the IT space. As business operations and offerings are substantially enabled through the various crucial accomplishments and advancements in the IT field, business executives across the globe are equally keen to experiment with and embrace this new and futuristic technology to reap a slew of business benefits. Interestingly, blockchain has the inherent potential and promise to bring forth a bevy of strategically sound implications for various industry verticals. Cryptocurrency is one of the finest and foremost applications of the blockchain technology. The supply chain domain is exploring this new phenomenon for realizing some crucial advantages. The IoT discipline is another one capable of attaining a number of distinct benefits out of all the trendsetting improvisations being realized in the blockchain space. This chapter is specially crafted to tell what, why, how, and where the indispensable blockchain paradigm is being used toward real digital transformations.}
}
@article{KIM2020107423,
title = {Damage detection of bottom-set gillnet using Artificial Neural Network},
journal = {Ocean Engineering},
volume = {208},
pages = {107423},
year = {2020},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.107423},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820304509},
author = {HanSung Kim and Chungkuk Jin and MooHyun Kim and Kiseon Kim},
keywords = {Digital twins, Artificial neural network (ANN), Machine learning, Gillnet, Damage detection, Real-time monitoring},
abstract = {This paper presents a method for detecting damage of a bottom-set gillnet based on sensor fusion and the Artificial Neural Network (ANN) model. In this regard, time-domain numerical simulations for a 300-m-long bottom-set gillnet with an equivalent-drag-net model are extensively performed. Various wave conditions as well as numerous damaged scenarios are considered in the numerical simulations, and extensive data are collected for the training and testing of the ANN-based machine-learning scheme. In training, representative sea states, net-assembly accelerations, and location-buoy displacements are selected as the input variables. The back-propagation learning algorithm is employed for training to maximize the damage-detection performance. The output of the ANN model is the identification of the type and location of the damaged net. The damage-detection capability is significantly enhanced by employing the moving standard deviation and median filter. The well-trained ANN models are shown to accurately (96% correct) detect the damage of the net even for the sea states not included in training. This study demonstrates that the real-time automatic monitoring of the underwater net systems can be designed by using the digital-twins technology with the applied ANN model and machine-learning scheme for the detection of damages and malfunctions.}
}
@article{RALPH2021335,
title = {MUL 4.0: Systematic Digitalization of a Value Chain from Raw Material to Recycling},
journal = {Procedia Manufacturing},
volume = {55},
pages = {335-342},
year = {2021},
note = {FAIM 2021},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.10.047},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921002444},
author = {Benjamin James Ralph and Manuel Woschank and Philipp Miklautsch and Alexander Kaiblinger and Corina Pacher and Marcel Sorger and Helmut Zsifkovits and Martin Stockinger},
keywords = {Digitalization, Industry 4.0, Industrial Logistics, Engineering Education, Digital Twin, Finite Element Analysis, Metal Forming},
abstract = {The digital revolution, also known as Industry 4.0, offers a variety of new technologies and technological concepts for the continuous optimization of production and logistics processes in manufacturing enterprises. Up to now, a multitude of scholars have investigated potential opportunities, barriers, threads, and necessary enablers of Industry 4.0 initiatives. However, most of the recent Industry 4.0 approaches can still not resist practical tests due to their limited view on a small range of relevant topics. This paper introduces the research project ‘MUL4.0’ which aims to digitalize an entire value chain, from raw material to recycling. Based on an action-research-orientated approach, the authors use a multi-case-study design to investigate the potential of digitalization approaches within production and logistics processes. Furthermore, the authors present future research activities and discuss the therefore necessary prerequisites, from a materials science, mechanical engineering, metallurgical, logistics engineering, and management perspective.}
}
@incollection{RAJ2021267,
title = {Chapter Thirteen - Empowering digital twins with blockchain},
editor = {Shubhani Aggarwal and Neeraj Kumar and Pethuru Raj},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {121},
pages = {267-283},
year = {2021},
booktitle = {The Blockchain Technology for Secure and Smart Applications across Industry Verticals},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2020.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0065245820300681},
author = {Pethuru Raj},
keywords = {Blockchain, Smart contracts, Decentralized applications, Artificial intelligence, Digital twins, The Internet of Things, Consensus algorithms, Hashing, Machine learning, Cloud computing},
abstract = {A digital twin is an exact digital/logical/cyber/virtual representation/replica of any tangible physical system or process. And the digital twin runs on a competent IT infrastructure (say, cloud centers). In essence, a digital twin is typically a software program that takes various real-world data about a ground-level physical system as prospective inputs and produces useful outputs in the form of insights. The outputs generally are the value-adding and decision-enabling predictions or simulations of how that physical system will act on those inputs. These help in quickly and easily realizing highly optimized and organized products with less cost and risk. The manufacturing industry had embraced the digital twin technology long time back to be modern in their operations, outputs, and offerings. The distinct contributions of the digital twin paradigm, since then, have gone up significantly with the seamless synchronization with a number of pioneering technologies such as the Internet of Things (IoT), artificial intelligence (AI), big and streaming data analytics, data lakes, software-defined cloud environments, blockchain, etc. With the concept of cyber physical systems (CPS) is being adopted and adapted widely and wisely, complicated yet sophisticated electronics devices at the ground level are being blessed with their corresponding digital twins. The digital twins enable data scientists and system designers to optimize a number of things including process excellence, knowledge discovery and dissemination in time, better system design, robust verification and validation, etc. In the recent past, with the flourishing of the blockchain technology, the scope for digital twins has gone up remarkably. This unique combination is bound to produce additional competencies and fresh use cases for enterprises. This chapter is to explain how they integrate and initiate newer opportunities to be grabbed and gained for a better tomorrow.}
}
@incollection{CLARK2021211,
title = {Chapter 11 - Adaptive Complex Systems: Digital Twins},
editor = {Gurjit Kaur and Pradeep Tomar and Marcus Tanque},
booktitle = {Artificial Intelligence to Solve Pervasive Internet of Things Issues},
publisher = {Academic Press},
pages = {211-238},
year = {2021},
isbn = {978-0-12-818576-6},
doi = {https://doi.org/10.1016/B978-0-12-818576-6.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185766000113},
author = {Tony Clark and Vinay Kulkarni},
keywords = {Complex systems, digital twins, machine learning, multiagent systems},
abstract = {The design, control, and maintenance of complex systems are a challenge. Often it is difficult to understand the whole-system behavior because the knowledge of component behavior and interaction is uncertain. Such systems are often deployed into dynamic environments whose behavior is liable to change. This chapter reviews the features of complex systems and proposes an approach based on creating digital twins of systems that are capable of adaptation. We discuss technologies for digital twins and propose that the adaptation should be based on machine learning. We provide a simple tutorial example of agents with machine learning using our proposed technology and describe how we have used the technology to build a digital twin for supply chain networks.}
}
@article{BENBARA2020115587,
title = {Bending waves focusing in arbitrary shaped plate-like structures: Application to spatial audio in cars},
journal = {Journal of Sound and Vibration},
volume = {487},
pages = {115587},
year = {2020},
issn = {0022-460X},
doi = {https://doi.org/10.1016/j.jsv.2020.115587},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X20304193},
author = {Nassim Benbara and Marc Rebillat and Nazih Mechbal},
keywords = {Spatial vibration control, Bending wave focusing, Multifunctional materials, Advanced signal processing, Inverse problems, Digital twin},
abstract = {Advanced automotive audio applications are more and more demanding with respect to the visual impact of loudspeakers while still requiring more and more channels for high quality spatial sound rendering. The use of arbitrary plate-like structures driven by electromagnetic actuators or by piezoelectric elements appears as a promising solution to tackle both issues. However, to meet spatial rendering audio constraints (i.e. to be as close as possible to omnidirectional piston-like sources), the generated bending waves must be focused at a given position and to a certain extent within the host plate which can be of arbitrary shape, material, and thickness. Theoretically, this means being able to invert the spatio-temporal wave propagation operator for the generated bending waves to fit a given target shape. There are several methods (modal control, time-reversal, and propagating waves operator inversion) that allow to focus bending waves in a media. However, there is scarce work on their adaption and performances assessment in the context of audio applications. These methods depend differently on the available knowledge of wave propagation in the plate (theoretical, partial spatial or full spatial knowledge) and are here investigated to perform this task. Their performances are assessed with respect to several aspects: geometrical complexity, thickness, and material damping of the host structure, number and type of actuators, position and extent of the focusing area. The various methods are presented in a unified theoretical framework and they are compared by means of two key performance indexes (focus localization error and spatial contrast). An experimental validation on a relevant industrial case is also carried out and learning through a digital twin instead of time consuming experimental data investigated. This work falls within the framework of research which tries to bridge the gap between laboratory research and industrial deployment of this kind of technologies.}
}
@article{VRABIC2021336,
title = {An architecture for sim-to-real and real-to-sim experimentation in robotic systems},
journal = {Procedia CIRP},
volume = {104},
pages = {336-341},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.057},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009550},
author = {Rok Vrabič and Gašper Škulj and Andreja Malus and Dominik Kozjek and Luka Selak and Drago Bračun and Primož Podržaj},
keywords = {robotics, simulation, machine learning, reinforcement learning, digital twin},
abstract = {Research in the area of robotic systems has greatly benefited from the use of simulation models. Recent approaches allow the transfer of developed algorithms from simulation to reality (sim-to-real) and increasingly accurate representations of real systems as simulation models (real-to-sim). The paper presents an architecture based on open software that supports simultaneous experiments on real robots and their simulation models. Two illustrative examples are shown: a digital twin of an industrial robot and a sim-to-real transfer in an autonomous mobile robot system. The possibilities of future research on the interaction between robotic systems and their simulation models are discussed.}
}
@article{WANG2020100717,
title = {Comparison of the effectiveness of Taiwanese college and high school students participating in creative contests},
journal = {Thinking Skills and Creativity},
volume = {38},
pages = {100717},
year = {2020},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2020.100717},
url = {https://www.sciencedirect.com/science/article/pii/S1871187120301917},
author = {Yu-Hung Wang and Mei-Chen Chang and Jia-Ru Liou},
keywords = {Creative contest, Motivation, Team composition, Knowledge},
abstract = {This study investigated the college students (N = 725) and high school students (N = 585) that participated in the National Energy Technology Creative Contest (NETCC) of Taiwan. AMOS analysis was performed to establish a model of students’ performances in creative contests and t-test was conducted to analyze and compare the differences in variables between the two groups of students in the model. The findings of this study are as follows: (1) the research model fits the model of college and high school students’ performances in creative contests; (2) for college students (Model 1), their performances in creative contests are directly affected by team composition and knowledge variables, and team composition variables can also directly affect knowledge variables, (3) for high school students (Model 2), their performances in creative contest are directly affected by team composition and knowledge variables, but there is no direct correlation between team composition variables and knowledge variables, (4) comparison and analysis of the mean of the variables that affect the students’ performances in creative contests show that, for most variables, the mean of high school students is significantly higher than that of college students.}
}
@incollection{VALEEV2021159,
title = {Chapter 5 - Simulation technologies for process safety},
editor = {Sagit Valeev and Natalya Kondratyeva},
booktitle = {Process Safety and Big Data},
publisher = {Elsevier},
pages = {159-208},
year = {2021},
isbn = {978-0-12-822066-5},
doi = {https://doi.org/10.1016/B978-0-12-822066-5.00006-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220665000066},
author = {Sagit Valeev and Natalya Kondratyeva},
keywords = {Simulation, Accuracy, Algorithms, Digital twin, Aggregated model, Hierarchical system of models, Real time modeling},
abstract = {The chapter discusses the features of the application of technologies of simulation based on big data to provide process safety. One of the important problems during the simulation is ensuring the accuracy of simulation models and finding potential sources of errors. Algorithms for modeling input random effects are based on the Monte Carlo method. Markov processes are considered as the most common method for modeling stochastic systems, in particular, in risk management. The following discusses the possibilities of using digital twins in process safety management. Constructing a hierarchical system of models within the framework of digital twins is a complex task, as well as solving the problem of reducing accuracy when complicating models. The relevance of real-time modeling of chemical processes while ensuring safety is substantiated. Edge computing is an effective tool for building digital twins in real time mode. Extreme learning machines are considered as modern data analysis tools based on statistical models. In conclusion, the role of big data in constructing and analyzing the results of simulation models is considered.}
}
@article{LUO2020107451,
title = {Every node counts: Self-ensembling graph convolutional networks for semi-supervised learning},
journal = {Pattern Recognition},
volume = {106},
pages = {107451},
year = {2020},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107451},
url = {https://www.sciencedirect.com/science/article/pii/S0031320320302545},
author = {Yawei Luo and Rongrong Ji and Tao Guan and Junqing Yu and Ping Liu and Yi Yang},
keywords = {Teacher-student models, Self-ensemble learning, Graph convolutional networks, Semi-supervised learning},
abstract = {Graph convolutional network (GCN) provides a powerful means for graph-based semi-supervised tasks. However, as a localized first-order approximation of spectral graph convolution, the classic GCN can not take full advantage of unlabeled data, especially when the unlabeled node is far from labeled ones. To capitalize on the information from unlabeled nodes to boost the training for GCN, we propose a novel framework named Self-Ensembling GCN (SEGCN), which marries GCN with Mean Teacher – a powerful self-ensemble learning mechanism for semi-supervised task. SEGCN contains a student model and a teacher model. As a student, it not only learns to correctly classify the labeled nodes, but also tries to be consistent with the teacher on unlabeled nodes in more challenging situations, such as a high dropout rate and graph corrosion. As a teacher, it averages the student model weights and generates more accurate predictions to lead the student. In such a mutual-promoting process, both labeled and unlabeled samples can be fully utilized for backpropagating effective gradients to train GCN. In a variety of semi-supervised classification benchmarks, i.e. Citeseer, Cora, Pubmed and NELL, we validate that the proposed method matches the state of the arts in the classification accuracy. The code is publicly available at https://github.com/RoyalVane/SEGCN.}
}
@article{OLUWASEGUN20202262,
title = {The application of machine learning for the prognostics and health management of control element drive system},
journal = {Nuclear Engineering and Technology},
volume = {52},
number = {10},
pages = {2262-2273},
year = {2020},
issn = {1738-5733},
doi = {https://doi.org/10.1016/j.net.2020.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S1738573319308654},
author = {Adebena Oluwasegun and Jae-Cheon Jung},
keywords = {Digital twin, PHM, Machine learning, Nuclear power plant},
abstract = {Digital twin technology can provide significant value for the prognostics and health management (PHM) of critical plant components by improving insight into system design and operating conditions. Digital twinning of systems can be utilized for anomaly detection, diagnosis and the estimation of the system's remaining useful life in order to optimize operations and maintenance processes in a nuclear plant. In this regard, a conceptual framework for the application of digital twin technology for the prognosis of Control Element Drive Mechanism (CEDM), and a data-driven approach to anomaly detection using coil current profile are presented in this study. Health management of plant components can capitalize on the data and signals that are already recorded as part of the monitored parameters of the plant's instrumentation and control systems. This work is focused on the development of machine learning algorithm and workflow for the analysis of the CEDM using the recorded coil current data. The workflow involves features extraction from the coil-current profile and consequently performing both clustering and classification algorithms. This approach provides an opportunity for health monitoring in support of condition-based predictive maintenance optimization and in the development of the CEDM digital twin model for improved plant safety and availability.}
}
@article{ZHOU202122,
title = {Digital-twin-driven geometric optimization of centrifugal impeller with free-form blades for five-axis flank milling},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {22-35},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301096},
author = {Yu Zhou and Tong Xing and Yue Song and Yajing Li and Xuefeng Zhu and Guo Li and Shuiting Ding},
keywords = {Smart manufacturing, Digital twin (DT), Centrifugal impeller (CI), Ruled-surface blade, Aerodynamic optimization, Flank milling, Machining experiment, Performance test},
abstract = {Centrifugal impeller (CI) manufacturing is moving toward a new paradigm, with the objective to improve efficiency and competitiveness through Industry 4.0 and smart manufacturing. Making a CI developable and ruled has become a crucial technology to obviously improve machining efficiency and save costs although it may bring negative effects on aerodynamic performance accordingly. Hence, it is extremely challenging to consider and balance both machinability and aerodynamic performance in the process of CI geometric optimization. Digital Twin (DT) provides an attractive option for the integrated design and manufacturing due to multi-dimension and real-time. This paper breaks traditional procedures and presents a DT-based optimization strategy on the consideration of both machining efficiency and aerodynamic performance, as well as builds a reified 5-dimensional DT model. The virtual model consists of three sub-functional modules, including geometric modeling, machining optimization and aerodynamic performance evaluation. A tool-path generation method for CI five-axis flank milling is proposed to improve machining efficiency. The negative influences on aerodynamic performance and internal flow field are simulated and analyzed. Reinforce Learning is introduced to determine the optimization decision-making. Machining experiment and performance test with respect to various CI workpieces are conducted to provide immediate feedback to DT model. Real world and virtual world are combined to make CI geometry dynamically updated and iteratively optimized, which is desirable and significative to effectively shorten cycles and save costs in CI development.}
}
@article{QIE2021135,
title = {A function-oriented surface reconstruction framework for reverse engineering},
journal = {CIRP Annals},
volume = {70},
number = {1},
pages = {135-138},
year = {2021},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2021.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0007850621000408},
author = {Yifan Qie and Sebastian Bickel and Sandro Wartzack and Benjamin Schleich and Nabil Anwer},
keywords = {Reverse engineering, Machine learning, Data-driven design},
abstract = {Reverse engineering can be considered as the methodological process of analyzing and reconstructing a digital model of a physical asset. It has gained considerable interest with the advent of sophisticated sensors and data processing technologies hence becoming an important enabler for the product digital twin. However, while existing approaches to reverse engineering focus on the mere geometric reconstruction, this paper presents a novel paradigm called function-oriented surface reconstruction, that is capable of reconstructing the underlying part and surface function and thus outperforms existing methods. The applicability of the presented method is demonstrated through a case study of a gearbox.}
}
@article{BEKELE2021420,
title = {Physics-informed deep learning for one-dimensional consolidation},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
volume = {13},
number = {2},
pages = {420-430},
year = {2021},
issn = {1674-7755},
doi = {https://doi.org/10.1016/j.jrmge.2020.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1674775520301384},
author = {Yared W. Bekele},
keywords = {Physics-informed deep learning, Consolidation, Forward problems, Inverse problems},
abstract = {Neural networks with physical governing equations as constraints have recently created a new trend in machine learning research. In this context, a review of related research is first presented and discussed. The potential offered by such physics-informed deep learning models for computations in geomechanics is demonstrated by application to one-dimensional (1D) consolidation. The governing equation for 1D problems is applied as a constraint in the deep learning model. The deep learning model relies on automatic differentiation for applying the governing equation as a constraint, based on the mathematical approximations established by the neural network. The total loss is measured as a combination of the training loss (based on analytical and model predicted solutions) and the constraint loss (a requirement to satisfy the governing equation). Two classes of problems are considered: forward and inverse problems. The forward problems demonstrate the performance of a physically constrained neural network model in predicting solutions for 1D consolidation problems. Inverse problems show prediction of the coefficient of consolidation. Terzaghi’s problem, with varying boundary conditions, is used as a numerical example and the deep learning model shows a remarkable performance in both the forward and inverse problems. While the application demonstrated here is a simple 1D consolidation problem, such a deep learning model integrated with a physical law has significant implications for use in, such as, faster real-time numerical prediction for digital twins, numerical model reproducibility and constitutive model parameter optimization.}
}
@article{WU2020100016,
title = {Battery digital twins: Perspectives on the fusion of models, data and artificial intelligence for smart battery management systems},
journal = {Energy and AI},
volume = {1},
pages = {100016},
year = {2020},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2020.100016},
url = {https://www.sciencedirect.com/science/article/pii/S2666546820300161},
author = {Billy Wu and W. Dhammika Widanage and Shichun Yang and Xinhua Liu},
abstract = {Effective management of lithium-ion batteries is a key enabler for a low carbon future, with applications including electric vehicles and grid scale energy storage. The lifetime of these devices depends greatly on the materials used, the system design and the operating conditions. This complexity has therefore made real-world control of battery systems challenging. However, with the recent advances in understanding battery degradation, modelling tools and diagnostics, there is an opportunity to fuse this knowledge with emerging machine learning techniques towards creating a battery digital twin. In this cyber-physical system, there is a close interaction between a physical and digital embodiment of a battery, which enables smarter control and longer lifetime. This perspectives paper thus presents the state-of-the-art in battery modelling, in-vehicle diagnostic tools, data driven modelling approaches, and how these elements can be combined in a framework for creating a battery digital twin. The challenges, emerging techniques and perspective comments provided here, will enable scientists and engineers from industry and academia with a framework towards more intelligent and interconnected battery management in the future.}
}
@article{ZHAO2021105183,
title = {IoT and digital twin enabled smart tracking for safety management},
journal = {Computers & Operations Research},
volume = {128},
pages = {105183},
year = {2021},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2020.105183},
url = {https://www.sciencedirect.com/science/article/pii/S0305054820303002},
author = {Zhiheng Zhao and Leidi Shen and Chen Yang and Wei Wu and Mengdi Zhang and George Q. Huang},
keywords = {Digital twin, Internet of things, Abnormal state detection, Indoor positioning, Safety management},
abstract = {Modern warehousing systems for fresh and cold-keeping storage, have presented characteristics of complex operation procedures, accelerated operating pace, and high labour intensity. Thus, the working environment has become complicated and hazardous. Two recent fatal accidents that occurred in cold warehouses have shifted wide focus to safety management. The invisibility of operators’ status and location causes late responsiveness for rescuing. This paper first proposes an IoT and digital twin-enabled tracking solution framework for safety management. Then an indoor safety tracking mechanism for detecting motionless behaviour and self-learning genetic positioning is developed for recognizing the abnormal condition and obtaining precise location information in a real-time manner. A real-life case study with physical and cyber world implementation is conducted to demonstrate the feasibility and effectiveness of our proposed techniques. The results show that the detection of abnormal motionless behaviour is fulfilled, and the indoor positioning algorithm with self-learning ability not only achieves high accuracy up to 96.5% but also ensures the long-term use through adaptation.}
}
@article{RAGHEB2021108426,
title = {Effects of extensible modelling on composite riser mechanical responses},
journal = {Ocean Engineering},
volume = {220},
pages = {108426},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.108426},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820313330},
author = {Hossam Ragheb and Adam Sobey},
keywords = {Composite risers, Cable dynamics, Time domain, Benchmarking},
abstract = {The change from steel risers to composites comes with uncertainties that led to large safety factors. One area of uncertainty is the predicted response and stresses derived from commercial packages that are based on formulations that assume in-extensible riser. However, composite pipes exhibit a lower axial stiffness and therefore the velocity of the axial waves will change with a corresponding change in dynamic response. To determine the effect of this assumption, this paper assesses the effect of extensibility on the time-domain response. It is found that the in-extensible model predicts 3 times the number of high frequency tension cycles in the 20 kN tension range. To determine the impact of this change on the stress, the accuracy of available composite pipe models is benchmarked using shell, continuum-shell and solid elements. The quadratic and continuum-shell elements provide a maximum percentage difference of 4% compared to solid elements but the continuum-shell is selected as it has a lower computational cost. The response from the extensible and in-extensible models are input into the pipe model, they provide similar Tsai-Wu failure factors, alleviating concerns when modelling the strength. However, the change in dynamics remains a concern for other applications such as machine-learning or digital-twins.}
}
@article{ZIMMERMAN2020102746,
title = {Factors influencing hand hygiene practice of nursing students: A descriptive, mixed-methods study},
journal = {Nurse Education in Practice},
volume = {44},
pages = {102746},
year = {2020},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2020.102746},
url = {https://www.sciencedirect.com/science/article/pii/S1471595318306565},
author = {Peta-Anne P. Zimmerman and Ishtar Sladdin and Ramon Zenel Shaban and Julia Gilbert and Lynne Brown},
keywords = {Undergraduate nursing, Infection prevention and control, Hand hygiene},
abstract = {Developing nursing students' knowledge and practice of infection prevention and control (IPC) is fundamental to safe healthcare. A two-phase descriptive, mixed-method study conducted within a Bachelor of Nursing program at an Australian university aimed to explore: (i) theoretical knowledge of IPC, highlighting hand hygiene, of nursing students and; (ii) nursing students' and clinical facilitators' perceptions of factors influencing these practices during clinical placement. Phase One utilised an anonymous validated questionnaire assessing students' knowledge; identifying variables influencing students' IPC practices, subjected to descriptive and inferential analysis. Phase Two were semi-structured interviews exploring clinical facilitators' experiences/perceptions of students during clinical placement, analysed thematically. Students' demonstrated satisfactory knowledge of IPC in their second and third year, but clinical facilitators perceived that. students lacked awareness of the importance of these practices. Five themes arose from the interviews: (i) understanding workplace culture; (ii) students' modelling local behaviour; (iii) enhancing and consolidating knowledge for practice; (iv) adjusting to practice reality and; (v) accessing additional hand hygiene resources. Factors specific to workplace setting and culture were perceived to influence nursing students' socialisation. Future practice/education strategies could address these factors by ensuring students receive adequate supervision during clinical placement, and having strong advocates/role models present in the workplace.}
}
@article{WANG2021107934,
title = {Advanced fault diagnosis method for nuclear power plant based on convolutional gated recurrent network and enhanced particle swarm optimization},
journal = {Annals of Nuclear Energy},
volume = {151},
pages = {107934},
year = {2021},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2020.107934},
url = {https://www.sciencedirect.com/science/article/pii/S0306454920306307},
author = {Hang Wang and Min-jun Peng and Abiodun Ayodeji and Hong Xia and Xiao-kun Wang and Zi-kang Li},
keywords = {Fault diagnosis, Deep learning, GRU network, Convolutional kernel, Particle swarm optimization},
abstract = {A predictive approach to fault diagnosis in complex systems such as the Nuclear power plant (NPP) is becoming popular because of the efficiency and accuracy it presents. However, there is still a huge gap between the proposed fault diagnosis techniques and engineering applications. To further optimize the fault diagnosis route and encourage real-time application, this paper presents a highly accurate and adaptable fault diagnosis technique based on the convolutional gated recurrent unit (CGRU) and enhanced particle swarm optimization (EPSO). Stacking convolutional kernel and GRU results in a model that speedily extract the local characteristics and learn the time-series information. The EPSO is utilized to adaptively search for optimal hyper-parameters for the CGRU. Finally, the accuracy is evaluated on a dataset obtained from experiments, and comparative analysis of the proposed model with existing architectures and models are presented. Relevant research results that show the usefulness of the proposed model are also presented, which highlights the enhanced intelligence and information level achieved in the NPP fault diagnosis.}
}
@article{LIU2020106,
title = {Adaptive multi-teacher multi-level knowledge distillation},
journal = {Neurocomputing},
volume = {415},
pages = {106-113},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.07.048},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220311565},
author = {Yuang Liu and Wei Zhang and Jun Wang},
keywords = {Knowledge distillation, Adaptive learning, Multi-teacher},
abstract = {Knowledge distillation (KD) is an effective learning paradigm for improving the performance of lightweight student networks by utilizing additional supervision knowledge distilled from teacher networks. Most pioneering studies either learn from only a single teacher in their distillation learning methods, neglecting the potential that a student can learn from multiple teachers simultaneously, or simply treat each teacher to be equally important, unable to reveal the different importance of teachers for specific examples. To bridge this gap, we propose a novel adaptive multi-teacher multi-level knowledge distillation learning framework (AMTML-KD), which consists two novel insights: (i) associating each teacher with a latent representation to adaptively learn instance-level teacher importance weights which are leveraged for acquiring integrated soft-targets (high-level knowledge) and (ii) enabling the intermediate-level hints (intermediate-level knowledge) to be gathered from multiple teachers by the proposed multi-group hint strategy. As such, a student model can learn multi-level knowledge from multiple teachers through AMTML-KD. Extensive results on publicly available datasets demonstrate the proposed learning framework ensures student to achieve improved performance than strong competitors.}
}
@article{TAN2021100032,
title = {Developing a gamified AI-enabled online learning application to improve students’ perception of university physics},
journal = {Computers and Education: Artificial Intelligence},
volume = {2},
pages = {100032},
year = {2021},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2021.100032},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X21000266},
author = {Da Yang Tan and Chin Wei Cheah},
keywords = {Intelligent tutoring systems, Gamification, University physics, Self-directed learning},
abstract = {This article discusses the current progress of the on-going design efforts in creating an AI-enabled gamified web-based online learning application in the university introductory physics courses. The application aims to cater to the learning needs of students with diversity of the background, particularly students with little or weak background. The overall design architecture and principles are discussed, focusing on the rationale and design of the gamified elements that have been incorporated within the application, namely: (i) incremental difficulties, (ii) points and streaks, (iii) leaderboard, and (iv) gamified and animated user interface. Particularly, the gamified elements are considered through the lens of the ‘Head, Hands, and Heart’ of the students, with the objective to first encourage and motivate participation, stimulate practice, and strengthen the students’ domain knowledge of physics. In this article, an architecture to incorporate AI program into the web-based gamification platform by including various AI models (learner model, pedagogy model and domain model) is proposed. With the architecture, we aim to create a personalized tutor that provide effective and unique learning experience to help a learner achieve learning objectives. The AI enabled gamification platform also serves as an efficient feedback tool to teacher for optimization and redesigning of curriculum to best suit the need of individual learner.}
}
@article{MASHALY2021299,
title = {Connecting the Twins: A Review on Digital Twin Technology & its Networking Requirements},
journal = {Procedia Computer Science},
volume = {184},
pages = {299-305},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921006694},
author = {Maggie Mashaly},
keywords = {Digital Twin, Digital Transformation, Data Analytics, Artificial Intelligence, Real-time Communications, Network Requirements},
abstract = {Digital twin technology can be considered as an innovation accelerator. By providing a live copy of physical systems, digital twins bring to the table numerous advantages such as accelerated business processes, enhanced productivity, and faster innovation with reduced costs. For these numerous advantages digital twin is an ideal solution for several problems in domains such as Industry 4.0, education, healthcare and smart cities. However, to make sure the digital twin contributes effectively to these domains by representing a synchronized real-time copy of the physical system, the network connecting the physical and digital twins should fulfill a set of requirements such as low latency of real-time communication, data security and quality. This paper provides an overview on the technology of digital twin and its application domains with a detailed discussion on its networking requirements and proposed enabling technologies to fulfill them.}
}
@article{FANG2020100955,
title = {A 2020 perspective on “A generalized stereotype learning approach and its instantiation in trust modeling”},
journal = {Electronic Commerce Research and Applications},
volume = {40},
pages = {100955},
year = {2020},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2020.100955},
url = {https://www.sciencedirect.com/science/article/pii/S1567422320300326},
author = {Hui Fang and Jie Zhang and Murat Sensoy},
keywords = {Data management, Few-shot learning, Learning with limited data, Recommender systems, User modeling, User profiling},
abstract = {Owing to the rapid increase of user data and development of machine learning techniques, user modeling has been explored in depth and exploited by both academia and industry. It has prominent impacts in e-commerce-related applications by facilitating users’ experience in online platforms and supporting business organizations’ decision-making. Among all the techniques and applications, user profiling and recommender systems are two representative and effective ones, which have also obtained growing attention. In view of its wide applications, researchers and practitioners should improve user modeling from two perspectives: (1) more effort should be devoted to obtain more user data via techniques like sensing devices and develop more effective ways to manage complex data; and (2) improving the ability of learning from a limited number of data samples (e.g., few-shot learning) has become an increasingly hot topic for researchers.}
}
@article{QU2020113857,
title = {Lithium-ion battery performance degradation evaluation in dynamic operating conditions based on a digital twin model},
journal = {Microelectronics Reliability},
volume = {114},
pages = {113857},
year = {2020},
note = {31st European Symposium on Reliability of Electron Devices, Failure Physics and Analysis, ESREF 2020},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2020.113857},
url = {https://www.sciencedirect.com/science/article/pii/S0026271420304765},
author = {X. Qu and Y. Song and D. Liu and X. Cui and Y. Peng},
abstract = {The performance of lithium-ion batteries degrades over time. Evaluating the performance degradation for lithium-ion batteries is essential to ensure the operational reliability and reduces the risk of host-system downtime. The battery capacity that is obtained by completely charging and discharging a battery cell, directly reflects the performance of a lithium-ion battery. But in practical applications, the battery is dynamically charged and discharged. This makes it difficult to measure the actual capacity and further evaluate battery performance degradation to ensure the battery operating safety. To address this challenging issue, this paper proposes a performance degradation evaluation model by estimating the battery actual capacity in dynamic operating conditions. A health indicator (HI) is extracted from the measurable parameters to reflect the battery performance degradation. A battery digital twin model that describes the relationship between the cell voltage and the cell state-of-charge (SOC) are modelled by the long short-term memory (LSTM) algorithm, which takes the HI as a temporal measurement. The battery actual capacity can be obtained by virtually completely discharging this digital twin model. The experimental results illustrate the potential of the proposed method applying in dynamic operating conditions.}
}
@article{MUELLERZHANG2021408,
title = {Integrated Planning and Scheduling for Customized Production using Digital Twins and Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {408-413},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.046},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321007631},
author = {Zai Mueller-Zhang and Pablo {Oliveira Antonino} and Thomas Kuhn},
keywords = {Digital Twin, Reinforcement Learning, Deep-Q-Network, Integrated Planning, Scheduling},
abstract = {For customized production in small lot-sizes, traditional production plants have to be reconfigured manually multiple times to be adapted to variable order changes, what significantly increases the production costs. One of the goals of Industry 4.0 is to enable flexible production, allowing for customer-specific production or even production with lot size 1 in order to react dynamically to changes in production orders. All of this with increased quality parameters such as optimized use of machines, conveyor belts and raw materials, which ultimately leads to optimized resource utilization and cost-efficiency. To address this challenge, in this paper, we present a digital twin based self-learning process planning approach using Deep-Q-Network that is capable of identifying optimized process plans and workflows for the simultaneous production of personalized products. We have evaluated our approach on a virtual aluminum cold milling factory from the SMS Group, in the context of the BaSys 4 project. The goal of the evaluation was to provide evidence that the proposed approach is able to handle large problem space effectively. Our approach ensures the efficiency of the personalized production and the adaptivity of the production system.}
}
@article{OGUNSANYA2021427,
title = {In-situ Droplet Monitoring of Inkjet 3D Printing Process using Image Analysis and Machine Learning Models},
journal = {Procedia Manufacturing},
volume = {53},
pages = {427-434},
year = {2021},
note = {49th SME North American Manufacturing Research Conference (NAMRC 49, 2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.06.045},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921000524},
author = {Michael Ogunsanya and Joan Isichei and Santosh Kumar Parupelli and Salil Desai and Yi Cai},
keywords = {additive manufacturing, inkjet 3D Printing, image analysis, machine learning, neural network},
abstract = {Additive manufacturing (AM) has yielded major innovations in the electronics, biomedical and energy domains. One of the AM techniques which has witnessed widespread use is the inkjet 3D printing (IJP). The IJP process fabricates parts by depositing colloidal liquid droplets on substrates. Despite its advantages, variations in input process parameters and fluid properties can have a profound impact on the print quality. This paper aims to address this issue by presenting a novel vision-based approach for in-situ monitoring of droplet formation. Further, a machine learning model was used to study the relationship between droplet attributes and droplet modes. A drop watcher camera was used to capture a sequence of videos obtained from different combinations of voltage and frequency. Custom source code was developed using python libraries to capture variations in droplet attributes (droplet size, velocity, aspect ratio, and presence of satellites) and their impact on the droplet modes (normal, satellite, and no-droplet) using computer vision. A backpropagation neural network mode (BPNN) was applied, with the droplet features as inputs, to classify output droplet modes. The BPNN classified droplet modes with 90% (high) accuracy. This research forms the basis for future development of digital twin model of inkjet 3D printing towards predictive analysis and process optimization.}
}
@incollection{ODELL20213,
title = {1 - The evolution of AI and the human-machine interface as a manager in Industry 4.0},
editor = {Hamid Jahankhani and Liam M. O’Dell and Gordon Bowen and Daniel Hagan and Arshad Jamal},
booktitle = {Strategy, Leadership, and AI in the Cyber Ecosystem},
publisher = {Academic Press},
pages = {3-22},
year = {2021},
isbn = {978-0-12-821442-8},
doi = {https://doi.org/10.1016/B978-0-12-821442-8.00015-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821442800015X},
author = {Liam M. O’Dell and Hamid Jahankhani},
keywords = {Artificial intelligence, Big data, Machine learning, Digital twin, Ethics and governance, Knowledge competency areas and frameworks},
abstract = {The role of project management is changing dramatically in the backdrop of Industry 4.0 and the digital revolution. This exciting transformation is taking place in the next few years whilst embracing artificial intelligence (AI) technology into the body of the knowledge competencies. With this intelligence explosion, the influence of AI technology and the key themes of machine learning, big data, and digital twin are evolving, creating the possibility of a cyber physical project professional. This brings with it further issues around ethics and governance with the development and use of AI technology. The aim of this chapter is to provides a useful initial insight whilst assisting the project management professional to gain further understanding of how AI innovation is entering the workplace and how to potentially engage with AI. In addition, this study will hopefully stimulate future researchers to develop ideas for innovation in the use of AI and the cyberphysical digital twin coworking relationships within the project management profession.}
}
@article{VASANTHAN20217,
title = {Combining Supervised Learning and Digital Twin for Autonomous Path-planning⁎⁎This work was sponsored by the Research Council of Norway through the Centre of Excellence funding scheme, project number 223254, AMOS.},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {16},
pages = {7-15},
year = {2021},
note = {13th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles CAMS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.066},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321014683},
author = {Chanjei Vasanthan and Dong T. Nguyen},
keywords = {Autonomy, Path-planning, Supervised learning, Digital twin, Maneuvering, Collision avoidance, COLREGs},
abstract = {Over the last decade, the evolution of autonomous automobiles based on artificial intelligence has increased rapidly with significant success. Naturally, this has caught the interest of the maritime industry and the development of autonomous vessels. However, unlike the highway, the ocean is considered a complex environment carrying unpredictable environmental forces, such as current, waves and wind-condition. For autonomous path-following and path-planning, particularly within the machine learning-field, Deep Reinforcement Learning (DRL) have generally been the favored approach. This follows from the fact that resulting models have demonstrated staggering performance. However, for practical implementations, Deep learning-based models are generally considered black box-solutions, and hence often introduce uncertainties in the operating domain. Therefore, in this paper an autonomous path-planner based on Supervised learning is proposed. Different Supervised learning models were investigated, and Gradient Boosting Regressor was found to be the most adequate model based on hyperparameter-tuning. The model was developed on constraints proposed by the class society DNV GL combined with International Regulations for Preventing Collision at Sea (COLREGs) rule 14 for collision-avoidance. Following this, the model was trained to design a suitable path based on parametrization of a cubic Bézier curve. To follow the parametrized path, a maneuvering-controller derived from the Maneuvering problem presented in Skjetne (2005) was applied. However, a drawback of Supervised learning is the necessity for large-scale training data. Hence, a digital twin of the own vessel was developed and utilized to generate sufficient training data. To demonstrate the performance of the autonomous path-planner, a number of simulation scenarios were introduced.}
}
@article{RAGAZZINI2021743,
title = {A Digital Twin-based Predictive Strategy for Workload Control},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {743-748},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.183},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321009538},
author = {Lorenzo Ragazzini and Elisa Negri and Marco Macchi},
keywords = {Digital Twin, Production Control, Workload Control, Order Release, Card Controlling, Reinforcement Learning},
abstract = {The paper aims at proposing a card controlling model to improve the standard CONWIP procedure, granting a similar system throughput while reducing Work In Progress (WIP) levels. To achieve this objective, the authors developed a Digital Twin-based production control system including a reinforcement learning algorithm (i.e. Q-Learning). The Digital Twin is responsible for short term predictions of the behavior of the system aimed at a what-if analysis with different numbers of cards. As there is lack of evidence of research related to Digital Twin applications for production control and for order release systems in particular, we aim at proposing this as an initial work to start the exploration of problems in this control area. The proposed model has been tested both in a Job Shop and in a Flow Shop systems with promising results.}
}
@article{MASCHLER2021127,
title = {Transfer learning as an enabler of the intelligent digital twin},
journal = {Procedia CIRP},
volume = {100},
pages = {127-132},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004790},
author = {Benjamin Maschler and Dominik Braun and Nasser Jazdi and Michael Weyrich},
keywords = {Case Study, Deep Learning, Industrial Transfer Learning, Intelligent Digital Twin, Lifecycle, Production Engineering, Qualitative Analysis, Sim2Real Transfer, Simulation, Use Cases},
abstract = {Digital Twins have been described as beneficial in many areas, such as virtual commissioning, fault prediction or reconfiguration planning. Equipping Digital Twins with artificial intelligence functionalities can greatly expand those beneficial applications or open up altogether new areas of application, among them cross-phase industrial transfer learning. In the context of machine learning, transfer learning represents a set of approaches that enhance learning new tasks based upon previously acquired knowledge. Here, knowledge is transferred from one lifecycle phase to another in order to reduce the amount of data or time needed to train a machine learning algorithm. Looking at common challenges in developing and deploying industrial machinery with deep learning functionalities, embracing this concept would offer several advantages: Using an intelligent Digital Twin, learning algorithms can be designed, configured and tested in the design phase before the physical system exists and real data can be collected. Once real data becomes available, the algorithms must merely be fine-tuned, significantly speeding up commissioning and reducing the probability of costly modifications. Furthermore, using the Digital Twin’s simulation capabilities virtually injecting rare faults in order to train an algorithm’s response or using reinforcement learning, e.g. to teach a robot, become practically feasible. This article presents several cross-phase industrial transfer learning use cases utilizing intelligent Digital Twins. A real cyber physical production system consisting of an automated welding machine and an automated guided vehicle equipped with a robot arm is used to illustrate the respective benefits.}
}
@article{HWANGBO2020106910,
title = {Design of control framework based on deep reinforcement learning and Monte-Carlo sampling in downstream separation},
journal = {Computers & Chemical Engineering},
volume = {140},
pages = {106910},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106910},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419310750},
author = {Soonho Hwangbo and Gürkan Sin},
keywords = {Liquid-liquid extraction column, Deep reinforcement learning, Monte-Carlo sampling, Control system, API production, Biopharmaceuticals},
abstract = {This paper proposes a systematic framework to develop deep reinforcement learning (RL)-based algorithms for control system of downstream separation in biopharmaceutical process as follows. First, a simulation model as a digital twin is built and Monte-Carlo sampling generates substantial amounts of samples considering disturbances. Second, the deep RL-based control system is designed and the optimization subject to sample datasets is conducted. The methodology is implemented in a prototype software and relevant codes are shared by Mendeley Data. The proposed model is successfully applied to control the liquid-liquid extraction column for the recovery of fusidic acid as part of downstream processing. The resulting deep RL algorithm provides an operation performance with a better API recovery yield (32 % higher than open loop operation) and lower deviations (23 % lower than open loop operation) against disturbances.}
}
@article{ZOHDI2020112907,
title = {A machine-learning framework for rapid adaptive digital-twin based fire-propagation simulation in complex environments},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {363},
pages = {112907},
year = {2020},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2020.112907},
url = {https://www.sciencedirect.com/science/article/pii/S0045782520300906},
author = {T.I. Zohdi},
keywords = {Fire-propagation, Ember flow, Digital-twin, Machine-learning algorithms},
abstract = {The objective of this work is to illustrate how to algorithmically integrate Machine-Learning Algorithms (MLA’s) with multistage/multicomponent fire spread models. In order to tangibly illustrate this process, this work develops a framework for a specific model problem combining: (I) a meshless discrete element “submodel” that tracks the trajectory of airborne hot particles/embers, subject to prevailing wind velocities and updrafts, (II) a topographical “submodel” of the ambient combustible material whereby airborne embers that make contact are allowed to start secondary fires (if conditions are appropriate), combined with ground-based surface spread and burn rates for generating new embers, new updrafts (due to hot air), etc., and (III) a Machine-Learning Algorithm to rapidly ascertain the multi-submodel system parameters that force the overall model to match observations. The submodels compute both ground and airborne hot-ember driven fire propagation, as well as subsequent distribution of debris/soot, which is important for air-quality assessment. The overall framework is designed for use in digital twin technology, which refers to an adaptive digital replica of a physical system, whereby model updates are continuously in near real-time. This necessitates a rapid simulation paradigm that can easily interface with telecommunications, cameras and sensors. The presented framework is designed to run quickly on laptops and hand held devices, with the guiding principle being to make it potentially useful for first-responders in real-time.}
}
@article{AI20201,
title = {Distributed stochastic configuration networks with cooperative learning paradigm},
journal = {Information Sciences},
volume = {540},
pages = {1-16},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.05.112},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520305375},
author = {Wu Ai and Dianhui Wang},
keywords = {Stochastic configuration networks, Randomized neural networks, Alternating direction method of multipliers, Distributed learning},
abstract = {As a new category of randomized neural networks (RNNs), stochastic configuration networks (SCNs) have demonstrated great potential for data analytics. Unlike conventional randomized learning techniques, e.g., random vector functional-link (RVFL) networks, SCNs provide a stochastic configuration mechanism on the assignment of input parameters which guarantees the universal approximation capability of a resulting learner model. In this paper, a distributed version of SCN is developed for decentralized datasets in cooperative learning paradigm. This paper proposes an approach to deal with datasets stored across a network of multiple learning agents without any fusion center. Specifically, we formulate the centralized learning problem as an equivalent form with the decomposition of subproblems coupled in a network and a consensus restriction. Then, a cooperative configuration scheme is proposed for randomly assigning the input weights and bias. Finally, based on the well-known parallel alternating direction method of multipliers (ADMM), the output weights are evaluated iteratively. Simulation studies with comparisons on three benchmark datasets are carried out. The experimental results indicate that our proposed learning scheme performs well and outperforms distributed RVFL networks.}
}
@article{BELDICEANU2021641,
title = {ASSISTANT: Learning and Robust Decision Support System for Agile Manufacturing Environments},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {641-646},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.074},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008041},
author = {Nicolas Beldiceanu and Alexandre Dolgui and Clemens Gonnermann and Gabriel Gonzalez-Castañé and Niki Kousi and Bart Meyers and Julien Prud’homme and Simon Thevenin and Eduardo Vyhmeister and Per-Olov Östberg},
keywords = {Artificial intelligence, Data analytics, Digital twins, Decision aid, Reconfigurable manufacturing systems, Process, production planning, Scheduling, real-time control},
abstract = {The European project ASSISTANT will provide a set of AI-based digital twins that helps process engineers and production planners to operate collaborative mixed-model assembly lines based on the data collected from IoT devices and external data sources. Such a tool will help planners to design the assembly line, plan the production, operate the line, and improve process tuning. In addition, the system monitors the line in real-time, ensures that all required resources are available, and allows fast re-planning when necessary. ASSISTANT aims to make cost-effective decisions while ensuring product quality, safety and wellbeing of the workers, and managing the various sources of uncertainties. The resulting digital twin systems will be data-driven, agile, autonomous, collaborative and explainable, safe but reactive.}
}
@incollection{RAJ20201,
title = {Chapter One - Stepping into the digitally instrumented and interconnected era},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {1-34},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300518},
author = {Pethuru Raj and Jenn-Wei Lin},
keywords = {The internet of things, Cyber-physical systems, Artificial intelligence, Edge computing, Real-time applications},
abstract = {This chapter is to tell all about the digitization-inspired possibilities and opportunities and how software-defined cloud centers are the best fit for hosting and running digital applications. Also, how the next-generation data analytics can be smartly accomplished through cloud platforms and infrastructures is also explained in detail. We are to describe some of the impactful developments and technological advancements brewing in the IT space, how the tremendous amount of data getting produced and processed through cloud systems is to impact the IT and business domains, and how next-generation IT infrastructures are accordingly getting refactored, remedied and readied for the impending big data-induced challenges, how likely the move of the data analytics discipline toward fulfilling the digital universe requirements of extracting and extrapolating actionable insights for the knowledge-parched is, and finally for the establishment and sustenance of the dreamt smarter planet. In short, the uninhibited explosion of digitized systems and connected devices pour out a tremendous amount of multi-structured data and the impending challenge is to make sense out of the data heaps. Data analytics is the way to go and in the recent past, the overwhelming trend is to empower our everyday systems with machine and deep learning algorithms to automatically learn out of data heaps and streams in order to be distinctively intelligent in their actions and reactions. This chapter is specially prepared to put a stimulating foundation for explaining the nitty-gritty of the Digital Twin paradigm.}
}
@article{WU2019127,
title = {Understanding students’ mimicry, emulation and imitation of genre exemplars: An exploratory study},
journal = {English for Specific Purposes},
volume = {54},
pages = {127-138},
year = {2019},
issn = {0889-4906},
doi = {https://doi.org/10.1016/j.esp.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0889490618300334},
author = {Zhiwei Wu},
keywords = {Genre exemplars, Imitative learning, Imitation, Emulation, Mimicry},
abstract = {Guided by the Vygotskian concepts of mimicry, emulation, and imitation, this study examines how eight Chinese EFL students modeled from genre exemplars when composing their first academic papers in university. The students were enrolled in a “Cultural Tourism Studies” course at a university in south China. The course was delivered in a co-teaching approach with an embedded six-week EAP workshop. Given the short time frame of the workshop, an ESP genre-based approach was adopted, and genre exemplars were used to scaffold the students' understanding of two focal genre features (i.e., citation and organization). At the end of the course, text-based interviews were conducted to understand how the students made sense of and used the genre exemplars in the composing process. The analysis of students' term papers revealed a varying amount of mimicry, emulation, and imitation of the target genre features. The introspective and retrospective accounts of students' engagement with the exemplars suggested that imitative learning was multifaceted, dynamic, and varied within and between individuals. These findings challenge the dichotomous characterization of exemplars as either an enabling scaffold or a constraining shackle. The paper also discusses how the tripartite Vygotskian framework is a useful heuristic for EAP instructors to assess the extent to which genre exemplars are attuned to students’ zone of proximal development, and how genre exemplars offer fluid affordances in the process of object-, other-, and self-regulation.}
}
@article{RALPH2020253,
title = {An Implementation Approach for an Academic Learning Factory for the Metal Forming Industry with Special Focus on Digital Twins and Finite Element Analysis},
journal = {Procedia Manufacturing},
volume = {45},
pages = {253-258},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.103},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920311458},
author = {Benjamin James Ralph and Andreas Schwarz and Martin Stockinger},
keywords = {Learning Factory, Digital Twin, Industry 4.0, Digitalization, Smart Factory},
abstract = {The requirements for the planning, implementation and operation of an academic learning factory vary depending on the specific area of the respective institution. This paper provides an approach for the planning and implementation of such a factory, specifically tailored to the requirements of the metal forming industry. This learning factory will then be operated at the Chair of Metalforming at the Montanuniversität Leoben (MUL). The objective is to monitor and control forming units of different technological maturity in a common system. The industrial software used, ibaPDA for data logging and ibaAnalyzer for automated further processing, is widespread in practice and enables students to learn the required skills as close to practice as possible. In addition, Analog to Digital (A/D) converters and machine hour counters will be implemented to illustrate the retrofitting approach in practice. For the planning and implementation of Digital Shadows and Digital Twins, common Finite Element (FE) simulation programs will be used and the possibilities of connectivity between machines, simulation programs and automation software will be demonstrated. The project presented here should thus make an important contribution to the training of future specialists with special consideration of the increasing interdisciplinarity in manufacturing technology.}
}
@article{ZAMBRANO2020100419,
title = {TWINKLE: A digital-twin-building kernel for real-time computer-aided engineering},
journal = {SoftwareX},
volume = {11},
pages = {100419},
year = {2020},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2020.100419},
url = {https://www.sciencedirect.com/science/article/pii/S2352711019300664},
author = {V. Zambrano and R. Rodríguez-Barrachina and S. Calvo and S. Izquierdo},
keywords = {Model order reduction, PARAFAC, Machine learning, Data analysis, Tensor decomposition},
abstract = {TWINKLE is a library for building families of solvers to perform Canonical Polyadic Decomposition (CPD) of tensors. The common characteristic of these solvers is that the data structure supporting the tuneable solution strategy is based on a Galerkin projection of the phase space. This allows processing and recovering tensors described by highly sparse and unstructured data. For achieving high performance, TWINKLE is written in C++ and uses the Armadillo open source library for linear algebra and scientific computing, based on LAPACK (Linear Algebra PACKage) and BLAS (Basic Linear Algebra Subprograms) routines. The library has been implemented keeping in mind its future extensibility and adaptability to fulfil the different users’ needs in academia and industry regarding Reduced Order Modelling (ROM) and data analysis by means of tensor decomposition. It is especially focused on post-processing data from Computer-Aided-Engineering (CAE) simulation tools.}
}
@incollection{SAINI2020273,
title = {Chapter 14 - Language learnability analysis of Hindi: a comparison with ideal and constrained learning approaches},
editor = {G.R. Sinha and Jasjit S. Suri},
booktitle = {Cognitive Informatics, Computer Modelling, and Cognitive Science},
publisher = {Academic Press},
pages = {273-290},
year = {2020},
isbn = {978-0-12-819445-4},
doi = {https://doi.org/10.1016/B978-0-12-819445-4.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012819445400014X},
author = {Sandeep Saini and Vineet Sahula},
keywords = {Second language acquisition, language processing, cognitive model for language},
abstract = {Native language acquisition is one of the first tasks undertaken by the human brain in the infant stage of life. The linguist community has always been interested in finding the method adopted by the human brain to acquire the native language. Word segmentation is one of the most important tasks in language acquisition. Statistical learning has been proposed to be one of the earliest strategies that an infant can adapt to segment a lot of different words. The language learnability theories are supposed to be universal in nature and work on all the languages. In this work, we have analyzed the learnability of the most popular Indian language, Hindi, based on ideal (universal) and constrained Bayesian learner models. We have analyzed the learnability of the language using unigram and bigram approaches by considering word, syllables, and phonemes as the smallest unit of the language. We demonstrate that Bayesian inference is indeed a viable crosslinguistic strategy and can be used on Hindi as well.}
}
@article{KHAN202013,
title = {On the requirements of digital twin-driven autonomous maintenance},
journal = {Annual Reviews in Control},
volume = {50},
pages = {13-28},
year = {2020},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2020.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578820300560},
author = {Samir Khan and Michael Farnsworth and Richard McWilliam and John Erkoyuncu},
keywords = {Digital twin, Autonomous systems, Maintenance, Fault detection and isolation, Reinforcement learning},
abstract = {Autonomy has become a focal point for research and development in many industries. Whilst this was traditionally achieved by modelling self-engineering behaviours at the component-level, efforts are now being focused on the sub-system and system-level through advancements in artificial intelligence. Exploiting its benefits requires some innovative thinking to integrate overarching concepts from big data analysis, digitisation, sensing, optimisation, information technology, and systems engineering. With recent developments in Industry 4.0, machine learning and digital twin, there has been a growing interest in adapting these concepts to achieve autonomous maintenance; the automation of predictive maintenance scheduling directly from operational data and for in-built repair at the systems-level. However, there is still ambiguity whether state-of-the-art developments are truly autonomous or they simply automate a process. In light of this, it is important to present the current perspectives about where the technology stands today and indicate possible routes for the future. As a result, this effort focuses on recent trends in autonomous maintenance before moving on to discuss digital twin as a vehicle for decision making from the viewpoint of requirements, whilst the role of AI in assisting with this process is also explored. A suggested framework for integrating digital twin strategies within maintenance models is also discussed. Finally, the article looks towards future directions on the likely evolution and implications for its development as a sustainable technology.}
}
@article{DAVID2019349,
title = {Attaining Learning Objectives by Ontological Reasoning using Digital Twins},
journal = {Procedia Manufacturing},
volume = {31},
pages = {349-355},
year = {2019},
note = {Research. Experience. Education. 9th Conference on Learning Factories 2019 (CLF 2019), Braunschweig, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.03.055},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919304196},
author = {Joe David and Andrei Lobov and Minna Lanz},
keywords = {Digital Twin, Learning Outcomes, ontology, Reasoning, Web Ontology Language (OWL), Learning, Pedagogy},
abstract = {Learning Factories provide a propitious learning environment for nurturing production related competencies. However, several problems continue to plague their widespread adoption. This study mentions these issues before proposing the use of digital twins as an alternative. The study presents an approach towards modelling such a digital twin and proposes a solution that uses ontologies to develop a formal representation of the domain (a flexible manufacturing system) and the learning that occurs in the environment. A reasoning mechanism is used deduce inferences from the ontology to facilitate automated assessment of the learner. A use-case for the pedagogic digital twin is presented and discussed before proposing future directions for work.}
}
@article{MIAO20202154,
title = {Humming-Query and Reinforcement-Learning based Modeling Approach for Personalized Music Recommendation},
journal = {Procedia Computer Science},
volume = {176},
pages = {2154-2163},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.252},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920321566},
author = {Dezhuang Miao and Xuesong Lu and Qiwen Dong and Daocheng Hong},
keywords = {Playlist Prediction, Recommendation, User Model, Interactive Learning},
abstract = {Music recommendation is a prominent application of recommender systems, which has been attracting more and more attentions. There are two research streams of music recommender systems: one is static recommendation based on learning user’s preference according to historical data, and the other is dynamic recommendation considering user’s feedback. But the individual music preference for a certain moment is closely related to personal experience of the music and music literacy, as well as temporal scenario with diversity. Thus, it’s necessary to design a new music recommendation framework by integrating static recommendation and dynamic recommendation. Therefore, we propose a novel approach for music recommendation HRRS (Humming-Query and Reinforcement-Learning based Recommender Systems) by integrating prior two research streams. This novel recommendation framework HRRS based on humming query and reinforcement learning is learning and adapting to user’s current preference continually by collecting interactive data in real time. This preliminary recommendation framework captures song characters, personal dynamic preferences, and yields a better listening experience with proper interaction.}
}
@article{BORDATCHEV2020159,
title = {Preliminary experimental analysis of the surface topography formation during laser polishing H13 tooling steel using statistical characteristics of the surface amplitude distribution},
journal = {Procedia Manufacturing},
volume = {48},
pages = {159-164},
year = {2020},
note = {48th SME North American Manufacturing Research Conference, NAMRC 48},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920314840},
author = {Evgueni V. Bordatchev and Srdjan J. Cvijanovic and Remus O. Tutunea-Fatan},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Surface finish is one of the most important quality characteristics of fabricated components. To complement that, laser polishing (LP) is one of the advanced manufacturing surface finishing techniques that has been recently developed and successfully employed for improving surface quality without deteriorating the overall structural form through surface smoothing by melting and redistributing a thin layer of molten material. This paper proposes a statistical digital twin of the LP process and demonstrates the applicability of amplitude distribution statistical characteristics in the experimental analysis of surface topography formation during LP process. Initially, the thermodynamic transformation of the initial surface topography is considered by means of technical cybernetics and machine learning approaches to describe two of the most critical LP process components, namely: thermodynamic melting and solidification of both solid material and surface topography. To exemplify the effective application of statistical amplitude distribution characteristics, LP experiments were conducted with two different laser powers (25 W and 100 W) on flat and ground initial surfaces and resulting surface topographies were measured. Several amplitude distribution characteristics, such as roughness average value, averaged transverse profile as a W-shape, averaged transverse roughness profile, and probability distribution function were calculated. After that, actual molten material area, volume redistribution and final surface quality were comparatively analyzed. It was shown that the proportion between two components of the LP thermodynamic transformation and surface topography is critically dependent on laser power. As such, during low-power conditions (< 25 W), surface quality is predominantly determined by the thermodynamic transformation of initial surface topography and therefore only this component can be used for statistically reliable LP process modelling and digital identification. In summary, amplitude distribution characteristics have several advantages in building a comprehensive understanding of the molten material redistributing along and across LP line.}
}
@article{BAKLIWAL20181237,
title = {A Multi Agent System architecture to implement Collaborative Learning for social industrial assets⁎⁎This research was funded by the Royal Academy of Engineering under the Newton Bhabha scheme (Project No. HEPI—1516—10). This research was supported by SustainOwner, a project sponsored by the EU Framework Programme Horizon 2020, MSCA-RISE-2014: Marie Skodowska-Curie Research and Innovation Staff Exchange (Rise) (grant agreement number 645733 Sustain-owner H2020-MSCA-RISE-2014).},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1237-1242},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.421},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318315477},
author = {Kshitij Bakliwal and Maharshi Harshadbhai Dhada and Adrià Salvador Palau and Ajith Kumar Parlikad and Bhupesh Kumar Lad},
keywords = {Cyber-Physical Systems, Industrial Internet of Things, Digital Twins, Collaborative Learning, Industry Automation, Multi Agent Systems, Distributed Computing},
abstract = {The ‘Industrial Internet of Things’ aims to connect industrial assets with one another and benefit from the data that is generated, and shared, among these assets. In recent years, the extensive instrumentation of machines and the advancements in Information Communication Technologies are re-shaping the role of assets in our industrial systems. An emerging concept here is that of ‘social assets’: assets that collaborate with each other in order to improve system optimisation. Cyber-Physical Systems (CPSs) are formed by embedding the assets with computers, or microcontrollers, which run real-time decision-making algorithms over the data originating from the asset. These are known as the ‘Digital Twins’ of the assets, and form the backbone of social assets. It is essential to have an architecture which enables a seamless integration of these technological advances for an industry. This paper proposes a Multi Agent System (MAS) architecture for collaborative learning, and presents the findings of an implementation of this architecture for a prognostics problem. Collaboration among assets is performed by calculating inter-asset similarity during operating condition to identify ‘friends’ and sharing operational data within these clusters of friends. The architecture described in this paper also presents a generic model for the Digital Twins of assets. Prognostics is demonstrated for the C-MAPSS turbofan engine degradation simulated data-set (Saxena and Goebel (2008)).}
}
@article{ERTVELDT2020456,
title = {MiCLAD as a platform for real-time monitoring and machine learning in laser metal deposition},
journal = {Procedia CIRP},
volume = {94},
pages = {456-461},
year = {2020},
note = {11th CIRP Conference on Photonic Technologies [LANE 2020]},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.09.164},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120313524},
author = {Julien Ertveldt and Patrick Guillaume and Jan Helsen},
keywords = {Laser Metal Deposition (LMD), Machine learning, Real-time monitoring, Feed-back control},
abstract = {The MiCLAD machine designed at the VUB, Belgium, allows for closed-loop controlled laser metal deposition including various in-situ optical based measurement systems. These integrated sensors collect information on deposition geometry and temperature during the building process. Hence, each cubic millimeter of material that is either added or removed is mapped to its digital twin with a millisecond temporal resolution in the machines database. This paper introduces the platform and its capabilities by focusing on the procedure of obtaining the necessary training data for the future application of machine learning algorithms, with the goal of controlling the geometry and temperature history during additive manufacturing.}
}
@article{TROUSSAS2020103698,
title = {Collaboration and fuzzy-modeled personalization for mobile game-based learning in higher education},
journal = {Computers & Education},
volume = {144},
pages = {103698},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2019.103698},
url = {https://www.sciencedirect.com/science/article/pii/S0360131519302519},
author = {Christos Troussas and Akrivi Krouska and Cleo Sgouropoulou},
keywords = {Advice generator, Collaborative learning, Fuzzy logic, Mobile game-based learning, Personalized learning},
abstract = {Mobile game-based learning constitutes a hot issue in the related scientific literature since it promotes learning through an entertaining way and fosters student motivation to increase engagement in the educational process. As such, it can enhance the learning process and improve student participation. Towards this direction, this paper investigates how mobile learning and game-based learning can be utilized in higher education settings and analyzes the pedagogical affordance of their adoption. As a testbed for our research, we designed and implemented Quiz Time! which is an intelligent mobile game-based learning application for assessing and advancing learners' knowledge in the programming language C#. Quiz Time! employs an assessing knowledge module for testing the knowledge of learners, a vectorial-based recommendation module for proposing personalized collaboration in group playing, a dynamic fuzzy logic-based advice generator for tailored assistance to learners' profile and misconceptions, and a cognitive learner modeler supporting the aforementioned modules. Quiz Time! was used in a higher education institution for an academic semester and was evaluated by students and computer science experts using an established framework and the statistical hypothesis test. Regarding the evaluation results, the computer science experts validated the pedagogical adequacy of the application and the students highlighted its positive impact on learning and its usefulness. A major conclusion is that incorporating personalization and collaboration in mobile game-based learning can further assist students in higher education towards advancing their knowledge level.}
}
@article{RIERA202017610,
title = {Experience feedback and innovative pedagogical applications with HOME I/O},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17610-17615},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2676},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320334388},
author = {B. Riera and T. Ranger and R. Saddem and F. Emprin and J.-P. Chemla and A. Philippot},
keywords = {simulation tool, control education, innovative pedagogical approaches, STEM education, virtual house, digital twin},
abstract = {In the previous IFAC World Congress at Toulouse in 2017, HOME I/O was introduced as an innovative pedagogical tool. This software is the result of a R&D project between CReSTIC lab from the University of Reims Champagne-Ardenne (URCA) and Real Games, partially founded by the French Ministry of National Education, in order to design a virtual house adapted to control and STEM (Science, Technology, Engineering and Mathematics) education. The main idea, from the beginning, has been to bring a virtual house into the classroom, adapted to learners and teachers and usable from middle schools to universities. To perform this goal, a free soft PLC called CONNECT I/O, enables to connect HOME I/O to external automation technologies (e.g. PLC, Modbus TCP, OPC DA, microcontrollers...). All over the world, around 800 middle, high schools and universities use HOME I/O. Teachers and students’ feed-back have been taken into account by updating HOME I/O with new add-ons and features like: a simpler licensing system (2016), Scratch 2.0 (2017) integration and Python 3.x integration (2019). This paper presents these new features and a selection of some innovative pedagogical applications performed by teachers from middle school, high school, university, and even primary school! HOME I/O seems to prove that it is possible to have one simulation tool adapted to different levels of training and enabling original pedagogical approaches: flipped classroom, pedagogic digital twin, learning from errors, projects…}
}
@article{VILORIA20191225,
title = {An intelligent approach for the design and development of a personalized system of knowledge representation},
journal = {Procedia Computer Science},
volume = {151},
pages = {1225-1230},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.176},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930643X},
author = {Amelec Viloria and Omar Bonerge {Pineda Lezama}},
keywords = {adaptive hypermedia, ontologies, knowledge representation, user modeling, interface design tools, teaching on the web, algorithm for advanced cluster vector page ranking},
abstract = {This article proposes a generic presentation system for hypermedia systems of adaptive teaching that is highly independent from the representation of domain knowledge and the application state maintenance. Generality is achieved by providing an application framework for the definition of ontologies that best fit a domain or a specific author. The presentation of the pages to be generated is described in terms of classes and relationships of the ontology. For this purpose, a web page ranking algorithm based on automatic learning is used, specifically, the algorithm for Advanced Cluster Vector Page Ranking (ACVPR). This algorithm provides the user a powerful meta-search tool that presents a ranking order of the web page to quickly meet custom needs, especially when the search is erroneous or incomplete.}
}
@article{BAUER2018147,
title = {Integration of Industrie 4.0 in Lean Manufacturing Learning Factories},
journal = {Procedia Manufacturing},
volume = {23},
pages = {147-152},
year = {2018},
note = {“Advanced Engineering Education & Training for Manufacturing Innovation”8th CIRP Sponsored Conference on Learning Factories (CLF 2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918304803},
author = {Harald Bauer and Felix Brandl and Christopher Lock and Gunther Reinhart},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Industrie 4.0 is referred to as an umbrella term for various digital concepts e.g. IoT, CPS, Big Data, Data Analytics, Digital Twin, Digital Shadow, HRC, etc. Said concepts promise new potentials for production planning and steering (PPS) optimization. In particular, data availability is an enabler for an efficiency increase in PPS. Managers of lean manufacturing systems question how to integrate these new possibilities into the existing philosophy and optimization projects. It is currently uncertain whether Industrie 4.0 approaches replace or revive lean manufacturing. Within the iwb’s learning factory, we illustrate lean and Industrie 4.0 as complementary approaches by postulating five theories concerning their interaction. This paper presents the introduction of Industrie 4.0 into the program of the learning factory by proposing two integrated teaching. The concept was successfully implemented within the iwb’s learning factory.}
}
@article{LOEKEN201862,
title = {Design Principles Behind the Construction of an Autonomous Laboratory-Scale Drilling Rig},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {8},
pages = {62-69},
year = {2018},
note = {3rd IFAC Workshop on Automatic Control in Offshore Oil and Gas Production OOGP 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.06.356},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318306864},
author = {Erik A. Loeken and Alexander Trulsen and Andrew M. Holsaeter and Ekaterina Wiktorski and Dan Sui and Robert Ewald},
keywords = {Drilling Automation, ROP Optimization, Modeling, Fault Detection, Drill String Dynamics},
abstract = {In recent years, hot topics such as digitalization, machine learning, digital twin and big data have evolved from being envisions on the paper to state of art solutions, expected to revolutionize drilling efficiency in the industry. Drilling automation tomorrow is all about exploiting the current state of technologies available to the entire operation of drilling a well. Not only can drilling automation limit costs and reduce the risk to rig personnel and the environment, but they also give access to locations of considerable potential that previously have been regarded unsafe or uneconomical to operate in. There are however some challenges in keeping up with the ever-increasing pace of the development. For one, testing of novel and innovative solutions is often very expensive because of non-productive rig time during implementation, trial runs and data evaluation. Also, the modern technologies require extensive R&D before on-site testing can even commence. While on land-rigs, some of these costs and risks can be greatly minimized, many offshore solutions lack that luxury. This paper presents an overview of the design principles that go into the construction of a fully autonomous laboratory-scale drilling rig at the University of Stavanger. It aims at describing 1) the engineering principles involved to resemble full-scale drilling operations on the laboratory scale, 2) design considerations and components, 3) component requirements for the rig, 4) control system algorithms for real-time optimization of drilling parameters and detection and handling of drilling anomalies, 5) development of drilling models (drill string dynamics, bit-vibration, etc.) and 6) benefits and future work with the laboratory-scale system. Some of the concepts that are presented in this paper have yet to be implemented during 2018.}
}
@article{UMEDA2020325,
title = {Exercise of digital kaizen activities based on ‘digital triplet’ concept},
journal = {Procedia Manufacturing},
volume = {45},
pages = {325-330},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920310635},
author = {Yasushi Umeda and Jun Ota and Shouhei Shirafuji and Fumio Kojima and Masahiro Saito and Hiroki Matsuzawa and Takuji Sukekawa},
keywords = {learning factory, digital triple, kaizen, cyber physical systems, manufacturing system},
abstract = {For supporting kaizen (continuous improvement) activities typically observed in Japanese manufacturing companies, we are developing the concept of ‘Digital Triple’ as an extension of Digital Twin. Digital Triplet contains intelligent activity world in addition to the cyber world and the physical world and emphasizes the ability of knowledge-based support for manufacturing system engineers. At the same time, we are developing an education program based on Digital Triplet. This program contains an exercise in which students execute kaizen with a prototype CPS system of a learning factory. This paper reports the contents of this exercise in relation with Digital Triplet and describes the results of two trial sessions of this exercise.}
}
@article{GAO2018107,
title = {On the teaching complexity of linear sets},
journal = {Theoretical Computer Science},
volume = {716},
pages = {107-123},
year = {2018},
note = {Special Issue on ALT 2015},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2017.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S030439751730854X},
author = {Ziyuan Gao and Hans Ulrich Simon and Sandra Zilles},
keywords = {Teaching complexity, Teaching dimension, Recursive teaching dimension, Linear sets},
abstract = {Linear sets are the building blocks of semilinear sets, which are in turn closely connected to automata theory and formal languages. Prior work has investigated the learnability of linear sets and semilinear sets in three models – Valiant's PAC-learning model, Gold's learning in the limit model, and Angluin's query learning model. This paper considers teacher–learner models of learning families of linear sets, in which a benevolent teacher presents a set of labelled examples to the learner. First, we study the classical teaching model, in which a teacher must successfully teach any consistent learner. Second, we will apply a generalisation of the recently introduced recursive teaching model to several infinite classes of linear sets, and show that thus the maximum sample complexity of teaching these classes can be drastically reduced compared to classical teaching. To this end, a major focus of the paper will be on determining two relevant teaching parameters, the teaching dimension and recursive teaching dimension, for various families of linear sets.}
}
@article{GARRIDO20191814,
title = {Integration of automatic generated simulation models, machine control projects and management tools to support whole life cycle of industrial digital twins.},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1814-1819},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.465},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319314466},
author = {J. Garrido and J. Sáez},
keywords = {Simulation, Industry Automation, Integration, CASE, Maintenance},
abstract = {The paper presents a framework of automatic generation of industrial digital twins. These digital twins will be suitable to support preliminary design phases of systems development, but also to support next phases of detailed designs implementation and systems running phases. These digital twin allow, from the preliminary designing phase, to generate a complete simulation of the target industrial system. But, at the same time, and without the need to develop and add any subsequent code, they should be a valuable support for the phases and tasks of exploitation: maintenance, machine or system learning, etc. The problem is that the requirements for first development phases are much more generic than those for later phases. For this reason, instead of incorporating specificities in the simulation system, the framework takes advantage of the applications which are being developed for the implementation of the real system. In these applications (the control program and the decisions and the high level management system), the specificities have had to be taken into account. The system has been specialized in industrial transportation and warehouse systems which, although have a finite number or building objects, they have an infinite set of final configurations, very different one from each other. The paper presents an evaluation of current simulation platforms suitable to be used as part of the framework, and the digital twin industrial system generation framework itself. An example of application is as well presented.}
}
@article{LUNDQVIST201924,
title = {The burden of smartness: Teacher's pet and classmates’ teasing in a Danish classroom},
journal = {Linguistics and Education},
volume = {52},
pages = {24-32},
year = {2019},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2019.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0898589817303145},
author = {Ulla Lundqvist},
keywords = {Classroom discourse, Social inequity, Student identities, Smartness, Linguistic ethnography, Social identification},
abstract = {Schools are sites of negotiation of what it means to be ''smart", and which students are viewed as smart. This is a pertinent problem for educational scholars, teacher educators, and teachers, because struggles concerning smartness foster social inequity. While much research accentuates the inequity that occurs when those students who do not fit the “smart” category are marginalized, the inequities that emerge when teachers prefer the smart student have not received much scholarly attention. Drawing on linguistic ethnographic fieldwork in a primary school in Copenhagen, Denmark, this paper explores how one student, over the course of two years and two months, comes to inhabit the “smart” role, and must then cope with being favoured by the teachers and ostracised by peers. Dual pressures such as these have implications for education and research.}
}
@article{CATAL201999,
title = {Aligning Education for the Life Sciences Domain to Support Digitalization and Industry 4.0},
journal = {Procedia Computer Science},
volume = {158},
pages = {99-106},
year = {2019},
note = {3rd WORLD CONFERENCE ON TECHNOLOGY, INNOVATION AND ENTREPRENEURSHIP"INDUSTRY 4.0 FOCUSED INNOVATION, TECHNOLOGY, ENTREPRENEURSHIP AND MANUFACTURE" June 21-23, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311901},
author = {Cagatay Catal and Bedir Tekinerdogan},
keywords = {Education, Internet of Things (IoT), Industry 4.0, data analytics, machine learning},
abstract = {Emerging technologies like Internet of Things, Data Science, Deep Learning, Augmented Reality, Edge Computing, and Digital Twins are bringing new opportunities, challenges, and solutions for many domains including agriculture, plant sciences, animal sciences, food sciences, and social sciences. These disruptive technologies are at the center of the fourth industrial revolution, but are we ready yet to educate and prepare new generations to help society, science, and humanity adapt them? How can we change the current curriculum to reflect these technological innovations? How can we help the new generation to develop not only left-brain skills but also right-brain skills? The Netherlands is the second largest food exporter in the world after the United States and the agricultural related exports generated €45 Billion in 2018 for the economy. R&D in Dutch companies and innovation in universities in the Netherlands play an important and active role in this context. In this paper, we provide a general framework for supporting education in the context of Industry 4.0. We adopt the case study of Wageningen University at which we were actively involved in designing and customizing academic courses related to Industry 4.0. Wageningen University, which has the highest rank in the field of Agriculture & Forestry according to influential university rankings and has a rank 59 according to Times Higher Education, is traditionally a life science university but has taken also an active strategy for aligning with the developments in IT and Artificial Intelligence. Apart from the content-wise shift, skills such as critical thinking, creativity, and problem-solving are addressed by applying project-based evaluations. We discuss the lessons learned and address the issues related to Industry 4.0 and education.}
}
@article{DING2019106957,
title = {Compressing CNN-DBLSTM models for OCR with teacher-student learning and Tucker decomposition},
journal = {Pattern Recognition},
volume = {96},
pages = {106957},
year = {2019},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0031320319302547},
author = {Haisong Ding and Kai Chen and Qiang Huo},
keywords = {Optical character recognition, CNN-DBLSTM Character model, Model compression, Teacher-student learning, Tucker decomposition},
abstract = {Integrated convolutional neural network (CNN) and deep bidirectional long short-term memory (DBLSTM) based character models have achieved excellent recognition accuracies on optical character recognition (OCR) tasks, along with large amount of model parameters and massive computation cost. To deploy CNN-DBLSTM model in products with CPU server, there is an urgent need to compress and accelerate it as much as possible, especially the CNN part, which dominates both parameters and computation. In this paper, we study teacher-student learning and Tucker decomposition methods to reduce model size and runtime latency for CNN-DBLSTM based character model for OCR. We use teacher-student learning to transfer the knowledge of a large-size teacher model to a small-size compact student model, followed by Tucker decomposition to further compress the student model. For teacher-student learning, we design a novel learning criterion to bring in the guidance of succeeding LSTM layer when matching the CNN-extracted feature sequences of the large teacher and small student models. Experimental results on large scale handwritten and printed OCR tasks show that, using teacher-student learning alone achieves 9.90 ×  footprint reduction and 15.23 ×  inference speedup yet without degrading recognition accuracy. Combined with Tucker decomposition method, we can compress and accelerate the model further. The decomposed model achieves 11.89 ×  footprint reduction and 22.16 ×  inference speedup while suffering no or only a small recognition accuracy degradation against the large-size baseline model.}
}
@article{BORANGIU2019150,
title = {Digital transformation of manufacturing through cloud services and resource virtualization},
journal = {Computers in Industry},
volume = {108},
pages = {150-162},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519300107},
author = {Theodor Borangiu and Damien Trentesaux and André Thomas and Paulo Leitão and Jose Barata},
keywords = {Digital manufacturing, Cloud services, Resource virtualization, Cloud manufacturing, Holonic manufacturing control, Multi-agent system, Real-time data analysis, Machine learning, Digital twin, Cyber physical production system, Industrial internet of things},
abstract = {This editorial introduces the special issue in the Elsevier journal Computers in Industry that analyses how the digital transformation of manufacturing is speeded up by two important drivers: cloud services and resource virtualization, which are vital for implementing the main building blocks - Cyber Physical Production Systems and Industrial Internet of Things - in the “Industry of the future” framework. The context of this special issue is firstly presented, with a specific focus on the federative concept of Industry 4.0. A framework characterizing research activities led in the field of the digital transformation of manufacturing processes and systems is then introduced. This framework is used to present and position the 12 papers composing the special issue. Perspectives are finally introduced as a guideline for future work in the digital transformation of manufacturing through cloud services and resource virtualization.}
}
@article{NIELSEN2019824,
title = {Low-Cost 3D Scanning in a Smart Learning Factory},
journal = {Procedia Manufacturing},
volume = {38},
pages = {824-831},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.163},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920301645},
author = {Christian P. Nielsen and Ali A. Malik and David G. Hansen and Arne Bilberg},
keywords = {Digital Twins, Point Cloud, Learning Factory, Laser Scanning},
abstract = {With the increased focus over the recent years on digitalizing the factory, 3D scanning has become more and more popular. Acquiring a point cloud of a given factory has shown several benefits, such as better documentation, realistic simulation models, collision detection of materials, visualization of factory development, and more. This paper investigates the testing of a developed low‐cost 3D scanner in a Smart Learning Factory based on parameters identified in literature. Furthermore, the paper compares the developed solution to a commercially available solution. This comparison indicates possible application areas for the developed low-cost solution. The 3D scanner is based on the Microsoft Kinect and a developed hardware platform combined with custom software for acquiring 360° point clouds. A discussion on the acquired results, as well as future works on the developed solution finalizes the paper.}
}
@article{AFININORMADHI2019168,
title = {Identification of personal traits in adaptive learning environment: Systematic literature review},
journal = {Computers & Education},
volume = {130},
pages = {168-190},
year = {2019},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0360131518303026},
author = {Nur Baiti {Afini Normadhi} and Liyana Shuib and Hairul Nizam {Md Nasir} and Andrew Bimba and Norisma Idris and Vimala Balakrishnan},
keywords = {Cooperative/collaborative learning, Intelligent tutoring systems, Interactive learning environments, Navigation},
abstract = {An adaptive learning environment provides personalised information to the learner through self-directed study. An adaptive learning environment model can be subdivided into a learner model, domain model, instructional model and adaptive engine. Personal traits comprise part of the components in a learner model and can be identified either explicitly or implicitly in an adaptive learning environment. In such an environment, the e-learning system should adapt to a learner's needs. However, even though academic research on adaptive learning environments has increased, the field lacks a comprehensive literature analysis of learners' personal traits in these environments. This study conducts a systematic literature review to identify the most commonly used personal traits in modelling the learner and the existing techniques suitable for identifying personal traits in an adaptive learning environment. A total of 140 articles spanning the years 2010–2017 are initially reviewed, from which 78 are selected based on the inclusion and exclusion criteria relevant to this study. This study provides an overview of learners' personal traits and the techniques used to identify them to provide a basis for improving adaptive learning environments. The findings indicate that most of the previous works used a learning style from the cognition learning domain category to model individual personal traits, while the computer-based detection technique was commonly applied to identify a learner's personal traits in adaptive learning environments. This study reveals the common learner characteristics used to develop learner models and the techniques for implementing such models. The findings of this paper can guide other researchers to recognise various personal traits and the identification technique for further studies, as well as assist developers in the development of the adaptive learning system.}
}
@article{SHAN2020113198,
title = {Learn#: A Novel incremental learning method for text classification},
journal = {Expert Systems with Applications},
volume = {147},
pages = {113198},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113198},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420300245},
author = {Guangxu Shan and Shiyao Xu and Li Yang and Shengbin Jia and Yang Xiang},
keywords = {Learn#, Incremental learning, Reinforcement learning},
abstract = {Deep learning is an effective method for extracting the underlying information in text. However, it performs better on closed datasets and is less effective in real-world scenarios for text classification. As the data is updated and the amount of data increases, the models need to be retrained, in what is often a long training process. Therefore, we propose a novel incremental learning strategy to solve these problems. Our method, called Learn#, includes four components: a Student model, a reinforcement learning (RL) module, a Teacher model, and a discriminator model. The Student models first extract the features from the texts, then the RL module filters the results of multiple Student models. After that, the Teacher model reclassifies the filtered results to obtain the final texts category. To avoid increasing the Student models unlimitedly as the number of samples increases, the discriminator model is used to filter the Student models based on their similarity. The Learn# method has the advantage of a shorter training time than the One-Time model, because it only needs to train a new Student model each time, without changing the existing Student models. Furthermore, it can also obtain feedback during application and tune the models parameters over time. Experiments on different datasets show that our method for text classification outperforms many traditional One-Time methods, reducing training time by nearly 80%.}
}
@incollection{WANG202033,
title = {Chapter 2 - Digital twin driven conceptual design},
editor = {Fei Tao and Ang Liu and Tianliang Hu and A.Y.C. Nee},
booktitle = {Digital Twin Driven Smart Design},
publisher = {Academic Press},
pages = {33-66},
year = {2020},
isbn = {978-0-12-818918-4},
doi = {https://doi.org/10.1016/B978-0-12-818918-4.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128189184000026},
author = {Yuchen Wang and Ang Liu and Fei Tao and A.Y.C. Nee},
keywords = {Digital twin, conceptual design, concept generation, design evaluation},
abstract = {In comparison to conventional conceptual design, capability of big data analysis, machine learning ability, and physical–virtual simulations, the digital twin (DT) assists designers to obtain earlier stage feedbacks, more reliable concept generations and evaluations. This chapter will envision the evolution of conceptual design supported by DT technology. Conceptual design methodologies are introduced first, including axiomatic design, systematic design, and function–behavior–structure ontology. Mainly based on the axiomatic design theory, the chapter clarifies how DT technology assists the functional modeling, concept generation, and concept evaluation. In addition, contradiction resolution and constraints management are also part of the discussion. The DT-driven conceptual design is illustrated with a robot vacuum cleaner as an example in each procedure.}
}
@article{JUNMIN201888,
title = {Research on the Construction and Application of Individual Learner Model},
journal = {Procedia Computer Science},
volume = {131},
pages = {88-92},
year = {2018},
note = {Recent Advancement in Information and Communication Technology:},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918305647},
author = {Ye Jun-min and Xu Song and Luo Da-Xiong and Wang Zhi-Feng and Huang Peng-Wei and Xu Chen},
keywords = {individual learner model, learning evaluation, evaluation model},
abstract = {In the background of extensive attention and research on education big data and learning analysis. The model construction method, the statistical analysis method, the empirical method, the inductive method and the deductive method are adopted in this research to probe in depth the problem of learning effect evaluation based on individual learner model: the learning assessment framework for individual learner model is studied, and the evaluation model of learning effect and evaluation index system are summarized of each sub-model, an empirical study is conducted. Compared with the actual learning effect of the learner, the method proposed in this paper is effective [1].}
}
@article{IRIONDO202017592,
title = {A proposal to introduce digitalization technologies within the automation learning process},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17592-17597},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2674},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320334352},
author = {N. Iriondo and D. Orive and O. Casquero and M. Marcos},
keywords = {education, digital factory, product lifecycle management, digital twin, virtual commissioning},
abstract = {Although the digital factory (DF) concept has raised high expectations since its inception, it is still missing industrial impact. One of the problems attributed to this issue is the lack of education curricula for enhancing the related digital competences of the future professionals. Higher education institutions, as major stakeholders in education, should introduce the new technologies for DF in practical courses. However, it is difficult to deal with the complexity of those technologies in a time-limited environment such us a bachelor or a master course. Instead of providing complete knowledge, this paper proposes to focus on the methodological aspects that allow students to acquire the skills needed to handle those technologies. Specifically, this paper illustrates this approach for teaching virtual commissioning (VC) within the automation learning process. The goal is to show the students how to use powerful industrial tools for performing VC through a set of methodological steps that help students manage the complexity of the VC process regardless of the specific tools used for it.}
}
@article{CIANCIOSA2020106671,
title = {Machine learning for analysis of atomic spectral data},
journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
volume = {240},
pages = {106671},
year = {2020},
issn = {0022-4073},
doi = {https://doi.org/10.1016/j.jqsrt.2019.106671},
url = {https://www.sciencedirect.com/science/article/pii/S0022407319302365},
author = {M. Cianciosa and K.J.H. Law and E.H. Martin and D.L. Green},
abstract = {Physics based forward models are the basis on which many experimental diagnostics are interpreted. For some diagnostics, models can be computationally expensive which precludes their use in real time analysis. Reduced models have the potential to capture sufficient physics thereby enabling the desired real time analysis. Using statistical inference and machine learning techniques the application of reduced models for inversion of atomic spectral data used to diagnose magnetic fields in a plasma will be examined. Two approaches are considered, (a) a reduction of the forward model where traditional inversion can be performed on the proxy model, and (b) a reduction of the direct inverse where parameters are a function of measured signal. The resulting inversion is sufficiently fast to be utilized in an online context for digital twinning, and ultimately real-time prediction, design, and control of plasma systems, such as tokamaks. These methods will be demonstrated on both simulated and experimentally measured data.}
}
@incollection{AUGUSTINE202079,
title = {Chapter Four - The industry use cases for the Digital Twin idea},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {79-105},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300610},
author = {Peter Augustine},
keywords = {Digital Twin, Use cases of Digital Twin, IoT, IIot, Smart cities, Virtual replicas, Artificial intelligence, Data analytics},
abstract = {Digital Twin Technology has taken the place in top 10 strategic technology trends in 2017 termed by Gartner Inc. Digital Twin concept brings out the virtual depiction or the digital representation of the real world equipment, device or system whereas the real world and the virtual world gets the highest synchronization. The digital representation of the complete life cycle of a product from its design phase to the maintenance phase will give the prophetic analysis of the problems to the business. This greatest advantage of foreseeing problems in the development of a device will give early warnings, foil downtime, cultivate novel prospects and inventing enhanced devices or gadgets for the later use at the lesser expense by means of digital representations. Indeed, these will devise a larger influence on conveying superior consumer feeling also in the enterprise. The emerging trends such as Artificial Intelligence, Machine Learning, Deep Learning, Internet of Things and Big Data used in Industry 4.0 play a vital role in Digital Twin and they are mostly adopted in the world of manufacturing, Industrial Internet of Things, and automobile business world. The penetration, wide coverage and the advancement of the Internet of Things in real-world have elevated the power of Digital Twins more economical and reachable for the world of various businesses.1.Manufacturing: Digital Twin has brought out the change in the existing manner of the manufacturing segment. Digital Twins have a substantial influence on the design of products and their manufacturing and maintenance. Because of its influence the manufacturing more competent and augmented while dropping throughput times.2.Industrial IoT: Integrating digital twin with industrial firms will facilitate the activities such as monitoring, tracking and controlling industrial systems in digital means. We can potentially experience the power of digital twin since it captures environmental data such as locality, settings of the devices, financial frameworks, etc., other than the operational data, which benefits in foreseeing the forthcoming operations and incongruities.3.Healthcare: Since the healthcare sector demands higher accuracy in diagnosis and treatment, with the important data from IoT, digital twins can play a vital role by reducing the expense for the patient, precautionary alerts to avoid health deterioration and giving tailored health support system. This will be great support especially in developing countries like India.4.Smart cities: Digital Twin coupled with IoT data can augment the efficient planning of the smart city and execution of its building by supplementing financial progress, effectual administration of resources, lessening of environmental impression and escalate the complete worth of a resident's life. The digital twin prototypical can aid city organizers and legislators in the smart city planning by retrieving the visions from numerous sensor networks and smart systems. The information received from the digital twins supports them in reaching well-versed choices concerning the future as well.5.Automobile: Automobile industry can get voluminous benefits out of Digital Twins for producing the simulated framework of a coupled vehicle. It retrieves the behavioral and functional information of the vehicle and services in examining the inclusive performance efficiency of the vehicle as well as the features connected along with it. Digital Twin also supports in supplying a justly enhance support and service for the consumers.6.Retail: Alluring client satisfaction is a fundamental factor in the merchandising world. Digital twin employment can play a key role in supplementing the retail customer experience by forming virtual twins for customers and modeling fashions for them on it. Digital Twins also supports enhanced planning of stock maintenance, safekeeping procedures, and human resource administration in an augmented means.}
}
@article{ZHANG2020105247,
title = {Deep learning-enabled intelligent process planning for digital twin manufacturing cell},
journal = {Knowledge-Based Systems},
volume = {191},
pages = {105247},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105247},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119305611},
author = {Chao Zhang and Guanghui Zhou and Junsheng Hu and Jing Li},
keywords = {Intelligent process planning, Deep learning, Residual networks, Evaluation twin, Digital twin manufacture cell},
abstract = {The transition to intelligent manufacturing provides a fulcrum for the revolution of product lifecycle like design, manufacturing and maintenance, so does it for process planning. Specifically, digital twin manufacturing cell (DTMC) is regarded as a new means of and also a basic unit for implementing intelligent manufacturing. Incorporating process planning in DTMC could improve the integrity of DTMC and enhance the feasibility of process planning. Consequently, this paper proposes a deep learning-enabled framework for intelligent process planning towards DTMC. Firstly, a process knowledge reuse network (PKR-Net) that takes deep residual networks as base architecture is embedding into the framework, which could understand design intents expressed in a drawing or a 3D computer-aided design (CAD) model via its views and automatically retrieve relevant knowledge for the quick generation of theorical processes. Then, an evaluation twin is constructed to transform the theorical processes into practical operations and produce an optimal process plan. Finally, a test bed of the framework is constructed and the experimental results demonstrate the feasibility and effectiveness of the approach.}
}
@article{QIAO20191388,
title = {Digital Twin for Machining Tool Condition Prediction},
journal = {Procedia CIRP},
volume = {81},
pages = {1388-1393},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.049},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119306638},
author = {Qianzhe Qiao and Jinjiang Wang and Lunkuan Ye and Robert X. Gao},
keywords = {tool system, digital twin, deep learning},
abstract = {Digital twin introduces new opportunities for predictive maintenance of manufacturing machines which can consider the influence of working condition on cutting tool and contribute to the understanding and application of the predicted results. This paper presents a data-driven model for digital twin, together with a hybrid model prediction method based on deep learning that creates a prediction technique for enhanced machining tool condition prediction. First, a five-dimensional digital twin model is introduced that highlights the performance of the data analytics in model construction. Next, a deep learning technique, termed Deep Stacked GRU (DSGRU), is demonstrated that enables system identification and prediction. Experimental studies using vibration data measured on milling machine tool have shown the effectiveness of the presented digital twin model for tool wear prediction.}
}
@article{ZANKER2019160,
title = {Measuring the impact of online personalisation: Past, present and future},
journal = {International Journal of Human-Computer Studies},
volume = {131},
pages = {160-168},
year = {2019},
note = {50 years of the International Journal of Human-Computer Studies. Reflections on the past, present and future of human-centred technologies},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2019.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S107158191930076X},
author = {Markus Zanker and Laurens Rook and Dietmar Jannach},
keywords = {Web personalisation, Adaptive systems, Recommender systems},
abstract = {Research on understanding, developing and assessing personalisation systems is spread over multiple disciplines and builds on methodologies and findings from several different research fields and traditions, such as Artificial Intelligence (AI), Machine Learning (ML), Human–Computer Interaction (HCI), and User Modelling based on (applied) social and cognitive psychology. The fields of AI and ML primarily focus on the optimisation of personalisation applications, and concentrate on creating ever more accurate algorithmic decision makers and prediction models. In the fields of HCI and Information Systems, scholars are primarily interested in the phenomena around the use and interaction with personalisation systems, while Cognitive Science (partly) delivers the theoretical underpinnings for the observed effects. The aim and contribution of this work is to put together the pieces about the impact of personalisation and recommendation systems from these different backgrounds in order to formulate a research agenda and provide a perspective on future developments.}
}
@incollection{JUNG2020721,
title = {The Role of Process Engineering in the Digital Transformation},
editor = {Sauro Pierucci and Flavio Manenti and Giulia Luisa Bozzano and Davide Manca},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {48},
pages = {721-726},
year = {2020},
booktitle = {30th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-823377-1.50121-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823377150121X},
author = {Norbert Jung},
keywords = {Process Simulation, Digital Transformation, Unified Engineering},
abstract = {The process engineering discipline has been largely excluded from Digital Transformation trends. The objective of this presentation is to provide an overview of the obstacles to Digital Transformation for the process discipline and explain how these can be overcome. A special emphasis will be given on the role of the process simulation tool (white box modelling) as a catalyst for transformational change. The presentation will examine several challenges specific to process engineering:1)Process simulators are divided into single-purpose point solutions. Separate models may be created for process design, control strategy design, operator training simulation, performance monitoring and online optimization.2)Process simulators are typically poorly integrated into engineering workflows beyond the process world, and if so, with a single directional information flow.3)Legacy process simulators are overloaded with niche features and functions only usable by experts.4)The potential benefits of Artificial Intelligence and Machine Learning for process engineering are not widely understood. Industry stakeholders see the Digital Twin as the most important building block for Digital Transformation of the process industries. While legacy simulators are well-suited to accurately simulate processes, their decades-old architectures mean they are not ideal to serve the entire plant lifecycle and support the Digital Transformation. We will use the AVEVA SimCentral Simulation Platform4 as an example of how the identified obstacles can be overcome with a next generation process simulator. Case examples from leading companies will be outlined.}
}
@incollection{PUSHPA202051,
title = {Chapter Three - Using fog computing/edge computing to leverage Digital Twin},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {51-77},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300464},
author = {J. Pushpa and S.A. Kalyani},
keywords = {Financial benefit, Tangible benefits, Connectivity model, Product management, Digital twin, Gateway, Load balancer, Propeller, Projection, Analyzer},
abstract = {Recreating a real-world entity as a virtual object has already been studied in most of the field, but making the cloned object more intelligent and healer of real-time physical object will give a new vision on technology. Digital Twin is fitting into the above statement. It uses a combination of machine learning, artificial intelligence, the IoT, and big data to evolve as a ubiquitous solution for all kinds of issues. Digital Twin can build on many form based on the requirement which is basically designed to resolve the challenges of the real world entity. Digital Twin is not limited to solving issues with standalone systems, single entities and machinery problems; it is also suitable for all kinds of data management and controlling issues. Digital Twin can extend its reach by embedding with edge or fog computing which can reduce connectivity and latency issues in networks. In this chapter, methodologies for leveraging Digital Twin using fog/edge computing will be discussed along with suitable use cases, such as wind turbines, product management, healthcare centers, and so on. We also discuss the financial benefits, tangible benefits, and connectivity model.}
}
@article{CORADDU2019106063,
title = {Data-driven ship digital twin for estimating the speed loss caused by the marine fouling},
journal = {Ocean Engineering},
volume = {186},
pages = {106063},
year = {2019},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2019.05.045},
url = {https://www.sciencedirect.com/science/article/pii/S0029801819302616},
author = {Andrea Coraddu and Luca Oneto and Francesco Baldi and Francesca Cipollini and Mehmet Atlar and Stefano Savio},
keywords = {Hull and propeller maintenance, Fouling, Condition based maintenance, ISO 19030, Digital twin, Data-Driven Models, Deep learning},
abstract = {Shipping is responsible for approximately the 90% of world trade leading to significant impacts on the environment. As a consequence, a crucial issue for the maritime industry is to develop technologies able to increase the ship efficiency, by reducing fuel consumption and unnecessary maintenance operations. For example, the marine fouling phenomenon has a deep impact, since to prevent or reduce its growth which affects the ship consumption, costly drydockings for cleaning the hull and the propeller are needed and must be scheduled based on a speed loss estimation. In this work a data driven Digital Twin of the ship is built, leveraging on the large amount of information collected from the on-board sensors, and is used for estimating the speed loss due to marine fouling. A thorough comparison between the proposed method and ISO 19030, which is the de-facto standard for dealing with this task, is carried out on real-world data coming from two Handymax chemical/product tankers. Results clearly show the effectiveness of the proposal and its better speedloss prediction accuracy with respect to the ISO 19030, thus allowing reducing the fuel consumption due to fouling.}
}
@article{XU201997,
title = {Formulating a learner model for evaluating construction workers’ learning ability during safety training},
journal = {Safety Science},
volume = {116},
pages = {97-107},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518313201},
author = {Sheng Xu and Mengge Zhang and Lei Hou},
keywords = {Safety training, Personalised training, Learner model, Learning process},
abstract = {The improvement of safety performance of construction workers heavily lies in safety training, and training technologies, materials and organisations. However, the traditional one-size-fit-all safety training does not cater for the needs of heterogeneous workers. Personalised training may proffer a better solution for heterogeneous workers in the construction sector. To understand the rationale of personalised training, this study formulated a learner model that can capture and evaluate the learning abilities of individual workers. Methodically, a survey on 170 construction workers was conducted, and evidenced that they were heterogeneous in safety training motivation, established knowledge, and emotions during the knowledge acquisition process; and were vulnerable to the model effect and convenience effect during the knowledge application process. The results also showed that workers generally perceived safety training as a mandatory requirement, rather than inherently motivated; emotional changes was the most influencing factor in the knowledge acquisition process; about 40% of the workers were strongly vulnerable to the model effect and convenience effect; and 18% of the workers needed to improve their ability of knowledge acquisition and knowledge application. The correlation analysis and t-test indicated that age, year of experience, trade, project type, organisation type and site environment influenced workers’ learning characteristics and abilities; which lead to the varied levels of safety understanding, awareness and performance. It was also concluded that the construction workers had unique characteristics in their safety learning process and the concept of adapted safety learning could potentially improve the efficiency of safety training.}
}
@article{UMEDA2019363,
title = {Development of an education program for digital manufacturing system engineers based on ‘Digital Triplet’ concept},
journal = {Procedia Manufacturing},
volume = {31},
pages = {363-369},
year = {2019},
note = {Research. Experience. Education. 9th Conference on Learning Factories 2019 (CLF 2019), Braunschweig, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.03.057},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919304214},
author = {Yasushi Umeda and Jun Ota and Fumio Kojima and Masahiro Saito and Hiroki Matsuzawa and Takuji Sukekawa and Akihide Takeuchi and Kazuya Makida and Shouhei Shirafuji},
keywords = {Cyber Physical Systems, digital manufacturing system engineers, education program, digital triplet},
abstract = {In the Industry 4.0 era, cyber physical manufacturing systems (CPPS) has started to change activities of manufacturing system engineers into CPS based ones. In typical Japanese factories, manufacturing system engineers are always stationed at the shop floor and continuously improve manufacturing systems with workers. For supporting such engineers’ activities, we are developing the concept of ‘Digital Triplet’ as an extension of Digital Twin. Digital Triplet consists of intelligent activity world in addition to the cyber world and the physical world and supports manufacturing system engineers in developing engineering processes with the cyber and physical worlds. Based on this, we are developing an education program. In this paper, we first describe the concept of Digital Triplet. Next, after explaining the overview of the education program, we introduce a course in which trainees (mainly novice engineers) execute ‘Kaizen’ with a prototype CPS system of a learning factory.}
}
@article{LAUZERAL201995,
title = {A model order reduction approach to create patient-specific mechanical models of human liver in computational medicine applications},
journal = {Computer Methods and Programs in Biomedicine},
volume = {170},
pages = {95-106},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718314676},
author = {Nathan Lauzeral and Domenico Borzacchiello and Michael Kugler and Daniel George and Yves Rémond and Alexandre Hostettler and Francisco Chinesta},
keywords = {Real-time simulation, Patient-specific modeling, Data-based modeling, Statistical shape analysis, Finite element modeling, Human liver},
abstract = {Background and objective
This paper focuses on computer simulation aspects of Digital Twin models in the medical framework. In particular, it addresses the need of fast and accurate simulators for the mechanical response at tissue and organ scale and the capability of integrating patient-specific anatomy from medical images to pinpoint the individual variations from standard anatomical models.
Methods
We propose an automated procedure to create mechanical models of the human liver with patient-specific geometry and real time capabilities. The method hinges on the use of Statistical Shape Analysis to extract the relevant anatomical features from a database of medical images and Model Order Reduction to compute an explicit parametric solution for the mechanical response as a function of such features. The Sparse Subspace Learning, coupled with a Finite Element solver, was chosen to create low-rank solutions using a non-intrusive sparse sampling of the feature space.
Results
In the application presented in the paper, the statistical shape model was trained on a database of 385 three dimensional liver shapes, extracted from medical images, in order to create a parametrized representation of the liver anatomy. This parametrization and an additional parameter describing the breathing motion in linear elasticity were then used as input in the reduced order model. Results show a consistent agreement with the high fidelity Finite Element models built from liver images that were excluded from the training dataset. However, we evidence in the discussion the difficulty of having compact shape parametrizations arising from the extreme variability of the shapes found in the dataset and we propose potential strategies to tackle this issue.
Conclusions
A method to represent patient-specific real-time liver deformations during breathing is proposed in linear elasticity. Since the proposed method does not require any adaptation to the direct Finite Element solver used in the training phase, the procedure can be easily extended to more complex non-linear constitutive behaviors - such as hyperelasticity - and more general load cases. Therefore it can be integrated with little intrusiveness to generic simulation software including more sophisticated and realistic models.}
}
@article{FAN2021102049,
title = {Disaster City Digital Twin: A vision for integrating artificial and human intelligence for disaster management},
journal = {International Journal of Information Management},
volume = {56},
pages = {102049},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.102049},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219302956},
author = {Chao Fan and Cheng Zhang and Alex Yahja and Ali Mostafavi},
keywords = {Digital twin, Machine learning, Information flow, Disaster management},
abstract = {This paper presents a vision for a Disaster City Digital Twin paradigm that can: (i) enable interdisciplinary convergence in the field of crisis informatics and information and communication technology (ICT) in disaster management; (ii) integrate artificial intelligence (AI) algorithms and approaches to improve situation assessment, decision making, and coordination among various stakeholders; and (iii) enable increased visibility into network dynamics of complex disaster management and humanitarian actions. The number of humanitarian relief actions is growing due to the increased frequency of natural and man-made crises. Various streams of research across different disciplines have focused on ICT and AI solutions for enhancing disaster management processes. However, most of the existing research is fragmented without a common vision towards a converging paradigm. Recognizing this, this paper presents the Disaster City Digital Twin as a unifying paradigm. The four main components of the proposed Digital Twin paradigm include: multi-data sensing for data collection, data integration and analytics, multi-actor game-theoretic decision making, and dynamic network analysis. For each component, the current state of the art related to AI methods and approaches are examined and gaps are identified.}
}
@article{YERA2019395,
title = {Modelling the interactive behaviour of users with a medication safety dashboard in a primary care setting},
journal = {International Journal of Medical Informatics},
volume = {129},
pages = {395-403},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619301662},
author = {Ainhoa Yera and Javier Muguerza and Olatz Arbelaitz and Iñigo Perona and Richard N. Keers and Darren M. Ashcroft and Richard Williams and Niels Peek and Caroline Jay and Markel Vigo},
keywords = {Patient safety, Primary health care, Supervised machine learning, User modelling, Human-Computer interaction},
abstract = {Objective
To characterise the use of an electronic medication safety dashboard by exploring and contrasting interactions from primary users (i.e. pharmacists) who were leading the intervention and secondary users (i.e. non-pharmacist staff) who used the dashboard to engage in safe prescribing practices.
Materials and methods
We conducted a 10-month observational study in which 35 health professionals used an instrumented medication safety dashboard for audit and feedback purposes in clinical practice as part of a wider intervention study. We modelled user interaction by computing features representing exploration and dwell time through user interface events that were logged on a remote database. We applied supervised learning algorithms to classify primary against secondary users.
Results
We observed values for accuracy above 0.8, indicating that 80% of the time we were able to distinguish a primary user from a secondary user. In particular, the Multilayer Perceptron (MLP) yielded the highest values of precision (0.88), recall (0.86) and F-measure (0.86). The behaviour of primary users was distinctive in that they spent less time between mouse clicks (lower dwell time) on the screens showing the overview of the practice and trends. Secondary users exhibited a higher dwell time and more visual search activity (higher exploration) on the screens displaying patients at risk and visualisations.
Discussion and conclusion
We were able to distinguish the interactive behaviour of primary and secondary users of a medication safety dashboard in primary care using timestamped mouse events. Primary users were more competent on population health monitoring activities, while secondary users struggled on activities involving a detailed breakdown of the safety of patients. Informed by these findings, we propose workflows that group these activities and adaptive nudges to increase user engagement.}
}
@article{GOMISPORQUERAS2018329,
title = {Teaching technologies, attendance, learning and the optimal level of access to online materials},
journal = {Economic Modelling},
volume = {73},
pages = {329-342},
year = {2018},
issn = {0264-9993},
doi = {https://doi.org/10.1016/j.econmod.2018.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0264999317308593},
author = {Pedro Gomis-Porqueras and José A. Rodrigues-Neto},
keywords = {Absenteeism, Attendance, Education, Technology, Online policies},
abstract = {A stylized game-theoretic model explores the relationship between a lecturer and a representative student in a university class. The lecturer moves first, choosing the student's level of access to online class materials, from zero to 100%. The student observes the lecturer's choice, and then chooses whether to attend or skip class. The student has a valuable outside option during class time, but she also values learning. Out-of-class learning cannot perfectly substitute in-class learning. The student's decision depends on her outside option and the level of access to online materials chosen by the lecturer. For extreme parameter values, the student's optimal action does not depend on the lecturer's choice; in these cases there is full access to online materials, as the lecturer anticipates the student's choice. If parameters lie in a range where the lecturer's action may influence the student, then the lecturer provides the maximum level of access to online materials that still incentivizes the student to attend class. Online policies adopted by universities may affect learning outcomes, the intensity of use of online technologies, and attendance. This paper analyzes two types of university policies about the access to online materials.}
}
@incollection{LAI2020109,
title = {Chapter 4 - Digital twin driven virtual verification},
editor = {Fei Tao and Ang Liu and Tianliang Hu and A.Y.C. Nee},
booktitle = {Digital Twin Driven Smart Design},
publisher = {Academic Press},
pages = {109-138},
year = {2020},
isbn = {978-0-12-818918-4},
doi = {https://doi.org/10.1016/B978-0-12-818918-4.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818918400004X},
author = {Yiling Lai and Yuchen Wang and Robert Ireland and Ang Liu},
keywords = {Digital twin, virtual verification, product life cycle, product design},
abstract = {The creation of high-fidelity, dynamic, and self-learning virtual models, known as digital twin (DT), will play a critical role in Industry 4.0 by reflecting the whole product life cycle. This ability enables designers to determine product functionality and configuration during the initial product design stages. The integration of virtual and physical elements of the product life cycle pioneers allows designers to model performance and detect issues of real-world function in a virtual space. This process, known as DT-driven virtual verification, will facilitate a significant leap in design efficiency. This chapter proposes a DT-driven virtual verification framework model to improve product design with respect to the five stages of a product life cycle (including design, manufacturing, usage, maintenance, and end-of-life). Furthermore, as demonstration of the framework’s efficacy two case studies are conducted: the first on a commercial espresso coffee machine and the second on a 3D printer. Development and application of the framework aim to highlight the revolutionary potential and expedite development of DT technology into real-world design.}
}
@article{YE2018233,
title = {Building feedforward neural networks with random weights for large scale datasets},
journal = {Expert Systems with Applications},
volume = {106},
pages = {233-243},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418302318},
author = {Hailiang Ye and Feilong Cao and Dianhui Wang and Hong Li},
keywords = {Large scale data, Neural networks, Learning, Approximate Newton-type method},
abstract = {With the explosive growth in size of datasets, it becomes more significant to develop effective learning schemes for neural networks to deal with large scale data modelling. This paper proposes an iterative approximate Newton-type learning algorithm to build neural networks with random weights (NNRWs) for problem solving, where the whole training samples are divided into some small subsets under certain assumptions, and each subset is employed to construct a local learner model for integrating a unified classifier. The convergence of the output weights of the unified learner model is given. Experimental results on UCI datasets with comparisons demonstrate that the proposed algorithm is promising for large scale datasets.}
}
@incollection{AMBIKA2020321,
title = {Chapter Thirteen - Machine learning and deep learning algorithms on the Industrial Internet of Things (IIoT)},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {321-338},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300609},
author = {P. Ambika},
keywords = {Machine learning, Deep learning, IIoT, Digital twin, Supervised, Unsupervised, Industry 4.0, Analytics, Interoperability, Contextual analysis},
abstract = {Deep transformation and human progress is a new industrial revolution that makes “Automation of Everything.” It connects all digital interfaces, data analysis and control of the physical world through networks of computers. This key revolution promises everyone to unlock trillions of opportunities in the next decade. Human could feel massive improvements in productivity in physical and digital industries that enhances quality life of a human healthier and more sustainable community. In the world of IIoT, the creation of massive amounts of data from a various sensors is common and there is lot of challenges. This goal of this chapter is to provide a comprehensive review about Machine learning and deep learning techniques, popular algorithms, and their impact on Industrial Internet of Things. This chapter also delves use cases where machine learning is used and to gain insights from IoT data.}
}
@article{DRUSHKU201979,
title = {Interest-based recommendations for business intelligence users},
journal = {Information Systems},
volume = {86},
pages = {79-93},
year = {2019},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917307032},
author = {Krista Drushku and Julien Aligon and Nicolas Labroche and Patrick Marcel and Verónika Peralta},
keywords = {User interest, Feature construction, Clustering, BI analyses, Collaborative recommender systems},
abstract = {It is quite common these days for experts, casual analysts, executives and data enthusiasts, to analyze large datasets through user-friendly interfaces on top of Business Intelligence (BI) systems. However, current BI systems do not adequately detect and characterize user interests, which may lead to tedious and unproductive interactions. In this paper, we propose a collaborative recommender system for BI interactions, specifically designed to take advantage of identified user interests. Such user interests are discovered by characterizing the intent of the interaction with the BI system. Building on user modeling for proactive search systems, we identify a set of features for an adequate description of intents, and a similarity measure for grouping intents into coherent clusters. On top of these automatically identified interests, we build a collaborative recommender system based on a Markov model that represents the probability for a user to switch from one interest to another. We validate our approach experimentally with an in-depth user study, where we analyze traces of BI navigation. Our results are two-fold. First, we show that our similarity measure outperforms a state-of-the-art query similarity measure and yields a very good precision with respect to expressed user interests. Second, we compare our recommender system to two state-of-the-art systems to demonstrate the benefit of relying on user interests.}
}
@article{YANG201897,
title = {Study on student performance estimation, student progress analysis, and student potential prediction based on data mining},
journal = {Computers & Education},
volume = {123},
pages = {97-108},
year = {2018},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2018.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0360131518300861},
author = {Fan Yang and Frederick W.B. Li},
keywords = {Evaluation methodologies, Intelligent tutoring systems, Teaching/learning strategies, Applications in subject areas, Simulations},
abstract = {Student performance, student progress and student potential are critical for measuring learning results, selecting learning materials and learning activities. However, existing work doesn't provide enough analysis tools to analyze how students performed, which factors would affect their performance, in which way students can make progress, and whether students have potential to perform better. To solve those problems, we have provided multiple analysis tools to analyze student performance, student progress and student potentials in different ways. First, this paper formulates student model with performance related attributes and non-performance related attributes by Student Attribute Matrix (SAM), which quantifies student attributes, so that we can use it to make further analysis. Second, this paper provides a student performance estimation tools using Back Propagation Neural Network (BP-NN) based on classification, which can estimate student performance/attributes according to students' prior knowledge as well as the performance/attributes of other students who have similar characteristics. Third, this paper proposes student progress indicators and attribute causal relationship predicator based on BP-NN to comprehensively describe student progress on various aspects together with their causal relationships. Those indicators and predicator can tell how much a factor would affect student performance, so that we can train up students on purpose. Finally, this paper proposes a student potential function that evaluates student achievement and development of such attributes. We have illustrated our analysis tools by using real academic performance data collected from 60 high school students. Evaluation results show that the proposed tools can give correct and more accurate results, and also offer a better understanding on student progress.}
}
@article{EPPERLEIN2019116,
title = {Recovering Markov models from closed-loop data},
journal = {Automatica},
volume = {103},
pages = {116-125},
year = {2019},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2019.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0005109819300299},
author = {Jonathan P. Epperlein and Sergiy Zhuk and Robert Shorten},
abstract = {Situations in which recommender systems are used to augment decision making are becoming prevalent in many application domains. Almost always, these prediction tools (recommenders) are created with a view to affecting behavioural change. Clearly, successful applications actuating behavioural change, affect the original model underpinning the predictor, leading to an inconsistency. This feedback loop is often not considered in standard machine learning techniques which rely upon machine learning/statistical learning machinery. The objective of this paper is to develop tools that recover unbiased user models in the presence of recommenders. More specifically, we assume that we observe a time series which is a trajectory of a Markov chain R modulated by another Markov chain S, i.e. the transition matrix of R is unknown and depends on the current state of S. The transition matrix of the latter is also unknown. In other words, at each time instant, S selects a transition matrix for R within a given set which consists of known and unknown matrices. The state of S, in turn, depends on the current state of R thus introducing a feedback loop. We propose an Expectation–Maximisation (EM) type algorithm, which estimates the transition matrices of S and R. Experimental results are given to demonstrate the efficacy of the approach.}
}
@article{DAI2018437,
title = {Cross-modal deep discriminant analysis},
journal = {Neurocomputing},
volume = {314},
pages = {437-444},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.09.059},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217315783},
author = {Xue-mei Dai and Sheng-Gang Li},
keywords = {Cross-modal analysis, Cross-media retrieval, Discriminant analysis, Deep learning},
abstract = {Cross-modal analysis has widespread applications ranging from cross-media retrieval to heterogeneous face recognition. The critical problem in cross-modal analysis is to correlate heterogeneous features originating from different modalities. Extensive studies have been focused on discovering shared feature space between modalities, while largely overlooked the discriminant information contained in the cross-modal data. Leveraging the discriminant information has been found effective in discovering the underlying semantic structure to facilitate the end applications. Considering this, we propose a deep learning-based method to simultaneously consider the cross-modal correlation and intra-modal discriminant information. Specifically, a unified objective function is introduced which consists of a LDA-like discriminant part and a CCA-like correlation part. The proposed method can be easily generalized to exploiting the unpaired samples. Extensive experiments are conducted on three representative cross-modal analysis problems: cross-media retrieval, cross-OSN user modeling and heterogeneous face recognition. By comparing with existing state-of-the-art algorithms, the results show that the proposed algorithm is robust to the feature dimension and achieves the best performance in all experiments.}
}
@article{BARTELT2020337,
title = {Automated production of individualized products for teaching I4.0 concepts},
journal = {Procedia Manufacturing},
volume = {45},
pages = {337-342},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920310659},
author = {Matthias Bartelt and Jannis Stecken and Bernd Kuhlenkötter},
keywords = {Automated production concepts, Individualization, I4.0},
abstract = {Many trends, such as cyber-physical systems, Internet of Things or digital twins are currently predominant in research and development. However, the teaching and training of these topics is often neglected. Especially, students or participants of a training course cannot develop and apply new technologies on their own. In order to meet this demand, this paper presents an approach that segments an automated production system into modules. The complexity of the system can be varied depending on the needs of the students or participants. With this, classical automation topics as well as state-of-the-art topics can be touched and trained. Additionally, the concept is designed in such a way that it is not only applicable in university education, but also that individual topics can be taught in half-day or one-day seminars with appropriate previous experience.}
}
@article{NIE2020539,
title = {3D Model classification based on few-shot learning},
journal = {Neurocomputing},
volume = {398},
pages = {539-546},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.03.105},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219310471},
author = {Jie Nie and Ning Xu and Ming Zhou and Ge Yan and Zhiqiang Wei},
keywords = {Few-shot, Meta-learner, 3D model classification},
abstract = {With the development of multimedia technology, 3D model has been applied in many fields such as mechanical design, construction industry, entertainment industry, medical treatment and so on. The number of 3D model is becoming more and more in our lives. Therefore, effective automatic management and classification of 3D models become more and more important. In this paper, we propose a dual-meta-learner model based on LSTM to learn the exact optimization algorithm used to train another two learner neural network classifier in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while it can also achieve a general initialization of the learner (classifier) network that allows for quick convergence of training. Our method attains state-of-the-art performance by significant margins.}
}
@article{STROER2018714,
title = {Combined development and test of product-service systems in early product development stages for customized, availability-oriented business models in the capital goods industry},
journal = {Procedia CIRP},
volume = {72},
pages = {714-719},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.246},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304177},
author = {Felix Ströer and Paaranan Sivasothy and Karl-G. Faißt and Hristo Apostolov and Thomas Eickhoff and Dani Bechev and Georgis Bulun and Jörg Seewig and Martin Eigner and Bernd Sauer},
keywords = {Product-Service Systems, Model-Based Systems Engineering, Digital Twin, Conditon Monitoring, Physical Modeling},
abstract = {Availability-Oriented Product-Service Systems (AOPSS) give manufacturers of capital goods the opportunity to expand their offerings while fulfilling the customers’ needs for product availability. In turn, they generate special requirements for the early product development phase. Condition monitoring has to be taken into account, which results in a contradiction since learning-based condition monitoring approaches require large amounts of data which isn’t available in that particular stage. Techniques from model-based systems engineering allow solving problems newly arising when developing parts or products in AOPSS. The approach proposed in the paper is being illustrated based on a use case from the agricultural industry.}
}
@article{ZHANG2019345,
title = {A data- and knowledge-driven framework for digital twin manufacturing cell},
journal = {Procedia CIRP},
volume = {83},
pages = {345-350},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.084},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119306985},
author = {Chao Zhang and Guanghui Zhou and Jun He and Zhi Li and Wei Cheng},
keywords = {digital twin, digital twin manufacturing cell, intelligent manufacturing, smart product-service systems},
abstract = {Intelligent manufacturing is regarded as the next generation manufacturing mode with powerful learning and cognitive capacities enabled by new generation information technologies such as Internet of Things, big data analytics, edge computing and artificial intelligence. To provide an insight into intelligent manufacturing, this paper takes autonomous manufacturing cell as implementation scenario and proposes a data- and knowledge-driven framework for digital twin manufacturing cell (DTMC), which could support autonomous manufacturing by an intelligent perceiving, simulating, understanding, predicting, optimizing and controlling strategy. In addition, three key enabling technologies including digital twin model, dynamic knowledge bases and knowledge-based intelligent skills for supporting the above strategy are analyzed. Then, the implementing methods of DTMC are introduced through a thus constructed digital twin robot, and the usage of data and knowledge for supporting the automous operations of DTMC is also discussed. Finally, benefits of DTMC in smart product-service systems (PSS) and its current challenges are summarized.}
}
@article{GRUBE2019219,
title = {SMEs can touch Industry 4.0 in the Smart Learning Factory},
journal = {Procedia Manufacturing},
volume = {31},
pages = {219-224},
year = {2019},
note = {Research. Experience. Education. 9th Conference on Learning Factories 2019 (CLF 2019), Braunschweig, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.03.035},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919303993},
author = {David Grube and Ali A. Malik and Arne Bilberg},
keywords = {I4.0, SME, Collaborative simulation},
abstract = {This paper describes how a Smart Learning Factory enables manufacturing SMEs of capturing the benefits of highly complex tools and enablers such as virtual simulation and the Digital Twin. The collaborative factory design approach is enabled by embedding the use of discrete event simulation connected with physical objects placed on a Digital Twin Module (DTM). The users can manipulate the physical objects as physical counterparts to the machines and equipment in the virtual space and visualize the designed factory and make further analysis. The method combines the use of dynamic discrete event simulation seamlessly connected with physical objects placed on the DTM to enable collaborative design. The bridging between simulation and physical objects is done by using a digital integration platform. Using physical artifacts as counterparts of the virtual objects in the simulation, participants confidently interact with the simulation regardless level of skills and competencies. The Smart Learning Factory is helping SMEs to get inspired using physical and virtual simulations for factory design and re-design, and develop solutions in a collaborative environment. This is in-line with the theory of lean automation, that suggests making simple and cheap automation and automating the correct value-adding processes. A demonstration case of designing a production setup in cooperation with a SME is developed and documented.}
}
@article{JAAKMA2019e02622,
title = {Auto-assessment tools for mechanical computer aided design education},
journal = {Heliyon},
volume = {5},
number = {10},
pages = {e02622},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e02622},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019362826},
author = {Kaur Jaakma and Panu Kiviluoma},
keywords = {Mechanical engineering, Education, Machine design, Computer-aided engineering, Pedagogy, Teaching research, Evaluation methodologies, Improving classroom teaching, STEP, Auto-assessment},
abstract = {Traditionally Computer Aided Design (CAD) courses have been carried out in computer classrooms requiring great amount of teaching personnel. Assessment of students' modeling exercises has been both time consuming and error prone. Utilization of the teaching resources could clearly benefit from online auto-assessment. The auto-assessment tools are widely in use in programming and language courses, but suitable tools for assessing 3D models used in CAD are lacking. This paper presents two new online auto-assessment tools to support the development of both command (“what steps are needed to create this shape?”) and strategic (“how should I model this shape?”) knowledge while learning CAD. The first tool is based on neutral file format (in this case STEP) and can recognize surface differences between student's model and reference model. This tool can assess student's skill to create certain predefined shape (i.e. command knowledge). The second auto-assessment tool utilizes commercial CAD software's API (Application Programming Interface) to test how student's model behaves when modeling parameters are changed. This tool assess student's capabilities to build and design a CAD model's design intent (i.e. strategic knowledge). Developed tools were tested on three mechanical engineering courses. This paper presents both the tools and the feedback received from the students and teachers. Overall, the auto-assessment tools functioned well and feedback from both students and teachers were positive. The most appreciated tool functionality was time and place independent submission and assessment of exercise works. These new tools able focusing teachers' workload from checking the basic exercises to guiding the learning process.}
}
@article{LI201973,
title = {Robust stochastic configuration networks with maximum correntropy criterion for uncertain data regression},
journal = {Information Sciences},
volume = {473},
pages = {73-86},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518307278},
author = {Ming Li and Changqin Huang and Dianhui Wang},
keywords = {Stochastic configuration networks, Robust data regression, Randomized algorithms, Maximum correntropy criterion, Alternating optimization},
abstract = {This paper develops a robust stochastic configuration network (RSCN) framework to cope with data modelling problems when the given samples contain noises or outliers. Technically, RSCNs are built by generalizing the objective function used in our original stochastic configuration networks with maximum correntropy criterion (MCC) induced losses (the proposed algorithm is termed as RSC-MCC). The half-quadratic (HQ) technique is employed to optimize the penalty weights for each training sample, aiming to weaken the impacts caused by the noisy data or outliers throughout the training session. Alternating optimization (AO) methodology is used to renew the RSCN model in company with updated penalty weights determined by HQ methods. The performance of RSC-MCC algorithm is compared with some existing methods, such as the probabilistic robust learning algorithm for neural networks with random weights (PRNNRW), RVFL networks, improved RVFL networks (Imp-RVFL), and our recent work RSCNs with kernel density estimation (RSC-KDE), on two synthetic function approximation examples, four benchmark datasets and one educational data modelling case study (for student learning performance prediction). The experimental results show that RSC-MCC performs more favourably in robust data analytics, and further indicate that our proposed RSCN framework (both RSC-KDE and RSC-MCC) has a good potential for real-world applications.}
}
@article{AISSAOUI201987,
title = {Combining supervised and unsupervised machine learning algorithms to predict the learners’ learning styles},
journal = {Procedia Computer Science},
volume = {148},
pages = {87-96},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919300122},
author = {Ouafae EL AISSAOUI and Yasser EL ALAMI {EL MADANI} and Lahcen OUGHDIR and Youssouf EL ALLIOUI},
keywords = {Web usage mining techniques, machine learning algorithms, K-modes clustering algorithm, Naive Bayes classifier, adaptive e-learning systems, Learning style model;},
abstract = {The implementation of an efficient adaptive e-learning system requires the construction of an effective student model that represents the student’s characteristics, among those characteristics, there is the learning style that refers to the way in which a student prefers to learn. Knowing learning styles helps adaptive E-learning systems to improve the learning process by providing customized materials to students. In this work, we have proposed an approach to identify the learning style automatically based on the existing learners’ behaviors and using web usage mining techniques and machine learning algorithms. The web usage mining techniques were used to pre-process the log file extracted from the E-learning environment and capture the learners’ sequences. The captured learners’ sequences were given as an input to the K-modes clustering algorithm to group them into 16 learning style combinations based on the Felder and Silverman learning style model. Then the naive Bayes classifier was used to predict the learning style of a student in real time. To perform our approach, we used a real dataset extracted from an e-learning system’s log file, and in order to evaluate the performance of the used classifier, the confusion matrix method was used. The obtained results demonstrate that our approach yields excellent results.}
}
@article{DERYABIN20203210,
title = {About some issues of developing Digital Twins for the intelligent process control in quarries},
journal = {Procedia Computer Science},
volume = {176},
pages = {3210-3216},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.128},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320287},
author = {Sergey A. Deryabin and Igor O. Temkin and Sergey V. Zykov},
keywords = {Digital Twin, Industry 4.0, software architecting, functional modeling, software prototyping},
abstract = {The present work is devoted to the problems of Digital Twin development of industrial enterprises in the field of mining. The main goal of this article is to formulate the principles of designing platform solutions for the integration of the most important functional elements that ensure the implementation of technological processes of a full production cycle. Various classification schemes for heterogeneous, poorly structured information spaces that form the distributed digital environment of a mining enterprise are proposed. Based on the results of structural and functional modeling, a number of principles and requirements are formulated for the implementation of the Digital Twin technology of the transport and technological process in quarry. A conceptual diagram of the functional structure of the Digital Twin platform is proposed, taking into account the need to include Industry 4.0 technologies such as Industrial Internet of Things, Big Data (including Predictive Analytics and Machine Learning), Autonomous Haulage Systems and Dynamics 3D Optimization Modeling. Some aspects of the implementation and functioning of the prototype version of the Digital Twin platform are considered in terms of the use of instrumental solutions based on the Unity visual modeling environment.}
}
@article{FRANCIOSA2020369,
title = {Deep learning enhanced digital twin for Closed-Loop In-Process quality improvement},
journal = {CIRP Annals},
volume = {69},
number = {1},
pages = {369-372},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.04.110},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620301323},
author = {Pasquale Franciosa and Mikhail Sokolov and Sumit Sinha and Tianzhu Sun and Dariusz Ceglarek},
keywords = {Digital Manufacturing System, Assembly, Digital Twin},
abstract = {A digital twin framework is presented for assembly systems with compliant parts fusing sensors with deep learning and CAE simulations. Its underlying concept, ‘process capability space,’ updates iteratively during evolving tasks of new product introduction with resulting model fidelity able to simulate dimensional, geometric and weld quality of parts and assemblies; isolate root causes of quality defects; and, suggest corrective actions for automatic defects mitigation; thereby, enabling ‘Closed-Loop In-Process (CLIP) quality improvement’ during assembly system development. Results, using the first fully digitally developed remote laser welding process for aluminium doors, yielded a right-first-time rate of >96% for door assembly cell development.}
}
@article{CALAFATO2019102700,
title = {The non-native speaker teacher as proficient multilingual: A critical review of research from 2009–2018},
journal = {Lingua},
volume = {227},
pages = {102700},
year = {2019},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0024384119300828},
author = {Raees Calafato},
keywords = {Multilingualism, Teacher identity, Non-native speaker teacher, Language awareness, Foreign language teaching},
abstract = {With many countries now implementing multilingual educational initiatives in schools, there is a need to reconfigure training programs and encourage teachers to develop a multilingual identity so that they can better promote multilingualism among their students. At present, monolingual ideologies dominate the language education landscape, particularly disadvantaging non-native speaker teachers (NNSTs), who are proficient multilinguals by default and who might best embody the successful language learner model for emergent multilinguals. Studies on NNSTs, however, have traditionally focused on their status as deficient native speakers instead of exploring their multilinguality, although this has started to change in recent years. This study represents a critical review of empirical studies (n=84) from 2009–2018 that reflect this change. The review indicates that a growing number of studies have started to document NNSTs’ unique affordances and multilingual practices in the classroom globally, that NNSTs can develop a reactive multilingual identity in response to native-speakerism, and that learners, when engaged by researchers, do recognize the multilingual affordances of NNSTs. The study's findings should contribute to the growing body of research on multilingual teacher identity, affordances, and practices, especially as this concerns NNSTs.}
}
@article{MCPAKE2019220,
title = {Radiographers' and students' experiences of undergraduate radiotherapy practice placement in the United Kingdom},
journal = {Radiography},
volume = {25},
number = {3},
pages = {220-226},
year = {2019},
issn = {1078-8174},
doi = {https://doi.org/10.1016/j.radi.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1078817418302104},
author = {M. McPake},
keywords = {Radiotherapy, Practice educator, Student, Practice placement model, Peer-assisted learning},
abstract = {Introduction
A three-phased, mixed-methods study was conducted to explore the experiences of undergraduate radiotherapy students and their supervising practice educators within U.K. radiotherapy practice placement.
Methods
Qualitative data were gathered from focus groups/interviews with volunteer participants to elicit in-depth perceptions about experiences of practice placement. Data were transcribed, verbatim, and manually coded and analysed by the researcher using the applied research methodology of framework analysis, enabling the investigation of the a priori theme ‘practice placement model’, and recommendations were made for improvement.
Results
Two radiotherapy placement models are confirmed, i.e. the single student model, and the paired student model, and advantages and disadvantages are identified for each. Study findings suggest that neither radiotherapy model is superior to the other in terms of placement education and experience. Previous and current experience of either model appears to bias students and practice educators towards that model, despite recognition of its disadvantages.
Conclusion
The experiences of students and practice educators using the radiotherapy models are consistent with the experiences of other AHPs and nursing using similar practice placement models. It is recommended that all students should have access to peer-assisted learning on placement to improve critical thinking skills, to enable time for reflection, and to consolidate learning.}
}
@article{DAMGRAVE2019341,
title = {Student driven learning in Synthetic Environments},
journal = {Procedia CIRP},
volume = {84},
pages = {341-346},
year = {2019},
note = {29th CIRP Design Conference 2019, 08-10 May 2019, Póvoa de Varzim, Portgal},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.270},
url = {https://www.sciencedirect.com/science/article/pii/S221282711930575X},
author = {R.G.J. Damgrave and E. Lutters},
keywords = {Digital twin, Virtual Reality, Virtual factory, Engineering education},
abstract = {113 master students of multiple engineering study backgrounds were challenged to develop solutions for the Fraunhofer Project Center at the University of Twente (FPC@UT). In 20 groups the students had to develop a synthetic environment to monitor, manage and control a pilot plant or virtual factory. The assignment was carried out in the context of the study Industrial Design Engineering, in the course ‘Virtual Reality’. During the course the students had to provide a strategic, yet concrete proposal and demonstrator on how to realize a solution using virtual and augmented reality technology and to find balance between generic tools and specific applications. The students had the Virtual Reality Lab and Smart Industry Lab to their disposal during this 10 week, 5 ECTS, course. Based on a brief introduction by the FPC@UT the students had to set their own goals and deliverables and convince the client that their envisaged solution would be beneficial. The course is based on student driven learning, in combination with project led education. This resulted in a situation where the students were in charge of their education and had to decide for themselves which knowledge and feedback they would need in order to achieve their deliverables. Eventually the resulted solutions could be used by FPC@UT to further integrate in their (future) clients.}
}
@article{FANG2018149,
title = {A generalized stereotype learning approach and its instantiation in trust modeling},
journal = {Electronic Commerce Research and Applications},
volume = {30},
pages = {149-158},
year = {2018},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2018.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1567422318300644},
author = {Hui Fang and Jie Zhang and Murat Şensoy},
keywords = {User modeling, Stereotype trust model, Fuzzy semantic framework, E-commerce},
abstract = {Owing to the lack of historical data regarding an entity in online communities, a user may rely on stereotyping to estimate its behavior based on historical data about others. However, these stereotypes cannot accurately reflect the user’s evaluation if they are based on limited historical data about other entities. In view of this issue, we propose a novel generalized stereotype learning approach: the fuzzy semantic framework. Specifically, we propose a fuzzy semantic process, incorporated with traditional machine-learning techniques to construct stereotypes. It consists of two sub-processes: a fuzzy process that generalizes over non-nominal attributes (e.g., price) by splitting their values in a fuzzy manner, and a semantic process that generalizes over nominal attributes (e.g., location) by replacing their specific values with more general terms according to a predefined ontology. We also implement the proposed framework on the traditional decision tree method to learn users’ stereotypes and validate the effectiveness of our framework for computing trust in e-marketplaces. Experiments on real data confirm that our proposed model can accurately measure the trustworthiness of sellers with which buyers have limited experience.}
}
@article{PROMYOO20191043,
title = {Innovative Digital Manufacturing Curriculum for Industry 4.0},
journal = {Procedia Manufacturing},
volume = {34},
pages = {1043-1050},
year = {2019},
note = {47th SME North American Manufacturing Research Conference, NAMRC 47, Pennsylvania, USA.},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.06.092},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919308194},
author = {Rapeepan Promyoo and Shashank Alai and Hazim El-Mounayri},
keywords = {Digital manufacturing, Digital thread, Industry 4.0, SDPD},
abstract = {Manufacturing companies across all major industries are facing serious challenges trying to competitively design and manage modern products, which are becoming increasingly complex multi-domain systems or “systems of systems”. Model-based systems driven product development (or SDPD, for Systems Driven Product Development) has been proposed as a solution based on driving the product lifecycle from the systems requirements and tracing back performance to stakeholders’ needs through a RFLP (Requirement, Functional, Logical, Physical) traceability process. The SDPD framework integrates system behavioral modeling with downstream product design and manufacturing process practices to support the verification/validation of the systems behavior as products progress through all phases of the lifecycle, as well as the optimization of trade-offs decisions by maintaining the cross-product digital twin and thread for global decision optimization in an efficient and effective way. We have developed an innovative digital manufacturing curriculum (designed around the SDPD paradigm) that is based on the digitalization of the SE (Systems Engineering) process through the integration of modelling and simulation continuum, in the form of Model-based Systems Engineering (MBSE), with Product lifecycle management (PLM). At the core of this curriculum is a shift of focus from theory to implementation and practice, through an applied synthesis of engineering fundamentals and systems engineering, that is driven by a state-of-the-art digital innovation platform for product (or system) development consisting of integrated software (digital) tools spanning the complete lifecycle. The curriculum consists of three key components, namely, modelling and simulation continuum, traceability, and digital thread. The curriculum provides a foundation for implementing the digital twin and supports the training of the next generation of engineers for Industry 4.0. The digital manufacturing (or SDPD) framework is applied in the design and optimization of an electric skateboard. The implementation demonstrates: 1) The benefits of digitalization/model-based engineering when developing complex multi-domain products or systems; 2) The ability of students to effectively complete a real-life modern product development within the time line of one semester; 3) The provision of MBSE curriculum for Engineering Education 4.0, characterized by key, integrated skills for the digital enterprise and Industry 4.0.}
}
@article{DAI2019367,
title = {Stochastic configuration networks with block increments for data modeling in process industries},
journal = {Information Sciences},
volume = {484},
pages = {367-386},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.01.062},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519300738},
author = {Wei Dai and Depeng Li and Ping Zhou and Tianyou Chai},
keywords = {Stochastic configuration networks, Process industries, Randomized learner model, Block incremental approach, Simulated annealing algorithm},
abstract = {Stochastic configuration networks (SCNs) that employ a supervisory mechanism to automatically and fast construct universal approximators can achieve promising performance for resolving regression problems. This paper develops an extension of the original SCNs with block increments to enhance learning efficiency, which has received considerable attention in industrial process modeling. This extension allows the learner model to add multiple hidden nodes (termed hidden node block) simultaneously to the network during construction process. To meet industrial demands, two block incremental implementations of SCNs are presented by adopting different strategies for setting the block size. Specifically, the first one adds the hidden node blocks with a fixed block size, which achieves the acceleration of convergence rate at the cost of model compactness; the second one can automatically set the block size by incorporating simulated annealing algorithm, achieving a good balance between efficiency and complexity. The two algorithms are suitable for industrial data modeling with distinct requirements on modeling speed and memory space. The improved methods for building SCNs are evaluated by two function approximations, four benchmark datasets and two real world applications in process industries. Experimental results with comparisons indicate that the proposed schemes perform favorably.}
}
@article{ROTGANS2019294,
title = {A Students’ Model of Team-based Learning},
journal = {Health Professions Education},
volume = {5},
number = {4},
pages = {294-302},
year = {2019},
issn = {2452-3011},
doi = {https://doi.org/10.1016/j.hpe.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2452301118301706},
author = {Jerome I. Rotgans and Preman Rajalingam and Michael A. Ferenczi and Naomi Low-Beer},
keywords = {Team-based learning, Medical education, Path analysis, Students׳ model},
abstract = {Background
Team-based learning (TBL) combines direct instruction with active, collaborative small group learning. This study aimed to elucidate-from the students’ perspective-the relations between different elements of TBL. This is expected to provide a better understanding of the inner workings of TBL in education.
Method
Three hundred and thirteen first- and second-year medical students participated in the study. Data about TBL were collected at the end of six teaching blocks, by means of a questionnaire. The data were then combined and subjected to path analysis, which enabled testing of hypothesised relations between three layers of TBL-relevant variables. These were (1) input variables: prior knowledge, teamwork, challenging application exercise, content expert and facilitator; (2) process variables: preparation materials, individual readiness assurance test (iRAT), team readiness assurance test (tRAT); and (3) output variables: learning and topic interest.
Results
Initial analysis resulted in amendments to the hypothesised model. An amended model fitted the data well and explained 43% of the variance in learning and 32% of the variance in topic interest. Content expert had a direct effect on topic interest, as did prior knowledge, teamwork, iRAT and application exercise. Learning was directly influenced by tRAT, application exercise and facilitator, but not content expert.
Conclusions
The results of this study demonstrate the inter-relationships of different elements of TBL. The results provide new insights in how TBL works from a students’ perspective. Implications of these findings are discussed.}
}
@article{BAZAZ2020288,
title = {The prediction method of tool life on small lot turning process – Development of Digital Twin for production},
journal = {Procedia Manufacturing},
volume = {51},
pages = {288-295},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.041},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920318965},
author = {Sara Moghadaszadeh Bazaz and Mika Lohtander and Juha Varis},
keywords = {Tool life, turning process, artificial intelligence, mathematical modelling, digital twin},
abstract = {Saving resources is one of the most significant factors in the manufacturing industry. There are in the factory, several different products under processing at the same time, therefore the handling of production conditions could be hard every now and then. Changing tools during operation might causes interruption and prolong production time. Estimation of a tool life during turning process is one of the key factors to avoid unnecessary unfinished parts and waste of resources. Overall research aiming to develop a machine learning method to predict tool life for any work-piece or tool material in the general turning process. The addressed method is important in modern small lot production when parts and materials changed constantly. The Purpose of this particular paper is to find out suitable machine learning method or several methods to evaluate tool-life in different turning conditions and circumstances. As a hypothesis of this research, we assume machine learning combine mathematical modelling is a proper method to estimate tool life in small-lot production with reasonable cost and operation time.}
}
@article{BURGUN2019913,
title = {Intelligence artificielle et radiothérapie : quelles bases et quelles perspectives ?},
journal = {Cancer/Radiothérapie},
volume = {23},
number = {8},
pages = {913-916},
year = {2019},
issn = {1278-3218},
doi = {https://doi.org/10.1016/j.canrad.2019.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1278321819303865},
author = {A. Burgun},
keywords = {Intelligence artificielle, Oncologie, Radiothérapie, Médecine de précision, Algorithme, Artificial intelligence, Precision oncology, Radiation therapy, Algorithm},
abstract = {Résumé
L’intelligence artificielle est une notion hautement polysémique. Pour réaliser un raisonnement complexe dans la vie réelle, et s’adapter à des connaissances et des situations nouvelles, deux grandes approches sont développées en informatique : les réseaux de neurones basés sur le modèle connexionniste (deep learning) pour l’apprentissage, et les méthodes symboliques et logiques capables de travailler à un niveau abstrait de description et de raisonnement. Les algorithmes d’intelligence artificielle reproduisant les processus de déduction, induction et abduction ont des applications en radiothérapie. Combinés à la radiomique, les réseaux de neurones ont obtenu de bons résultats en classification d’images, traitement du langage naturel, phénotypage à partir des dossiers patients, adaptation des traitements. Les approches logiques ont produit des ontologies formelles, des algorithmes déterministes pour la décision et des méthodes de vérification de cohérence des systèmes complexes. Une intelligence artificielle hybride conjuguant apprentissage et logique est nécessaire pour réaliser des tâches complexes allant au delà de l’intelligence artificielle qui réalise des tâches restreintes et spécialisées. Combinée à des modèles formalisant les connaissances physicobiologiques, l’intelligence artificielle est au cœur de nouveaux outils comme les jumeaux numériques (digital twins) nécessaires à la médecine de précision en oncologie.
Artificial intelligence is a highly polysemic term. In computer science, with the objective of being able to solve totally new problems in new contexts, artificial intelligence includes connectionism (neural networks) for learning and logics for reasoning. Artificial intelligence algorithms mimic tasks normally requiring human intelligence, like deduction, induction, and abduction. All apply to radiation oncology. Combined with radiomics, neural networks have obtained good results in image classification, natural language processing, phenotyping based on electronic health records, and adaptive radiation therapy. General adversial networks have been tested to generate synthetic data. Logics based systems have been developed for providing formal domain ontologies, supporting clinical decision and checking consistency of the systems. Artificial intelligence must integrate both deep learning and logic approaches to perform complex tasks and go beyond the so-called narrow artificial intelligence that is tailored to perform some highly specialized task. Combined together with mechanistic models, artificial intelligence has the potential to provide new tools such as digital twins for precision oncology.}
}
@article{ELJANATI2018436,
title = {SMART Education Framework for Adaptation Content Presentation},
journal = {Procedia Computer Science},
volume = {127},
pages = {436-443},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.141},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918301534},
author = {Salma {El Janati} and Abdelilah Maach and Driss {El Ghanami}},
keywords = {E-learning, ES, ALS, Physical disability, Media Adaptation, DAHS, Learner Model, Transcoding},
abstract = {The rapid growth of the Educational System (ES) has changed traditional learning behavior and presented a new situation to learners. To address the ever increasing needs and challenges associated with ES, a strategic methods and techniques of Adaptive Learning System (ALS) is adapted, which the primary objective is the adaptation of content, presentation, and navigation. In this paper, we suggest a new framework to adapt the Content Presentation to the preferences of learners and physical disability with learners who suffer from the visual and hearing limitations to stimulate the content. This framework is intended to be integrated in Dynamic Adaptive Hypermedia System (DAHS)in a manner that increases the learner’s perceived quality, high level of adaptivity and reliability.}
}
@article{YANG2020106598,
title = {A comparative analysis of bubble point pressure prediction using advanced machine learning algorithms and classical correlations},
journal = {Journal of Petroleum Science and Engineering},
volume = {185},
pages = {106598},
year = {2020},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2019.106598},
url = {https://www.sciencedirect.com/science/article/pii/S0920410519310198},
author = {Xi Yang and Birol Dindoruk and Ligang Lu},
keywords = {Bubble point pressure, PVT, Fluid properties, Machine learning methods (XGBoost, LightGBM, random forest regressor, MLP neural network, and super learner), Correlations},
abstract = {The need for fluid properties or PVT (Pressure-Volume-Temperature) properties, is part of the entire Exploration and Production (E&P) lifecycle from exploration to mature asset management to the typical later life events such as, Improved Oil Recovery (IOR). As the projects mature, the need for such data and its integration for various discipline-specific workflows and its interpretation in the light of reservoir performance varies. Among all the key PVT properties, bubble point pressure is probably the most important parameter. Bubble point pressure is important because it is the point at which constant composition and variable composition portions of the depletion paths merge. Geometrically, bubble point pressure appears to be a discontinuity. In addition, it dictates the existence (or not) of the incipient phase (i.e., gas phase) leading to the changes in the flow characteristics both in porous media and as well as within the wellbore and the facilities. Furthermore, it is also a good indicative of a possible gas cap when the reservoir is at saturation (reservoir pressure is equal to the bubble point pressure) or near-saturated. Among the highlighted uses, there are many more used such as the determination of the elements of miscibility, gas lift design, etc. Therefore, it is very important to estimate the bubble point pressure accurately. In this study, tree-based advanced machine learning algorithm including XGBoost, LightGBM, and random forest regressor, and multi-layer perceptron (neural network) regressor are implemented to predict bubble point pressure (Pbp). A novel super learner model which is also known as stacking ensemble is used to enhance base machine learning model performance on predicting bubble point pressure. Three datasets with different predictors are prepared to study machine learning algorithms' performance for three situations: only compositional data are available; only bulk properties (Gas-Oil-Ratio, gas gravity, API gravity and reservoir Temperature) are available; both compositional data and bulk properties are available. Through literature review, there is no research on using only compositional data and temperature to predict bubble point pressure. Our super learner model offers an accurate solution for oil bubble point pressure when only compositional data and temperature are available. Machine learning models perform better than empirical correlations with limited input data (i.e., bulk properties). When compositional data and bulk properties are all used as predictors, super learner reaches about 5.146% mean absolute relative error on predicting the bubble point pressure from global samples with bubble point pressures in the range of 100 to 10,000 psi, which is a wider range compared to most ANN models published in literature.}
}
@article{ZHANG2020105958,
title = {Knowledge distilling based model compression and feature learning in fault diagnosis},
journal = {Applied Soft Computing},
volume = {88},
pages = {105958},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105958},
url = {https://www.sciencedirect.com/science/article/pii/S1568494619307392},
author = {Wenfeng Zhang and Gautam Biswas and Qi Zhao and Hongbo Zhao and Wenquan Feng},
keywords = {Fault diagnosis, Neural networks, Knowledge distilling, Feature learning},
abstract = {Recently, there has been interest in developing diagnosis methods that combine model-based and data-driven diagnosis. In both approaches, selecting the relevant measurements or extracting important features from historical data is a key determiner of the success of the algorithm. Recently, deep learning methods have been effective in automating the feature selection process. Autoencoders have been shown to be an effective neural network configuration for extracting features from complex data, however, they may also learn irrelevant features. In addition, end-to-end classification neural networks have also been used for diagnosis, but like autoencoders, this method may also learn unimportant features thus making the diagnostic inference scheme inefficient. To rapidly extract significant fault features, this paper employs end-to-end networks and develops a new feature extraction method based on importance analysis and knowledge distilling. First, a set of cumbersome neural network models are trained to predict faults and some of their internal values are defined as features. Then an occlusion-based importance analysis method is developed to select the most relevant input variables and learned features. Finally, a simple student neural network model is designed based on the previous analysis results and an improved knowledge distilling method is proposed to train the student model. Because of the way the cumbersome networks are trained, only fault features are learned, with the importance analysis further pruning the relevant feature set. These features can be rapidly generated by the student model. We discuss the algorithms, and then apply our method to two typical dynamic systems, a communication system and a 10-tank system employed to demonstrate the proposed approach.}
}
@article{SUN202030,
title = {A comprehensive hybrid first principles/machine learning modeling framework for complex industrial processes},
journal = {Journal of Process Control},
volume = {86},
pages = {30-43},
year = {2020},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2019.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S095915241930592X},
author = {Bei Sun and Chunhua Yang and Yalin Wang and Weihua Gui and Ian Craig and Laurentz Olivier},
keywords = {Comprehensive state space, Descriptive system, Modeling, Machine learning},
abstract = {The selection of an appropriate descriptive system and modeling framework to capture system dynamics and support process control applications is a fundamental problem in the operation of industrial processes. In this study, to account for the highly complex dynamics of industrial process and additional requirements imposed by smart and optimal manufacturing systems, an extended state space descriptive system, named comprehensive state space, is first designed. Then, based on the descriptive system, a hybrid first principles/machine learning modeling framework is proposed. The hybrid model is formulated as a combination of a nominal term and a deviation term. The nominal term covers the underlying physicochemical principles. The deviation term handles the effects of high-dimensional influence factors using regression of low-dimensional deep process features. To handle the multimodal and time-varying properties of process dynamics, the comprehensive state space is divided into subspaces indicating different operating conditions. The model parameters are identified and trained for each operating condition to form the sub-models. Then the system dynamics are formulated as a weighted sum of sub-models, with the weights being the probabilities that the current operating point belongs to different operating conditions. The weights update with the movement of the operating point in the comprehensive state space. Moreover, the descriptive system provides a platform for visualization, and can act as a digital twin of the physical process. A case study illustrates the feasibility and performance of the proposed descriptive system.}
}
@incollection{LAZZARI2020187,
title = {Chapter Six - Toward a digital polymer reaction engineering},
editor = {Davide Moscatelli and Mattia Sponchioni},
series = {Advances in Chemical Engineering},
publisher = {Academic Press},
volume = {56},
number = {1},
pages = {187-227},
year = {2020},
booktitle = {Advances in Polymer Reaction Engineering},
issn = {0065-2377},
doi = {https://doi.org/10.1016/bs.ache.2020.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065237720300211},
author = {Stefano Lazzari and Andree Lischewski and Yury Orlov and Peter Deglmann and Andreas Daiss and Eduard Schreiner and Hugo Vale},
keywords = {Polymer, Digitalization, Digital twin, Modeling, Kinetics, Thermodynamics, Computational fluid dynamics, Quantum chemistry, Molecular dynamics, Machine learning},
abstract = {What is digitalization, and why do we need it? What does digitalization mean for research and development in polymer reaction engineering (PRE)? In this chapter, we address these questions, starting from a global perspective, briefly analyzing the current situation. We then illustrate our views on how the virtual representation of a polymerization process, i.e., a digital twin, would enable us to widen the spectrum of research and development in PRE. The main tasks of the digital twin are, in our opinion, to approach two yet unsolved problems: linking reaction conditions to final material properties and producing materials in an optimal way. Our strategy to tackle these issues is to combine physical and data science models. We analyze how the predictive ability of deterministic approaches, such as kinetic, thermodynamic and fluid mechanics models is improved by parameters and mechanistic understanding provided by quantum chemistry and molecular dynamics. While we constantly aim for a deeper physicochemical understanding to improve these physical models, we recognize the power of statistics and machine learning, as they enable us to approach highly complex problems, thanks to their large flexibility. When analyzing the current limitations of each technique, we believe that concrete progress can only be achieved if we systematically centralize our knowledge. Therefore, we conclude our perspective on digitalization in PRE by advocating for initiatives that foster an open source sharing of published experimental data, parameters, models, and algorithms.}
}
@article{TACOMA2020106276,
title = {Enhancing learning with inspectable student models: Worth the effort?},
journal = {Computers in Human Behavior},
volume = {107},
pages = {106276},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106276},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220300327},
author = {Sietske Tacoma and Corine Geurts and Bert Slof and Johan Jeuring and Paul Drijvers},
keywords = {Feedback-seeking behavior, Higher education, Inspectable student model, Log file analysis, Statistics education},
abstract = {In electronic learning environments, information about a student's performance can be provided to the student in the form of an inspectable student model. While relatively easy to implement, little is known about whether students use the feedback provided by such models and whether they benefit from it. In this study, the use of inspectable student models in an introductory university statistics course by 599 first-year social science students was monitored. Research questions focused on whether students sought feedback from the student models, which decisions for subsequent study steps they made, and how this feedback seeking and decision making related to results on their statistics exams. Results showed a large variety among students in feedback-seeking and decision-making behavior. Lower student model scores seemed to encourage students to practice more on the same topic and higher scores seemed to evoke the decision to move to a different topic. Viewing frequency and amount of variety in decision making were positively related to exam results, even when controlling for total time students worked. These findings imply that inspectable student models can be a valuable addition to electronic learning environments and suggest that more intensive use of inspectable student models may contribute to learning.}
}
@article{JING2020644,
title = {A Learner Model Integrating Cognitive and Metacognitive And Its Application on Scratch Programming Projects},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {5},
pages = {644-649},
year = {2020},
note = {3rd IFAC Workshop on Cyber-Physical & Human Systems CPHS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.154},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321002913},
author = {Sifeng Jing and Ying Tang and Xiwei Liu and Xiaoyan Gong},
keywords = {learner model, cognitive state, metacognitive ability, individualized teaching},
abstract = {learner’s cognitive and metacognitive are key personal profile for individualized teaching. To evaluate learner’s comprehensive characteristics, existing learner model were reviewed. Two challenges of constructing an accurate and comprehensive learner model integrating cognitive and metacognitive were summarized. A plan of constructing a comprehensive learner model was made based on analysis of existing massive online learning environment, sensor information technology and educational data-mining. As a case study, a method of how to map learning data onto learners’ cognitive and metacognitive was proposed based on an analysis of a number of pupils’ Scratch projects. Three mapping table were established. Pupil’s cognitive skill could be evaluated from technology shown from Scratch project, namely, data structure, algorithm, computational practices and overall evaluation. Content shown from Scratch project were used to infer pupil’s cognitive style. Meta-cognitive ability can be measured from computational practices and behavior in programming process.}
}
@article{WU2020101695,
title = {Multi-teacher knowledge distillation for compressed video action recognition based on deep learning},
journal = {Journal of Systems Architecture},
volume = {103},
pages = {101695},
year = {2020},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2019.101695},
url = {https://www.sciencedirect.com/science/article/pii/S1383762119305028},
author = {Meng-Chieh Wu and Ching-Te Chiu},
keywords = {Deep convolutional model compression, Action recognition, Knowledge distillation, Transfer learning},
abstract = {Recently, Convolutional Networks have great progress in classifying images. While action recognition is different from still image classification, video data contains temporal information which plays an important role in video understanding. Currently most CNN-based approaches for action recognition has excessive computational costs, an explosion of parameters and computation time. The most efficient method currently trained a deep network directly on the compressed video contains the motion information. However, this method has a large number of parameters. We propose a multi-teacher knowledge distillation framework for compressed video action recognition to compress this model. With this framework, the model is compressed by transferring the knowledge from multiple teachers to a single small student model. With multi-teacher knowledge distillation, students learn better than single-teacher knowledge distillation. Experiments show that we can reach a 2.4 ×  compression rate in number of parameters and 1.2 ×  computation reduction with 1.79% loss of accuracy on the UCF-101 dataset and 0.35% loss of accuracy on the HMDB51 dataset.}
}
@article{MIN2019502,
title = {Machine Learning based Digital Twin Framework for Production Optimization in Petrochemical Industry},
journal = {International Journal of Information Management},
volume = {49},
pages = {502-519},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218311484},
author = {Qingfei Min and Yangguang Lu and Zhiyong Liu and Chao Su and Bo Wang},
keywords = {digital twin, machine learning, internet of things, petrochemical industry, production control optimization},
abstract = {Digital twins, along with the internet of things (IoT), data mining, and machine learning technologies, offer great potential in the transformation of today’s manufacturing paradigm toward intelligent manufacturing. Production control in petrochemical industry involves complex circumstances and a high demand for timeliness; therefore, agile and smart controls are important components of intelligent manufacturing in the petrochemical industry. This paper proposes a framework and approaches for constructing a digital twin based on the petrochemical industrial IoT, machine learning and a practice loop for information exchange between the physical factory and a virtual digital twin model to realize production control optimization. Unlike traditional production control approaches, this novel approach integrates machine learning and real-time industrial big data to train and optimize digital twin models. It can support petrochemical and other process manufacturing industries to dynamically adapt to the changing environment, respond in a timely manner to changes in the market due to production optimization, and improve economic benefits. Accounting for environmental characteristics, this paper provides concrete solutions for machine learning difficulties in the petrochemical industry, e.g., high data dimensions, time lags and alignment between time series data, and high demand for immediacy. The approaches were evaluated by applying them in the production unit of a petrochemical factory, and a model was trained via industrial IoT data and used to realize intelligent production control based on real-time data. A case study shows the effectiveness of this approach in the petrochemical industry.}
}
@article{NAFEE2020230,
title = {Machine learning to predict venous thrombosis in acutely ill medical patients},
journal = {Research and Practice in Thrombosis and Haemostasis},
volume = {4},
number = {2},
pages = {230-237},
year = {2020},
issn = {2475-0379},
doi = {https://doi.org/10.1002/rth2.12292},
url = {https://www.sciencedirect.com/science/article/pii/S2475037922019690},
author = {Tarek Nafee and C. Michael Gibson and Ryan Travis and Megan K. Yee and Mathieu Kerneis and Gerald Chi and Fahad AlKhalfan and Adrian F. Hernandez and Russell D. Hull and Ander T. Cohen and Robert A. Harrington and Samuel Z. Goldhaber},
keywords = {acute medically ill, machine learning, personalized medicine, super learner, venous thromboembolism},
abstract = {Background
The identification of acutely ill patients at high risk for venous thromboembolism (VTE) may be determined clinically or by use of integer‐based scoring systems. These scores demonstrated modest performance in external data sets.
Objectives
To evaluate the performance of machine learning models compared to the IMPROVE score.
Methods
The APEX trial randomized 7513 acutely medically ill patients to extended duration betrixaban vs. enoxaparin. Including 68 variables, a super learner model (ML) was built to predict VTE by combining estimates from 5 families of candidate models. A “reduced” model (rML) was also developed using 16 variables that were thought, a priori, to be associated with VTE. The IMPROVE score was calculated for each patient. Model performance was assessed by discrimination and calibration to predict a composite VTE end point. The frequency of predicted risks of VTE were plotted and divided into tertiles. VTE risks were compared across tertiles.
Results
The ML and rML algorithms outperformed the IMPROVE score in predicting VTE (c‐statistic: 0.69, 0.68 and 0.59, respectively). The Hosmer‐Lemeshow goodness‐of‐fit P‐value was 0.06 for ML, 0.44 for rML, and <0.001 for the IMPROVE score. The observed event rate in the lowest tertile was 2.5%, 4.8% in tertile 2, and 11.4% in the highest tertile. Patients in the highest tertile of VTE risk had a 5‐fold increase in odds of VTE compared to the lowest tertile.
Conclusion
The super learner algorithms improved discrimination and calibration compared to the IMPROVE score for predicting VTE in acute medically ill patients.}
}
@article{FERRARIO2019663,
title = {A Multipurpose Small-Scale Smart Factory For Educational And Research Activities},
journal = {Procedia Manufacturing},
volume = {38},
pages = {663-670},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.085},
url = {https://www.sciencedirect.com/science/article/pii/S235197892030086X},
author = {Andrea Ferrario and Matteo Confalonieri and Andrea Barni and Gabriele Izzo and Giuseppe Landolfi and Paolo Pedrazzoli},
keywords = {Smart Factories, Industrial Internet of Thing, Engineering Education, Production Planning, Scheduling, Inventory Control, Engineering Design Methods, Tools for Industry 4.0},
abstract = {As manufacturing industry is moving its steps towards a more digital, smart and flexible scenario, the changes required to achieve the expectations of the Industry 4.0 (I4.0) framework are numerous and extensive. Though, a general lack of understanding of how new technologies could be integrated and shall be implemented is present, limiting the rate of adoption of such changes and the related beneficial impacts. This paper describes the implementation of a smart-factory at the University of Applied Sciences and Arts of Southern Switzerland (SUPSI) aiming at filling the gap in I4.0 related skills development thorough a learning by doing approach, and providing a research platform that could foster collaboration of practitioners and academia on the development and testing of new technologies. The developed factory integrates different production technologies such as additive manufacturing, laser processing and milling, in order to produce a highly customizable item consisting in a TANGRAM game-set, packaged into personalized boxes. The entire factory is coupled with its digital twin, which is fed by an exhaustive monitoring infrastructure composed by vision systems and high precision measurement instruments, allowing to track in real time plant processes. The design has been carried out in order to make the smart factory serving as a mean to face both educational and research challenges at many different levels. As an educational mean, students and professionals have the chance to dive into manufacturing history experiencing both classical automation topics (PLC, MES and SCADA programming, precision axes control and pneumatics), as well as more advanced technologies, typical of the most advanced smart-factories (IoT, vision systems, simulation and digital twin, advanced measuring methods and smart production management systems). From a research point of view, the factory functions as a pilot plant for internal research and applied industrial projects, on the top of which applications, manufacturing methods and technologies are developed, tested and integrated.}
}
@article{MUKHERJEE201959,
title = {A digital twin for rapid qualification of 3D printed metallic components},
journal = {Applied Materials Today},
volume = {14},
pages = {59-65},
year = {2019},
issn = {2352-9407},
doi = {https://doi.org/10.1016/j.apmt.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2352940718304931},
author = {T. Mukherjee and T. DebRoy},
keywords = {Additive manufacturing, Digital twin, Machine learning, Big data, Mechanistic model},
abstract = {The customized production of complex components by 3D printing has been hailed as a potentially transformative tool in manufacturing with important applications in health care, automotive and aerospace industries. However, after about a quarter of a century of research and development, only a handful of commercial alloys can be printed and the market value of all 3D printed products now amounts to a negligible portion of the manufacturing economy. This difficulty is attributable to a remarkable diversity in structure and properties of the printed components and susceptibility to defects. In addition, the current practice of qualifying components by prolonged trial and error with expensive printing equipment and feed stock material confine the printed products to a niche market where the high product cost and the delay in the qualification are not critical factors. Here we explain how a digital twin or a digital replica of the printing machine will reduce the number of trial and error tests to obtain desired product attributes and reduce the time required for part qualification to make the printed components cost effective. It is shown that a comprehensive digital twin of 3D printing machine consisting of mechanistic, control and statistical models of 3D printing, machine learning and big data can reduce the volume of trial and error testing, reduce defects and shorten time between the design and production.}
}
@article{XIONG2018481,
title = {Vehicle grid integration for demand response with mixture user model and decentralized optimization},
journal = {Applied Energy},
volume = {231},
pages = {481-493},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.09.139},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918314508},
author = {Yingqi Xiong and Bin Wang and Chi-cheng Chu and Rajit Gadh},
keywords = {Electric vehicle, Demand response, Distributed optimization, Machine learning, Topic mining},
abstract = {With the rapidly growing electric vehicle adoption rate and increasing number of public electric vehicle charging stations in recent years, electric vehicle becomes more and more critical in the demand response programs. Development in Vehicle to Grid technology has converted electric vehicle to distributed energy resources. Using the Electric Vehicle Smart Charging Infrastructures on UCLA campus and city of Santa Monica as testbeds, we have collected real-world datasets of electric vehicle usage, based on which, we proposed optimal bi-directional charging control strategies to integrate electric vehicle in commercial and public parking facilities into the power grid as distributed energy resources for demand response programs by two-stage distributed optimization and water-filling algorithm. Driver behavioral uncertainties have been considered in our approach. Specifically, electric vehicle users are clustered by their behavioral patterns using a modified Latent Semantic Analysis. The first-stage optimization is performed to minimize energy cost using day-ahead wholesale energy price with predictions on energy demand and electric vehicle availability which generated by a mixture user model. Decentralized optimization (second-stage) is carried out on the next day in real-time to control individual electric vehicle so that the aggregated load can follow the first-stage optimal profile. As an alternative, a fast converging water filling algorithm is proposed and compared with two-stage optimization. Extensive simulation results show that proposed charging controls can utilize electric vehicle as distributed energy resource to accommodate demand response program while satisfying electric vehicle energy demands and providing significant energy cost savings.}
}
@article{YEN2019547,
title = {Design of a computational model for social learning support and analystics},
journal = {Computers in Human Behavior},
volume = {92},
pages = {547-561},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.07.042},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218303649},
author = {Neil Y. Yen and Jason C. Hung and Chia-Chen Chen and Qun Jin},
keywords = {Social learning, Social network analytics, Social knowledge, Crowdsourcing, Human-centered computing, User modeling},
abstract = {Conventional online learning typically allows an instructor to deliver instruction to students via a predefined curriculum and within a fixed knowledge structure (i.e., explaining the instructional subject). With the dramatic growth of social media technology and correlated data aggregation, some sort of instant knowledge is obtained by daily users. An emerging type of knowledge (i.e., social knowledge) has been identified and may lead to self-paced learning from social networks, which is simply defined as social learning. This article points out three important issues for social learning, namely, knowledge retrieval via temporal social factors, and the connection between social network and the knowledge domain. Two significant automation mechanisms, lecture generation for self-regulated learning and influencing domain computation for opportunity finding, are suggested to facilitate the process of social learning. A prototype system based on Elgg was implemented, sourced by a federated repository that has stored and shared more than 1.5 millions transactions (e.g., content, interactions, etc.). We conclude that timely social knowledge (or crowdsourcing results) can be widely applied in the next era of online learning environment. Findings through the statistical analysis are prospective to support understanding of phenomenon of social learning and design of future learning platform for followup researchers.}
}
@article{BHATT2018144,
title = {iABC: Towards a hybrid framework for analyzing and classifying behaviour of iOS applications using static and dynamic analysis},
journal = {Journal of Information Security and Applications},
volume = {41},
pages = {144-158},
year = {2018},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S2214212617303812},
author = {Arpita Jadhav Bhatt and Chetna Gupta and Sangeeta Mittal},
keywords = {iOS applications, Reverse engineering, Machine learning, Static analysis, Dynamic analysis, Static risk score},
abstract = {Is this app safe to use? - A wrong decision can result in privacy breach in iOS devices. In this digital era users extensively use smart devices to store their personal and important information. To ease users’ tasks, thousands of free or paid apps are available in app store. However, recent studies reveal startling facts about various attacks and data harvesting incidents through these apps, where personal data is put at risk. Through this paper, we propose a permission induced risk model- iOS Application analyzer and Behavior Classifier (iABC), for iOS devices to detect privacy violations arising due to granting permissions during installation of applications. It is a two-layer process comprising of static and dynamic analysis. It uses reverse engineering to extract permission variables from applications and computes a risk score for each application using ranking algorithms. The approach considers application's category as a key feature for detecting malicious applications while computing static risk score. Different machine learning classifiers were employed to evaluate 1,150 applications. The empirical results show that our proposed model gives detection rate of 97.04%. Furthermore, to assess privacy breaches by applications at run time, dynamic analysis on 50 applications has been performed to obtain dynamic risk scores of installed apps.}
}
@article{ALGEDDAWY20201799,
title = {A Digital Twin Creation Method for an Opensource Low-cost Changeable Learning Factory},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1799-1805},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.250},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321284},
author = {Tarek Al-Geddawy},
keywords = {Learning Factory, Digital Twin, Changeable Manufacturing},
abstract = {Learning factories demonstrate applications and technology to students in a real industrial environment. While turnkey changeable learning factories are supplied by many vendors, with some digital twin capabilities, they are mostly a closed box, with very little flexibility to change the underlying architecture or technology, hindering the maximum benefit of student hands-on experience. This paper presents a method to build a simulated changeable learning factory and link it to the physical system to create a digital twin. The studied learning factory (LEAF) is an opensource low-cost changeable automated system. The suggested digital environment is ‘RoboDK’, which is a 3D simulation and offline/online programming environment, mainly for industrial robots, but it also offers an open source ‘Python’ programming library, allowing the extension of the capabilities of the software to adapt to LEAF. The method is also using the opensource Modbus TCP and OPC UA industrial communication protocols to establish the connection between the physical modules and the digital objects. The results show a capable digital system that is accurately mirroring the physical system layout and material flow, with a flexible structure to allow future extensions.}
}
@article{WEI2019102915,
title = {A vision and learning-based indoor localization and semantic mapping framework for facility operations and management},
journal = {Automation in Construction},
volume = {107},
pages = {102915},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102915},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519301219},
author = {Yujie Wei and Burcu Akinci},
keywords = {Facility operation and management, Image, Indoor localization, Semantic understanding, Multi-task deep learning, Convolutional neural network},
abstract = {Recent research on facility management has focused on leveraging location-based services (LBS) to assist on-demand access to model information and on-site documentation. Fast and robust indoor localization is of great importance for location-based facility management services, such as the ones used in mobile computing settings. However, there are several challenges in achieving fast and robust indoor localization: 1) Signal-based indoor localization methods, such as WIFI, RFID, Bluetooth and Ultrasound, require installation of extra infrastructures in a building to support localization; 2) Visual-based indoor localization methods, such as LiDAR and camera, depend on feature point detection and matching, which require heavy computation and can also be impacted by environmental conditions, such as lighting and texture richness. In addition to these, existing localization methods do not support semantic understanding which is of great importance when associating a component with its digital twin. To address the stated problems above, this paper presents a vision and learning-based framework that utilizes a shared convolutional neural network to perform localization and semantic segmentation simultaneously. The proposed framework can support facility management by locating facility components within a building and associate them with their digital twins in an information repository. Compared to conventional methods, the developed image-based indoor localization and semantic mapping framework has the following advantages: 1) It only requires image as input to support localization, semantic understanding, and association, which eliminates the need for extra infrastructure, such as deployment of RFID tags, etc.; 2) It reuses the feature extraction network for simultaneous localization and semantic understanding, which saves computing resources; 3) With 6-DoF poses and semantic labels, it supports component-level association. The authors evaluated the proposed framework on publicly available data sets using three metrics: localization accuracy, semantic segmentation accuracy, and association success rate. The results show that the proposed image-based method can achieve 6-DoF localization and semantic segmentation concurrently. Also, formalized experiments on a synthetic data set with different noise levels introduced to localization and semantic segmentation showed that a main factor affecting the performance of association of an image to its digital twin is the accuracy of its localization.}
}
@article{MOCHIZUKI201936,
title = {The lived experience of thesis writers in group writing conferences: The quest for “perfect” and “critical”},
journal = {Journal of Second Language Writing},
volume = {43},
pages = {36-45},
year = {2019},
note = {Special Issue: Thesis and dissertation writing in a second language: Context, identity, genre},
issn = {1060-3743},
doi = {https://doi.org/10.1016/j.jslw.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1060374317302539},
author = {Naoko Mochizuki},
keywords = {Thesis writing, Perezhivanie (lived experience), Writing conferences, Social interactions, Sociocultural theory},
abstract = {While group writing conferences have become a popular means of encouraging social interactions among doctoral students, little is known about how these group interactions influence a writer’s learning of the thesis genre. Taking a genre as social practice perspective (Tardy, 2009), this study uses the analytical lens of perezhivanie (lived experience) (Vygotsky, 1994) to investigate how doctoral students perceive group writing conferences. Previous studies of thesis writers’ experiences have suggested the interconnectedness of cognition, emotions, and the social contexts in their learning processes. The study focuses on the role that the oral interactions around the text play within doctoral students’ social situations of learning thesis writing. The participants are two L2 doctoral students in group writing conferences run by the learning centre at an Australian university. Data were collected through observation and audio-recording of group discussions, interviews with students and facilitators, and students’ writing drafts. The findings reveal the students’ ‘models’ in their minds as drivers of their learning thesis writing. Underlying those perceived ‘models’ were some social and ideological forces related to ‘native-speaker’ English. The study illuminates the role of writing conferences in assisting students’ co-constructing processes of the thesis genre in their social situations.}
}
@article{HOSE201938,
title = {Cardiovascular models for personalised medicine: Where now and where next?},
journal = {Medical Engineering & Physics},
volume = {72},
pages = {38-48},
year = {2019},
note = {Special issue to commemorate the 40th anniversary of Medical Engineering & Physics},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1350453319301626},
author = {D. Rodney Hose and Patricia V. Lawford and Wouter Huberts and Leif Rune Hellevik and Stig W. Omholt and Frans N. {van de Vosse}},
keywords = {Cardiovascular modelling, Model personalisation, Model, Uncertainity, Physiological modelling, Clinical descision support},
abstract = {The aim of this position paper is to provide a brief overview of the current status of cardiovascular modelling and of the processes required and some of the challenges to be addressed to see wider exploitation in both personal health management and clinical practice. In most branches of engineering the concept of the digital twin, informed by extensive and continuous monitoring and coupled with robust data assimilation and simulation techniques, is gaining traction: the Gartner Group listed it as one of the top ten digital trends in 2018. The cardiovascular modelling community is starting to develop a much more systematic approach to the combination of physics, mathematics, control theory, artificial intelligence, machine learning, computer science and advanced engineering methodology, as well as working more closely with the clinical community to better understand and exploit physiological measurements, and indeed to develop jointly better measurement protocols informed by model-based understanding. Developments in physiological modelling, model personalisation, model outcome uncertainty, and the role of models in clinical decision support are addressed and ‘where-next’ steps and challenges discussed.}
}
@article{MERZ2020215,
title = {A Cloud-Based Research and Learning Factory for Industrial Production},
journal = {Procedia Manufacturing},
volume = {45},
pages = {215-221},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.097},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920311392},
author = {Robert Merz and Ralph Hoch and Damian Drexel},
keywords = {Cloud-based research, Learning Factory, Digital Factory},
abstract = {The Digital Factory Vorarlberg is the youngest Research Center of Vorarlberg University of Applied Sciences. In the lab of the research center a research and learning factory has been established for educating students and employees of industrial partners. We devised learning scenarios and developed courses addressing a wide variety of topics related to Industry 4.0 and showcase best practice scenarios for various topics of digitalization. In addition, novel methods and technologies for digital production, cloud-based manufacturing, data analytics, IT- and OT-security or digital twins are being developed. A centralized SCADA (Supervisory Control and Data Acquisition)-System is the core data hub for the factory. As an alternative to on premise manufacturing, orders can be pushed into a cloud-based manufacturing platform, which has been developed at the Digital Factory. In this paper, we present the basic concept of the Digital Factory Vorarlberg, some of the newly developed topics as well as learning scenarios for students and industry staff.}
}
@article{YAGO201848,
title = {ON-SMMILE: Ontology Network-based Student Model for MultIple Learning Environments},
journal = {Data & Knowledge Engineering},
volume = {115},
pages = {48-67},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17301945},
author = {Hector Yago and Julia Clemente and Daniel Rodriguez and Pedro Fernandez-de-Cordoba},
keywords = {Ontological engineering, Student modeling, Ontology network, Learning supervision, Semantic web},
abstract = {Currently, many educational researchers focus on the extraction of information about the learning progress to properly assist students. We present ON-SMMILE, a student-centered and flexible student model which is represented as an ontology network combining information related to (i) students and their knowledge state, (ii) assessments that rely on rubrics and different types of objectives, (iii) units of learning and (iv) information resources previously employed as support for the student model in intelligent virtual environment for training/instruction and here extended. The aim of this work is to design and build methodologically, throughout ontological engineering, the ON-SMMILE model to be used as support of future works closely linked to supervision of student's learning as competence-based recommender system. For this purpose, our model is designed as a set of ontological resources that have been extended, standardized, interrelated and adapted to be used in multiple learning environments. In this paper, we also analyze the available approaches based on instructional design which can be added to ontology network to build the proposed model. As a case study, a chemical experiment in a virtual environment and its instantiation are described in terms of ON-SMMILE.}
}
@article{SYKES201866,
title = {Reasoning about ideal interruptible moments: A soft computing implementation of an interruption classifier in free-form task environments},
journal = {International Journal of Human-Computer Studies},
volume = {120},
pages = {66-93},
year = {2018},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918303471},
author = {Edward R. Sykes},
keywords = {Human computer interaction, Interruption, Machine learning, Soft computing applications, Situationally appropriate interaction, Workload},
abstract = {Current trends in society and technology make the concept of interruption a central human computer interaction problem. In this work, a novel soft computing implementation for an Interruption Classifier was designed, developed and evaluated that draws from a user model and real-time observations of the user's actions as s/he works on computer-based tasks to determine ideal times to interact with the user. This research is timely as the number of interruptions people experience daily has grown considerably over the last decade. Thus, systems are needed to manage interruptions by reasoning about ideal timings of interactions. This research shows: (1) the classifier incorporates a user model in its’ reasoning process. Most of the research in this area has focused on task-based contextual information when designing systems that reason about interruptions; (2) the classifier performed at 96% accuracy in experimental test scenarios and significantly outperformed other comparable systems; (3) the classifier is implemented using an advanced machine learning technology—an Adaptive Neural-Fuzzy Inference System—this is unique since all other systems use Bayesian Networks or other machine learning tools; (4) the classifier does not require any direct user involvement—in other systems, users must provide interruption annotations while reviewing video sessions so the system can learn; and (5) a promising direction for reasoning about interruptions for free-form tasks–this is largely an unsolved problem.}
}
@article{HARRIS2018103,
title = {Spatial evolution of regularization in learned behavior of animals},
journal = {Mathematical Biosciences},
volume = {299},
pages = {103-116},
year = {2018},
issn = {0025-5564},
doi = {https://doi.org/10.1016/j.mbs.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0025556417306648},
author = {Dakari Harris and Dominik Wodarz and Natalia L. Komarova},
abstract = {Stochastic population dynamics of learned traits are studied, where individual learners behave according to a reinforcement learner model, which is a nonlinear version of the Bush–Mosteller model. Depending on a regularization parameter (parameter a), the learners may possess different degrees of overmatching (regularization behavior, 0 ≤ a < 1), frequency matching (corresponding to a=1), or undermatching behavior (a > 1). Both non-spatial and spatial models are considered, to study the interplay of individual heterogeneity of behavior, spatial and temporal effects of learning, and the possibility of emergence of regional culture. In non-spatial models, we observe that populations of individuals learning from each other converge to a universally shared, deterministic rule (either rule “1” or rule “0”), only if they to some extent possess the ability to generalize (a < 1). Otherwise, a low-coherence solution where both rules are used intermittently by everyone, is achieved. If the evolution of the regularization ability is included, then we find that a initially evolves toward lower values, and a shared solution is established when everyone reliably uses the same rule. The spatial (2D) model has two well known limiting cases: if a=0 (the strongest degree of regularization), the model converges to a threshold voter model, and if a=1 (frequency matching), it is equivalent to the discrete diffusion equation. If 0 < a < 1 (the case where individuals regularize), spatial patterns emerge, where patches of different usage of the rule are formed. Smaller values of a lead to sharper and longer lived patches. Values of a < 1 close to unity result in probabilistic outcomes where patches only survive if they are attached to the boundary. Analytical treatment of the 1D case reveals the existence of approximate equilibria that have front structure, where spatially intermittent deterministic usage of one and the other rule are separated by interfaces whose analytical form is derived.}
}
@incollection{AGAPAKI202065,
title = {Chapter 3 - Scene understanding and model generation},
editor = {Ioannis Brilakis and Carl Haas},
booktitle = {Infrastructure Computer Vision},
publisher = {Butterworth-Heinemann},
pages = {65-167},
year = {2020},
isbn = {978-0-12-815503-5},
doi = {https://doi.org/10.1016/B978-0-12-815503-5.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128155035000036},
author = {Eva Agapaki and Mohammad Nahangi},
keywords = {3D object fitting, BIM, Data management, Deep learning, Digital twins, IFC, Segmentation, Shape classification},
abstract = {ICV scene understanding provides an overview about various object detection and fitting techniques that can produce accurate information for the digital twin (DT) generation of existing assets. The aim of this chapter is to acquire digital twins of existing infrastructure with the least amount of human intervention and give readers the appropriate tools to apply ICV scene understanding and fitting techniques for specific domain applications. The chapter gives an overview of scene understanding for infrastructure applications and user requirements and explores deep learning methods used so far on shape detection and the potential for its use in ICV scene understanding.}
}
@incollection{SUSILA2020247,
title = {Chapter Ten - Impact of cloud security in digital twin},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {247-263},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300488},
author = {Nagarajan Susila and Anand Sruthi and Sakthivel Usha},
keywords = {Cloud security, Digital twin prototype, Digital twin instance, Prognostics, Skyhigh security, Virtualization, Internet of things},
abstract = {Digital Twin is a way to virtually represent or model a physical object using the real time data. This innovation sets up a way to deal with industries and organizations to supervise their products, consequently bridging the gap between design and implementations. As the name suggests, “Digital Twin” infers that a reproduction of the product is made in order to have a nearby relationship with the live item. The procedure of computerized twin begins by gathering real time data, processed data, and operational data and performs distinctive investigation which helps in anticipating the future. This additionally enhances the customer experiences by giving a digital feel of their product. The objective behind all these is the job of gathering information and putting them in a place, i.e., the cloud which could store exorbitant data. The user experience gets enhanced by the intervention of digital twin technology which could help in the successful working of the products geographically distributed. The impact of Internet of Things and Cloud Computing lifts up the digital twin. The information gathered from the sources can be arranged in terms of utilization and prospect to change on a timely basis. These data, as they are stored require proper coordination and a legitimate use. Digital Twin innovation assumes incredible opportunities in the field of manufacturing, healthcare, smart cities, automobile and so on. The effect of having a digital twin for the product makes it simple for activities and recognize the blemishes, if any happened. This approach can help reduce the workload and furthermore can get trained on the virtual machine without the need of a specific training. With the most prevailing technologies of today, like Artificial Intelligence, Machine Learning and Internet of Things more prominent approach to train and monitor products, taking care of its own execution, collaborating to different frameworks, performing self-repairs are made possible. Hence the future is getting unfolded with the emerging DIGITAL TWIN era. The massive data utilized in the field of digital twin is prone to severe security breaches. Thus digital twin technology should be handled with extreme care so as to protect the data. Hence, this chapter identifies the ways and means of collecting, organizing and storing the data in a secured cloud environment. The data is filtered according to the use and priority and pushed into the cloud. It is determined to implement an exclusive algorithm for a secured cloud which would greatly benefit the users and the providers to handle and process it effectively.}
}
@article{TVENGE202036,
title = {Added value of a virtual approach to simulation-based learning in a manufacturing learning factory},
journal = {Procedia CIRP},
volume = {88},
pages = {36-41},
year = {2020},
note = {13th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 17-19 July 2019, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S221282712030322X},
author = {Nina Tvenge and Olga Ogorodnyk and Niels Peter Østbø and Kristian Martinsen},
keywords = {learning, simulation-based learning, manufacturing learning factory, cognitive processes, VR, AR},
abstract = {More and more learning factories (LF) are set up supporting the vision of Industry 4.0 connectivity and automation levels; thus, digital twins, virtual and augmented reality are emerging tools. Literature review is the basis for the discourse on possible constraints and opportunities these tools have for the cognitive learning processes in a simulation. State of the art give insight in the lack of research on value of digital learning activities in learning factory setting. This paper is a concept description, giving input to the community on aspects to be considered regarding the use VR/AR/digital twins in a learning factory context.}
}
@article{TOIVONEN2018135,
title = {The FMS Training Center - a versatile learning environment for engineering education},
journal = {Procedia Manufacturing},
volume = {23},
pages = {135-140},
year = {2018},
note = {“Advanced Engineering Education & Training for Manufacturing Innovation”8th CIRP Sponsored Conference on Learning Factories (CLF 2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918304785},
author = {Ville Toivonen and Minna Lanz and Hasse Nylund and Harri Nieminen},
keywords = {Production system design, digital twin},
abstract = {Digital twins are created by combining a design model with the use phase data of a product or a system. They are developed in order to observe and test the behavior of the target over its life cycle. This paper introduces a versatile learning environment focusing on a Flexible Manufacturing System (FMS). The FMS Training center phenomenon based learning environment consists of a physical training system and its digital twin. The realization of the digital twin is a virtual model linked to a system simulator and an industrial controller. Students can thus become familiar with the fully automated production system and develop and test programs in a virtual environment before visiting the production facility. The digital twin also meets an industrial need as it facilitates virtual commissioning of new system installations. Evaluation of the benefits in educational context, and the largest obstacles, of developing system level digital twins are described. A full system description of the learning environment is presented with the pedagogical objectives of the associated exercises. Future development and extendibility of the learning environment are also discussed.}
}
@article{BUISSON2019110197,
title = {Towards an integrated machine-learning framework for model evaluation and uncertainty quantification},
journal = {Nuclear Engineering and Design},
volume = {354},
pages = {110197},
year = {2019},
note = {Special Issue on TRENDS AND PERSPECTIVES IN NUCLEAR THERMAL-HYDRAULICS},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2019.110197},
url = {https://www.sciencedirect.com/science/article/pii/S0029549319302067},
author = {Bertrand Buisson and Djamel Lakehal},
keywords = {Fluid flow simulation, Wall boiling, Data analytics, Digital Twin, Machine-learning, Data-driven models (DDM)},
abstract = {We introduce a new paradigm for treating and exploiting simulation data, serving in parallel as an alternative workflow for model evaluation and uncertainty quantification. Instead of reporting simulations of base-case and specific variations scenarios, databases covering a wide spectrum of operational conditions are built by means of machine-learning using sophisticated mathematical algorithms. While the approach works for all sorts of computer-aided engineering applications, the present contribution addresses the CFD/CMFD sub-branch, with application to a widely used benchmark of convective flow boiling. In addition to comparing simulation and experimental results on a case-by-case basis, machine-learning is used to create their respective (CFD and experiment) data-driven models (DDM), which will in a later stage serve for assessing the predictive performance of the CFD models over a wider range of experimental conditions, hence providing a high-level classification of their range of applicability.}
}
@incollection{VIJAYAKUMAR2020265,
title = {Chapter Eleven - Digital twin in consumer choice modeling},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {265-284},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300531},
author = {D. Sudaroli Vijayakumar},
keywords = {Digital Twin, Retail, Consumer choice modeling, Tarantool},
abstract = {“Digital twin” more often perceived as a twin terminology along with industry virtualization of physical assets. The usage of digital twin on physical asset is well known, such as to predict when the individual parts of a machine must be replaced. However, digital twin technology in non-physical modeling is a vibrant research area. One area where digital twin can be effective is predicting the customer's needs. Most businesses to predict the customer's needs uses risk analysis and profitability assessment which holds its own pitfalls. One of the major downfalls arises during the analysis on historical data is the time consumed. Time is one of the crucial factors that determines the profit a company makes, holding of customers, satisfying the customers' needs at the right time, fails because of the static behavior. This can be made more effective by enforcing Digital twin to track the customer behavior dynamically such as the products they consume, their satisfaction. So instead of relying on the historical data, the data for digital twin will be from CRMs, logs, order processing info etc. Right product at right time can be achieved by creating suitable machine learning models on this dynamic dataset and this trained model are held in the digital twin, which runs them in real time. For achieving this approach, a specific technology called Tarantool Data Grid is very useful. In this chapter, we will explore how this technology can be used to create consumer choice modeling using Digital Twin with suitable use cases.}
}
@article{YILDIZ2020216,
title = {Virtual Factory: Digital Twin Based Integrated Factory Simulations},
journal = {Procedia CIRP},
volume = {93},
pages = {216-221},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120306077},
author = {Emre Yildiz and Charles Møller and Arne Bilberg},
keywords = {virtual factory, digital twin, modelling, simulation, virtual reality, industry 4.0},
abstract = {The co-evolution problem, which is known as the concurrent evolution of products, processes and production systems, along with increased complexity and shorter manufacturing operation lifecycles, makes modelling, simulation and evaluation of such operations challenging activities for industry players. This paper presents the concept of a digital twin-based virtual factory (VF) and its architecture to support modelling, simulation and evaluation of manufacturing systems while employing multi-user (collaborative and coordinated) virtual reality (VR) learning/training scenarios. This paper also addresses how digital twin-based virtual factory can support factory lifecycle processes by demonstrating the concept in a wind turbine manufacturing plant, including preliminary evaluation by industry experts.}
}
@article{DRODER2018187,
title = {A Machine Learning-Enhanced Digital Twin Approach for Human-Robot-Collaboration},
journal = {Procedia CIRP},
volume = {76},
pages = {187-192},
year = {2018},
note = {7th CIRP Conference on Assembly Technologies and Systems (CATS 2018)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118300295},
author = {Klaus Dröder and Paul Bobka and Tomas Germann and Felix Gabriel and Franz Dietrich},
keywords = {Human-Robot-Collaboration, Safety, Machine Learning},
abstract = {A key problem in human robot collaboration is a safe movement of the robot. The reason for this lies mainly in the variety of possible different events that can occur in an unstructured environment. Especially the description of a variable working space and the movements of humans are difficult to represent deterministically. In this paper, an approach to machine learning to enable industrial robots to bypass obstacles or people in the workspace is presented. First, a machine learning-enhanced robot control strategy is presented, which combines a nearest neighbor approach for path planning, clustering analysis and artificial neural networks for obstacle detection. Finally, a proof of concept is presented describing adaptive path planning for the protection of a human being.}
}
@article{UHLEMANN2017113,
title = {The Digital Twin: Demonstrating the Potential of Real Time Data Acquisition in Production Systems},
journal = {Procedia Manufacturing},
volume = {9},
pages = {113-120},
year = {2017},
note = {7th Conference on Learning Factories, CLF 2017},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2017.04.043},
url = {https://www.sciencedirect.com/science/article/pii/S2351978917301610},
author = {Thomas H.-J. Uhlemann and Christoph Schock and Christian Lehmann and Stefan Freiberger and Rolf Steinhilper},
keywords = {Digital Twin, Industry 4.0, Process optimization},
abstract = {The acquisition of data and the development of different options in production system and factory planning requires up to 2/3rds of the total needed time resources. The digitization of production systems offers the possibility of automated data acquisition. Nevertheless, approaches concerning fully automated data acquisition systems are not widely spread among SME (small and medium sized enterprises). On the one hand, this is caused by the heterogeneous databases, on the other hand by insufficient data processing systems. Furthermore, the advantages of The Digital Twin are not sufficiently known due to the lack of competence in SME concerning matters of Industry 4.0. In order to transfer knowledge about the benefits of digitalization, the development of demonstrating platforms is crucial. This paper introduces a learning factory based concept to demonstrate the potentials and advantages of real time data acquisition and subsequent simulation based data processing. Therefore, an existing learning factory will be upgraded regarding both, multi-modal data acquisition technologies as well as a locally independent optimization environment. Thereby the requirements of SME concerning flexible, easy to use, scalable and service oriented digitization applications are met. The approach is part of a concept for the realization of a Cyber Physical Production System (CPPS) in SME that ensures the development of an image of the production with the aid of a multi-modal data acquisition.}
}
@article{MULDNER2015127,
title = {Utilizing sensor data to model students’ creativity in a digital environment},
journal = {Computers in Human Behavior},
volume = {42},
pages = {127-137},
year = {2015},
note = {Digital Creativity: New Frontier for Research and Practice},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.10.060},
url = {https://www.sciencedirect.com/science/article/pii/S074756321300410X},
author = {Kasia Muldner and Winslow Burleson},
keywords = {Creativity, Student modeling, Eye tracking, EEG, Skin conductance, Intelligent Tutoring Systems},
abstract = {While creativity is essential for developing students’ broad expertise in Science, Technology, Engineering, and Math (STEM) fields, many students struggle with various aspects of being creative. Digital technologies have the unique opportunity to support the creative process by (1) recognizing elements of students’ creativity, such as when creativity is lacking (modeling step), and (2) providing tailored scaffolding based on that information (intervention step). However, to date little work exists on either of these aspects. Here, we focus on the modeling step. Specifically, we explore the utility of various sensing devices, including an eye tracker, a skin conductance bracelet, and an EEG sensor, for modeling creativity during an educational activity, namely geometry proof generation. We found reliable differences in sensor features characterizing low vs. high creativity students. We then applied machine learning to build classifiers that achieved good accuracy in distinguishing these two student groups, providing evidence that sensor features are valuable for modeling creativity.}
}
@article{WESIAK201413,
title = {Iterative augmentation of a medical training simulator: Effects of affective metacognitive scaffolding},
journal = {Computers & Education},
volume = {76},
pages = {13-29},
year = {2014},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2014.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S036013151400061X},
author = {Gudrun Wesiak and Christina M. Steiner and Adam Moore and Declan Dagger and Gordon Power and Marcel Berthold and Dietrich Albert and Owen Conlan},
keywords = {Adult learning, Evaluation of CAL systems, Simulations, Teaching/learning strategies, Metacognition},
abstract = {Experiential training simulators are gaining increasing popularity for job-related training due to their potential to engage and motivate adult learners. They are designed to provide learning experiences that are directly connected to users' work environments and support self-regulated learning. Nevertheless, learners often fail to transfer the knowledge gained in the simulated environment to real-world contexts. The EU-funded ImREAL project aimed to bridge that gap by developing a suite of intelligent services designed to enhance existing training simulators. This paper presents work that was a subset of this research project, reporting the iterative development and evaluation of a scaffolding service, which was integrated into a simulator for training medical students to perform diagnostic interviews. The study comprises three evaluation phases, comparing the pure simulator to a first version with metacognitive scaffolding and then to a final version with affective metacognitive scaffolding and enriched user modelling. The scaffolding service provides the learner with metacognitive prompts; affective elements are realized by an integrated affect reporting tool and affective prompts. Using a mixed-method approach by analysing questionnaires (N = 106) and log-data (N = 426), the effects of the services were investigated with respect to real-world relevance, self-regulated learning support, learning experience, and integration. Despite some limitations, the outcomes of this study demonstrate the usefulness of affective metacognitive scaffolding in the context of experiential training simulators; significant post-simulation increases in perceived relevance of the simulator, reflective note-taking, overall motivation, and feeling of success could be identified. Perceived usability and flow of the simulation increased, whereas overall workload and frustration decreased. However, low response rates to specific functions of the simulation point to a need to further investigate how to raise users' awareness and understanding of the provided tools, to encourage interaction with the services, and to better convey the benefits of using them. Thus, future challenges concern not so much technological developments for personalizing learning experiences, but rather new ways to change user attitudes towards an open approach to learning systems that enables them to benefit from all offered features.}
}
@article{KORDAKI201526,
title = {A Constructivist, Modeling Methodology for the Design of Educational Card Games},
journal = {Procedia - Social and Behavioral Sciences},
volume = {191},
pages = {26-30},
year = {2015},
note = {The Proceedings of 6th World Conference on educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.04.669},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815029365},
author = {Maria Kordaki},
keywords = {Contructivism, design, modelling, educational card games ;},
abstract = {This paper presents a specific modeling methodology for the creation of educational card games. This methodology is based on the creation of the following three models: (a) the model of the subject matter; including all aspects of the learning subject in question, (b) the learners’ model; including learners’ non scientific conceptions about the aforementioned learning subject, and (c) the learning model; consisting of an appropriate learning strategy for the learning of the subject in question through card-game play, taking into account basic social and constructivist views of learning in combination with as well as key structural characteristics of games which can contribute to players’ engagement. Based on the aforementioned methodology, the design of various types of cards is proposed.}
}
@article{CHENG201658,
title = {Understanding and enhancing personal transfer of creative learning},
journal = {Thinking Skills and Creativity},
volume = {22},
pages = {58-73},
year = {2016},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1871187116300955},
author = {Vivian M.Y. Cheng},
keywords = {Personal transfer, Creative learning, Creativity, Thematic curriculum, Higher education},
abstract = {The ‘actor-orientated transfer’ approach was adopted to examine the personal transfer of creative learning in a thematic course on toys in a higher education of Hong Kong. The personal transfers of creative learning were found to be spontaneous, far, diverse, multidirectional, highly individual and, sometimes, quite unexpected. Creativity-related attitudes, conceptions, habits and thinking strategies were commonly transferred from the initial study over to daily life, new learning and teaching, all because the learning was impressive, useful, full of surprises and easy to remember. A number of curricular, contextual and student's personal factors were identified to be either facilitating or hindering these transfers. It is concluded that significant parts of creative learning can be transferred in personal ways, if the curriculum is suitably designed and implemented to meet the needs of students in local context. A model for personal transfer of creative learning, which consisted of four core themes (i.e. context, content, impact and factor of personal transfer) and their sub-themes, was depicted. The ‘transfer in pieces’ theory was applied to explain the nature of personal transfer. Ultimately, this study helps to establish a new conception of creativity education as a type of education that values and facilitates personal transfer.}
}
@article{LIU201744,
title = {Scientific modeling with mobile devices in high school physics labs},
journal = {Computers & Education},
volume = {105},
pages = {44-56},
year = {2017},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2016.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360131516302019},
author = {Chia-Yu Liu and Chao-Jung Wu and Wing-Kwong Wong and Yunn-Wen Lien and Tsung-Kai Chao},
keywords = {Architectures for educational technology system, Teaching/learning strategies, Improving classroom teaching, Pedagogical issues, Applications in subject areas},
abstract = {Scientific modeling is thought to help students understand the world and scientific phenomenon. Science laboratory in school should provide well-designed activities to promote students' model building skills. Thus, this study aims to propose microcomputer based labs with several data acquisition tools and a modeling tool, which can assist students to collect and visualize data in faster and fancier ways, and generate mathematical models to fit the data, thus exercising their skills of scientific modeling. Thirty-two high school students participated in the science laboratory courses within two semesters for four labs. Results showed that students' overall success rates of model building were approaching 50%; the duration of participants' modeling time decreased with the increase of the experimental labs; the benefits of doing physics labs with smartphones were confirmed by the success rates, personal preferences, and students' feedback. Regarding students' spontaneous model building behavior in the first lab, almost 90% of the participants fitted data with linear equation; most participants adjusted coefficients to fit the data, instead of changing the highest degree of equation; and different strategies were used by successful participants and the others. These results indicated that the combination of modern data acquisition tools and fitting data with a modeling tool would provide an alternative and meaningful approach to doing physics labs at high school.}
}
@article{PAIVA2016769,
title = {What do students do on-line? Modeling students' interactions to improve their learning experience},
journal = {Computers in Human Behavior},
volume = {64},
pages = {769-781},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.07.048},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216305386},
author = {Ranilson Paiva and Ig Ibert Bittencourt and Thyago Tenório and Patricia Jaques and Seiji Isotani},
keywords = {User modeling, Gamified learning environments, Interactional characteristics, Pedagogical decision-making, Data-informed decision-making},
abstract = {In this work, we present an approach to model and analyze students interactions, within a gamified on-line learning environment, in order to assist teachers and tutors (education professionals) decision-making, regarding their students learning experience. We noticed that asking students this information might not bring precise and dynamic results for all students. This way, we characterize the educational resources available in the studied environment, and collected data from students interactions with these resources. Our objective was to generate the students’ interactional profile (a model of their interactions). The information, then, is presented to teachers and tutors, who should use it to guide their pedagogical decision-making process. In this study, the types of interactions were used to personalize gamification elements named missions. We experimented the approach with two groups of users from the studied environment (MeuTutor). The data analysis showed differences in the way these groups were performing, where group B was considerably above group A. We sent the personalized missions (following our approach) to every student from group A, and waited some time for them to interact with it. In the end we checked the effect of this treatment, which, according to the results, promoted relevant improvement in group A interactions.}
}
@article{BRENNER2017198,
title = {Digital Twin as Enabler for an Innovative Digital Shopfloor Management System in the ESB Logistics Learning Factory at Reutlingen - University},
journal = {Procedia Manufacturing},
volume = {9},
pages = {198-205},
year = {2017},
note = {7th Conference on Learning Factories, CLF 2017},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2017.04.039},
url = {https://www.sciencedirect.com/science/article/pii/S2351978917301579},
author = {Beate Brenner and Vera Hummel},
keywords = {Digital twin, business ecosystem, cloud- and app-based platform, digital factory, software agents, indoor localization, sensor combinations, digital shopfloor management},
abstract = {Technologies for mapping the “digital twin” have been under development for approximately 20 years. Nowadays increasingly intelligent, individualized products encourages companies to respond innovatively to customer requirements and to handle the rising product variations quickly. An integrated engineering network, spanning across the entire value chain, is operated to intelligently connect various company divisions, and to generate a business ecosystem for products, services and communities. The conditions for the digital twin are thereby determined in which the digital world can be fed into the real, and the real world back into the digital to deal such intelligent products with rising variations. The term digital twin can be described as a digital copy of a real factory, machine, worker etc., that is created and can be independently expanded, automatically updated as well as being globally available in real time. Every real product and production site is permanently accompanied by a digital twin. First prototypes of such digital twins already exist in the ESB Logistics Learning Factory on a cloud- and app-based software that builds on a dynamic, multidimensional data and information model. A standardized language of the robot control systems via software agents and positioning systems has to be integrated. The aspect of the continuity of the real factory in the digital factory as an economical means of ensuring continuous actuality of digital models looks as the basis of changeability. For the indoor localization sensor combinations that in addition to the hardware already contain the software required for the sensor data fusion should be used. Processing systems, scenario-live-simulations and digital shop floor management results in a mandatory procedural combination. Essential to the digital twin is the ability to consistently provide all subsystems with the latest state of all required information, methods and algorithms.}
}
@article{ZHAO2015159,
title = {A local learning algorithm for random weights networks},
journal = {Knowledge-Based Systems},
volume = {74},
pages = {159-166},
year = {2015},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2014.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0950705114004067},
author = {Jianwei Zhao and Zhihui Wang and Feilong Cao and Dianhui Wang},
keywords = {Random weights networks, Local learning algorithm, Regularization model, Moving least squares method, Feedforward neural networks},
abstract = {Robust modelling is significant to deal with complex systems with uncertainties. This paper aims to develop a novel learning algorithm for training regularized local random weights networks (RWNs). The learner model, terms as RL-RWN, is built on regularized moving least squares method and generalizes the solution obtained from the standard least square technique. Simulations are carried out using two benchmark datasets, including Auto-MPG data and surface reconstruction data. Results demonstrate that our proposed RL-RWN outperforms the original RWN and radial basis function networks.}
}
@article{ARAR2016106,
title = {Deriving thresholds of software metrics to predict faults on open source software: Replicated case studies},
journal = {Expert Systems with Applications},
volume = {61},
pages = {106-121},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416302366},
author = {Ömer Faruk Arar and Kürşat Ayan},
keywords = {Machine learning, Software quality metrics, Threshold, Software fault prediction, Logistic regression, Bender method},
abstract = {Object-oriented metrics aim to exhibit the quality of source code and give insight to it quantitatively. Each metric assesses the code from a different aspect. There is a relationship between the quality level and the risk level of source code. The objective of this paper is to empirically examine whether or not there are effective threshold values for source code metrics. It is targeted to derive generalized thresholds that can be used in different software systems. The relationship between metric thresholds and fault-proneness was investigated empirically in this study by using ten open-source software systems. Three types of fault-proneness were defined for the software modules: non-fault-prone, more-than-one-fault-prone, and more-than-three-fault-prone. Two independent case studies were carried out to derive two different threshold values. A single set was created by merging ten datasets and was used as training data by the model. The learner model was created using logistic regression and the Bender method. Results revealed that some metrics have threshold effects. Seven metrics gave satisfactory results in the first case study. In the second case study, eleven metrics gave satisfactory results. This study makes contributions primarily for use by software developers and testers. Software developers can see classes or modules that require revising; this, consequently, contributes to an increment in quality for these modules and a decrement in their risk level. Testers can identify modules that need more testing effort and can prioritize modules according to their risk levels.}
}
@article{SHAHBAZI201739,
title = {Generation of rhythmic hand movements in humanoid robots by a neural imitation learning architecture},
journal = {Biologically Inspired Cognitive Architectures},
volume = {19},
pages = {39-48},
year = {2017},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300354},
author = {Hamed Shahbazi and Reyhaneh Parandeh and Kamal Jamshidi},
keywords = {Imitation learning, Neural networks, Central pattern generator},
abstract = {This paper presents a two layer system for imitation learning in humanoid robots. The first layer of this system records complicated and rhythmic movement of the trainer using a motion capture device. It solves an inverse kinematic problem with the help of an adaptive Neuro-Fuzzy Inference system. Then it can achieve angles records of any joints involved in the desired motion. The trajectory is given as input to the systems second layer. The layer deals with extracting optimal parameters of the trajectories obtained from the first layer using a network of oscillator neurons and Particle Swarm Optimization algorithm. This system is capable to obtain any complex motion and rhythmic trajectory via first layer and learns rhythmic trajectories in the second layer then converge towards all these movements. Moreover, this two layer system is able to provide various features of a learner model, for instance resistance against perturbations, modulation of trajectories amplitude and frequency. The simulation results of the learning system is performed in the robot simulator WEBOTS linked with MATLAB software. Practical implementation on an NAO robot demonstrate that the robot has learned desired motion with high accuracy. These results show that proposed system in this paper produces high convergence rate and low test error.}
}
@article{KAR2014243,
title = {Applications of neuro fuzzy systems: A brief review and future outline},
journal = {Applied Soft Computing},
volume = {15},
pages = {243-259},
year = {2014},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2013.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S1568494613003487},
author = {Samarjit Kar and Sujit Das and Pijush Kanti Ghosh},
keywords = {Neuro fuzzy system, Neuro fuzzy system applications, Methodologies, Literature review},
abstract = {This paper surveys neuro fuzzy systems (NFS) development using classification and literature review of articles for the last decade (2002–2012) to explore how various NFS methodologies have been developed during this period. Based on the selected journals of different NFS applications and different online database of NFS, this article surveys and classifies NFS applications into ten different categories such as student modeling system, medical system, economic system, electrical and electronics system, traffic control, image processing and feature extraction, manufacturing and system modeling, forecasting and predictions, NFS enhancements and social sciences. For each of these categories, this paper mentions a brief future outline. This review study indicates mainly three types of future development directions for NFS methodologies, domains and article types: (1) NFS methodologies are tending to be developed toward expertise orientation. (2) It is suggested that different social science methodologies could be implemented using NFS as another kind of expert methodology. (3) The ability to continually change and learning capability is the driving power of NFS methodologies and will be the key for future intelligent applications.}
}
@article{WAGNER20142023,
title = {A model for profile management applied to ubiquitous learning environments},
journal = {Expert Systems with Applications},
volume = {41},
number = {4, Part 2},
pages = {2023-2034},
year = {2014},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.08.098},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413007203},
author = {André Wagner and Jorge Luis Victória Barbosa and Débora Nice Ferrari Barbosa},
keywords = {Profile management, Ubiquitous computing, User modeling, Trails, Profile inference, Entity-adapted interaction},
abstract = {Ubiquitous systems have the challenge of implicitly collect relevant information about entities, and use this information to understand and predict their behavior. This allows the applications to adapt themselves to the entities, thus avoiding to overflow them with inquires and information. The analysis of trails, the context-aware history of actions, can further improve the relevance of information. This paper proposes a model that allows applications to register entities’ actions in trails and infer profile information from these trails, using semantic interoperability and thus allowing different applications to share information and infer a unified profile. An application was developed and integrated with two different softwares in a scenario of ubiquitous learning, where the student profiles were dynamically updated, allowing them to better adapt to the environment. The contributions of this model are the use of trails for extracting profiles and the capability of managing dynamic inference rules for profile generation.}
}
@article{MCCUTCHEON2017605,
title = {Interprofessional objective structured teaching exercise (iOSTE) to train preceptors},
journal = {Currents in Pharmacy Teaching and Learning},
volume = {9},
number = {4},
pages = {605-615},
year = {2017},
issn = {1877-1297},
doi = {https://doi.org/10.1016/j.cptl.2017.03.023},
url = {https://www.sciencedirect.com/science/article/pii/S1877129716300806},
author = {Livia R.M. McCutcheon and Kathryn Whitcomb and Craig D. Cox and Mary S. Klein and Hansel Burley and Terrance Youngblood and Cynthia Raehl},
keywords = {Preceptor development, Objective structured teaching exercise, Objective structured teaching examination, Simulation, Interprofessional education, Debriefing},
abstract = {Background and purpose
Interprofessional education (IPE) is important in the education of all health care students, yet limited IPE training has been provided to preceptors who train these students in the clinical setting. Simulation using the standardized student model has been used to train health care preceptors in medicine. To our knowledge, there are no reports utilizing interprofessional objective structured teaching exercises (iOSTE) to train pharmacy preceptors. The primary objectives of this pilot study were to evaluate the effects of iOSTE on the pharmacy preceptors’ perceived importance of the Interprofessional Education Collaborative (IPEC) core competencies and confidence in precepting interprofessional students. Additionally, data were collected regarding pharmacy preceptors' prior experiences in simulation and debriefing.
Educational activity and setting
Preceptors (n=23) participated in an iOSTE and debriefed with trained standardized nursing and pharmacy students caring for a trained standardized asthma patient.
Findings
Pre- versus post-iOSTE survey data showed statistically significant improvements in all self-confidence related items, including the following abilities: precept students from different disciplines (p=0.004), facilitate a simulation activity (p=0.001), conduct the debriefing process (p<0.001), and discuss with students the IPE core competencies (p=0.001). Additionally, responses to post-iOSTE survey questions assessing the learning activity showed high ratings (median=5, interquartile range=4 to 5). Pharmacy preceptors increased their teaching ability and confidence level in communicating with students from other health care professions.
Summary
These findings indicate that iOSTE is a useful and well-received method for preceptor development.}
}
@article{WANG2017210,
title = {Robust stochastic configuration networks with kernel density estimation for uncertain data regression},
journal = {Information Sciences},
volume = {412-413},
pages = {210-222},
year = {2017},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.05.047},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517307636},
author = {Dianhui Wang and Ming Li},
keywords = {Stochastic configuration networks, Robust data regression, Randomized algorithms, Kernel density estimation, Alternating optimization techniques},
abstract = {Neural networks have been widely used as predictive models to fit data distribution, and they could be implemented through learning a collection of samples. In many applications, however, the given dataset may contain noisy samples or outliers which may result in a poor learner model in terms of generalization. This paper contributes to a development of robust stochastic configuration networks (RSCNs) for resolving uncertain data regression problems. RSCNs are built on original stochastic configuration networks with weighted least squares method for evaluating the output weights, and the input weights and biases are incrementally and randomly generated by satisfying with a set of inequality constrains. The kernel density estimation (KDE) method is employed to set the penalty weights for each training samples, so that some negative impacts, caused by noisy data or outliers, on the resulting learner model can be reduced. The alternating optimization technique is applied for updating a RSCN model with improved penalty weights computed from the kernel density estimation function. Performance evaluation is carried out by a function approximation, four benchmark datasets and a case study on engineering application. Comparisons to other robust randomised neural modelling techniques, including the probabilistic robust learning algorithm for neural networks with random weights and improved RVFL networks, indicate that the proposed RSCNs with KDE perform favourably and demonstrate good potential for real-world applications.}
}
@article{SADOWSKI201810,
title = {Pull-off adhesion prediction of variable thick overlay to the substrate},
journal = {Automation in Construction},
volume = {85},
pages = {10-23},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517308816},
author = {Łukasz Sadowski and Jerzy Hoła and Sławomir Czarnecki and Dianhui Wang},
keywords = {Layered concrete elements, Variable thickness, Interlayer bond, Pull-off adhesion, Non-destructive testing, Artificial intelligence},
abstract = {Non-destructive identification of the pull-off adhesion of a concrete substrate to an overlay mortar with variable thickness using artificial neural networks (ANNs) is studied in this paper. Selected ANNs with various training algorithms were tested on the basis of the parameter which describes the thickness of the overlay and also the parameters specified experimentally using non-destructive testing (NDT) methods. Real world data collected from experiments of pull-off adhesion were used for building our learner models. The tests were carried out in the same place where tests using NDT methods were performed. Three variant analyses of the possibility of such identification were conducted. The variance was calculated for these testing methods and parameters obtained with their usage, without considering the parameter that describes the thickness of the overlay in this work.}
}
@article{PENAAYALA2014131,
title = {Activity theory as a framework for building adaptive e-learning systems: A case to provide empirical evidence},
journal = {Computers in Human Behavior},
volume = {30},
pages = {131-145},
year = {2014},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.07.057},
url = {https://www.sciencedirect.com/science/article/pii/S0747563213002963},
author = {Alejandro Peña-Ayala and Humberto Sossa and Ignacio Méndez},
keywords = {Activity theory, Adaptive e-learning systems, Proactive student model, Anticipation principle, Teaching–learning experience},
abstract = {We apply activity theory (AT) to design adaptive e-learning systems (AeLS). AT is a framework to study human’s behavior at learning; whereas, AeLS enhance students’ apprenticeship by the personalization of teaching–learning experiences. AeLS depict users’ traits and predicts learning outcomes. The approach was successfully tested: Experimental group took lectures chosen by the anticipation AT principle; whilst, control group received randomly selected lectures. Learning achieved by experimental group reveals a correlation quite significant and high positive; but, for control group the correlation it is not significant and medium positive. We conclude: AT is a useful framework to design AeLS and provide student-centered education.}
}
@article{ORTIGOSA2014527,
title = {Sentiment analysis in Facebook and its application to e-learning},
journal = {Computers in Human Behavior},
volume = {31},
pages = {527-541},
year = {2014},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0747563213001751},
author = {Alvaro Ortigosa and José M. Martín and Rosa M. Carro},
keywords = {Sentiment analysis, Social networks, User modeling, Adaptive e-learning},
abstract = {This paper presents a new method for sentiment analysis in Facebook that, starting from messages written by users, supports: (i) to extract information about the users’ sentiment polarity (positive, neutral or negative), as transmitted in the messages they write; and (ii) to model the users’ usual sentiment polarity and to detect significant emotional changes. We have implemented this method in SentBuk, a Facebook application also presented in this paper. SentBuk retrieves messages written by users in Facebook and classifies them according to their polarity, showing the results to the users through an interactive interface. It also supports emotional change detection, friend’s emotion finding, user classification according to their messages, and statistics, among others. The classification method implemented in SentBuk follows a hybrid approach: it combines lexical-based and machine-learning techniques. The results obtained through this approach show that it is feasible to perform sentiment analysis in Facebook with high accuracy (83.27%). In the context of e-learning, it is very useful to have information about the users’ sentiments available. On one hand, this information can be used by adaptive e-learning systems to support personalized learning, by considering the user’s emotional state when recommending him/her the most suitable activities to be tackled at each time. On the other hand, the students’ sentiments towards a course can serve as feedback for teachers, especially in the case of online learning, where face-to-face contact is less frequent. The usefulness of this work in the context of e-learning, both for teachers and for adaptive systems, is described too.}
}
@article{LABIB2017433,
title = {On the way to learning style models integration: a Learner's Characteristics Ontology},
journal = {Computers in Human Behavior},
volume = {73},
pages = {433-445},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.03.054},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217302133},
author = {A. Ezzat Labib and José H. Canós and M. Carmen Penadés},
keywords = {Learning style models, Ontology, E-learning, Adaptive learning, Personalized learning, Learning characteristics},
abstract = {On the way to increasing customization in e-learning systems, the learner model is the main source of variability. Such a model includes a number of psychological characteristics and study preferences that describe the learner's personality traits related to learning. During the last decades, the design methods and tools for e-learning have been designed assuming specific learner models. Therefore, in the search for a learning environment suitable for as many learner models as possible, we need tools to explore -and exploit- such models. In general, the learner's characteristics can be linked to the so-called learner's learning style (which is a part of the learner model) to provide the instructor with extensive knowledge about the learner's characterization in perceiving and processing information. Numerous learning styles have been proposed in the last decades, in some cases with overlapping characteristics with the same or different names. Thus, the heterogeneity of the learning style space makes it difficult to handle customization effectively. In this paper, we introduce a Learner's Characteristics Ontology based on creating interconnections between the different learning style model dimensions and learning styles with the relevant learner's characteristics, that: (1) helps instructors to improve and personalize the learning content; (2) can recommend learning materials to learners according to their learning characteristics and preferences; (3) can provide both instructors and learners with extensive knowledge about how they can improve their teaching and learning abilities; and (4) can improve communications and interaction between humans and computers by specifying the semantics of the learning style models' characteristics.}
}
@article{CLEMENTE2014508,
title = {Applying a student modeling with non-monotonic diagnosis to Intelligent Virtual Environment for Training/Instruction},
journal = {Expert Systems with Applications},
volume = {41},
number = {2},
pages = {508-520},
year = {2014},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.07.077},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413005642},
author = {Julia Clemente and Jaime Ramírez and Angélica {de Antonio}},
keywords = {Intelligent Tutoring System, Student model, Pedagogic diagnosis},
abstract = {We present a student modeling approach that has been designed to be part of an Intelligent Virtual Environment for Training and/or Instruction (IVET). In order to provide the proper tutoring to a student, an IVET needs to keep and update dynamically a student model taking into account the student’s behaviour in the Virtual Environment. For that purpose, the proposed student model employs a student ontology, a pedagogic diagnosis module and a Conflict Solver module. The goal of the pedagogic diagnosis module is to infer which learning objectives have been acquired or not by the student. Nevertheless, the diagnosis process can be complicated by the fact that while learning the student will not only acquire new knowledge, but he/she may also forget some previously acquired knowledge, or he/she may have some oversights that could mislead the tutor about the true state of the student’s knowledge. All of these situations will lead to contradictions in the student model that must be solved so that the diagnosis can continue. Thus, our approach consists in applying diagnosis rules until a contradiction arises. At that moment, a conflict solver module is responsible of classifying and solving the contradiction. Next, the student ontology is updated according to the resolution adopted by the Conflict Solver and the diagnosis can continue. This paper mainly focuses on the design of the proper mechanisms of the student model to deal with the non monotonic nature of the pedagogic diagnosis.}
}
@article{LI201567,
title = {Integrating representation learning and skill learning in a human-like intelligent agent},
journal = {Artificial Intelligence},
volume = {219},
pages = {67-91},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214001349},
author = {Nan Li and Noboru Matsuda and William W. Cohen and Kenneth R. Koedinger},
keywords = {Agent learning, Representation learning, Student modeling},
abstract = {Building an intelligent agent that simulates human learning of math and science could potentially benefit both cognitive science, by contributing to the understanding of human learning, and artificial intelligence, by advancing the goal of creating human-level intelligence. However, constructing such a learning agent currently requires manual encoding of prior domain knowledge; in addition to being a poor model of human acquisition of prior knowledge, manual knowledge-encoding is both time-consuming and error-prone. Previous research has shown that one of the key factors that differentiates experts and novices is their different representations of knowledge. Experts view the world in terms of deep functional features, while novices view it in terms of shallow perceptual features. Moreover, since the performance of learning algorithms is sensitive to representation, the deep features are also important in achieving effective machine learning. In this paper, we present an efficient algorithm that acquires representation knowledge in the form of “deep features”, and demonstrate its effectiveness in the domain of algebra as well as synthetic domains. We integrate this algorithm into a machine-learning agent, SimStudent, which learns procedural knowledge by observing a tutor solve sample problems, and by getting feedback while actively solving problems on its own. We show that learning “deep features” reduces the requirements for knowledge engineering. Moreover, we propose an approach that automatically discovers student models using the extended SimStudent. By fitting the discovered model to real student learning curve data, we show that it is a better student model than human-generated models, and demonstrate how the discovered model may be used to improve a tutoring system's instructional strategy.}
}
@article{SALMERONMAJADAS2014691,
title = {An Evaluation of Mouse and Keyboard Interaction Indicators towards Non-intrusive and Low Cost Affective Modeling in an Educational Context},
journal = {Procedia Computer Science},
volume = {35},
pages = {691-700},
year = {2014},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.08.151},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914011168},
author = {Sergio Salmeron-Majadas and Olga C. Santos and Jesus G. Boticario},
keywords = {Affective Computing, Affective States, User Modeling, Human-Computer Interaction, Keyboard, Mouse.},
abstract = {In this paper we propose a series of indicators, which derive from user's interactions with mouse and keyboard. The goal is to evaluate their use in identifying affective states and behavior changes in an e-learning platform by means of non-intrusive and low cost methods. The approach we have followed study user's interactions regardless of the task being performed and its presentation, aiming at finding a solution applicable in any domain. In particular, mouse movements and clicks, as well as keystrokes were recorded during a math problem solving activity where users involved in the experiment had not only to score their degree of valence (i.e., pleasure versus displeasure) and arousal (i.e., high activation versus low activation) of their affective states after each problem by using the Self-Assessment-Manikin scale, but also type a description of their own feelings. By using that affective labeling, we evaluated the information provided by these different indicators processed from the original user's interactions logs. In total, we computed 42 keyboard indicators and 96 mouse indicators.}
}
@article{CUI2016505,
title = {High dimensional data regression using Lasso model and neural networks with random weights},
journal = {Information Sciences},
volume = {372},
pages = {505-517},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.08.060},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516306314},
author = {Caihao Cui and Dianhui Wang},
keywords = {High dimensional data regression, Lasso model, Ensemble features, Neural networks with random weights, NMR data},
abstract = {This paper aims to develop a framework for high dimensional data regression, where the model interpretation and prediction accuracy are regularized. Taking application background into account, we supposed that the collected samples for building learner models are expensive and limited. Our technical contributions include the generation of ensemble features (EF) using Lasso models with some selective regularizing factors estimated via a cross-validation procedure; and predictive model building using neural networks with random weights, where the weights and biases of the hidden nodes are assigned randomly in a specific interval, and the output weights are evaluated analytically by a regularized least square method. Experiments with comparisons on estimating protein content of milk from its NMR spectrum are carried out by a data set with 31,570 dimensions (spectrum size) and 120 samples. Results demonstrate that our proposed solution for data regression problems with small samples and high dimensionality is promising, and the learning system performs robustly with respect to a key parameter setting in the ensemble feature generation.}
}
@article{CAO2016546,
title = {An iterative learning algorithm for feedforward neural networks with random weights},
journal = {Information Sciences},
volume = {328},
pages = {546-557},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0020025515006568},
author = {Feilong Cao and Dianhui Wang and Houying Zhu and Yuguang Wang},
keywords = {Neural networks with random weights, Learning algorithm, Stability, Convergence},
abstract = {Feedforward neural networks with random weights (FNNRWs), as random basis function approximators, have received considerable attention due to their potential applications in dealing with large scale datasets. Special characteristics of such a learner model come from weights specification, that is, the input weights and biases are randomly assigned and the output weights can be analytically evaluated by a Moore–Penrose generalized inverse of the hidden output matrix. When the size of data samples becomes very large, such a learning scheme is infeasible for problem solving. This paper aims to develop an iterative solution for training FNNRWs with large scale datasets, where a regularization model is employed to potentially produce a learner model with improved generalization capability. Theoretical results on the convergence and stability of the proposed learning algorithm are established. Experiments on some UCI benchmark datasets and a face recognition dataset are carried out, and the results and comparisons indicate the applicability and effectiveness of our proposed learning algorithm for dealing with large scale datasets.}
}
@article{RUIPEREZVALIENTE2015139,
title = {ALAS-KA: A learning analytics extension for better understanding the learning process in the Khan Academy platform},
journal = {Computers in Human Behavior},
volume = {47},
pages = {139-148},
year = {2015},
note = {Learning Analytics, Educational Data Mining and data-driven Educational Decision Making},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2014.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0747563214003689},
author = {José A. Ruipérez-Valiente and Pedro J. Muñoz-Merino and Derick Leony and Carlos {Delgado Kloos}},
keywords = {Learning analytics, Architectures, Decision making, Visualizations, Data processing},
abstract = {The Khan Academy platform enables powerful on-line courses in which students can watch videos, solve exercises, or earn badges. This platform provides an advanced learning analytics module with useful visualizations. Nevertheless, it can be improved. In this paper, we describe ALAS-KA, which provides an extension of the learning analytics support for the Khan Academy platform. We herein present an overview of the architecture of ALAS-KA. In addition, we report the different types of visualizations and information provided by ALAS-KA, which have not been available previously in the Khan Academy platform. ALAS-KA includes new visualizations for the entire class and also for individual students. Individual visualizations can be used to check on the learning styles of students based on all the indicators available. ALAS-KA visualizations help teachers and students to make decisions in the learning process. The paper presents some guidelines and examples to help teachers make these decisions based on data from undergraduate courses, where ALAS-KA was installed. These courses (physics, chemistry, and mathematics) for freshmen were developed at Universidad Carlos III de Madrid (UC3M) and were taken by more than 300 students.}
}
@incollection{KALZ201593,
title = {Lifelong Learning and Its Support with New Technologies},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {93-99},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.92006-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868920063},
author = {Marco Kalz},
keywords = {Contextualized learning, Learning analytics, Learning networks, Lifelong learning, Mobile learning, Open educational practices, Open educational resources, Open learner models, Technology-enhanced learning},
abstract = {This article provides an overview of the use of new technologies for lifelong learning. While in the past learning technologies were mostly provided by educational institutions to support a specific lifetime or shorter learning episodes, nowadays more personal technologies are used for lifelong learning to support self-organized learning. Four important developments are introduced in this article, namely, open learner models and learning analytics, learning networks and networked learning, open educational resources and practices, and last but not least, mobile and contextualized learning. The state of the art in these research fields is summarized and the future potential and requirements for lifelong learning are highlighted.}
}
@article{KORDAKI20141631,
title = {On the Design of Educational Digital Stories: The Ed-W Model},
journal = {Procedia - Social and Behavioral Sciences},
volume = {116},
pages = {1631-1635},
year = {2014},
note = {5th World Conference on Educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.01.447},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814004649},
author = {Maria Kordaki},
keywords = {Constructivism, storyboarding, modelling, educational digital stories},
abstract = {This paper focuses on the presentation of an educational, modeling methodology (the Ed-W methodology) that is appropriate for the design of storyboards for educational digital stories. This methodology is based on the creation of the following three models:(a) the model of the subject matter; including all aspects of the learning subject in question, (b) the learners’ model; including learners’ non scientific conceptions about the aforementioned learning subject, and (c) the Ed-W learning model; consisting of a 5-step digital story boarding strategy for the learning of the subject in question, while at the same time acknowledging the students’ non scientific conceptions. The aforementioned 5 steps are: (a) the hero faces a problematic situation where she/he needs to use the knowledge of the subject matter in question, (b) the situation is worsened, due to actions which are based on the hero's non scientific conceptions described in the learners’ model, (c) the situation is improved due to external, uncontrollable factors, (d) the situation becomes terrible because the hero coninues to act in the previously mentioned way, and (e) the hero is forced to reflect on her/his thoughts and practices, and makes appropriate corrections. Then, all problems are finally resolved. To illustrate the aforementioned design methodology, an example of the design of a concrete digital story will be also demonstrated.}
}
@article{WANG201755,
title = {Stochastic configuration networks ensemble with heterogeneous features for large-scale data analytics},
journal = {Information Sciences},
volume = {417},
pages = {55-71},
year = {2017},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516319739},
author = {Dianhui Wang and Caihao Cui},
keywords = {Stochastic configuration networks, Large-scale data analytics, Heterogeneous features, Ensemble learning, Negative correlation learning},
abstract = {This paper presents a fast decorrelated neuro-ensemble with heterogeneous features for large-scale data analytics, where stochastic configuration networks (SCNs) are employed as base learner models and the well-known negative correlation learning (NCL) strategy is adopted to evaluate the output weights. By feeding a large number of samples into the SCN base models, we obtain a huge sized linear equation system which is difficult to be solved by means of computing a pseudo-inverse used in the least squares method. Based on the group of heterogeneous features, the block Jacobi and Gauss–Seidel methods are employed to iteratively evaluate the output weights, and a convergence analysis is given with a demonstration on the uniqueness of these iterative solutions. Experiments with comparisons on two large-scale datasets are carried out, and the system robustness with respect to the regularizing factor used in NCL is given. Results indicate that the proposed ensemble learning techniques have good potential for resolving large-scale data modelling problems.}
}
@article{LUNA20151387,
title = {An ontology-based approach for representing the interaction process between user profile and its context for collaborative learning environments},
journal = {Computers in Human Behavior},
volume = {51},
pages = {1387-1394},
year = {2015},
note = {Computing for Human Learning, Behaviour and Collaboration in the Social and Mobile Networks Era},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2014.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0747563214005275},
author = {Vladimir Luna and Rolando Quintero and Miguel Torres and Marco Moreno-Ibarra and Giovanni Guzmán and Imelda Escamilla},
keywords = {Collaborative learning, User profile ontology, Context ontology, Interaction process},
abstract = {Recent researches about the personalized content generation have focused their efforts on two main topics: the first topic is the user model definition, i.e. the dimensions to be taken into account to represent the user, and the second topic is about the techniques used by recommender systems to provide recommendations according to the user requirements, such as adaptive approaches for context-aware systems, collaborative learning, and recommender systems for mobile environments. In this work, an approach based on ontologies to represent the interaction process between user profile and its context for collaborative learning is presented. We also analyzed the role assignments, permissions, restrictions and the definition of rules that are applied to the user, particularly in the collaborative learning context where the subject is involved. A case study related to the context of a school as well as the defined roles by the occupations in the context of locations is proposed.}
}
@article{CHOU2015215,
title = {Negotiation based adaptive learning sequences: Combining adaptivity and adaptability},
journal = {Computers & Education},
volume = {88},
pages = {215-226},
year = {2015},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2015.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S036013151500127X},
author = {Chih-Yueh Chou and K. Robert Lai and Po-Yao Chao and Chung Hsien Lan and Tsung-Hsin Chen},
keywords = {Architectures for educational technology system, Evaluation of CAL systems, Intelligent tutoring systems, Interactive learning environments},
abstract = {This study proposes a negotiation-based approach to combine the notion of adaptivity (system-controlled adaptation) and adaptability (user-controlled adaptation) for an adaptive learning system. The system suggests adaptations and the student also submits his/her adaptation preference. When the student preference opposes the system suggestion, the student then negotiates with the system to reach an agreement of adaptation. A negotiation-based adaptive learning system (NALS) is implemented to support the generation of personalized adaptive learning sequences by system negotiations with students regarding assessments of learning performance (i.e. negotiated open student model) of the current content and choices of the next learning content (i.e. negotiation of adaptation). Students require two metacognitions in deciding adaptive learning sequences: self-assessment for evaluating their understanding of the current content and regulation for choosing appropriate learning content. Negotiated open student model are used for assist student self-assessment and negotiation of adaptation are used for assist student regulation of content choices. An experiment was conducted to compare a system-controlled adaptive learning system (SALS, adaptivity), a user-controlled adaptive learning system (UALS, adaptability), and a NALS. The results revealed that NALS promoted better metacognitions in student calibration (i.e. self-assessment) accuracy and learning content choices (i.e. regulation). Preliminary evidences also showed that NALS promoted better student performance in a delay test. The results further suggested that students with poor calibration accuracy and inappropriate content choices were not suitable to use UALS and were suitable to use SALS. The NALS can also be used for training students to make appropriate adaptation for learning.}
}
@article{SYKES2014625,
title = {A Cloud-based Interaction Management System Architecture for Mobile Devices},
journal = {Procedia Computer Science},
volume = {34},
pages = {625-632},
year = {2014},
note = {The 9th International Conference on Future Networks and Communications (FNC'14)/The 11th International Conference on Mobile Systems and Pervasive Computing (MobiSPC'14)/Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.07.086},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914009417},
author = {Edward R. Sykes},
keywords = {Human Computer Interaction, mobile devices, user modeling, intelligent agents, machine learning.},
abstract = {The number of interruptions people experience on a daily basis has grown considerably over the last decade and this growth has not shown any signs of subsiding. In fact, with the exponential growth of mobile computing, interruptions are permeating the user experience. Systems must be developed to manage interruptions by reasoning about ideal timings of interactions and determining appropriate notification formats. In this work, an architecture for a cloud-based interruption management system for mobile device users is presented. The system draws from rich contextual information from the mobile device (i.e., user, task and environment dimensions) and real-time observations of the user's activities and then reasons about ideal times to interact with the user. The reasoning component (interruption algorithm) is situated in the cloud and implemented using a novel machine learning technique (an Adaptive Neuro Fuzzy Inference System). This research addresses the complex problem of determining the precise time to interact with a mobile device user and in so doing aims to reduce the negative aspects of interruptions. This paper also presents a new interruption taxonomy built on an existing framework, and a report on the current prototype developed.}
}
@article{OZYURT2015349,
title = {Learning style based individualized adaptive e-learning environments: Content analysis of the articles published from 2005 to 2014},
journal = {Computers in Human Behavior},
volume = {52},
pages = {349-358},
year = {2015},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215004604},
author = {Özcan Özyurt and Hacer Özyurt},
keywords = {Adaptive educational hypermedia, Individualized e-learning, Learning style, Content analysis},
abstract = {The studies on creating learning environments based on differences in learning styles have gained importance in recent years. Learning styles are one of the most important parameters in determining individual differences. Accordingly, traditional web-based learning environments have been replaced by individualized adaptive e-learning environments on the basis of learning styles which are more innovative. This study deals with the content analysis of the recent studies on Adaptive Educational Hypermedia (AEH) based on learning styles. 69 articles published from 2005 to 2014 were obtained through a comprehensive and detailed review. Afterwards, these studies were subjected to document analysis. The studies were categorized under the titles of purpose, nature, method, characteristics of examinees, level, data collection tool, learner modelling, learning styles, subject, and findings. Some of the studies offered a framework or proposed a model for AEH while others focused on the influence of AEH on academic achievement and learning outputs as well as learning satisfaction. This study examines the existing tendencies and gaps in the literature and discusses the potential research topics.}
}
@article{SHAHBAZI201694,
title = {Implementation of Imitation Learning using Natural Learner Central Pattern Generator Neural Networks},
journal = {Neural Networks},
volume = {83},
pages = {94-108},
year = {2016},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2016.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608016300983},
author = {Hamed Shahbazi and Reyhaneh Parandeh and Kamal Jamshidi},
keywords = {Imitation learning, Neural networks, Oscillatory neurons, Central pattern generator, Natural policy gradient},
abstract = {In this paper a new design of neural networks is introduced, which is able to generate oscillatory patterns. The fundamental building block of the neural network is O-neurons that can generate an oscillation in its transfer functions. Since the natural policy gradient learning has been used in training a central pattern generator paradigm, it is called Natural Learner CPG Neural Networks (NLCPGNN). O-neurons are connected and coupled to each other in order to shape a network and their unknown parameters are found by a natural policy gradient learning algorithm. The main contribution of this paper is design of this learning algorithm which is able to simultaneously search for the weights and topology of the network. This system is capable to obtain any complex motion and rhythmic trajectory via first layer and learn rhythmic trajectories in the second layer and converge towards all these movements. Moreover this two layers system is able to provide various features of a learner model for instance resistance against perturbations, modulation of trajectories amplitude and frequency. Simulation of the learning system in the robot simulator (WEBOTS) that is linked with MATLAB software has been done. Implementation on a real NAO robot demonstrates that the robot has learned desired motion with high accuracy. These results show proposed system produces high convergence rate and low test errors.}
}
@article{PAPAMITSIOU2017423,
title = {Exhibiting achievement behavior during computer-based testing: What temporal trace data and personality traits tell us?},
journal = {Computers in Human Behavior},
volume = {75},
pages = {423-438},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.05.036},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217303576},
author = {Zacharoula Papamitsiou and Anastasios A. Economides},
keywords = {Assessment analytics, BFI, Computer-based testing, Personality traits, Student behavior modelling, Supervised classification},
abstract = {Personalizing computer-based testing services to examinees can be improved by considering their behavioral models. This study aims to contribute towards deeper understanding the examinee’s time-spent and achievement behavior during testing according to the five personality traits by exploiting assessment analytics. Further, it aims to investigate assessment analytics appropriateness for classifying students and generating enhanced student models to guide personalization of testing services. In this study, the LAERS assessment environment and the Big Five Inventory were used to track the response times of 112 undergraduate students and to extract their personality traits respectively. Partial Least Squares was used to detect fundamental relationships between the collected data, and Supervised Learning Algorithms were used to classify students. Results indicate a positive effect of extraversion and agreeableness on goal-expectancy, a positive effect of conscientiousness on both goal-expectancy and level of certainty, and a negative effect of neuroticism and openness on level of certainty. Further, extraversion, agreeableness and conscientiousness have statistically significant indirect impact on students’ response-times and level of achievement. Moreover, the ensemble RandomForest method provides accurate classification results, indicating that a time-spent driven description of students’ behavior could have added value towards dynamically reshaping the respective models. Further implications of these findings are also discussed.}
}
@article{PELANEK2016169,
title = {Applications of the Elo rating system in adaptive educational systems},
journal = {Computers & Education},
volume = {98},
pages = {169-179},
year = {2016},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2016.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S036013151630080X},
author = {Radek Pelánek},
keywords = {Architectures for educational technology system, Interactive learning environments, Applications in subject areas, Student modeling},
abstract = {The Elo rating system was originally developed for rating chess players, nowadays it is widely used for ranking players of many other games. The system can be used in educational systems when we interpret student's answer to an item as a match between the student and the item. In this way we can easily dynamically estimate the skill of students and difficulty of items. We provide a systematic overview of different variants of the Elo rating system and their application in education. We compare the Elo rating system to alternative methods and describe a specific case study (an adaptive practice of geography facts) to illustrate the application of the Elo rating system in education. We argue that the Elo rating system is simple, robust, and effective and thus suitable for use in the development of adaptive educational systems. We provide specific guidelines for such applications.}
}
@article{HEUNG201751,
title = {Comparing the use of training data derived from legacy soil pits and soil survey polygons for mapping soil classes},
journal = {Geoderma},
volume = {290},
pages = {51-68},
year = {2017},
issn = {0016-7061},
doi = {https://doi.org/10.1016/j.geoderma.2016.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0016706116309491},
author = {Brandon Heung and Matúš Hodúl and Margaret G. Schmidt},
keywords = {Digital soil mapping, Machine-learning, Soil classification, Data-mining, Model comparison, Ensemble-learning},
abstract = {Machine-learners used for digital soil mapping are generally trained using either data derived from field-observed soil pits or from soil survey polygons - although no direct comparison of the accuracy resulting from the two methods has yet to be undertaken. This study examined such a comparison over the Okanagan Valley and Kamloops region of British Columbia where good quality soil pit and soil survey data were available. A standard set of environmental variables including vegetative, climatic, and topographic indices were used to predict soil Great Groups in accordance with the Canadian System of Soil Classification. The pit-derived training dataset was developed using n=478 points from the British Columbia Soil Information System while the polygon-derived training dataset was developed through random sampling of single-component soil survey map units based on an area-weighted approach. In both cases, the training points were intersected with a suite of 18 environmental covariates, reduced from 27 covariates using principal component analysis, and submitted to a machine-learner for predictions at a 100m spatial resolution. Four single-model learners (CART, k-nearest neighbor, multinomial logistic regression, and logistic model tree) and five ensemble-model learners (CART with bagging, k-nearest neighbor with bagging, multinomial logistic regression with bagging, logistic model trees with bagging, and Random Forest) were compared. Surfaces of prediction uncertainty were produced using ignorance uncertainty and results were validated using a 5-fold cross-validation procedure. Predictions made using polygon-derived training data were consistently higher in accuracy across all models where the Random Forest model was the most effective learner with C=61% accuracy when using pit-derived training data and C=68% accuracy when using polygon-derived training data. Comparing single-model and ensemble-learner models, the bagging algorithm resulted in a 2–11% increase in accuracy when using pit-derived training data. Ensemble-models allowed for the visualization of prediction uncertainty. This study provides further insight into the use of legacy soil data and the development of training data for digital soil mapping.}
}
@article{ORTIGOSA201457,
title = {Predicting user personality by mining social interactions in Facebook},
journal = {Journal of Computer and System Sciences},
volume = {80},
number = {1},
pages = {57-71},
year = {2014},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2013.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S002200001300072X},
author = {Alvaro Ortigosa and Rosa M. Carro and José Ignacio Quiroga},
keywords = {Data mining in social networks, User modeling, Personality inference},
abstract = {Adaptive applications may benefit from having models of usersʼ personality to adapt their behavior accordingly. There is a wide variety of domains in which this can be useful, i.e., assistive technologies, e-learning, e-commerce, health care or recommender systems, among others. The most commonly used procedure to obtain the user personality consists of asking the user to fill in questionnaires. However, on one hand, it would be desirable to obtain the user personality as unobtrusively as possible, yet without compromising the reliability of the model built. On the other hand, our hypothesis is that users with similar personality are expected to show common behavioral patterns when interacting through virtual social networks, and that these patterns can be mined in order to predict the tendency of a user personality. With the goal of inferring personality from the analysis of user interactions within social networks, we have developed TP2010, a Facebook application. It has been used to collect information about the personality traits of more than 20,000 users, along with their interactions within Facebook. Based on all the collected data, automatic classifiers were trained by using different machine-learning techniques, with the purpose of looking for interaction patterns that provide information about the usersʼ personality traits. These classifiers are able to predict user personality starting from parameters related to user interactions, such as the number of friends or the number of wall posts. The results show that the classifiers have a high level of accuracy, making the proposed approach a reliable method for predicting the user personality}
}
@article{FENG20165,
title = {Social network regularized Sparse Linear Model for Top-N recommendation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {51},
pages = {5-15},
year = {2016},
note = {Mining the Humanities: Technologies and Applications},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2016.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952197616000233},
author = {Xiaodong Feng and Ankit Sharma and Jaideep Srivastava and Sen Wu and Zhiwei Tang},
keywords = { recommendation, Social network, User modeling, Sparse Linear Model, Local learning},
abstract = {Social recommendation techniques have been developed to employ user׳s social connections for both rating prediction and Top-N recommendation. However, they are mostly using social network enhanced matrix factorization (MF) where the objective is to minimize the prediction error of rating scores, which makes it impractical and unsuccessful for Top-N recommendation. This paper thus focuses on developing more effective methods to utilize social network information for Top-N recommendation. Social network regularized Sparse LInear Model (SocSLIM) with its extensions incorporating local learning (LocSocSLIM) to improve efficiency are proposed. SocSLIM learns sparse coefficient matrix for users by solving a sparse representation problem over user-item rating/purchase matrix and user–user social network׳s adjacency matrix at the same time by sharing coefficient matrix. The coefficient matrix is used to predict the recommendation scores, which are then combined with a proposed item based Distance regularized Sparse LInear Model (DSLIM) to generate recommendations for the users. The experimental results demonstrate that SocSLIM effectively uses the social information to outperform the state-of-the-art methods by at least 12%. Moreover, the local weight learning extension LocSocSLIM significantly improves the efficiency up to 10 times as compared to SocSLIM as the original SLIM while achieving the close performance guarantees.}
}
@article{SCHADENBERG2017222,
title = {Personalising game difficulty to keep children motivated to play with a social robot: A Bayesian approach},
journal = {Cognitive Systems Research},
volume = {43},
pages = {222-231},
year = {2017},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716300523},
author = {B.R. Schadenberg and M.A. Neerincx and F. Cnossen and R. Looije},
keywords = {Social robotics, User modeling, Rating system, Child-robot interaction, Motivation},
abstract = {For effective child education, playing games with a social robot should be motivating for a longer period of time. One aspect that can affect the motivation of a child is the difficulty of a game. The game should be perceived as challenging, while at the same time, the child should be confident to meet the challenge. We designed a user modelling module that adapts the difficulty of a game to the child’s skill level, in order to provide children with the optimal challenge. This module applies a Bayesian rating method that estimates the child’s skill and game item’s difficulty levels to personalise the game progress. In an experiment with 22 children (aged between 10 and 12years old), we tested whether the personalisation leads to a higher motivation to play with the robot. Although the personalised system did not challenge the participants optimally, this study shows that the Bayesian rating system is in principle able to measure the skill and performance of children in playing a game with a robot (even without accurate estimates of the difficulty of items). We outline multiple ways in which the rating method and module can be used to further personalise and enhance the child-robot interaction, other than adapting the difficulty of games (e.g. by adapting the dialogue and feedback).}
}
@article{LI2017170,
title = {Insights into randomized algorithms for neural networks: Practical issues and common pitfalls},
journal = {Information Sciences},
volume = {382-383},
pages = {170-178},
year = {2017},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S002002551631917X},
author = {Ming Li and Dianhui Wang},
keywords = {Randomized algorithms, Neural networks, Incremental learning, Function approximation},
abstract = {Random Vector Functional-link (RVFL) networks, a class of learner models, can be regarded as feed-forward neural networks built with a specific randomized algorithm, i.e., the input weights and biases are randomly assigned and fixed during the training phase, and the output weights are analytically evaluated by the least square method. In this paper, we provide some insights into RVFL networks and highlight some practical issues and common pitfalls associated with RVFL-based modelling techniques. Inspired by the folklore that “all high-dimensional random vectors are almost always nearly orthogonal to each other”, we establish a theoretical result on the infeasibility of RVFL networks for universal approximation, if a RVFL network is built incrementally with random selection of the input weights and biases from a fixed scope, and constructive evaluation of its output weights. This work also addresses the significance of the scope setting of random weights and biases in respect to modelling performance. Two numerical examples are employed to illustrate our findings, which theoretically and empirically reveal some facts and limits of such class of randomized learning algorithms.}
}
@article{SHAKSHUKI2015356,
title = {Dynamic Healthcare Interface for Patients},
journal = {Procedia Computer Science},
volume = {63},
pages = {356-365},
year = {2015},
note = {The 6th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2015)/ The 5th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2015)/ Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.354},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915024898},
author = {Elhadi M. Shakshuki and Malcolm Reid and Tarek R. Sheltami},
keywords = {Multi-Agent Systems, Healthcare Technology ;Adaptive User Interface, Reinforcement Learning},
abstract = {Canadian healthcare is a fundamental part of society. Challenges such as the aging baby boomer generation require the healthcare industry to meet higher demands while using fewer resources. Computer systems designed to record and report physical health properties of an individual personcan be used in part to accomplish this task. In this paper, we present the architecture of a hypothetical multi-agent system designed to provide healthcare information about specific patients through continuous monitoring. The resulting data from the system is accessible by the patient to whom it belongs as well as his or her healthcare professional. Furthermore, the proposed system utilizes an adaptive user interface for the purpose of improving the overall experience for users with poor vision or motor skills. Specifically, we focus on the implementation of several of the key components involved in the adaptive user interface: learning component and the user model. To demonstrate the feasibility of the implementation two scenarios are provided. We conclude with several possible future directions for this research.}
}
@article{SCARDAPANE2015271,
title = {Distributed learning for Random Vector Functional-Link networks},
journal = {Information Sciences},
volume = {301},
pages = {271-284},
year = {2015},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2015.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0020025515000298},
author = {Simone Scardapane and Dianhui Wang and Massimo Panella and Aurelio Uncini},
keywords = {Random Vector Functional-Link, Distributed learning, Consensus, Distributed Optimization},
abstract = {This paper aims to develop distributed learning algorithms for Random Vector Functional-Link (RVFL) networks, where training data is distributed under a decentralized information structure. Two algorithms are proposed by using Decentralized Average Consensus (DAC) and Alternating Direction Method of Multipliers (ADMM) strategies, respectively. These algorithms work in a fully distributed fashion and have no requirement on coordination from a central agent during the learning process. For distributed learning, the goal is to build a common learner model which optimizes the system performance over the whole set of local data. In this work, it is assumed that all stations know the initial weights of the input layer, the output weights of local RVFL networks can be shared through communication channels among neighboring nodes only, and local datasets are blocked strictly. The proposed learning algorithms are evaluated over five benchmark datasets. Experimental results with comparisons show that the DAC-based learning algorithm performs favorably in terms of effectiveness, efficiency and computational complexity, followed by the ADMM-based learning algorithm with promising accuracy but higher computational burden.}
}
@article{YANG201739,
title = {ExploreTree: Interactive tree modeling in semantic trait space with online intent learning},
journal = {Graphical Models},
volume = {91},
pages = {39-51},
year = {2017},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2017.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1524070317300103},
author = {Yinhui Yang and Rui Wang and Hongxin Zhang and Hujun Bao},
keywords = {Tree modeling, Parametirc space exploration, Online learning, Semantic traits},
abstract = {Perceptually modeling realistic trees is important for many graphics applications. However, existing methods are mainly rule-based. Few have directly associated control parameters with user modeling intent and semantic tree shape descriptions. In this paper, we propose a new interactive tree modeling system, ExploreTree, that automatically deduces user modeling intent and supports iteratively design of 3D tree models. It consists of two major phases. The first phase is an off-line learning process, where semantic tree traits perceived by humans are learned. Crowdsourced data on example tree models are collected and analyzed to construct the semantic trait space as well as the embedding of trees into this space. Built upon it, the second phase is an interactive exploration of tree models via a few user clicks, where a user intent evaluation model is learned online to guide the modeling process. Modeled trees and user studies demonstrate the efficiency and capability of ExploreTree.}
}
@article{RAYBOURN2014471,
title = {A new paradigm for serious games: Transmedia learning for more effective training and education},
journal = {Journal of Computational Science},
volume = {5},
number = {3},
pages = {471-481},
year = {2014},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2013.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877750313001014},
author = {Elaine M. Raybourn},
keywords = {Transmedia learning, Serious games, Transmedia campaigns, Storytelling, Social media, Data mining, xAPI, MOOC},
abstract = {Serious games present a relatively new approach to training and education for international organizations such as NATO (North Atlantic Treaty Organization), non-governmental organizations (NGOs), the U.S. Department of Defense (DoD) and the U.S. Department of Homeland Security (DHS). Although serious games are often deployed as stand-alone solutions, they can also serve as entry points into a comprehensive training pipeline in which content is delivered via different media to rapidly scale immersive training and education for mass audiences. The present paper introduces a new paradigm for more effective and scalable training and education called transmedia learning. Transmedia learning leverages several new media trends including the peer communications of social media, the scalability of massively openonline course (MOOCs), and the design of transmedia storytelling used by entertainment, advertising, and commercial game industries to sustain audience engagement. Transmedia learning is defined as the scalable system of messages representing a narrative or core experience that unfolds from the use of multiple media, emotionally engaging learners by involving them personally in the story. In the present paper, we introduce the transmedia learning paradigm as offering more effective use of serious games for training and education. This approach is consistent with the goals of international organizations implementing approaches similar to those described by the Army Learning Model (ALM) to deliver training and education to Soldiers across multiple media. We discuss why the human brain is wired for transmedia learning and demonstrate how the Simulation Experience Design Method can be used to create transmedia learning story worlds for serious games. We describe how social media interactions and MOOCs may be used in transmedia learning, and how data mining social media and experience tracking can inform the development of computational learner models for transmedia learning campaigns. Examples of how the U.S. Army has utilized transmedia campaigns for strategic communication and game-based training are provided. Finally, we provide strategies the reader can use today to incorporate transmedia storytelling elements such as Internet, serious games, video, social media, graphic novels, machinima, blogs, and alternate reality gaming into a new paradigm for training and education: transmedia learning.}
}
@article{OUF2017796,
title = {A proposed paradigm for smart learning environment based on semantic web},
journal = {Computers in Human Behavior},
volume = {72},
pages = {796-818},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216305957},
author = {Shimaa Ouf and Mahmoud {Abd Ellatif} and S.E. Salama and Yehia Helmy},
keywords = {E-Learning ecosystem, Personalization, Ontology, Software architecture, Semantic Web Rule Language, Learner model},
abstract = {The current approaches of e-learning face challenges, in isolation of learners from learning process, and shortage of learning process quality. The researchers mentioned that the next generation of e-learning is e-learning ecosystem. E-learning ecosystem has many advantages, in which, learners form groups, collaborate with each other and with educators, and content designed for interaction. E-learning ecosystem faces some issues. It applies teacher-student model, in which, fixed learning pathway is considered suitable for all learners. Consequently, learners are presented with limited personalized materials. E-learning ecosystem needs to merge the personalization's concept. Semantic web ontology based personalization of learning environment plays a leading role to build smart e-learning ecosystem. This paper previews a detailed study which addresses research papers that apply ontology within learning environment. Most of these studies focus on personalizing e-learning by providing learners with suitable learning objects, ignoring the other learning process components. This paper proposes and implements framework for smart e-learning ecosystem using ontology and SWRL. A new direction is proposed. This direction fosters the creation of a separate four ontologies for the personalized full learning package which is composed of learner model and all the learning process components (learning objects, learning activities and teaching methods).}
}
@article{VALORMIRO201565,
title = {Efficiency and usability study of innovative computer-aided transcription strategies for video lecture repositories},
journal = {Speech Communication},
volume = {74},
pages = {65-75},
year = {2015},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2015.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167639315001016},
author = {Juan Daniel {Valor Miró} and Joan Albert Silvestre-Cerdà and Jorge Civera and Carlos Turró and Alfons Juan},
keywords = {Video lecture repositories, Usability study, Computer-assisted transcription, Interface design strategies, Automatic speech recognition},
abstract = {Video lectures are widely used in education to support and complement face-to-face lectures. However, the utility of these audiovisual assets could be further improved by adding subtitles that can be exploited to incorporate added-value functionalities such as searchability, accessibility, translatability, note-taking, and discovery of content-related videos, among others. Today, automatic subtitles are prone to error, and need to be reviewed and post-edited in order to ensure that what students see on-screen are of an acceptable quality. This work investigates different user interface design strategies for this post-editing task to discover the best way to incorporate automatic transcription technologies into large educational video repositories. Our three-phase study involved lecturers from the Universitat Politècnica de València (UPV) with videos available on the poliMedia video lecture repository, which is currently over 10,000 video objects. Simply by conventional post-editing automatic transcriptions users almost reduced to half the time that would require to generate the transcription from scratch. As expected, this study revealed that the time spent by lecturers reviewing automatic transcriptions correlated directly with the accuracy of said transcriptions. However, it is also shown that the average time required to perform each individual editing operation could be precisely derived and could be applied in the definition of a user model. In addition, the second phase of this study presents a transcription review strategy based on confidence measures (CM) and compares it to the conventional post-editing strategy. Finally, a third strategy resulting from the combination of that based on CM with massive adaptation techniques for automatic speech recognition (ASR), achieved to improve the transcription review efficiency in comparison with the two aforementioned strategies.}
}
@article{COCEA201548,
title = {Participatory Learner Modelling Design: A methodology for iterative learner models development},
journal = {Information Sciences},
volume = {321},
pages = {48-70},
year = {2015},
note = {Security and privacy information technologies and applications for wireless pervasive computing environments},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2015.05.032},
url = {https://www.sciencedirect.com/science/article/pii/S0020025515003916},
author = {Mihaela Cocea and George D. Magoulas},
keywords = {Learner/user models, Adaptive systems, Participatory design, Methodology, Iterative development},
abstract = {Learner models are built to offer personalised solutions related to learning. They are often developed in parallel to the development of adaptive learning systems and thus, linked to the system’s development. The adaptive learning systems literature reports numerous accounts of learner model development, but there are no reports on the methodological aspects of developing learner models and the relation between the development of the learner model component and the rest of the system. This paper presents the Participatory Learner Modelling Design methodology, which outlines the steps for learner model development and their relation to the development of the system. The methodology is illustrated with a case study of an adaptive educational system.}
}
@article{COSTA2017247,
title = {Evaluating the effectiveness of educational data mining techniques for early prediction of students' academic failure in introductory programming courses},
journal = {Computers in Human Behavior},
volume = {73},
pages = {247-256},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.01.047},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217300596},
author = {Evandro B. Costa and Baldoino Fonseca and Marcelo Almeida Santana and Fabrísia Ferreira {de Araújo} and Joilson Rego},
keywords = {Artificial intelligence in education, Automatic instructional planner, Automatic prediction, Educational data mining, Interactive learning environment, Learner modeling},
abstract = {The data about high students' failure rates in introductory programming courses have been alarming many educators, raising a number of important questions regarding prediction aspects. In this paper, we present a comparative study on the effectiveness of educational data mining techniques to early predict students likely to fail in introductory programming courses. Although several works have analyzed these techniques to identify students' academic failures, our study differs from existing ones as follows: (i) we investigate the effectiveness of such techniques to identify students likely to fail at early enough stage for action to be taken to reduce the failure rate; (ii) we analyse the impact of data preprocessing and algorithms fine-tuning tasks, on the effectiveness of the mentioned techniques. In our study we evaluated the effectiveness of four prediction techniques on two different and independent data sources on introductory programming courses available from a Brazilian Public University: one comes from distance education and the other from on-campus. The results showed that the techniques analyzed in our study are able to early identify students likely to fail, the effectiveness of some of these techniques is improved after applying the data preprocessing and/or algorithms fine-tuning, and the support vector machine technique outperforms the other ones in a statistically significant way.}
}
@article{QUIROGABAQUERO201618,
title = {Efectos de diferentes tipos de entrenamiento por modelado en tareas de igualación a la muestra},
journal = {Revista Latinoamericana de Psicología},
volume = {48},
number = {1},
pages = {18-29},
year = {2016},
issn = {0120-0534},
doi = {https://doi.org/10.1016/j.rlp.2015.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0120053415000333},
author = {Luis Alberto {Quiroga Baquero} and María Antonia {Padilla Vargas} and Santiago {Ordoñez Riaño} and Luis Carlos {Fonseca León}},
keywords = {Modelamiento, Igualación a la muestra, Control abstracto de estímulos, Transferencia, Modelling, Matching-to-sample, Abstract stimulus control, Transfer},
abstract = {Resumen
Se evaluó el efecto de tres tipos de entrenamiento por modelado, sobre el aprendizaje y la transferencia en una tarea de igualación a la muestra de segundo orden. Participaron 30 estudiantes de psicología (15 mexicanos y 15 colombianos). Cada muestra fue dividida en tres condiciones experimentales: entrenamiento por exposición a modelo experto, modelo aprendiz o modelo antiexperto. Los resultados muestran que: (a) en las dos muestras, los desempeños en pruebas de aprendizaje y transferencia fueron significativamente superiores a los obtenidos en las prepuebas; (b) se encontraron mejores ejecuciones en las pruebas de aprendizaje y transferencia para los grupos con exposición a modelo experto; (c) la condición de exposición a modelo antiexperto produjo porcentajes de aciertos bajos en pruebas de aprendizaje y transferencia; (d) entre las pruebas de transferencia, las ejecuciones en la prueba extrarrelacional fueron las más bajas; y (e) en la prueba de construcción, la proporción de relaciones construidas fue homogénea. Estos hallazgos son discutidos en términos de la identificación de algunos factores implicados en el establecimiento de control abstracto de estímulo por modelado, y del efecto de las condiciones de remuneración sobre el desempeño en tareas experimentales de resolución de problemas.
This study evaluated the effects of three types of training models on the learning and transfer of a second-order matching-to-sample task. Thirty psychology students (15 Mexicans and 15 Colombians) took part. Each sample was divided into three experimental conditions: training by exposure to an expert model, learner model or anti-expert model. The results show that: (a) in both samples, performances in transfer and learning tests are significantly higher than those obtained in the pre-tests; (b) best performances were found in tests of learning and transfer for groups with exposure to the expert model; (c) the anti-expert model condition produced lower percentages of correct answers on tests of learning and transfer; (d) performance in extra-relational tests were the lowest in transfer tests; and (e) the ratio of relationships constructed was homogeneous in the construction test. These findings are discussed in terms of the identification of some factors involved in the establishment of abstract stimulus control by modelling, and the effect of the conditions of remuneration on performance in experimental problem-solving tasks.}
}
@article{BENT2017456,
title = {Modeling user behavior data in systems of engagement},
journal = {Future Generation Computer Systems},
volume = {68},
pages = {456-464},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1630156X},
author = {Oliver Bent and Prasenjit Dey and Komminist Weldemariam and Mukesh K. Mohania},
keywords = {User engagement, User modeling, Instrumentation, Mobile sensors},
abstract = {The proliferation of mobile devices has changed the way digital information is consumed and its efficacy measured. These personal devices know a lot about user behavior from embedded sensors along with monitoring the daily activities users perform through various applications on these devices. This data can be used to get a deep understanding of the context of the users and provide personalized services to them. However, there are a lot of challenges in capturing, modeling, storing, and processing such data from these systems of engagement, both in terms of achieving the right balance of redundancy in the captured and stored data, along with ensuring the usefulness of the data for analysis. There are additional challenges in balancing how much of the captured data should be processed through client or server applications. In this article, we present the modeling of user behavior in the context of personalized education which has generated a lot of recent interest. More specifically, we present an architecture and the issues of modeling student behavior data, captured from different activities the student performs during the process of learning. The user behavior data is modeled and sent to the cloud-enabled backend where detailed analytics are performed to understand different aspects of a student, such as engagement, difficulties, and preferences and to also analyze the quality of the data.}
}
@article{SM201610,
title = {Efficient online and offline template update mechanisms for speaker recognition},
journal = {Computers & Electrical Engineering},
volume = {50},
pages = {10-25},
year = {2016},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2015.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045790615004309},
author = {Anzar S.M. and Amala K. and Remya Rajendran and Ashwin Mohan and Ajeesh P.S. and Mohammed Sabeeh K. and Febin Aziz},
keywords = {Template update, Mel frequency Cepstral coefficient, MFCC super template, GMM super model, Online update, Offline update},
abstract = {Sample variations are one of the main problems associated with speaker recognition. Most approaches use multiple templates in the gallery database. But, this requires enormous memory space. In order to minimize classification errors and intra-class variations, adaptive online and offline template update methods using vector quantization (VQ) and Gaussian mixture model (GMM) are proposed. Online and offline feature update as well as model update techniques are considered here. Feature update utilizes the vector quantization approach, while Gaussian mixture model approach is considered for model updating. The proposed methods automatically update the feature (model) in accordance with the biometric sample variations over time and they continually adapt the templates (user model) based on semi-supervised learning strategies. Experiments with 50 subjects reveal that the proposed template update strategies, improve the recognition accuracy and reduce the classification errors for voice recognition systems, even under sample variations.}
}
@article{WANG2016490,
title = {A computer vision-based algorithm to predict false positive errors in radiology trainees when interpreting digital breast tomosynthesis cases},
journal = {Expert Systems with Applications},
volume = {64},
pages = {490-499},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416304171},
author = {Mengyu Wang and Meng Wang and Lars J. Grimm and Maciej A. Mazurowski},
keywords = {Radiology education, False positive error prediction, Training plan optimization, Digital breast tomosynthesis, Image processing, Clustering},
abstract = {Objectives
Digital breast tomosynthesis (DBT) is a new imaging modality that improves invasive cancer detection rates compared to mammography. In this work, we aim to advance adaptive computer-based education in DBT by computer algorithm.
Methods
First, a set of potentially difficult locations are identified based on locations marked by other trainees using a regional clustering algorithm. Second, the candidate location is segmented to identify potential abnormal objects. Third, 18 features are extracted from the location from the segmented image. Finally, a classifier uses the 18 features to predict whether the candidate location would result in a false positive error for the trainee. The classifier is personalized for each trainee by using data from the trainee's prior DBT interpretations.
Results
Our algorithm successfully identified locations more likely associated with false positive errors as compared to randomly identified locations. The prevalence of errors among the difficult locations was 20.7% when 1 location per trainee was predicted and 17.2% when 10 locations were predicted. In comparison, the prevalence of errors for random locations generated within a breast region with 1 and 10 identified locations was 0% and 4.8%, respectively.
Conclusions
We developed an algorithm to successfully identify locations on DBT where trainees are more likely to commit false positive errors.
Advances in knowledge
Our user model can be used to select the most challenging cases for each trainee from the perspective of committing false positive errors. Our model improved the status quo of case presentation with random selection to trainee in breast tomosynthesis.}
}
@article{CESTA201498,
title = {Training for crisis decision making – An approach based on plan adaptation},
journal = {Knowledge-Based Systems},
volume = {58},
pages = {98-112},
year = {2014},
note = {Intelligent Decision Support Making Tools and Techniques: IDSMT},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2013.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950705113003614},
author = {Amedeo Cesta and Gabriella Cortellessa and Riccardo {De Benedictis}},
keywords = {Strategic decision making, Training systems, Crisis management, Continuous plan adaptation, Biofeedback, Mixed-initiative system, User modeling},
abstract = {The human ability to take the right decisions is very important in real world critical situations. An interesting problem always worth being investigated concerns how to teach decision making skills to humans. The real nature of taking decisions is extremely difficult to describe in detail and, as a consequence, training it according to fixed protocols is also challenging. This is because it comes out as a combination of natural talent, competence from previous experience, ability to quick reasoning, leadership, resilience to stress, and so on. We have addressed this problem while building a new learning environment to train crisis decision makers. The environment, called Pandora, is grounded on Artificial Intelligence planning techniques known as “timeline-based”. This technology is used to create and manipulate segments of lesson’s content over time. Planning a lesson corresponds to logically organize events over time that are then rendered in front of trainees during the lesson’s actual enactment. This paper shows how the machinery of continuous plan adaptation is functional to create variety and novelty in the lessons thus engaging the trainees during the teaching interaction. In particular, it shows the different uses of plan adaptation to take into account the basic reactivity of the trainees, the background deductions from user modeling, and the mixed-initiative interactions guided by the trainer.}
}
@article{KISTNER2016446,
title = {Model development in scientific discovery learning with a computer-based physics task},
journal = {Computers in Human Behavior},
volume = {59},
pages = {446-455},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.02.041},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216300930},
author = {Saskia Kistner and Regina Vollmeyer and Bruce D. Burns and Ulrich Kortenkamp},
keywords = {Scientific discovery learning, Multiple problem spaces, Computer simulations, Physics concepts, Misconceptions, Conceptual change},
abstract = {Based on theories of scientific discovery learning (SDL) and conceptual change, this study explores students' preconceptions in the domain of torques in physics and the development of these conceptions while learning with a computer-based SDL task. As a framework we used a three-space theory of SDL and focused on model space, which is supposed to contain the current conceptualization/model of the learning domain, and on its change through hypothesis testing and experimenting. Three questions were addressed: (1) What are students' preconceptions of torques before learning about this domain? To do this a multiple-choice test for assessing students' models of torques was developed and given to secondary school students (N = 47) who learned about torques using computer simulations. (2) How do students' models of torques develop during SDL? Working with simulations led to replacement of some misconceptions with physically correct conceptions. (3) Are there differential patterns of model development and if so, how do they relate to students’ use of the simulations? By analyzing individual differences in model development, we found that an intensive use of the simulations was associated with the acquisition of correct conceptions. Thus, the three-space theory provided a useful framework for understanding conceptual change in SDL.}
}
@article{SHIN20171826,
title = {The role of affordance in the experience of virtual reality learning: Technological and affective affordances in virtual reality},
journal = {Telematics and Informatics},
volume = {34},
number = {8},
pages = {1826-1836},
year = {2017},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2017.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0736585317301223},
author = {Dong-Hee Shin},
keywords = {Virtual reality learning, Critical incident technique, Amultimixed approach, Educational affordance, Virtual reality, Affordance},
abstract = {As virtual reality becomes more and more mainstream, the role of affordances in virtual environments becomes an important question. The goal of this study is to explicate users’ motivational affordances and examine how they influence the acceptance of a virtual reality learning environment (VLE). It examines how motivational affordances in an educational virtual reality (VR) system affect user experience to track and achieve goals for users. A multimixed approach was used by combining qualitative methods and a quantitative survey. First, a critical incident technique was used to explore a range of affordance factors related to VLE. Second, based on the affordance factors identified from the qualitative methods, a survey was conducted to examine the effects of affordance on user cognitive processes and the influence of affordance on the learning process. The results of the user model confirmed the heuristic role of presence and immersion affordance regarding their underlying link to educational affordances, such as empathy and embodied cognition. The findings imply the embodied cognition process of VLE in which technological qualities are shaped by users’ perception and context. The results establish a foundation for VR technologies through a heuristic assessment tool from a user-embodied cognitive process. They confirm the validity and utility of applying affordances to the design of VR as a useful concept and prove that the optimum mix of affordances is crucial to the success of VR design.}
}
@article{WANG20161,
title = {Predicting false negative errors in digital breast tomosynthesis among radiology trainees using a computer vision-based approach},
journal = {Expert Systems with Applications},
volume = {56},
pages = {1-8},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.01.053},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416300173},
author = {Mengyu Wang and Jing Zhang and Lars J. Grimm and Sujata V. Ghate and Ruth Walsh and Karen S. Johnson and Joseph Y. Lo and Maciej A. Mazurowski},
keywords = {Radiology education, False negative error prediction, Training protocol optimization, Tomosynthesis},
abstract = {Purpose
Digital breast tomosynthesis (DBT) can improve lesion visibility in comparison to mammography by eliminating breast tissue superimposition. While the benefits of DBT in breast cancer screening rely on well trained radiologists, the optimal training regimen in DBT is unknown. We propose a computer-aided educational system that individually selects the optimal training cases for each trainee. The first step towards this goal is to capture the individual weaknesses of each trainee. In this study, we present and evaluate a computer algorithm for this purpose with particular focus on false negative errors.
Methods
We developed an algorithm (a user model) that predicted the likelihood of a trainee missing an abnormal location. An individual model is applied for each trainee. The algorithm consists of three steps. First, the lesions on DBT images are segmented by a 3D active contour method with a level set algorithm. Then, 16 features are extracted automatically for the segmented lesions. Finally a multivariate logistic regression classifier predicts the likelihood of error based on the extracted features. The classifier is trained using the previous interpretation data of the trainee. We evaluated the individual predictive algorithms experimentally using data from a reader study in which 29 trainees and 3 expert breast radiologists read 60 DBT cases. Receiver operating characteristic (ROC) analysis, along with a repeated holdout approach, was used to evaluate the predictive performance of our algorithm.
Results
The average area under the ROC curve (AUC) of the algorithms which predicted which lesions will be detected and which will be missed by a specific trainee was 0.627 (95% CI: 0.579–0.675). The average performance was statistically significantly better than chance (p<0.001). Under the status quo, training involves no specific strategy for case presentation, and this random behavior corresponds to AUC of 0.5. Therefore, the proposed algorithm may provide a significant improvement in distinguishing abnormal locations that will be detected by a trainee from those that will be missed.
Conclusions
Our algorithm was able to distinguish abnormal locations that will be detected by a trainee from those that will be missed. This could be used to enrich the training set with cases that are likely to prompt error for the individual trainee while still maintaining a range of cases necessary for comprehensive education.}
}
@article{SADOWSKI201649,
title = {Non-destructive neural identification of the bond between concrete layers in existing elements},
journal = {Construction and Building Materials},
volume = {127},
pages = {49-58},
year = {2016},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2016.09.146},
url = {https://www.sciencedirect.com/science/article/pii/S0950061816315975},
author = {Łukasz Sadowski and Jerzy Hoła and Sławomir Czarnecki},
keywords = {Concrete layers, Non-destructive testing, Acoustic methods, Impact-echo, Impulse response, Artificial intelligence, Neural networks, Interlayer bond, Pull-off adhesion, Methodology},
abstract = {The paper presents the results regarding the identification of the value of the pull-off adhesion between a concrete added layer with a constant thickness and a substrate concrete layer in existing elements. A method of identification, which is based solely on the use of artificial neural networks (ANNs) and two non-destructive acoustic methods: impact-echo and impulse response on the surface of the added layer, was developed. The methodology of identifying the pull-off adhesion between a concrete added layer and a substrate layer in existing elements was developed and presented in the paper and is useful in construction practice.}
}