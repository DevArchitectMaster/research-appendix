@article{LIN2022108715,
title = {Digital-twin-based improvements to diagnosis, prognosis, strategy assessment, and discrepancy checking in a nearly autonomous management and control system},
journal = {Annals of Nuclear Energy},
volume = {166},
pages = {108715},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2021.108715},
url = {https://www.sciencedirect.com/science/article/pii/S0306454921005910},
author = {Linyu Lin and Paridhi Athe and Pascal Rouxelin and Maria Avramova and Abhinav Gupta and Robert Youngblood and Jeffrey Lane and Nam Dinh},
keywords = {autonomous control, digital twin, diagnosis, prognosis},
abstract = {The Nearly Autonomous Management and Control System (NAMAC) is a comprehensive control system that assists plant operations by furnishing control recommendations to operators in a broad class of situations. This study refines a NAMAC system for making reasonable recommendations during complex loss-of-flow scenarios with a validated Experimental Breeder Reactor II simulator, digital twins improved by machine-learning algorithms, a multi-attribute decision-making scheme, and a discrepancy checker for identifying unexpected recommendation effects. We assess the performance of each NAMAC component, while we demonstrate and evaluated the capability of NAMAC in a class of loss-of-flow scenarios.}
}
@article{MA2021220,
title = {Progressive Mimic Learning: A new perspective to train lightweight CNN models},
journal = {Neurocomputing},
volume = {456},
pages = {220-231},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.086},
url = {https://www.sciencedirect.com/science/article/pii/S092523122100638X},
author = {Hongbin Ma and Shuyuan Yang and Dongzhu Feng and Licheng Jiao and Luping Zhang},
keywords = {Lightweight CNN models, Progressive Mimic Learning, Knowledge distillation},
abstract = {Knowledge distillation (KD) builds a lightweight Student Model (SM) and trains it to approximate a large Teacher Model (TM) by exploring knowledge learned by the TM, which shows effectiveness to train lightweight CNN models. However, training a small SM to achieve better performance remains a challenging problem. Recent researches on human learning behaviors show that both the knowledge from teachers and the knowledge learning processes of teachers are significant for students. Inspired by this characteristic, in this paper, we propose a new perspective, called Progressive Mimic Learning (PML), to train lightweight CNN models by mimicking the learning trajectory of the TM. In order to obtain a more powerful SM, the useful hints in the learning process of the TM are explored. To start with, the TM learning process is divided into multiple stages, and the last state of the TM in each stage is recorded as a landmark. The learning trajectory of the TM is composed of these landmarks. Then, a landmark loss is defined to constrain the SM to progressively mimic the learning process of the TM, by employing landmarks in the learning trajectory as a training hint of the SM. Several experiments are conducted on four benchmark data sets, CIFAR-10, CIFAR-100, Fashion-MNIST, and ImageNet-10, to investigate the performance of the PML. The results show that the PML can make SMs generate more accurate predictions than SMs trained by its counterparts.}
}
@article{HALLAJI2022104049,
title = {Predictive maintenance of pumps in civil infrastructure: State-of-the-art, challenges and future directions},
journal = {Automation in Construction},
volume = {134},
pages = {104049},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104049},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005008},
author = {Seyed Mostafa Hallaji and Yihai Fang and Brandon K. Winfrey},
keywords = {Pump, Predictive maintenance, Digital twin, Building information modelling, Machine learning},
abstract = {Predictive maintenance (PdM) is a technique that employs data-driven analysis to detect anomalous working conditions and predict future failure risks of assets. Despite wide applications in the manufacturing and oil and gas industries, the application of PdM in infrastructure facilities, such as wastewater treatment plants, is scarce. Recent advent of information and communication technologies and artificial intelligence presents a great opportunity to enhance the practice in infrastructure maintenance by integrating PdM techniques. This study aims to investigate the potentials and challenges of integrating emerging technologies in the PdM of pumps. A quantitative review of the literature was conducted to identify primary research themes and knowledge domains. A qualitative review was conducted to assess their potentials for realizing PdM in pump maintenance. Findings from this research are expected to point out key technical and practical challenges and future research directions.}
}
@article{LI2021529,
title = {A data-based soft-sensor approach to estimating raceway depth in ironmaking blast furnaces},
journal = {Powder Technology},
volume = {390},
pages = {529-538},
year = {2021},
issn = {0032-5910},
doi = {https://doi.org/10.1016/j.powtec.2021.05.072},
url = {https://www.sciencedirect.com/science/article/pii/S0032591021004812},
author = {Wangyan Li and Yuting Zhuo and Jie Bao and Yansong Shen},
keywords = {Raceway, Soft-sensor, Ironmaking blast furnace, Principal component analysis (PCA), Support vector machine (SVM), Thermal image},
abstract = {Raceway is a key region in ironmaking blast furnace (BF). While the raceway depth is extremely difficult to measure, thermal images near tuyeres may be available. In this study, inspired by the concept of digital-twin, a soft-sensor approach is proposed to estimate the raceway depth from thermal images. This approach includes (1) The representative thermal images are generated through a raceway CFD model under industry-scale conditions of a specific BF; (2) A principal component analysis (PCA) method is used to reduce data dimension and extract key features from the thermal images of high dimension; (3) A model-learning tool, support vector machine (SVM) is developed to learn the underlying data-driven soft-sensor model between extracted features from PCA and raceway depth from CFD simulations. The result shows that the soft-sensor model can effectively capture the latent relationship between thermal images and the raceway depth, which can be used to estimate raceway depth in real-time in practice.}
}
@article{WERNECK2021114775,
title = {Effective and diverse POI recommendations through complementary diversification models},
journal = {Expert Systems with Applications},
volume = {175},
pages = {114775},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114775},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002165},
author = {Heitor Werneck and Rodrigo Santos and Nícollas Silva and Adriano C.M. Pereira and Fernando Mourão and Leonardo Rocha},
keywords = {Recommender systems and algorithms, Knowledge management, Point-of-interest, Location-based social networks, Diversity},
abstract = {Nowadays, recommender systems play an important role in several Location-Based Social Networks (LBSNs). The current advances have considered the trade-off between accuracy and diversity to help users to discover and explore new points-of-interest (POI). However, differently from traditional recommendation scenarios, other equally relevant dimensions (e.g., social and geographical user information) have to be considered to understand how the characteristics of services offered by each POI fit the user needs. Specifically, this work sheds light upon naive failures introduced by traditional recommendation methods while they handle this trade-off between diversity and accuracy in POI recommendations. We hypothesize that some efforts on POI recommendations somehow are deviating from basic learnings from the area. In this context, this work addresses four characteristics inherent to the POI domain that previous efforts have failed to recognize: (1) POI categories and locations are complementary dimensions of diversification that should be simultaneously addressed; (2) Diversity is a complex concept that should be modeled by distinct and non-orthogonal models; (3) Distinct users have different biases and willingness to move to fulfill their needs; (4) POI recommendation is a multi-objective task. In order to demonstrate the gains of properly addressing these aspects, we also propose DisCovER, a straightforward re-ordering method that linearly combines geographical and categorical diversification. DisCovER results demonstrate that even simple strategies to exploit simultaneously these complementary dimensions can increase diversification while keeping accuracy high. Differently from state-of-the-art diversification methods, DisCovER does not penalize any quality dimension in favor of others. It allows us to discuss future directions towards more robust user modeling and preference elicitation in POI domains.}
}
@article{CHAKRABARTY2021111460,
title = {Scalable Bayesian optimization for model calibration: Case study on coupled building and HVAC dynamics},
journal = {Energy and Buildings},
volume = {253},
pages = {111460},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111460},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821007441},
author = {Ankush Chakrabarty and Emilio Maddalena and Hongtao Qiao and Christopher Laughman},
keywords = {Parameter estimation, Gaussian processes, Bayesian methods, Sensitivity analysis, Digital twins, Building energy systems, Probabilistic machine learning},
abstract = {Model calibration for building systems is a key step to achieving accurate and reliable predictions that reflect the dynamics of real systems under study. Calibration becomes particularly challenging when integrating building and HVAC dynamics, due to large-scale, nonlinear, and stiff underlying differential algebraic equations. In this paper, we describe a framework for calibrating multiple parameters of coupled building/HVAC models using scalable Bayesian optimization (BO), whose advantages include global optimization without requiring gradient information, and data-efficiency. The proposed methodology is improved online via two additional steps: domain tightening and domain slicing, both of which leverage the learned calibration cost function to reduce the search space volume and dimension, respectively. We demonstrate effectiveness of the proposed algorithm by simultaneously calibrating 17 parameters (including emissivities, heat transfer coefficients, and thickness of walls/floors) of a Modelica model of joint building and HVAC dynamics, with 2 weeks worth of building data. This high-dimensional calibration task is solved via our proposed scalable BO calibration method, and yields parameters that are >90% accurate with <1000 model simulations; additionally, the outputs of the final calibrated model on unseen testing data complies with standard ASHRAE calibration guidelines.}
}
@article{CHOI2022102258,
title = {An integrated mixed reality system for safety-aware human-robot collaboration using deep learning and digital twin generation},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {73},
pages = {102258},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102258},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521001381},
author = {Sung Ho Choi and Kyeong-Beom Park and Dong Hyeon Roh and Jae Yeol Lee and Mustafa Mohammed and Yalda Ghasemi and Heejin Jeong},
keywords = {Human-robot collaboration, Mixed reality, Deep learning, Safety distance calculation, Digital twin, Cyber-physical system},
abstract = {For human-robot collaboration (HRC), one of the most practical methods to ensure human safety with a vision-based system is establishing a minimum safe distance. This study proposes a novel integrated mixed reality (MR) system for safety-aware HRC using deep learning and digital twin generation. The proposed approach can accurately measure the minimum safe distance in real-time and provide MR-based task assistance to the human operator. The approach integrates MR with safety-related monitoring by tracking the shared workplace and providing user-centric visualization through smart MR glasses for safe and effective HRC. Two RGB-D sensors are used to reconstruct and track the working environment. One sensor scans one area of the physical environment through 3D point cloud data. The other also scans another area of the environment and tracks the user's 3D skeletal information. In addition, the two partially scanned environments are registered together by applying a fast global registration method to two sets of the 3D point cloud. Furthermore, deep learning-based instance segmentation is applied to the target object's 3D point cloud to increase the registration between the real robot and its virtual robot, the digital twin of the real robot. While only 3D point cloud data are widely used in previous studies, this study proposes a simple yet effective 3D offset-based safety distance calculation method based on the robot's digital twin and the human skeleton. The 3D offset-based method allows for real-time applicability without sacrificing the accuracy of safety distance calculation for HRI. In addition, two comparative evaluations were conducted to confirm the originality and advantage of the proposed MR-based HRC.}
}
@article{WANG2021103786,
title = {Semi-supervised semantic segmentation network for surface crack detection},
journal = {Automation in Construction},
volume = {128},
pages = {103786},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103786},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002375},
author = {Wenjun Wang and Chao Su},
keywords = {Deep learning, Convolutional neural network, Semi-supervised network, Crack detection, Pixel-wise segmentation},
abstract = {The detection of surface crack is essential to ensure the safety and the serviceability of civil infrastructure. The automatic method is highly efficient and the test results are objective, which makes it gradually replace conventical manual inspection. Recently, semantic segmentation algorithms based on deep learning have shown excellent performance in crack detection tasks. However, the commonly used fully supervised segmentation method requires manual annotation of large amounts of data, which is time-consuming. In order to solve this problem, we propose a semi-supervised semantic segmentation network for crack detection. The proposed method consists of student model and teacher model. The two models have the same network structure and use the EfficientUNet to extract multi-scale crack feature information, reducing the loss of image information. The student model updates weights through the gradient descent of loss function, and the teacher model uses the exponential moving average weights of the student model. During training, the robustness of the model is improved by adding noise to the input data. When using only 60% of the annotated data, our method achieves an F1 score of 0.6540 on the concrete crack dataset and 0.8321 on the Crack500 dataset. The results show that our method can greatly reduce the workload of annotation while maintaining high accuracy.}
}
@article{LEUNG2022108353,
title = {From traditional warehouses to Physical Internet hubs: A digital twin-based inbound synchronization framework for PI-order management},
journal = {International Journal of Production Economics},
volume = {244},
pages = {108353},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108353},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321003297},
author = {Eric K.H. Leung and Carmen Kar Hang Lee and Zhiyuan Ouyang},
keywords = {Hyperconnected city logistics, Physical internet, Synchronization, Joint order fulfillment and replenishment, Digital twins, Internet of things},
abstract = {Physical Internet (PI) is a new concept to ensure global mobility of physical objects. Conventionally, logistics networks are closed and independent. Under the concept of PI, they are transformed into an open logistics network, providing an efficient way to relocate physical goods to a given place in a short period of time. A hyperconnected city logistics system is conceptualized as the final segment of a PI-network. It uses regional and city hubs as the final leg of last-mile delivery. Inventory at PI-hubs has to be managed efficiently so as to maximize the benefits of PI. This paper proposes a digital twin-based inbound synchronization framework to streamline the operations of a PI-hub in a hyperconnected city logistics system. Digital twins and Internet of Things technologies are proposed for data acquisition and virtualization of real conditions of physical objects, followed by machine learning-integrated models to optimize a joint order fulfillment and replenishment operation in the PI-hubs. Adopting the proposed framework can formulate a Total Inbound Synchronization at three levels: order synchronization, process synchronization and information synchronization. Simulation results show a significant reduction of traveling distance in PI-hubs if the interdependent order fulfillment and replenishment operations are considered as a joint operation. In addition, this paper provides practical implications for logistics service providers to manage information flows within a PI-network driven by digital twins.}
}
@article{OVERENG2021109433,
title = {Dynamic Positioning using Deep Reinforcement Learning},
journal = {Ocean Engineering},
volume = {235},
pages = {109433},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109433},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821008398},
author = {Simen Sem Øvereng and Dong Trong Nguyen and Geir Hamre},
keywords = {Dynamic Positioning, Deep Reinforcement Learning, Proximal policy optimization, Reward shaping},
abstract = {This paper demonstrates the implementation and performance testing of a Deep Reinforcement Learning based control scheme used for Dynamic Positioning of a marine surface vessel. The control scheme encapsulated motion control and control allocation by using a neural network, which was trained on a digital twin without having any prior knowledge of the system dynamics, using the Proximal Policy Optimization learning algorithm. By using a multivariate Gaussian reward function for rewarding small errors between the vessel and the various setpoints, while encouraging small actuator outputs, the proposed Deep Reinforcement Learning based control scheme showed good positioning performance while being energy efficient. Both simulations and model scale sea trials were carried out to demonstrate performance compared to traditional methods, and to evaluate the ability of neural networks trained in simulation to perform on real life systems.}
}
@article{CHEN2021118568,
title = {MTANS: Multi-Scale Mean Teacher Combined Adversarial Network with Shape-Aware Embedding for Semi-Supervised Brain Lesion Segmentation},
journal = {NeuroImage},
volume = {244},
pages = {118568},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118568},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921008417},
author = {Gaoxiang Chen and Jintao Ru and Yilin Zhou and Islem Rekik and Zhifang Pan and Xiaoming Liu and Yezhi Lin and Beichen Lu and Jialin Shi},
keywords = {Brain Lesion Segmentation, Semi-Supervised Learning, MRI, Deep Learning},
abstract = {The annotation of brain lesion images is a key step in clinical diagnosis and treatment of a wide spectrum of brain diseases. In recent years, segmentation methods based on deep learning have gained unprecedented popularity, leveraging a large amount of data with high-quality voxel-level annotations. However, due to the limited time clinicians can provide for the cumbersome task of manual image segmentation, semi-supervised medical image segmentation methods present an alternative solution as they require only a few labeled samples for training. In this paper, we propose a novel semi-supervised segmentation framework that combines improved mean teacher and adversarial network. Specifically, our framework consists of (i) a student model and a teacher model for segmenting the target and generating the signed distance maps of object surfaces, and (ii) a discriminator network for extracting hierarchical features and distinguishing the signed distance maps of labeled and unlabeled data. Besides, based on two different adversarial learning processes, a multi-scale feature consistency loss derived from the student and teacher models is proposed, and a shape-aware embedding scheme is integrated into our framework. We evaluated the proposed method on the public brain lesion datasets from ISBI 2015, ISLES 2015, and BRATS 2018 for the multiple sclerosis lesion, ischemic stroke lesion, and brain tumor segmentation respectively. Experiments demonstrate that our method can effectively leverage unlabeled data while outperforming the supervised baseline and other state-of-the-art semi-supervised methods trained with the same labeled data. The proposed framework is suitable for joint training of limited labeled data and additional unlabeled data, which is expected to reduce the effort of obtaining annotated images.}
}
@article{CENTORRINO2021101357,
title = {Managing crowded museums: Visitors flow measurement, analysis, modeling, and optimization},
journal = {Journal of Computational Science},
volume = {53},
pages = {101357},
year = {2021},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2021.101357},
url = {https://www.sciencedirect.com/science/article/pii/S1877750321000521},
author = {P. Centorrino and A. Corbetta and E. Cristiani and E. Onofri},
keywords = {IoT, Machine learning, Clustering, Tracking system, Museum simulator, Museum optimization},
abstract = {We present an all-around study of the visitors flow in crowded museums: a combination of Lagrangian field measurements and statistical analyses enable us to create stochastic digital-twins of the guest dynamics, unlocking comfort- and safety-driven optimizations. Our case study is the Galleria Borghese museum in Rome (Italy), in which we performed a real-life data acquisition campaign. We specifically employ a Lagrangian IoT-based visitor tracking system based on Raspberry Pi receivers, displaced in fixed positions throughout the museum rooms, and on portable Bluetooth Low Energy beacons handed over to the visitors. Thanks to two algorithms: a sliding window-based statistical analysis and an MLP neural network, we filter the beacons RSSI and accurately reconstruct visitor trajectories at room-scale. Via a clustering analysis, hinged on an original Wasserstein-like trajectory-space metric, we analyze the visitors paths to get behavioral insights, including the most common flow patterns. On these bases, we build the transition matrix describing, in probability, the room-scale visitor flows. Such a matrix is the cornerstone of a stochastic model capable of generating visitor trajectories in silico. We conclude by employing the simulator to enhance the museum fruition while respecting numerous logistic and safety constraints. This is possible thanks to optimized ticketing and new entrance/exit management.}
}
@article{PARK2021100702,
title = {Bioprocess digital twins of mammalian cell culture for advanced biomanufacturing},
journal = {Current Opinion in Chemical Engineering},
volume = {33},
pages = {100702},
year = {2021},
issn = {2211-3398},
doi = {https://doi.org/10.1016/j.coche.2021.100702},
url = {https://www.sciencedirect.com/science/article/pii/S2211339821000344},
author = {Seo-Young Park and Cheol-Hwan Park and Dong-Hyuk Choi and Jong Kwang Hong and Dong-Yup Lee},
abstract = {Today’s biomanufacturing processes are still operated based on experience, and thus can hardly cope with increasing bioprocess complexity. Recently, there is a growing interest in industrial applications of the digital twins (DT) which integrate physical and virtual systems via real-time data monitoring, thus enabling their interactive communications for the enhanced operational efficiency towards advanced biomanufacturing. We suggest that bioprocess DT of cell cultures can be developed by incorporating in-line monitoring, advanced data analytics with machine and deep learning, and mechanistic models representing the mammalian cells and bioreactor for virtually mirroring their behaviors under adjustable process conditions. In this review, we summarize and highlight recent advances in the key components within bioprocess DT platform and discuss the current challenges with future research direction.}
}
@article{CHABANET2021103529,
title = {Coupling digital simulation and machine learning metamodel through an active learning approach in Industry 4.0 context},
journal = {Computers in Industry},
volume = {133},
pages = {103529},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103529},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001366},
author = {Sylvain Chabanet and Hind {Bril El-Haouzi} and Philippe Thomas},
keywords = {Stream based active learning, K-nearest neighbors, Simulation metamodel, Sawmill, Smart data, Artificial intelligence},
abstract = {Although digital simulations are becoming increasingly important in the industrial world owing to the transition toward Industry 4.0, as well as the development of digital twin technologies, they have become increasingly computationally intensive. Many authors have proposed the use of machine learning (ML) metamodels to alleviate this cost and take advantage of the enormous amount of data that are currently available in industry. In an industrial context, it is necessary to continuously train predictive models integrated into decision support systems to ensure the consistency of their prediction quality over time. This led the authors to investigate active learning (AL) concepts in the particular context of the sawmilling industry. In this paper, a method based on AL is proposed to combine simulation and an ML metamodel that is trained incrementally using only selected data (smart data). A case study based on the sawmilling industry and experiments are shown, the results of which prove the possible advantages of this approach.}
}
@article{LI2022122178,
title = {Data-driven hybrid petri-net based energy consumption behaviour modelling for digital twin of energy-efficient manufacturing system},
journal = {Energy},
volume = {239},
pages = {122178},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122178},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221024269},
author = {Hongcheng Li and Dan Yang and Huajun Cao and Weiwei Ge and Erheng Chen and Xuanhao Wen and Chongbo Li},
keywords = {Energy management, Energy behaviour modelling, Digital twin, Data-driven hybrid petri-net, Gaussian kernel extreme learning machine},
abstract = {Advances in energy-saving technology is main way to achieve carbon neutrality. With the development of digital twin, building the physical-virtual data space for improving energy management capacity of enterprises has received tremendous attention. The energy behaviour model implementing accurate simulation and prediction of energy state is the core meta-model of energy-efficient manufacturing digital twin (EMDT). The widely used state-based energy modelling assumes constant power in operation state and approximately fits the energy behaviour without considering uncertain operation environment, resulting in energy behaviour distortion. A data-driven hybrid petri-net (DDHPN) inspired by both the state-based energy modelling and machine learning was developed for establishing the energy behaviour meta-model. Gaussian kernel extreme learning machine is proposed to fit the instantaneous firing speed of energy consumption continuous transitions in DDHPN. DDHPN-based energy behaviour model is driven by physical data under real-time working conditions, operating parameters, and production load for generating a virtual data space of energy management. Finally, DDHPN was integrated into the EMDT model using unified modelling language. The application in extrusion process and die casting process show that the presented model has higher accuracy in energy behaviour prediction. Furthermore, a digital-twin-based energy management prototype system for extrusion workshop demonstrates its potential.}
}
@article{TIAN2022108284,
title = {Real-time model calibration with deep reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {165},
pages = {108284},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108284},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021006506},
author = {Yuan Tian and Manuel Arias Chao and Chetan Kulkarni and Kai Goebel and Olga Fink},
keywords = {Model calibration, Reinforcement learning, Model-based diagnostics, Deep learning},
abstract = {The real-time, and accurate inference of model parameters is of great importance in many scientific and engineering disciplines that use computational models (such as a digital twin) for the analysis and prediction of complex physical processes. However, fast and accurate inference for processes of complex systems cannot easily be achieved in real-time with state-of-the-art methods under noisy real-world conditions with the requirement of a real-time response. The primary reason is that the inference of model parameters with traditional techniques based on optimization or sampling often suffers from computational and statistical challenges, resulting in a trade-off between accuracy and deployment time. In this paper, we propose a novel framework for inference of model parameters based on reinforcement learning. The proposed methodology is demonstrated and evaluated on two different physics-based models of turbofan engines. The experimental results demonstrate that the proposed methodology outperforms all other tested methods in terms of speed and robustness, with high inference accuracy.}
}
@article{SADEGHIAN2021101389,
title = {Happiness recognition from smartphone usage data considering users’ estimated personality traits},
journal = {Pervasive and Mobile Computing},
volume = {73},
pages = {101389},
year = {2021},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2021.101389},
url = {https://www.sciencedirect.com/science/article/pii/S157411922100050X},
author = {Alireza Sadeghian and Marjan Kaedi},
keywords = {User modeling, Happiness, Smartphone usage, Personality traits, Machine learning},
abstract = {The daily and routine interactions of people with their smartphones in various situations make these devices valuable data sources to understand user behaviors. Passive users’ emotion recognition is one of the most essential user modeling areas and has been studied for various purposes so far. Psychological studies, on the other hand, show that the personality of users can influence their behavior when they experience different emotions. Individuals with varying types of personality exhibit different reactions in the same emotional situation. It is concluded that if we consider the user’s personality in passive recognition of his/her emotion, the emotion can be identified more accurately. However, researchers have not paid enough attention to the users’ personality traits when identifying the users’ emotions based on their interaction with cell phones. In the present study, we strive to address this research gap. Among the various emotions, our focus is on happiness recognition. In our proposed method, the user’s personality traits are first estimated based on his/her interactions with the smartphone. Then the estimated personality of the user, along with the data of his/her interactions with the smartphone, is taken into account to recognize his/her happiness. Evaluations showed that taking into account the users’ personality traits reduces the happiness recognition error.}
}
@article{AHMED2021104895,
title = {A nudged hybrid analysis and modeling approach for realtime wake-vortex transport and decay prediction},
journal = {Computers & Fluids},
volume = {221},
pages = {104895},
year = {2021},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2021.104895},
url = {https://www.sciencedirect.com/science/article/pii/S004579302100061X},
author = {Shady E. Ahmed and Suraj Pawar and Omer San and Adil Rasheed and Mandar Tabib},
keywords = {Wake vortex, Digital twins, Nudging, Data fusion, Nonlinear filtering, Galerkin projection, Model order reduction, Proper orthogonal decomposition, Sparse reconstruction, Closure modeling},
abstract = {We put forth a long short-term memory (LSTM) nudging framework for the enhancement of reduced order models (ROMs) of fluid flows utilizing noisy measurements for air traffic improvements. Toward emerging applications of digital twins in aviation, the proposed approach allows for constructing a realtime predictive tool for wake-vortex transport and decay systems. We build on the fact that in realistic application, there are uncertainties in initial and boundary conditions, model parameters, as well as measurements. Moreover, conventional nonlinear ROMs based on Galerkin projection (GROMs) suffer from imperfection and solution instabilities, especially for advection-dominated flows with slow decay in the Kolmogorov n-width. In the presented LSTM nudging (LSTM-N) approach, we fuse forecasts from a combination of imperfect GROM and uncertain state estimates, with sparse Eulerian sensor measurements to provide more reliable predictions in a dynamical data assimilation framework. We illustrate our concept by solving the two-dimensional vorticity transport equation. We investigate the effects of measurements noise and state estimate uncertainty on the performance of the LSTM-N behavior. We also demonstrate that it can sufficiently handle different levels of temporal and spatial measurement sparsity, and offer a huge potential in developing next-generation digital twin technologies for aerospace applications.}
}
@article{ALANNE2022103445,
title = {An overview of machine learning applications for smart buildings},
journal = {Sustainable Cities and Society},
volume = {76},
pages = {103445},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103445},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721007186},
author = {Kari Alanne and Seppo Sierla},
keywords = {Smart building, Intelligent building, Learning, HVAC, Reinforcement learning, Energy efficiency},
abstract = {The efficiency, flexibility, and resilience of building-integrated energy systems are challenged by unpredicted changes in operational environments due to climate change and its consequences. On the other hand, the rapid evolution of artificial intelligence (AI) and machine learning (ML) has equipped buildings with an ability to learn. A lot of research has been dedicated to specific machine learning applications for specific phases of a building's life-cycle. The reviews commonly take a specific, technological perspective without a vision for the integration of smart technologies at the level of the whole system. Especially, there is a lack of discussion on the roles of autonomous AI agents and training environments for boosting the learning process in complex and abruptly changing operational environments. This review article discusses the learning ability of buildings with a system-level perspective and presents an overview of autonomous machine learning applications that make independent decisions for building energy management. We conclude that the buildings’ adaptability to unpredicted changes can be enhanced at the system level through AI-initiated learning processes and by using digital twins as training environments. The greatest potential for energy efficiency improvement is achieved by integrating adaptability solutions at the timescales of HVAC control and electricity market participation.}
}
@article{LIU2021108140,
title = {Certainty driven consistency loss on multi-teacher networks for semi-supervised learning},
journal = {Pattern Recognition},
volume = {120},
pages = {108140},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108140},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321003277},
author = {Lu Liu and Robby T. Tan},
keywords = {Semi-supervised learning, Certainty-driven consistency loss, Uncertainty estimation, Decoupled student-teacher, Reliable targets, Noisy labels},
abstract = {One of the successful approaches in semi-supervised learning is based on the consistency regularization. Typically, a student model is trained to be consistent with teacher prediction for the inputs under different perturbations. To be successful, the prediction targets given by teacher should have good quality, otherwise the student can be misled by teacher. Unfortunately, existing methods do not assess the quality of the teacher targets. In this paper, we propose a novel Certainty-driven Consistency Loss (CCL) that exploits the predictive uncertainty in the consistency loss to let the student dynamically learn from reliable targets. Specifically, we propose two approaches, i.e. Filtering CCL and Temperature CCL to either filter out uncertain predictions or pay less attention on them in the consistency regularization. We further introduce a novel decoupled framework to encourage model difference. Experimental results on SVHN, CIFAR-10, and CIFAR-100 demonstrate the advantages of our method over a few existing methods.}
}
@article{BI2022100316,
title = {New digital triad (DT-II) concept for lifecycle information integration of sustainable manufacturing systems},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100316},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100316},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21001096},
author = {Zhuming Bi and Chris W.J. Zhang and Chong Wu and Ling Li},
keywords = {Information integration (II), Digital manufacturing (DM), Digital twin (DT-I), Digital triad (DT-II), Internet of Things (IoT), Internet of digital triad things (IoDTT), Enterprise architecture (EA), Cyber-physical system (CPS), Cloud computing (CC), Big data analytics (BDA), Machine learning (ML), Blockchain technology (BCT)},
abstract = {A system paradigm is a typical pattern or model of enterprise architectures (EAs) that describes constitutive system elements and their relations in achieving missions and goals of enterprises. A well-defined system paradigm and EA help enterprises in adopting appropriate manufacturing resources and technologies, optimizing plans, schedules, and controls of production lines, and coping with complexity, changes, and uncertainties of business processes cost-effectively. It is understandable that a system paradigm should be evolved along with the availability and advancement of advanced manufacturing technologies; on the other hand, new system paradigms are mostly inspired by some successful manufacturing applications of new information technologies (ITs). One of the recently developed system paradigms is Digital Manufacturing (DM) where Digital Twin (DT-I) is used to describe the interactions of virtual and physical entities. This paper discusses the concepts of DM and DT-I and rationalizes the relations of DM and DT-I; in particular, the origin and evolution of DT-I are explored in details to identify its limitations to be used as an EA for DM. It is our finding that existing IT concepts show their limitations in supporting smooth transitions when systems have to be reconfigured in dealing with long-term changes in Sustainable Manufacturing, and this is evidenced by the fact of the trend of the shortened lifespans of modern enterprises even with the aids of rapidly developed digital technologies in decades. To overcome these limitations, a new concept so called Digital Triad (DT-II) is coined and the Internet of Digital Triad Things (IoDTT) is proposed as an information integration (II) solution for digital manufacturing enterprises, and their application is illustrated through a reconfigurable robotic system example. The rationales and significances of DT-II and IoDTT as well as future research directions of DM are summarized as a conclusion.}
}
@article{LIU2022108341,
title = {Weakly Supervised Segmentation of COVID19 Infection with Scribble Annotation on CT Images},
journal = {Pattern Recognition},
volume = {122},
pages = {108341},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108341},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005215},
author = {Xiaoming Liu and Quan Yuan and Yaozong Gao and Kelei He and Shuo Wang and Xiao Tang and Jinshan Tang and Dinggang Shen},
keywords = {COVID-19, infection segmentation, weakly supervised learning, transformation consistency, uncertainty},
abstract = {Segmentation of infections from CT scans is important for accurate diagnosis and follow-up in tackling the COVID-19. Although the convolutional neural network has great potential to automate the segmentation task, most existing deep learning-based infection segmentation methods require fully annotated ground-truth labels for training, which is time-consuming and labor-intensive. This paper proposed a novel weakly supervised segmentation method for COVID-19 infections in CT slices, which only requires scribble supervision and is enhanced with the uncertainty-aware self-ensembling and transformation-consistent techniques. Specifically, to deal with the difficulty caused by the shortage of supervision, an uncertainty-aware mean teacher is incorporated into the scribble-based segmentation method, encouraging the segmentation predictions to be consistent under different perturbations for an input image. This mean teacher model can guide the student model to be trained using information in images without requiring manual annotations. On the other hand, considering the output of the mean teacher contains both correct and unreliable predictions, equally treating each prediction in the teacher model may degrade the performance of the student network. To alleviate this problem, the pixel level uncertainty measure on the predictions of the teacher model is calculated, and then the student model is only guided by reliable predictions from the teacher model. To further regularize the network, a transformation-consistent strategy is also incorporated, which requires the prediction to follow the same transformation if a transform is performed on an input image of the network. The proposed method has been evaluated on two public datasets and one local dataset. The experimental results demonstrate that the proposed method is more effective than other weakly supervised methods and achieves similar performance as those fully supervised.}
}
@article{ZHOU20211274,
title = {Intelligent Ironmaking Optimization Service on a Cloud Computing Platform by Digital Twin},
journal = {Engineering},
volume = {7},
number = {9},
pages = {1274-1281},
year = {2021},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2021.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S209580992100299X},
author = {Heng Zhou and Chunjie Yang and Youxian Sun},
keywords = {Cloud factory, Blast furnace, Multi-objective optimization, Distributed computation},
abstract = {The shortage of computation methods and storage devices has largely limited the development of multi-objective optimization in industrial processes. To improve the operational levels of the process industries, we propose a multi-objective optimization framework based on cloud services and a cloud distribution system. Real-time data from manufacturing procedures are first temporarily stored in a local database, and then transferred to the relational database in the cloud. Next, a distribution system with elastic compute power is set up for the optimization framework. Finally, a multi-objective optimization model based on deep learning and an evolutionary algorithm is proposed to optimize several conflicting goals of the blast furnace ironmaking process. With the application of this optimization service in a cloud factory, iron production was found to increase by 83.91 t∙d−1, the coke ratio decreased 13.50 kg∙t−1, and the silicon content decreased by an average of 0.047%.}
}
@article{DEGIORGIO202122,
title = {Towards online reinforced learning of assembly sequence planning with interactive guidance systems for industry 4.0 adaptive manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {22-34},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521000984},
author = {Andrea {de Giorgio} and Antonio Maffei and Mauro Onori and Lihui Wang},
keywords = {Reinforcement learning, Adaptive assembly, Assembly sequence planning, Assembly guidance system, Manufacturing, Industry 4.0, Optimization, Knowledge retrieval},
abstract = {Literature shows that reinforcement learning (RL) and the well-known optimization algorithms derived from it have been applied to assembly sequence planning (ASP); however, the way this is done, as an offline process, ends up generating optimization methods that are not exploiting the full potential of RL. Today’s assembly lines need to be adaptive to changes, resilient to errors and attentive to the operators’ skills and needs. If all of these aspects need to evolve towards a new paradigm, called Industry 4.0, the way RL is applied to ASP needs to change as well: the RL phase has to be part of the assembly execution phase and be optimized with time and several repetitions of the process. This article presents an agile exploratory experiment in ASP to prove the effectiveness of RL techniques to execute ASP as an adaptive, online and experience-driven optimization process, directly at assembly time. The human-assembly interaction is modelled through the input-outputs of an assembly guidance system built as an assembly digital twin. Experimental assemblies are executed without pre-established assembly sequence plans and adapted to the operators’ needs. The experiments show that precedence and transition matrices for an assembly can be generated from the statistical knowledge of several different assembly executions. When the frequency of a given subassembly reinforces its importance, statistical results obtained from the experiments prove that online RL applications are not only possible but also effective for learning, teaching, executing and improving assembly tasks at the same time. This article paves the way towards the application of online RL algorithms to ASP.}
}
@article{LI2022167,
title = {Big data analysis of the Internet of Things in the digital twins of smart city based on deep learning},
journal = {Future Generation Computer Systems},
volume = {128},
pages = {167-177},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003964},
author = {Xiaoming Li and Hao Liu and Weixi Wang and Ye Zheng and Haibin Lv and Zhihan Lv},
keywords = {Deep learning, Smart city, Digital twins, Internet of Things, Big data analysis},
abstract = {The study aims to conduct big data analysis (BDA) on the massive data generated in the smart city Internet of things (IoT), make the smart city change to the direction of fine governance and efficient and safe data processing. Aiming at the multi-source data collected in the smart city, the study introduces the deep learning (DL) algorithm while using BDA, and puts forward the distributed parallelism strategy of convolutional neural network (CNN). Meantime, the digital twins (DTs) and multi-hop transmission technology are introduced to construct the smart city DTs multi-hop transmission IoT-BDA system based on DL, and further simulate and analyze the performance of the system. The results reveal that in the energy efficiency analysis of model data transmission, the energy efficiency first increases and then decrease as the minimum energy collected α0 increases. But a more suitable power diversion factor ρ is crucial to the signal transmission energy efficiency of the IoT-BDA system. The prediction accuracy of the model is analyzed and it suggests that the accuracy of the constructed system reaches 97.80%, which is at least 2.24% higher than the DL algorithm adopted by other scholars. Regarding the data transmission performance of the constructed system, it is found that when the successful transmission probability is 100% and the exponential distribution parameters λ is valued 0.01∼0.05, it is the closest to the actual result, and the data delay is the smallest, which is maintained at the ms level. To sum up, improving the smart city’s IoT-BDA system using the DL approach can reduce data transmission delay, improve data forecasting accuracy, and offer actual efficacy, providing experimental references for the digital development of smart cities in the future.}
}
@article{SVABENSKY2021107398,
title = {Dataset of shell commands used by participants of hands-on cybersecurity training},
journal = {Data in Brief},
volume = {38},
pages = {107398},
year = {2021},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2021.107398},
url = {https://www.sciencedirect.com/science/article/pii/S2352340921006806},
author = {Valdemar Švábenský and Jan Vykopal and Pavel Seda and Pavel Čeleda},
keywords = {Cybersecurity education, Cybersecurity exercise, Linux shell, Metasploit, Command-line history, Host-based data collection, Educational data mining, Learning analytics},
abstract = {We present a dataset of 13446 shell commands from 175 participants who attended cybersecurity training and solved assignments in the Linux terminal. Each acquired data record contains a command with its arguments and metadata, such as a timestamp, working directory, and host identification in the emulated training infrastructure. The commands were captured in Bash, ZSH, and Metasploit shells. The data are stored as JSON records, enabling vast possibilities for their further use in research and development. These include educational data mining, learning analytics, student modeling, and evaluating machine learning models for intrusion detection. The data were collected from 27 cybersecurity training sessions using an open-source logging toolset and two open-source interactive learning environments. Researchers and developers may use the dataset or deploy the learning environments with the logging toolset to generate their own data in the same format. Moreover, we provide a set of common analytical queries to facilitate the exploratory analysis of the dataset.}
}
@article{WU2022104301,
title = {An intelligent tunnel firefighting system and small-scale demonstration},
journal = {Tunnelling and Underground Space Technology},
volume = {120},
pages = {104301},
year = {2022},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2021.104301},
url = {https://www.sciencedirect.com/science/article/pii/S0886779821004922},
author = {Xiqiang Wu and Xiaoning Zhang and Yishuo Jiang and Xinyan Huang and George G.Q. Huang and Asif Usmani},
keywords = {Smart firefighting, IoT system, Artificial intelligence, Tunnel fire prediction, Fire modelling},
abstract = {Disastrous fire event in the confined tunnel is a fatal hazard, threatening the lives of trapped people and firefighters. Considering the rapid development of fire and the complex environment of tunnels, an accurate and timely fire identification system is in urgent need for guiding the evacuation, rescue, and firefighting actions. This study proposes an intelligent system and digital twin composed of four main components to collect, manage, process and visualize the tunnel fire information. As demonstrated in a laboratory-scale tunnel model, the AI model is trained with a large numerical database to successfully identify the fire size and location. The whole system is assessed in terms of accuracy, timeliness and robustness. The AI model attained an overall accuracy of 98% in predicting the tunnel fire scenarios. The total time delay is around 1 s from the on-site measurement of temperature to the final display of the tunnel fire scenario on a remote user interface. Moreover, the system is robust enough to predict fire, even if part of the temperature sensors is failed or destroyed by fire. The proposed intelligent system will be a valuable step for smart firefighting from the concept to practice.}
}
@article{LIAO2022102797,
title = {Group event recommendation based on graph multi-head attention network combining explicit and implicit information},
journal = {Information Processing & Management},
volume = {59},
number = {2},
pages = {102797},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102797},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002752},
author = {Guoqiong Liao and Xiaobin Deng and Changxuan Wan and Xiping Liu},
keywords = {Explicit information, Implicit information, Group recommendation, Graph attention network},
abstract = {In event-based social networks (EBSN), group event recommendation has become an important task for groups to quickly find events that they are interested in. Existing methods on group event recommendation either consider just one type of information, explicit or implicit, or separately model the explicit and implicit information. However, these methods often generate a problem of data sparsity or of model vector redundancy. In this paper, we present a Graph Multi-head Attention Network (GMAN) model for group event recommendation that integrates the explicit and implicit information in EBSN. Specifically, we first construct a user-explicit graph based on the user's explicit information, such as gender, age, occupation and the interactions between users and events. Then we build a user-implicit graph based on the user's implicit information, such as friend relationships. The incorporated both explicit and implicit information can effectively describe the user's interests and alleviate the data sparsity problem. Considering that there may be a correlation between the user's explicit and implicit information in EBSN, we take the user's explicit vector representation as the input of the implicit information aggregation when modeling with graph neural networks. This unified user modeling can solve the aforementioned problem of user model vector redundancy and is also suitable for event modeling. Furthermore, we utilize a multi-head attention network to learn richer implicit information vectors of users and events from multiple perspectives. Finally, in order to get a higher level of group vector representation, we use a vanilla attention mechanism to fuse different user vectors in the group. Through experimenting on two real-world Meetup datasets, we demonstrate that GMAN model consistently outperforms state-of-the-art methods on group event recommendation.}
}
@article{MARINI2021102165,
title = {Semi-supervised training of deep convolutional neural networks with heterogeneous data and few local annotations: An experiment on prostate histopathology image classification},
journal = {Medical Image Analysis},
volume = {73},
pages = {102165},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.102165},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521002115},
author = {Niccolò Marini and Sebastian Otálora and Henning Müller and Manfredo Atzori},
keywords = {Computational pathology, Deep learning, Semi-supervision, Prostate cancer},
abstract = {Convolutional neural networks (CNNs) are state-of-the-art computer vision techniques for various tasks, particularly for image classification. However, there are domains where the training of classification models that generalize on several datasets is still an open challenge because of the highly heterogeneous data and the lack of large datasets with local annotations of the regions of interest, such as histopathology image analysis. Histopathology concerns the microscopic analysis of tissue specimens processed in glass slides to identify diseases such as cancer. Digital pathology concerns the acquisition, management and automatic analysis of digitized histopathology images that are large, having in the order of 100′0002 pixels per image. Digital histopathology images are highly heterogeneous due to the variability of the image acquisition procedures. Creating locally labeled regions (required for the training) is time-consuming and often expensive in the medical field, as physicians usually have to annotate the data. Despite the advances in deep learning, leveraging strongly and weakly annotated datasets to train classification models is still an unsolved problem, mainly when data are very heterogeneous. Large amounts of data are needed to create models that generalize well. This paper presents a novel approach to train CNNs that generalize to heterogeneous datasets originating from various sources and without local annotations. The data analysis pipeline targets Gleason grading on prostate images and includes two models in sequence, following a teacher/student training paradigm. The teacher model (a high-capacity neural network) automatically annotates a set of pseudo-labeled patches used to train the student model (a smaller network). The two models are trained with two different teacher/student approaches: semi-supervised learning and semi-weekly supervised learning. For each of the two approaches, three student training variants are presented. The baseline is provided by training the student model only with the strongly annotated data. Classification performance is evaluated on the student model at the patch level (using the local annotations of the Tissue Micro-Arrays Zurich dataset) and at the global level (using the TCGA-PRAD, The Cancer Genome Atlas-PRostate ADenocarcinoma, whole slide image Gleason score). The teacher/student paradigm allows the models to better generalize on both datasets, despite the inter-dataset heterogeneity and the small number of local annotations used. The classification performance is improved both at the patch-level (up to κ=0.6127±0.0133 from κ=0.5667±0.0285), at the TMA core-level (Gleason score) (up to κ=0.7645±0.0231 from κ=0.7186±0.0306) and at the WSI-level (Gleason score) (up to κ=0.4529±0.0512 from κ=0.2293±0.1350). The results show that with the teacher/student paradigm, it is possible to train models that generalize on datasets from entirely different sources, despite the inter-dataset heterogeneity and the lack of large datasets with local annotations.}
}
@article{WANG2022110523,
title = {The replacement of dysfunctional sensors based on the digital twin method during the cutter suction dredger construction process},
journal = {Measurement},
volume = {189},
pages = {110523},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.110523},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121014020},
author = {Bin Wang and Shidong Fan and Yong Chen and Liangyan Zheng and Hanhua Zhu and Zhenlong Fang and Min Zhang},
keywords = {Cutter suction dredger, Sensor failure, Digital twin, Regression prediction, Stacking generalization},
abstract = {In practice, the construction environment of the cutter suction dredger (CSD) is inclement, which results in a high frequency of the key parameter sensor failure. If the key parameter data is missing or false, it will affect the continuity of the construction. This paper proposed a sensor network regression prediction method based on the “Digital Twin” to establish a correlated model between the key sensor and other highly reliable sensors in the CSD construction. The stacking model is trained by learning the CSD construction data that can synchronously calculate any key parameters when the dredger is running. The proposed method is validated on the “Changshi 12” CSD construction case. The results indicate that the method has high prediction accuracy and computes less expensively. Thus, the proposed method could better solve the problem of construction discontinuity caused by key sensor failure.}
}
@article{KAMARI2022104091,
title = {AI-based risk assessment for construction site disaster preparedness through deep learning-based digital twinning},
journal = {Automation in Construction},
volume = {134},
pages = {104091},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104091},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005422},
author = {Mirsalar Kamari and Youngjib Ham},
keywords = {Visual sensing and analytics, Construction site disaster preparedness, Scene understanding, Artificial intelligence},
abstract = {Hurricanes are among the most devastating natural disasters in the United States, causing billions of dollars of property damage and insured losses. During extreme wind events, unsecured objects in jobsites can easily become airborne debris, which results in substantial loss to construction projects and neighboring communities. Towards a systematic disaster preparedness in construction jobsites, this paper presents a novel vision-based digital twinning and threat assessment framework. We encode the context of disaster risk into deep-learning architectures to identify and analyze the characteristics and impacts of potential wind-borne debris in construction site digital twin models. Case studies on nine piles of construction materials are presented to demonstrate and discuss the fidelity of the proposed computational modules. The proposed methods are expected to help provide heads up for practitioners to quickly recognize, localize, and assess potential wind-borne derbies in construction jobsites, and thereby implementing hurricane preparedness in an effective and timely manner.}
}
@article{GUNASEGARAM2021102089,
title = {Towards developing multiscale-multiphysics models and their surrogates for digital twins of metal additive manufacturing},
journal = {Additive Manufacturing},
volume = {46},
pages = {102089},
year = {2021},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.102089},
url = {https://www.sciencedirect.com/science/article/pii/S2214860421002542},
author = {D.R. Gunasegaram and A.B. Murphy and A. Barnard and T. DebRoy and M.J. Matthews and L. Ladani and D. Gu},
keywords = {Additive manufacturing, Artificial intelligence, Digital twins, Machine learning, Multiscale modeling, Multiphysics modeling, Industry 4.0},
abstract = {Artificial intelligence (AI) embedded within digital models of manufacturing processes can be used to improve process productivity and product quality significantly. The application of such advanced capabilities particularly to highly digitalized processes such as metal additive manufacturing (AM) is likely to make those processes commercially more attractive. AI capabilities will reside within Digital Twins (DTs) which are living virtual replicas of the physical processes. DTs will be empowered to operate autonomously in a diagnostic control capacity to supervise processes and can be interrogated by the practitioner to inform the optimal processing route for any given product. The utility of the information gained from the DTs would depend on the quality of the digital models and, more importantly, their faster-solving surrogates which dwell within DTs for consultation during rapid decision-making. In this article, we point out the exceptional value of DTs in AM and focus on the need to create high-fidelity multiscale-multiphysics models for AM processes to feed the AI capabilities. We identify technical hurdles for their development, including those arising from the multiscale and multiphysics characteristics of the models, the difficulties in linking models of the subprocesses across scales and physics, and the scarcity of experimental data. We discuss the need for creating surrogate models using machine learning approaches for real-time problem-solving. We further identify non-technical barriers, such as the need for standardization and difficulties in collaborating across different types of institutions. We offer potential solutions for all these challenges, after reflecting on and researching discussions held at an international symposium on the subject in 2019. We argue that a collaborative approach can not only help accelerate their development compared with disparate efforts, but also enhance the quality of the models by allowing modular development and linkages that account for interactions between the various sub-processes in AM. A high-level roadmap is suggested for starting such a collaboration.}
}
@article{ATTENBOROUGH20211,
title = {Clinical assessment during a global pandemic – Transitioning to a COVID safe hybrid OSCE},
journal = {International Journal of Osteopathic Medicine},
volume = {42},
pages = {1-4},
year = {2021},
issn = {1746-0689},
doi = {https://doi.org/10.1016/j.ijosm.2021.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1746068921000808},
author = {Paul Attenborough and Jacquelyn Towns and Azharuddin Fazalbhoy and Kylie Fitzgerald},
keywords = {COVID-19, Educational measurement, Educational technology, Learning methods, Osteopathic medicine},
abstract = {Objective structured clinical examinations (OSCEs) are often used to assess the clinical competence of students in preprofessional osteopathy training. During the COVID-19 global pandemic, the final year OSCE in the RMIT University osteopathy program was redeveloped leveraging online technologies within COVID-19 guidelines such as hygiene and occupancy limitations. Final year osteopathy students were assessed using a hybrid ten station OSCE, comprising both online and face-to-face components. The examination was led by a pre-recorded narrated PowerPoint video. The video contained instructions, case information for five cases and prompts for the practical stations. A student model stepped into the room as needed for practical stations. The examiner assessed students from another room via video streaming, with limited interaction with students. The hybrid OSCE was conducted safely during Stage 4 restrictions adhering to COVID Safe guidelines, allowing robust competency assessment of final year students, enabling timely graduation and transition to practice. Institutional support, technology infrastructure, clear communication and stakeholder collaboration are key to successful implementation. The hybrid OSCE format offers a potential solution for institutions delivering high-stakes assessment in the continuing challenges of clinical assessment in the post COVID landscape. Adopting hybrid assessment formats may facilitate remote assessment of students in clinical placements.}
}
@article{BANG2021743,
title = {Distilling from professors: Enhancing the knowledge distillation of teachers},
journal = {Information Sciences},
volume = {576},
pages = {743-755},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521008203},
author = {Duhyeon Bang and Jongwuk Lee and Hyunjung Shim},
keywords = {Knowledge distillation, Professor model, Conditional adversarial autoencoder},
abstract = {Knowledge distillation (KD) is a successful technique for transferring knowledge from one machine learning model to another model. Specifically, the idea of KD has been widely used for various tasks such as model compression and knowledge transfer between different models. However, existing studies in KD have overlooked the possibility that dark knowledge (i.e., soft targets) obtained from a complex and large model (a.k.a., a teacher model) may be either incorrect or insufficient. Such knowledge can hinder the effective learning of another small model (a.k.a., a student model). In this paper, we propose the professor model, which refines the soft target from the teacher model to improve KD. The professor model aims to achieve two goals; 1) improving the prediction accuracy and 2) capturing the inter-class correlation of the soft target from the teacher model. We first design the professor model by reformulating a conditional adversarial autoencoder (CAAE). Then, we devise two KD strategies using both teacher and professor models. Our empirical study demonstrates that the professor model effectively improves KD in three benchmark datasets: CIFAR100, TinyImagenet, and ILSVRC2015. Moreover, our comprehensive analysis shows that the professor model is much more effective than employing the stronger teacher model, in which parameters are greater than the sum of the teacher’s and professor’s parameters. Since the proposed model is model-agnostic, our model can be combined with any KD algorithm and consistently improves various KD techniques.}
}
@article{MOHAMMED2021106912,
title = {FASTory digital twin data},
journal = {Data in Brief},
volume = {35},
pages = {106912},
year = {2021},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2021.106912},
url = {https://www.sciencedirect.com/science/article/pii/S2352340921001967},
author = {Wael M. Mohammed and Jose L. {Martinez Lastra}},
keywords = {Digital twin, Data engineering, Linked data, Discrete manufacturing process, Assembly process},
abstract = {The vast adoption of machine learning techniques in developing smart solutions increases the need of training and testing data. This data can be either collected from physical systems or created using simulation tools. In this regard, this paper presents a set of data collected using a digital twin known as the FASTory Simulator. The data contains more than 100 K events which are collected during a simulated assembly process. The FASTory simulator is a replica of a real assembly line with web-based industrial controllers. The data have been collected using specific-developed orchestrator. During the simulated process, the orchestrator was able to record all the events that occurred in the system. The provided data contains raw JavaScript Object Notation (JSON) formatted data and filtered Comma Separated Values (CSV) formatted data. This data can be exploited in machine learning for modelling the behaviour of the production systems or as testing data for optimization solution for the production system. Finally, this data has been utilized in a research for comparing different data analysis approaches including Knowledge-based systems and data-based systems.}
}
@article{QUILODRANCASAS202211,
title = {Digital twins based on bidirectional LSTM and GAN for modelling the COVID-19 pandemic},
journal = {Neurocomputing},
volume = {470},
pages = {11-28},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.10.043},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221015290},
author = {César Quilodrán-Casas and Vinicius L.S. Silva and Rossella Arcucci and Claire E. Heaney and YiKe Guo and Christopher C. Pain},
keywords = {Reduced order models, Digital twins, Deep learning, Long short-term memory networks, Generative adversarial networks},
abstract = {The outbreak of the coronavirus disease 2019 (COVID-19) has now spread throughout the globe infecting over 150 million people and causing the death of over 3.2 million people. Thus, there is an urgent need to study the dynamics of epidemiological models to gain a better understanding of how such diseases spread. While epidemiological models can be computationally expensive, recent advances in machine learning techniques have given rise to neural networks with the ability to learn and predict complex dynamics at reduced computational costs. Here we introduce two digital twins of a SEIRS model applied to an idealised town. The SEIRS model has been modified to take account of spatial variation and, where possible, the model parameters are based on official virus spreading data from the UK. We compare predictions from one digital twin based on a data-corrected Bidirectional Long Short-Term Memory network with predictions from another digital twin based on a predictive Generative Adversarial Network. The predictions given by these two frameworks are accurate when compared to the original SEIRS model data. Additionally, these frameworks are data-agnostic and could be applied to towns, idealised or real, in the UK or in other countries. Also, more compartments could be included in the SEIRS model, in order to study more realistic epidemiological behaviour.}
}
@article{WEN202125,
title = {Preparing lessons: Improve knowledge distillation with better supervision},
journal = {Neurocomputing},
volume = {454},
pages = {25-33},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.102},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221006603},
author = {Tiancheng Wen and Shenqi Lai and Xueming Qian},
keywords = {Knowledge distillation, Label regularization, Hard example mining},
abstract = {Knowledge distillation (KD) is widely applied in the training of efficient neural network. A compact model, which is trained to mimic the representation of a cumbersome model for the same task, generally obtains a better performance compared with being trained with the ground truth label. Previous KD-based works mainly focus on two aspects: (1) designing various feature representation for knowledge transfer; (2) introducing different training mechanism such as progressive learning or adversarial learning. In this paper, we revisit the standard KD and observe that training with teacher’s logits might suffer from incorrect and uncertain supervision. To tackle these problems, we propose two novel approaches to deal with incorrect logits and uncertain logits respectively, which are called Logits Adjustment (LA) and Dynamic Temperature Distillation (DTD). To be specific, LA rectifies the incorrect logits according to ground truth label and certain rules. While DTD treats the temperature of KD as a dynamic sample wise parameter rather than a static and global hyper-parameter, which actually notes the uncertainty for each sample’s logits. With iteratively updating the sample wise temperature, the student model could pay more attention on the samples that confuse the teacher model. Experiments on CIFAR-10/100, CINIC-10 and Tiny ImageNet verify that the proposed methods yield encouraging improvement compared with the standard KD. Furthermore, considering the simple implementations, LA and DTD can be easily attached to many KD-based frameworks and bring improvements without extra cost of training time and computing resources.}
}
@article{GARG2021103173,
title = {Machine learning based digital twin for stochastic nonlinear multi-degree of freedom dynamical system},
journal = {Probabilistic Engineering Mechanics},
volume = {66},
pages = {103173},
year = {2021},
issn = {0266-8920},
doi = {https://doi.org/10.1016/j.probengmech.2021.103173},
url = {https://www.sciencedirect.com/science/article/pii/S0266892021000576},
author = {Shailesh Garg and Ankush Gogoi and Souvik Chakraborty and Budhaditya Hazra},
keywords = {Digital twin, Bayesian filters, Gaussian process, Stochastic, Non-linear MDOF systems},
abstract = {The potential of digital twin technology is immense, specifically in the infrastructure, aerospace, and automotive sector. However, practical implementation of this technology is not at an expected speed, specifically because of lack of application-specific details. In this paper, we propose a novel digital twin framework for stochastic nonlinear multi-degree of freedom (MDOF) dynamical systems. The proposed digital twin has four modules — (a) a physics-based nominal model, (b) a data collection module, (c) algorithm for real-time update of the digital twin and (d) module for predicting future state. The modules for real-time update and prediction are based on the so-called gray-box modeling approach, and utilizes both physics based and data driven frameworks; this enables the proposed digital twin to generalize and predict future responses. The gray box modeling framework used within the digital twin is developed by coupling Bayesian filtering and machine learning algorithm. Although, the proposed digital twin can be used with any machine learning regression algorithm, we have used Gaussian process in this study. Performance of the proposed approach is illustrated using two examples. Results obtained indicate the applicability and excellent performance of the proposed digital twin framework.}
}
@article{TONG2021113871,
title = {Experiment analysis and computational optimization of the Atkinson cycle gasoline engine through NSGA Ⅱ algorithm using machine learning},
journal = {Energy Conversion and Management},
volume = {238},
pages = {113871},
year = {2021},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2021.113871},
url = {https://www.sciencedirect.com/science/article/pii/S0196890421000480},
author = {Ji Tong and Yangyang Li and Jingping Liu and Ran Cheng and Jinhuan Guan and Shuqian Wang and Shujing Liu and Song Hu and Tao Guo},
keywords = {Atkinson cycle gasoline engine, NSGA Ⅱ algorithm, Support vector machine, Machine learning, Digital twins},
abstract = {This paper is pioneered in developing digital twins by GT-Power software and multi-objective evolutionary optimization (MOEO) using NSGA II algorithm for an Atkinson cycle gasoline engine under China VI emissions standards. Firstly, an experimental investigation is conducted and relevant experimental data is obtained. Based on this, the corresponding 1D GT-Power simulation model is established and calibrated according to the obtained test data. Secondly, the four decision variables including the spark advance angle (SA), exhaust gas recirculation (EGR) rate, exhaust variable valve timing (VVT-E) and intake variable valve timing (VVT-I) are input into the simulation model, the optimal values of the decision variables will be determined via MOEO to minimize NOx emissions and the brake specific fuel consumption (BSFC). Thirdly, based on the data obtained from the scanning test, a machine learning method is used to build an engine performance prediction model through the support vector machine (SVM) regression algorithm. The inputs (control parameters obtained from the optimization process) including SA, EGR, VVT-I and VVT-E are imported to predict the performance output of the engine. The results show that under the engine control parameters obtained by the NSGA II algorithm, the simulation values of engine performance parameters have been greatly optimized, the decreasing extent of fuel consumption is about 5.0%, besides, the decreasing extent of NOx is about 70%. What is more, the increased EER and EEE is up to 6.21% and 2.26%, respectively. And then most of the predicted values obtained by machine learning have been optimized. For BSFC, in general, the simulation value and the predicted value are in good agreement at the smaller value, indicating that the simulation model and the regression prediction model basically achieve the same value at the lower BSFC of the engine. For NOx, the simulated and predicted values have all been optimized. Furthermore, the method and platform developed in this paper will help to carry out a series of related work in the field of vehicle energy flow distribution and optimization when changing different control strategies and optimization methods in the future. Besides, the above work provides a reliable theoretical basis and digital model support for the development of energy-saving and efficient Atkinson cycle engines, which further drives the application of Atkinson cycle engines in new energy vehicles.}
}
@article{SOLOMON2021101596,
title = {Analyzing movement predictability using human attributes and behavioral patterns},
journal = {Computers, Environment and Urban Systems},
volume = {87},
pages = {101596},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101596},
url = {https://www.sciencedirect.com/science/article/pii/S019897152100003X},
author = {Adir Solomon and Amit Livne and Gilad Katz and Bracha Shapira and Lior Rokach},
keywords = {Location prediction, User modeling, Spatial information, Deep learning},
abstract = {The ability to predict human mobility, i.e., transitions between a user's significant locations (the home, workplace, etc.) can be helpful in a wide range of applications, including targeted advertising, personalized mobile services, and transportation planning. Most studies on human mobility prediction have focused on the algorithmic perspective rather than on investigating human predictability. Human predictability has great significance, because it enables the creation of more robust mobility prediction models and the assignment of more accurate confidence scores to location predictions. In this study, we propose a novel method for detecting a user's stay points from millions of GPS samples. Then, after detecting these stay points, a long short-term memory (LSTM) neural network is used to predict future stay points. We explore the use of two types of stay point prediction models (a general model that is trained in advance and a personal model that is trained over time) and analyze the number of previous locations needed for accurate prediction. Our evaluation on two real-world datasets shows that by using our preprocessing approach, we can detect stay points from routine trajectories with higher accuracy than the methods commonly used in this domain, and that by utilizing various LSTM architectures instead of the traditional Markov models and advanced deep learning models, our method can predict human movement with high accuracy of more than 40% when using the Acc@1 measure and more than 59% when using the Acc@3 measure. We also demonstrate that the movement prediction accuracy varies for different user populations based on their trajectory characteristics and demographic attributes.}
}
@article{XIE2022108550,
title = {KD-CLDNN: Lightweight automatic recognition model based on bird vocalization},
journal = {Applied Acoustics},
volume = {188},
pages = {108550},
year = {2022},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2021.108550},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X21006447},
author = {Jiangjian Xie and Sibo Zhao and Xingguang Li and Dongming Ni and Junguo Zhang},
keywords = {Bird vocalization, Lightweight, CLDNN, Recognition, Knowledge distillation},
abstract = {Passive acoustic monitoring (PAM) equipment embedded automatic bird recognition is conducive to the real-time monitoring of birds. In this paper, the lightweight bird recognition model KD-CLDNN is proposed to adapt to the monitoring equipment with limited computing power. The KD-CLDNN model is obtained by knowledge distillation. The teacher model CS-CLDNN (CBAM-Switch-CLDNN) is built by introducing Convolutional Block Attention Module (CBAM) and Swish activation function to CLDNN model. Furthermore, the student model Net-S is constructed by simplifying the CS-CLDNN model. The recordings of 20 bird species are used as the dataset in this study, and the performances of six complex models are evaluated with this dataset. The results show that the CS-CLDNN model outperforms other complex models. Compared with the CS-CLDNN model, the performance of the KD-CLDNN model in accuracy, recall, precision, and F1 score decrease slightly by 0.028, 0.025, 0.01, and 0.023, respectively. However, the training GPU time of KD-CLDNN is reduced about seven times, and the total number of parameters is reduced about three times. On the edge computing platform NVIDIA Jetson TX2, the inference speed of KD-CLDNN is 3.2 times faster than that of CS-CLDNN. Therefore, the lightweight KD-CLDNN model can significantly reduce calculation requirement on the premise of ensuring recognition accuracy, which can be helpful for the development of intelligent bird monitoring equipment based on deep learning.}
}
@article{LEE2021126681,
title = {A stacking ensemble model for hydrological post-processing to improve streamflow forecasts at medium-range timescales over South Korea},
journal = {Journal of Hydrology},
volume = {600},
pages = {126681},
year = {2021},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2021.126681},
url = {https://www.sciencedirect.com/science/article/pii/S0022169421007290},
author = {Dong-Gi Lee and Kuk-Hyun Ahn},
keywords = {Stacking generalization, Hydrological forecast, South Korea, Medium-range forecast, Hydrological post-processing},
abstract = {This study presents the potential of hydrological ensemble forecasts over South Korea for medium-range forecast lead times (1–7 days). To generate hydrological forecasts, this study utilizes a framework based on stacking ensemble learning, an emerging machine learning technique that includes a two-level structure: base-learner and meta-learner models. In particular, the present research contributes to hydrological post-processing techniques by: (1) introducing a penalized quantile regression-based meta-learner to generate probabilistic predictions, (2) considering modeled climate predictions and antecedent hydrologic conditions simultaneously for regional hydrological forecast development, and (3) quantifying the skill enhancements from the multi-model forecasts under the stacking generalization. The proposed model is evaluated in massive 473 grid cells along with nine additional simpler models to test the specific hypotheses introduced in this study. Results indicate that our proposed forecasts can be used for relatively short lead times. In addition, results demonstrate that utilizing a penalized probabilistic meta-learner and antecedent conditions contributes to the forecast skill improvements. Lastly, we find that base-model diversity outperforms increased ensemble size alone in enhancing the forecast abilities under the stacking ensemble generalization. We conclude this paper with a discussion of possible forecast model improvements from an adaptation of additional information from input and model structures under the stacking generalization.}
}
@article{PAN2022117271,
title = {Data-centric Engineering: integrating simulation, machine learning and statistics. Challenges and opportunities},
journal = {Chemical Engineering Science},
volume = {249},
pages = {117271},
year = {2022},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2021.117271},
url = {https://www.sciencedirect.com/science/article/pii/S0009250921008368},
author = {Indranil Pan and Lachlan R. Mason and Omar K. Matar},
keywords = {Digital twins, Artificial Intelligence, CFD, FEM, Data-centric Engineering, SimOps},
abstract = {Recent advances in machine learning, coupled with low-cost computation, availability of cheap streaming sensors, data storage and cloud technologies, has led to widespread multi-disciplinary research activity with significant interest and investment from commercial stakeholders. Mechanistic models, based on physical equations, and purely data-driven statistical approaches represent two ends of the modelling spectrum. New hybrid, data-centric engineering approaches, leveraging the best of both worlds and integrating both simulations and data, are emerging as a powerful tool with a transformative impact on the physical disciplines. We review the key research trends and application scenarios in the emerging field of integrating simulations, machine learning, and statistics. We highlight the opportunities that such an integrated vision can unlock and outline the key challenges holding back its realisation. We also discuss the bottlenecks in the translational aspects of the field and the long-term upskilling requirements for the existing workforce and future university graduates.}
}
@article{MOWBRAY2021108054,
title = {Machine learning for biochemical engineering: A review},
journal = {Biochemical Engineering Journal},
volume = {172},
pages = {108054},
year = {2021},
issn = {1369-703X},
doi = {https://doi.org/10.1016/j.bej.2021.108054},
url = {https://www.sciencedirect.com/science/article/pii/S1369703X21001303},
author = {Max Mowbray and Thomas Savage and Chufan Wu and Ziqi Song and Bovinille Anye Cho and Ehecatl A. {Del Rio-Chanona} and Dongda Zhang},
keywords = {Machine learning, Data-driven modelling, Biochemical engineering, Industrial biotechnology, Digitalisation, Digital twin},
abstract = {The field of machine learning is comprised of techniques, which have proven powerful approaches to knowledge discovery and construction of ‘digital twins’ in the highly dimensional, nonlinear and stochastic domains common to biochemical engineering. We review the use of machine learning within biochemical engineering over the last 20 years. The most prevalent machine learning methods are demystified, and their impact across individual biochemical engineering subfields is outlined. In doing so we provide insights into the true benefits of each technique, and obstacles for their wider deployment. Finally, core challenges into the application of machine learning in biochemical engineering are thoroughly discussed, and further insight into adoption of innovative hybrid modelling and transfer learning strategies for development of new digital biotechnologies is provided.}
}
@article{LEE2021108443,
title = {Development of the Machine Learning-based Safety Significant Factor Inference Model for Diagnosis in Autonomous Control System},
journal = {Annals of Nuclear Energy},
volume = {162},
pages = {108443},
year = {2021},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2021.108443},
url = {https://www.sciencedirect.com/science/article/pii/S0306454921003194},
author = {Joomyung Lee and Linyu Lin and Paridhi Athe and Nam Dinh},
keywords = {Diagnosis, Digital twin, Recurrent Neural Network, Safety significant factor, Machine Learning},
abstract = {As a critical component to the autonomous control system, Digital Twin for Diagnosis (DT-D) is a virtual replica of physical systems for an accurate understanding of reactor states. Since the physical damage state cannot be measured directly in transient or accident conditions, safety significant factor (SSF) is introduced as a surrogate index for physical damage states to support safety-related decision making. This study develops a machine learning (ML) based SSF inference model (SSFIM) using the Recurrent Neural Network (RNN) with acceptable accuracy, generalization capability, effectiveness, and robustness against sensor errors. To demonstrate the capability of the ML-based SSFIM, case studies are implemented on a plant simulator for Experimental Breeder Reactor – II. For partial loss of flow accident scenarios, the SSFIM is able to infer the peak fuel centerline temperature with minimally one sensor. Meanwhile the SSFIM is also found to be robust against manipulated sensor drifts and/or random noises.}
}
@article{CHEN202231,
title = {Artificial intelligence enabled Digital Twins for training autonomous cars},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {2},
pages = {31-41},
year = {2022},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2022.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667345222000116},
author = {Dongliang Chen and Zhihan Lv},
keywords = {Deep learning, Autonomous cars, Digital twins, Information security, Artificial intelligence},
abstract = {This exploration is aimed at the system prediction and safety performance of the Digital Twins (DTs) of autonomous cars based on artificial intelligence technology, and the intelligent development of transportation in the smart city. On the one hand, considering the problem of safe driving of autonomous cars in intelligent transportation systems, it is essential to ensure the transmission safety of vehicle data and realize the load balancing scheduling of data transmission resources. On the other hand, convolution neural network (CNN) of the deep learning algorithm is adopted and improved, and then, the DTs technology is introduced. Finally, an autonomous cars DTs prediction model based on network load balancing and spatial-temporal graph convolution network is constructed. Moreover, through simulation, the performance of this model is analyzed from perspectives of Accuracy, Precision, Recall, and F1-score. The experimental results demonstrate that in comparative analysis, the accuracy of road network prediction of the model reported here is 92.70%, which is at least 2.92% higher than that of the models proposed by other scholars. Through the analysis of the security performance of network data transmission, it is found that this model achieves a lower average delay time than other comparative models. Besides, the message delivery rate is basically stable at 80%, and the message leakage rate is basically stable at about 10%. Therefore, the prediction model for autonomous cars constructed here not only ensures low delay but also has excellent network security performance, so that information can interact more efficiently. The research outcome can provide an experimental basis for intelligent development and safety performance improvement in the transportation field of smart cities.}
}
@article{XU2022242,
title = {Contrastive adversarial knowledge distillation for deep model compression in time-series regression tasks},
journal = {Neurocomputing},
volume = {485},
pages = {242-251},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.139},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016374},
author = {Qing Xu and Zhenghua Chen and Mohamed Ragab and Chao Wang and Min Wu and Xiaoli Li},
keywords = {Knowledge distillation, Contrastive learning, Adversarial learning, Time series regression},
abstract = {Knowledge distillation (KD) attempts to compress a deep teacher model into a shallow student model by letting the student mimic the teacher’s outputs. However, conventional KD approaches can have the following shortcomings. First, existing KD approaches align the global distribution between teacher and student models and overlook the fine-grained features. Second, most of existing approaches focus on classification tasks and require the architecture of teacher and student models to be similar. To address these limitations, we propose a contrastive adversarial knowledge distillation called CAKD for time series regression tasks where the student and teacher are using different architectures. Specifically, we first propose adversarial adaptation to automatically align the feature distribution between student and teacher networks respectively. Yet, adversarial adaptation can only align the global feature distribution without considering the fine-grained features. To mitigate this issue, we employ a novel contrastive loss for instance-wise alignment between the student and teacher. Particularly, we maximize similarity between teacher and student features that originate from the same sample. Lastly, a KD loss is used to for the knowledge distillation where the teacher and student have two different architectures. We used a turbofan engine dataset that consists of four sub-datasets to evaluate the model performance. The results show that the proposed CAKD method consistently outperforms state-of-the-art methods in terms of two different metrics.}
}
@article{BHARDWAJ2021172,
title = {Empowering Knowledge Distillation via Open Set Recognition for Robust 3D Point Cloud Classification},
journal = {Pattern Recognition Letters},
volume = {151},
pages = {172-179},
year = {2021},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2021.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167865521002701},
author = {Ayush Bhardwaj and Sakshee Pimpale and Saurabh Kumar and Biplab Banerjee},
keywords = {Knowledge Distillation, Open Set Recognition, 3D Object Recognition, Point Cloud Classification},
abstract = {Real-world scenarios pose several challenges to deep learning based computer vision techniques despite their tremendous success in research. Deeper models provide better performance, but are challenging to deploy and knowledge distillation allows us to train smaller models with minimal loss in performance. A model also has to deal with open set samples from classes outside the ones it was trained on and should be able to identify them as unknown samples while classifying the known ones correctly. Finally, most existing image recognition research focuses only on using two-dimensional snapshots of the three-dimensional real world objects. In this work, we attempt to bridge these three research fields, which have been developed independently until now, despite being deeply interrelated in practice. We propose a joint knowledge distillation and open set recognition training methodology for three-dimensional object recognition. We demonstrate the effectiveness of the proposed method via various experiments on how it allows us to obtain a much smaller model, which takes a minimal hit in performance while being capable of open set recognition for 3D point cloud data.}
}
@article{SINGH2021105407,
title = {Highway 4.0: Digitalization of highways for vulnerable road safety development with intelligent IoT sensors and machine learning},
journal = {Safety Science},
volume = {143},
pages = {105407},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105407},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521002514},
author = {Rajesh Singh and Rohit Sharma and Shaik {Vaseem Akram} and Anita Gehlot and Dharam Buddhi and Praveen Kumar Malik and Rajeev Arya},
keywords = {Highway, DL, Vulnerable Road Safety, Smart city, IoT, Renewable energy, And vision node},
abstract = {According to United Nations (UN) 2030 agenda, the transportation system needs to be enhanced for the establishment of access to safe, affordable, accessible, and sustainable transport systems along with enhanced road safety. The highway road transport system is one of the transport systems that enables to transits goods and humans from one location to another location. The agenda of UN 2030 for the transport system will be accomplished with the assistance of digital technologies like the internet of things (IoT) and artificial intelligence (AI). The implementation of these digital technologies on highways empowers to provide reliable, smarter, intelligent, and renewable energy sources experience to the users travelling along the highways. This study discusses the significance of the digitalization of highways that supporting and realizing a sustainable environment on the highways. To discuss the significance of digitalization, the study has categorized digitalization into five subcomponents namely smart highway lighting system, smart traffic and emergency management system, renewable energy sources on highways, smart display and AI in highways. An architecture-for smart highway lighting, smart traffic, and emergency management are proposed and discussed in the study. The significance of implementing smart display boards and renewable sources with real-time applications is also addressed in this study. Moreover, the integration of AI in highways is addressed with the perspective of enhancing road safety. The integration of deep learning (DL) in the edge-based vision node for predicting the patterns of traffic flow, highway road safety, and maintenance of quality roads have been addressed in the discussion section. Embedding the deep learning techniques in the vison node at the traffic junction and the highway lighting controller is able to deliver an intelligent system that provides sustained experience and management of the highways. Smart reflectors, adoption of renewable energy, developing vehicle-to-vehicle communication in vehicles, and smart lamppost are the few recommendations for the implementation of digitalizing highways.}
}
@article{LIN2021108362,
title = {Uncertainty quantification and software risk analysis for digital twins in the nearly autonomous management and control systems: A review},
journal = {Annals of Nuclear Energy},
volume = {160},
pages = {108362},
year = {2021},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2021.108362},
url = {https://www.sciencedirect.com/science/article/pii/S0306454921002383},
author = {Linyu Lin and Han Bao and Nam Dinh},
keywords = {Digital twin, Autonomous control, Uncertainty quantification, Software risk analysis},
abstract = {A nearly autonomous management and control (NAMAC) system is designed to furnish recommendations to operators for achieving particular goals based on NAMAC’s knowledge base. As a critical component in a NAMAC system, digital twins (DTs) are used to extract information from the knowledge base to support decision-making in reactor control and management during all modes of plant operations. With the advancement of artificial intelligence and data-driven methods, machine learning algorithms are used to build DTs of various functions in the NAMAC system. To evaluate the uncertainty of DTs and its impacts on the reactor digital instrumentation and control systems, uncertainty quantification (UQ) and software risk analysis is needed. As a comprehensive overview of prior research and a starting point for new investigations, this study selects and reviews relevant UQ techniques and software hazard and software risk analysis methods that may be suitable for DTs in the NAMAC system.}
}
@article{KLYMKOWSKY2021308,
title = {Making mechanistic sense: are we teaching students what they need to know?},
journal = {Developmental Biology},
volume = {476},
pages = {308-313},
year = {2021},
issn = {0012-1606},
doi = {https://doi.org/10.1016/j.ydbio.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0012160621000993},
author = {Michael W. Klymkowsky},
abstract = {Evaluating learning outcomes depends upon objective and actionable measures of what students know – that is, what can they do with what they have learned. In the context of a developmental biology course, a capstone of many molecular biology degree programs, I asked students to predict the behaviors of temporal and spatial signaling gradients. Their responses led me to consider an alternative to conventional assessments, namely a process in which students are asked to build and apply plausible explanatory mechanistic models (“PEMMs”). A salient point is not whether students' models are correct, but whether they “work” in a manner consistent with underlying scientific principles. Analyzing such models can reveal the extent to which students recognize and accurately apply relevant ideas. An emphasis on model building, analysis and revision, an authentic scientific practice, can be expected to have transformative effects on course and curricular design as well as on student engagement and learning outcomes.}
}
@article{WANG2022116036,
title = {Attention-based dynamic user modeling and Deep Collaborative filtering recommendation},
journal = {Expert Systems with Applications},
volume = {188},
pages = {116036},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116036},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421013816},
author = {Ruiqin Wang and Zongda Wu and Jungang Lou and Yunliang Jiang},
keywords = {Short-term preferences, Long-term preferences, Dynamic preference modeling, Matching score prediction, Time-aware attention},
abstract = {Deep learning (DL) techniques have been widely used in recommender systems for user modeling and matching function learning based on historical interaction matrix. However, existing DL-based recommendation methods usually perform static user preference modeling by using historical interacted items of the user. In this article, we present a time-aware deep CF framework which contains two stages: dynamic user preference modeling based on attention mechanism and matching score prediction based on DL. In the first stage, short-term user preferences are modeled by the time-aware attention mechanism that fully considered the predicted item, the recent interacted items and their interaction time. The resulting short-term preferences are combined with long-term preferences for dynamic user preference modeling. In the second stage, high-order user-item feature interactions are learned by two types of DL models, Deep Matrix Factorization (DMF) and Multiple-Layer Perception (MLP), and the feature interaction vectors of the two models are fused in the last layer of the model to predict the matching score. Extensive experiments on five datasets indicate that our method is superior to the existing time-aware and DL-based recommendation methods in top-k recommendations significantly and consistently.}
}
@article{DIPALMA2021102136,
title = {Resolution-based distillation for efficient histology image classification},
journal = {Artificial Intelligence in Medicine},
volume = {119},
pages = {102136},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102136},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721001299},
author = {Joseph DiPalma and Arief A. Suriawinata and Laura J. Tafe and Lorenzo Torresani and Saeed Hassanpour},
keywords = {Deep neural networks, Digital pathology, Knowledge distillation, Self-supervised learning},
abstract = {Developing deep learning models to analyze histology images has been computationally challenging, as the massive size of the images causes excessive strain on all parts of the computing pipeline. This paper proposes a novel deep learning-based methodology for improving the computational efficiency of histology image classification. The proposed approach is robust when used with images that have reduced input resolution, and it can be trained effectively with limited labeled data. Moreover, our approach operates at either the tissue- or slide-level, removing the need for laborious patch-level labeling. Our method uses knowledge distillation to transfer knowledge from a teacher model pre-trained at high resolution to a student model trained on the same images at a considerably lower resolution. Also, to address the lack of large-scale labeled histology image datasets, we perform the knowledge distillation in a self-supervised fashion. We evaluate our approach on three distinct histology image datasets associated with celiac disease, lung adenocarcinoma, and renal cell carcinoma. Our results on these datasets demonstrate that a combination of knowledge distillation and self-supervision allows the student model to approach and, in some cases, surpass the teacher model's classification accuracy while being much more computationally efficient. Additionally, we observe an increase in student classification performance as the size of the unlabeled dataset increases, indicating that there is potential for this method to scale further with additional unlabeled data. Our model outperforms the high-resolution teacher model for celiac disease in accuracy, F1-score, precision, and recall while requiring 4 times fewer computations. For lung adenocarcinoma, our results at 1.25× magnification are within 1.5% of the results for the teacher model at 10× magnification, with a reduction in computational cost by a factor of 64. Our model on renal cell carcinoma at 1.25× magnification performs within 1% of the teacher model at 5× magnification while requiring 16 times fewer computations. Furthermore, our celiac disease outcomes benefit from additional performance scaling with the use of more unlabeled data. In the case of 0.625× magnification, using unlabeled data improves accuracy by 4% over the tissue-level baseline. Therefore, our approach can improve the feasibility of deep learning solutions for digital pathology on standard computational hardware and infrastructures.}
}
@article{DING2021104790,
title = {Control performance monitoring and degradation recovery in automatic control systems: A review, some new results, and future perspectives},
journal = {Control Engineering Practice},
volume = {111},
pages = {104790},
year = {2021},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.104790},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121000678},
author = {Steven X. Ding and Linlin Li},
keywords = {Control performance monitoring, Control performance degradation recovery, Loop performance degradation index},
abstract = {This paper addresses control performance monitoring (CPM) and degradation recovering in automatic control systems. It begins with a re-visit of CPM techniques and a summary of the major limitations of the existing CPM methods. They are (i) deficit in assessing control performance degradation caused by different types of disturbances and environment uncertainties, (ii) incapability for predicting performance degradation, and (iii) deficiency of efficient performance degradation recovering methods. In order to meet increasing demands of next generation automatic control systems for higher system performance, novel CPM methods have been developed in recent years, including performance assessment of control systems with deterministic disturbances and uncertainties, prediction of control performance degradation, and recovery of control performance degradation. Some of these methods and algorithms are introduced in the second part of this paper. The basis of these methods is a so-called residual centred model of feedback control systems, which allows a unified handling of control, monitoring and diagnosis in feedback control systems corrupted by disturbances and uncertainties. The focuses of these methods are on (i) introduction of the loop performance degradation index for the assessment and prediction of performance degradation in automatic control systems, (ii) predictive detection and estimation of loop performance degradation, and (iii) a data-driven performance degradation recovering scheme. The paper is concluded by a short summary of three future perspective topics, (i) prediction of economic system performance monitoring and estimation, (ii) reinforcement learning aided system performance recovery, and (iii) CPM digital twin.}
}
@article{MIRAZ2021100363,
title = {Adaptive user interfaces and universal usability through plasticity of user interface design},
journal = {Computer Science Review},
volume = {40},
pages = {100363},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100363},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000034},
author = {Mahdi H. Miraz and Maaruf Ali and Peter S. Excell},
keywords = {User interface (UI), Dynamic UI Design, Plasticity, Adaptive user interface (AUI), Universal usability, Interaction design, Inclusive Design, Design for All},
abstract = {A review of research on universal usability, plasticity of user interface design and facilitation of interface development with universal usability is presented. The survey was based on 165 research papers spanning over fifty-five years. The foundations of adaptive or intelligent user interfaces (AUI or IUI) are presented, three core domains being focused upon: Artificial Intelligence (AI), User Modelling (UM) and Human–Computer Interaction (HCI). For comparison of the various AUIs, a proposed taxonomy is given. One conclusion is that an efficient training vector for fast optimal convergence of the machine-learning algorithm is a necessity, but key to this is the bounding of the dataset, the goal being to achieve an accurate user preference model, which has to be built from a limited number of datasets obtained from the human interaction. More research also needs to be conducted to ascertain the usefulness and effectiveness of IUIs compared against AUIs. With the global mobility of users, interface design must take account of the abilities and cultures of users, derived from actual user behaviour and not on their feedback. A key question is whether the interface should be adaptive under system control or be made adaptable under user control. A need is identified for an “afferential component” that stores a priori information about the end user, an “inferential component” that determines to what extent the user interface actually needs to be adapted, and the “efferential component” that actually determines how the adaptivity is applied seamlessly to the system. Application to e-learning is a priority: the use of machine intelligence to achieve appropriate learnability, ideally enhanced by “Playful interaction”, was found to be desirable. Universal application of adaptation lies in the future, but AUI properties cannot be ascertained while disregarding the other parameters of the system in which it will be used. A more complete understanding of the human mental model is necessary, requiring a highly multidisciplinary approach and cooperation between diverse researchers. Finally, a performance evaluation of plasticity of user interface was conducted: it is concluded that the use of dynamic techniques can enhance the user experience to a much greater extent than more basic approaches, although optimisation of usability parameter trade-offs needs further attention. It is noted that most of the work reviewed originated from a limited range of cultural perspectives. To make an interface simultaneously usable for users from a diverse range of cultural backgrounds will require a very large amount of adaptation, but the powerful principles of plasticity of user interface design hold the future promise of an optimum tool to achieve cross-cultural usability.}
}
@article{GAWADE2022875,
title = {Leveraging simulated and empirical data-driven insight to supervised-learning for porosity prediction in laser metal deposition},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {875-885},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001503},
author = {Vidita Gawade and Vani Singh and Weihong “Grace” Guo},
keywords = {Additive manufacturing, Machine learning, Porosity prediction},
abstract = {The advent of digital-twin manufacturing in additive manufacturing (AM) is to integrate the physical world of real-time 3D printing with the digital world of a simulated print. This paper contributes to digital-twin manufacturing in laser-based additive manufacturing by combining melt pools’ simulated thermal behavior via finite element analysis (FEA) and melt pools’ empirical thermal behavior via pyrometry-based sensors. Studying the thermal behavior of melt pools based on heat transfer characteristics determines melt pools’ porosity and part quality. FEA uses Godak’s moving heat flux to capture the melt pools’ physically bound temperature profile in three dimensions. Simulated data helps to mitigate the influence of measuring errors from real-world data and provides non-observable data such as gradient changes of thermal behavior at the curvature of the 3D melt pool. The pyrometer captures empirical temperature behavior, including uncertainty and randomness introduced to the process. A significant knowledge gap exists when predicting melt pool porosity accurately with theoretical FEA and empirical in situ evidence alone. The gap is bridged by combining the data sources, specifically, feature engineering via functional principal component analysis (empirical data source) and capturing the melt pool's 3-D temperature shape profile via FEA (simulated data source). A hybrid model predicts melt pool porosity by capturing the strengths of prior simulated and posterior in situ empirical data by matching simulated melt pools to real-world empirical melt pools. Moreover, comparing predicted porosity labels with true porosity labels of Ti–6Al–4V thin-wall structure from laser metal deposition verified the proposed interpretable and robust supervised-learning model's validity. This methodology can apply to other materials and part shapes printed under various additive-manufactured printers.}
}
@article{LI2021107862,
title = {Hierarchical distillation learning for scalable person search},
journal = {Pattern Recognition},
volume = {114},
pages = {107862},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.107862},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321000492},
author = {Wei Li and Shaogang Gong and Xiatian Zhu},
keywords = {Person search, Person re-identification, Person detection, Knowledge distillation, Scalability, Model inference efficiency},
abstract = {Existing person search methods typically focus on improving person detection accuracy. This ignores the model inference efficiency, which however is fundamentally significant for real-world applications. In this work, we address this limitation by investigating the scalability problem of person search involving both model accuracy and inference efficiency simultaneously. Specifically, we formulate a Hierarchical Distillation Learning (HDL) approach. With HDL, we aim to comprehensively distil the knowledge of a strong teacher model with strong learning capability to a lightweight student model with weak learning capability. To facilitate the HDL process, we design a simple and powerful teacher model for joint learning of person detection and person re-identification matching in unconstrained scene images. Extensive experiments show the modelling advantages and cost-effectiveness superiority of HDL over the state-of-the-art person search methods on three large person search benchmarks: CUHK-SYSU, PRW, and DukeMTMC-PS.}
}
@article{LI2022104957,
title = {A flexible manufacturing assembly system with deep reinforcement learning},
journal = {Control Engineering Practice},
volume = {118},
pages = {104957},
year = {2022},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.104957},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121002343},
author = {Junzheng Li and Dong Pang and Yu Zheng and Xinping Guan and Xinyi Le},
keywords = {Reinforcement learning, Digital twin, Flexible manufacture, Assembly line},
abstract = {Traditional assembly line requires a significant amount of designs from engineers, especially in the case of multi-species and small-lot production. Recently, intelligent algorithms based on reinforcement learning are proposed to address this issue. However, the lower success rate and safety reasons limit their industrial applications. In this article, we proposed a systematic solution, including the automatic planning of assembly motions and the monitoring system of the production lines. In the planning stage, we built the digital twin model of the assembly line, then trained a deep reinforcement learning agent to assembly the workpieces. In the production stage, the digital twin model is used to monitor the assembly lines and predict failures. To validate the system we proposed, we conducted a peg-in-hole assembly experiment, and reached a 90% success rate for a single assembly attempt. During the whole experiment, no collision happens in the real world.}
}
@article{MATULIS2021106,
title = {A robot arm digital twin utilising reinforcement learning},
journal = {Computers & Graphics},
volume = {95},
pages = {106-114},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S009784932100011X},
author = {Marius Matulis and Carlo Harvey},
keywords = {Robot arm, Reinforcement learning, Artificial intelligence, Digital twin},
abstract = {For many industry contexts, the implementation of Artificial Intelligence (AI) has contributed to what has become known as the fourth industrial revolution or “Industry 4.0” and creates an opportunity to deliver significant benefit to both businesses and their stakeholders. Robot arms are one of the most common devices utilised in manufacturing and industrial processes, used for a wide variety of automation tasks on, for example, a factory floor but the effective use of these devices requires AI to be appropriately trained. One approach to support AI training of these devices is the use of a “Digital Twin”. There are, however, a number of challenges that exist within this domain, in particular, success depends upon the ability to collect data of what are considered as observations within the environment and the application of one or many trained AI policies to the task that is to be completed. This project presents a case-study of creating and training a Robot Arm Digital Twin as an approach for AI training in a virtual space and applying this simulation learning within physical space. A virtual space, created using Unity (a contemporary Game Engine), incorporating a virtual robot arm was linked to a physical space, being a 3D printed replica of the virtual space and robot arm. These linked environments were applied to solve a task and provide training for an AI model. The contribution of this work is to provide guidance on training protocols for a digital twin together with details of the necessary architecture to support effective simulation in a virtual space through the use of Tensorflow and hyperparameter tuning. It provides an approach to addressing the mapping of learning in the virtual domain to the physical robot twin.}
}
@article{GHOSH2021100242,
title = {Developing sensor signal-based digital twins for intelligent machine tools},
journal = {Journal of Industrial Information Integration},
volume = {24},
pages = {100242},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100242},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000418},
author = {Angkush Kumar Ghosh and AMM Sharif Ullah and Roberto Teti and Akihiko Kubo},
keywords = {Digital twin, Sensor signal, Cyber-physical systems, Machine tool, Monitoring},
abstract = {Digital twins can assist machine tools in performing their monitoring and troubleshooting tasks autonomously from the context of smart manufacturing. For this, a special type of twin denoted as sensor signal-based twin must be constructed and adapted into the cyber-physical systems. The twin must (1) machine-learn the required knowledge from the historical sensor signal datasets, (2) seamlessly interact with the real-time sensor signals, (3) handle the semantically annotated datasets stored in clouds, and (4) accommodate the data transmission delay. The development of such twins has not yet been studied in detail. This study fills this gap by addressing sensor signal-based digital twin development for intelligent machine tools. Two computerized systems denoted as Digital Twin Construction System (DTCS) and Digital Twin Adaptation System (DTAS) are proposed to construct and adapt the twin, respectively. The modular architectures of the proposed DTCS and DTAS are presented in detail. The real-time responses and delay-related computational arrangements are also elucidated for both systems. The systems are also developed using a Java™-based platform. Milling torque signals are used as an example to demonstrate the efficacy of DTCS and DTAS. This study thus contributes toward the advancement of intelligent machine tools from the context of smart manufacturing.}
}
@article{YANG2021100088,
title = {Implementation for a cloud battery management system based on the CHAIN framework},
journal = {Energy and AI},
volume = {5},
pages = {100088},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000422},
author = {Shichun Yang and Zhengjie Zhang and Rui Cao and Mingyue Wang and Hanchao Cheng and Lisheng Zhang and Yinan Jiang and Yonglin Li and Binbin Chen and Heping Ling and Yubo Lian and Billy Wu and Xinhua Liu},
keywords = {Battery, CHAIN, Cloud, Battery management system, SOX estimation, end-edge-cloud architecture},
abstract = {Summary
An intelligent battery management system is a crucial enabler for energy storage systems with high power output, increased safety and long lifetimes. With recent developments in cloud computing and the proliferation of big data, machine learning approaches have begun to deliver invaluable insights, which drives adaptive control of battery management systems (BMS) with improved performance. In this paper, a general framework utilizing an end-edge-cloud architecture for a cloud-based BMS is proposed, with the composition and function of each link described. Cloud-based BMS leverages from the Cyber Hierarchy and Interactional Network (CHAIN) framework to provide multi-scale insights, more advanced and efficient algorithms can be used to realize the state-of-X estimation, thermal management, cell balancing, fault diagnosis and other functions of traditional BMS system. The battery intelligent monitoring and management platform can visually present battery performance, store working-data to help in-depth understanding of the microscopic evolutionary law, and provide support for the development of control strategies. Currently, the cloud-based BMS requires more effects on the multi-scale integrated modeling methods and remote upgrading capability of the controller, these two aspects are very important for the precise management and online upgrade of the system. The utility of this approach is highlighted not only for automotive applications, but for any battery energy storage system, providing a holistic framework for future intelligent and connected battery management.}
}
@article{YOU2022117899,
title = {Digital twins based day-ahead integrated energy system scheduling under load and renewable energy uncertainties},
journal = {Applied Energy},
volume = {305},
pages = {117899},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117899},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921012125},
author = {Minglei You and Qian Wang and Hongjian Sun and Iván Castro and Jing Jiang},
keywords = {Digital twins, Multi-vector energy system, Integrated energy system, Machine learning},
abstract = {By constructing digital twins (DT) of an integrated energy system (IES), one can benefit from DT’s predictive capabilities to improve coordinations among various energy converters, hence enhancing energy efficiency, cost savings and carbon emission reduction. This paper is motivated by the fact that practical IESs suffer from multiple uncertainty sources, and complicated surrounding environment. To address this problem, a novel DT-based day-ahead scheduling method is proposed. The physical IES is modelled as a multi-vector energy system in its virtual space that interacts with the physical IES to manipulate its operations. A deep neural network is trained to make statistical cost-saving scheduling by learning from both historical forecasting errors and day-ahead forecasts. Case studies of IESs show that the proposed DT-based method is able to reduce the operating cost of IES by 63.5%, comparing to the existing forecast-based scheduling methods. It is also found that both electric vehicles and thermal energy storages play proactive roles in the proposed method, highlighting their importance in future energy system integration and decarbonisation.}
}
@article{SHI2022108316,
title = {Explainable scale distillation for hyperspectral image classification},
journal = {Pattern Recognition},
volume = {122},
pages = {108316},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108316},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004969},
author = {Cheng Shi and Li Fang and Zhiyong Lv and Minghua Zhao},
keywords = {Hyperspectral image classification, Knowledge distillation, Scale distillation, Explainable scale network},
abstract = {The land-covers within an observed remote sensing scene are usually of different scales; therefore, the ensemble of multi-scale information is a commonly used strategy to achieve more accurate scene interpretation; however, this process suffers from being time-consuming. In terms of this issue, this paper proposes a scale distillation network to explore the possibility that single-scale classification network can achieve the same (or even better) classification performance compared with multi-scale one. The proposed scale distillation network consists of a cumbersome multi-scale teacher network and a lightweight single-scale student network. The former is trained for multi-scale information learning, and the latter improves the classification accuracy by accepting the knowledge from the multi-scale teacher network and its true label. The experimental results show the advantages of scale distillation on hyperspectral image classification. The single-scale student network can even achieve higher evaluation accuracy than the multi-scale teacher network. In addition, a faithful explainable scale network is designed to visually explain the trained scale distillation network. The traditional deep neural network is a black-box and lacks interpretability. The explanation of the trained network can explore more hidden information from the predictions. We visually explain the prediction results of scale distillation network, and the results show that the explainable scale network can more precisely analyze the relationship between the learned scale features and the land-cover categories. Moreover, the possible application of the explainable scale network on classification is further discussed in this study.}
}
@article{BERMEOAYERBE2022121691,
title = {Data-driven energy prediction modeling for both energy efficiency and maintenance in smart manufacturing systems},
journal = {Energy},
volume = {238},
pages = {121691},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121691},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221019393},
author = {Miguel Angel Bermeo-Ayerbe and Carlos Ocampo-Martinez and Javier Diaz-Rozo},
keywords = {Non-intrusive load monitoring, Data-driven model, Subspace identification, Energy models, Concept drift, Digital twin, Gaussian mixture models, Energy efficiency, Machine fault diagnosis},
abstract = {The optimization and monitoring of the energy consumption of machinery lead to a sustainable and efficient industry. For this reason and following a digital twin strategy, an online data-driven energy modeling approach with adaptive capabilities has been proposed and described throughout this paper. This approach is useful in developing robust energy management systems that enhance the energy efficiency of industrial machinery. In this way, the dynamic behavior of their energy consumption is modeled without using phenomenological laws. In contrast, traditional methodologies hardly consider such dynamic behavior or use an exhaustive modeling process. The proposed approach includes an adaptive mechanism to consider the natural degradation of machinery. This mechanism is based on a concept drift detector, which detects when the current consumption of the machine is not correctly represented by the model estimation and adapts the model to account for these new behaviors. The concept drift detector has broad applicability in the face of reducing maintenance costs, measuring the impact and evolution of either abnormal behaviors (e.g., failures) or degradation, and identify which elements change. The proposed methodology has been validated in an industrial testbed. An experiment with three emulated concept drifts was carried out in the testbed. As a result, the proposed adaptive approach obtained more than doubled the fit rate of the energy prediction/estimation compared to the non-adaptive model and successfully detected these changes in energy consumption.}
}
@article{WANG2022124,
title = {Digital twin and cloud-side-end collaboration for intelligent battery management system},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {124-134},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521002284},
author = {Yujie Wang and Ruilong Xu and Caijie Zhou and Xu Kang and Zonghai Chen},
keywords = {Digital twin, Battery management system, Cloud computing, Internet of Things, Distributed computing},
abstract = {Nowadays the wave of digital economy has swept the world, and the competition in the field of battery management has become increasingly vigorous. The application of digital twin technology gives a new concept of networked management and service of lithium-ion batteries. In this paper, the digital twin technology and cloud-side-end collaboration for the future battery management system is discussed. A four layer networked architecture of cloud-side-end collaboration for battery management system is presented which breaks through the computing capacity and storage space limitations of the conventional battery management and enables high performance algorithms. The digital twin model of the battery is established, which enables refined and safety management of the batteries in their entire life cycle. Furthermore, the digital twin model and key technologies such as state estimation and cloud assisted equalization of the batteries are introduced. The results indicate that digital twin models are helpful for battery management and the full life cycle data are useful to build the upgrade route of the battery.}
}
@article{PYLIANIDIS2022105274,
title = {Simulation-assisted machine learning for operational digital twins},
journal = {Environmental Modelling & Software},
volume = {148},
pages = {105274},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105274},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221003169},
author = {Christos Pylianidis and Val Snow and Hiske Overweg and Sjoukje Osinga and John Kean and Ioannis N. Athanasiadis},
keywords = {Machine learning, Digital twin, Data availability, Data resolution, APSIM, Metamodel},
abstract = {In the environmental sciences, there are ongoing efforts to combine multiple models to assist the analysis of complex systems. Combining process-based models, which have encoded domain knowledge, with machine learning models, which can flexibly adapt to input data, can improve modeling capabilities. However, both types of models have input data limitations. We propose a methodology to overcome these issues by using a process-based model to generate data, aggregating them to a lower resolution to mimic real situations, and developing machine learning models using a fraction of the process-based model inputs. We showcase this method with a case study of pasture nitrogen response rate prediction. We train models of different scales and test them in sampled and unsampled location experiments to assess their practicality in terms of accuracy and generalization. The resulting models provide accurate predictions and generalize well, showing the usefulness of the proposed method for tactical decision support.}
}
@article{MEIXEDO2021112189,
title = {Damage detection in railway bridges using traffic-induced dynamic responses},
journal = {Engineering Structures},
volume = {238},
pages = {112189},
year = {2021},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2021.112189},
url = {https://www.sciencedirect.com/science/article/pii/S0141029621003394},
author = {Andreia Meixedo and João Santos and Diogo Ribeiro and Rui Calçada and Michael Todd},
keywords = {Damage detection, Unsupervised learning, Structural Health Monitoring, Traffic-induced dynamic responses, Autoregressive models, PCA, Regression},
abstract = {This paper aims at detecting damage in railway bridges based on traffic-induced dynamic responses. To achieve this goal, an unsupervised automatic data-driven methodology is proposed, consisting of a combination of time series analysis methods and multivariate statistical techniques. Damage-sensitive features of train-induced responses are extracted and allow taking advantage, not only of the repeatability of the loading, but also, and more importantly, of its great magnitude, thus enhancing the sensitivity to small-magnitude structural changes. The efficiency of the proposed methodology is validated in a long-span steel-concrete composite bowstring-arch railway bridge with a permanent structural monitoring system installed. An experimentally validated finite element model was used, along with experimental values of temperature, noise, and train loadings and speeds, to realistically simulate baseline and damage scenarios. The proposed methodology proved to be highly sensitive in detecting early damage, even when it consists of small stiffness reductions that do not impair the safety or use of the structure, and highly robust to false detections. The analysis and validation allowed concluding that the ability to identify early damage, imperceptible in the original signals, while avoiding observable changes induced by variations in train speed or temperature, was achieved by carefully defining the modelling and fusion sequence of the information. A single-value damage indicator, proposed as a tool for real-time structural assessment of bridges without interfering with the normal service condition, proved capable of characterizing multi-sensor data while being sensitive to identify local changes.}
}
@article{LI2021104939,
title = {A numerical integrated approach for the estimation of the uniaxial compression strength of rock from point load tests},
journal = {International Journal of Rock Mechanics and Mining Sciences},
volume = {148},
pages = {104939},
year = {2021},
issn = {1365-1609},
doi = {https://doi.org/10.1016/j.ijrmms.2021.104939},
url = {https://www.sciencedirect.com/science/article/pii/S1365160921003233},
author = {Yi-Ming Li and Gao-Feng Zhao},
keywords = {Point load test, Uniaxial compressive strength, Digital twin, Numerical modelling, Lattice spring model},
abstract = {The point load test (PLT) has been considered a flexible approach to estimate the uniaxial compressive strength (UCS) of rocks. Previously, empirical equations were obtained by mathematical fitting or machine learning to predict the UCS of rocks. The acquisition of these equations usually required a large amount of experimental data, while the corresponding parameters often lacked clear physical meanings, and the applicability of these empirical equations was limited. In this work, we attempted to develop a new method to predict the UCS of rocks by using the concept of a digital twin (numerical modelling). First, an automatic calibration procedure was used to obtain the numerical parameters of a digital twin for the PLT. Next, the UCS was predicted numerically by using a digital twin of the UCS test with the calibrated parameters. We performed a comprehensive comparison of our proposed method with previously obtained empirical equations and showed the superiority of our approach in better predicting the UCS of rocks. In this paper, we also discuss the influence of particle size and heterogeneity of rock material to illustrate the possible merits of the proposed method. Our work also shows the possible benefits of integrating numerical modelling into physical experimental tests of rocks.}
}
@article{ABBASI2021102176,
title = {Classification of diabetic retinopathy using unlabeled data and knowledge distillation},
journal = {Artificial Intelligence in Medicine},
volume = {121},
pages = {102176},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102176},
url = {https://www.sciencedirect.com/science/article/pii/S093336572100169X},
author = {Sajjad Abbasi and Mohsen Hajabdollahi and Pejman Khadivi and Nader Karimi and Roshanak Roshandel and Shahram Shirani and Shadrokh Samavi},
keywords = {Convolutional neural networks (CNN), Transfer learning, Knowledge distillation, Teacher-student model, Unlabeled data, Diabetic retinopathy},
abstract = {Over the last decade, advances in Machine Learning and Artificial Intelligence have highlighted their potential as a diagnostic tool in the healthcare domain. Despite the widespread availability of medical images, their usefulness is severely hampered by a lack of access to labeled data. For example, while Convolutional Neural Networks (CNNs) have emerged as an essential analytical tool in image processing, their impact is curtailed by training limitations due to insufficient labeled data availability. Transfer Learning enables models developed for one task to be reused for a second task. Knowledge distillation enables transferring knowledge from a pre-trained model to another. However, it suffers from limitations, and the two models' constraints need to be architecturally similar. Knowledge distillation addresses some of the shortcomings of transfer learning by generalizing a complex model to a lighter model. However, some parts of the knowledge may not be distilled by knowledge distillation sufficiently. In this paper, a novel knowledge distillation approach using transfer learning is proposed. The proposed approach transfers the complete knowledge of a model to a new smaller one. Unlabeled data are used in an unsupervised manner to transfer the new smaller model's maximum amount of knowledge. The proposed method can be beneficial in medical image analysis, where labeled data are typically scarce. The proposed approach is evaluated in classifying images for diagnosing Diabetic Retinopathy on two publicly available datasets, including Messidor and EyePACS. Simulation results demonstrate that the approach effectively transfers knowledge from a complex model to a lighter one. Furthermore, experimental results illustrate that different small models' performance is improved significantly using unlabeled data and knowledge distillation.}
}
@article{LWANDE2021e07701,
title = {Identifying learning styles and cognitive traits in a learning management system},
journal = {Heliyon},
volume = {7},
number = {8},
pages = {e07701},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e07701},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021018041},
author = {Charles Lwande and Lawrence Muchemi and Robert Oboko},
keywords = {Learning style, Cognitive trait, Learner behavior, Learning management system, Learner modeling},
abstract = {Investigating learner behavior is an increasingly important research topic in online learning. Learning styles and cognitive traits have been the subjects of research in this area. Although learning institutions use Learning Management Systems such as Moodle, Claroline, and Blackboard to facilitate teaching, the platforms do not have features for analyzing data and identifying behavior such as learning styles and cognitive traits. Instead, they only produce certain statistical reports from the daily access records. Even though complex models have been proposed in the literature, most studies are based on a single behavior such as learning styles or cognitive traits but not both. Only a few have investigated a combination of cognition-based theories such as working memory capacity and psychology-based ones such as learning styles. Thus, this study sought to answer the research question of whether it was possible to establish a methodology for the estimation of learning styles and cognitive traits from a learning management system. The study combined the Felder-Silverman Learning Style Model and Cognitive Trait Model as theoretical frameworks to identify behavior in a Learning Management System. This study designed a model for extracting records from Learning Management Systems access records to estimate learning style and cognitive traits. From this, a prototype was developed to estimate the learning style and cognitive traits for each student. The model was evaluated by administering manual tools to students in a classroom environment then comparing the results gathered against those estimated by the model. The results analyzed using Kappa statistics demonstrated the interrater reliability results were moderately in agreement. Taken together, these results suggest that it is possible to estimate the learning styles and cognitive traits of a learner in a Learning Management System. The information generated by the model can be used by tutors to provide a conducive online learning environment where learners with similar behavior ask each other for help. This can reduce the teaching load for online tutors because learners themselves act as a teaching resource. Information on learning styles and cognitive styles can also facilitate online group formation by isolating the individual factors that contribute to team success.}
}
@article{PEI2021127,
title = {The digital twin of the quality monitoring and control in the series solar cell production line},
journal = {Journal of Manufacturing Systems},
volume = {59},
pages = {127-137},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S027861252100025X},
author = {Feng-Que Pei and Yi-Fei Tong and Ming-Hai Yuan and Kun Ding and Xi-Hui Chen},
keywords = {Digital twin, Quality monitoring & control, Series solar cell production line},
abstract = {With the development of intelligent manufacturing (IM), the Digital twin (DT) has become an important means to the evolution mechanism of the process. Many researchers pay attention on the realization of DT in different industries. Based on the DT and Digital Twin Shop Floor (DTS) model, a novel, high throughput metrology method is proposed in the process quality monitoring and control of the Series Solar Cell Production Line (SSCPL) for detailed performance analysis. The variance of individual loss parameters and their impact on quality performance are quantified and mapped into the virtual space. The nature of their distributions and correlations provide great insights about quality loss mechanisms in process monitoring, helping to prioritize efforts for optimizing the control of the SSCPL in the physical space. Additionally, the parameters can be tied back to the physical space, allowing the data to be used directly for the control in the manufacturing. The data-loop of “Autonomous perception of process parameters - Dynamic behaver mapping - Online monitoring - Online data analysis - Parameters configuration & control” can be obtained in the model. This paper provides an application paradigm for DT and IM.}
}
@article{SHRIVASTAVA2021107404,
title = {Addressing the challenges in remanufacturing by laser-based material deposition techniques},
journal = {Optics & Laser Technology},
volume = {144},
pages = {107404},
year = {2021},
issn = {0030-3992},
doi = {https://doi.org/10.1016/j.optlastec.2021.107404},
url = {https://www.sciencedirect.com/science/article/pii/S0030399221004928},
author = {Ankit Shrivastava and Sumanta Mukherjee and Shitanshu S. Chakraborty},
keywords = {Remanufacturing, Laser additive manufacturing, Process monitoring, Laser remelting, Laser cladding},
abstract = {Increased focus on reduction of impact on the environment has put the aspect of remanufacturing in the spotlight, and remanufacturing of high-value engineering components is gradually becoming a mainstream practice. Out of different alternatives, laser-based deposition has been the central choice for remanufacturing, thanks to its accuracy, and precision. However, considering the complex process physics involved in laser-based remanufacturing processes, it is essential to establish the reliability of the process so that certifiable remanufactured parts can be produced. This work provides a comprehensive analysis of the issues encountered during laser-based remanufacturing, and the different approaches to address them. Apart from covering the state-of-the-art of remanufacturing by laser-based deposition, this article also discusses tools like deep learning, and digital twin which are still in their early phases in terms of applications in the remanufacturing domain.}
}
@article{ELMGHOUCHI2022100157,
title = {On the prediction of daily global solar radiation using temperature as input. An application of hybrid machine learners to the six climatic Moroccan zones},
journal = {Energy Conversion and Management: X},
volume = {13},
pages = {100157},
year = {2022},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2021.100157},
url = {https://www.sciencedirect.com/science/article/pii/S2590174521000829},
author = {Y. {El Mghouchi}},
keywords = {Solar radiation forecasting, Temperature-based models, Least Square regression, Machines learning, Moroccan climatic zones},
abstract = {Forecasting of solar radiation intensity is a necessity for the establishment of solar energy projects and for decision-making in other related fields. Current prediction models/methods are site-dependent and their performance/accuracy outside the area of application is debatable. Temperature-based solar radiation models are highly recommended in areas where only air temperature data is available. Therefore, the purpose of this study is to evaluate the prediction accuracy of 42 existing temperature-based solar radiation models in forecasting the daily global solar radiation (DGSR) on horizontal surface for the six climatic zones of Morocco. In the first time, the models were assessed using only the least square method. Then, four Machine Learners models (SVM, Decision Tree, Gaussian Regression and Linear Regression) were employed as optimizers to improve the accuracy prediction of the models. The results differ from model to another based on their values of MBE, MSE, RMSE, σ and R2. Two methods were employed for ordering the studied models: the Performance score and the Taylor diagram. Long term meteorological data was used in the evaluation processes. The correlation R2 of the optimized models changes from 0.80 to 0.95 for all skies and from 0.95 to 0.98 for clear skies.}
}
@article{ABDELRAHMAN2022108532,
title = {Personal thermal comfort models using digital twins: Preference prediction with BIM-extracted spatial–temporal proximity data from Build2Vec},
journal = {Building and Environment},
volume = {207},
pages = {108532},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108532},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321009240},
author = {Mahmoud M. Abdelrahman and Adrian Chong and Clayton Miller},
keywords = {Spatial–temporal modeling, Building information models, Graph network structure, Personal thermal comfort model, Digital twin},
abstract = {Conventional thermal preference prediction in buildings has limitations due to the difficulty in capturing all environmental and personal factors. New model features can improve the ability of a machine learning model to classify a person’s thermal preference. The spatial context of a building can provide information to models about the windows, walls, heating and cooling sources, air diffusers, and other factors that create micro-environments that influence thermal comfort. Due to spatial heterogeneity, it is impractical to position sensors at a high enough resolution to capture all conditions. This research aims to build upon an existing vector-based spatial model, called Build2Vec, for predicting spatial–temporal occupants’ indoor environmental preferences. Build2Vec utilizes the spatial data from the Building Information Model (BIM) and indoor localization in a real-world setting. This framework uses longitudinal intensive thermal comfort subjective feedback from smart watch-based ecological momentary assessments (EMA). The aggregation of these data is combined into a graph network structure (i.e., objects and relations) and used as input for a classification model to predict occupant thermal preference. The results of a test implementation show 14%–28% accuracy improvement over a set of baselines that use conventional thermal preference prediction input variables.}
}
@article{ALSAIHATI2022109335,
title = {Rate of penetration prediction while drilling vertical complex lithology using an ensemble learning model},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109335},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109335},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521009852},
author = {Ahmed Alsaihati and Salaheldin Elkatatny and Hany Gamal},
keywords = {Rate of penetration, Complex lithology, Principal component analysis, Artificial intelligence, Ensemble learning},
abstract = {The rate of penetration (ROP) accounts for a substantial portion of the overall drilling cost. The drilling optimization process, which mostly involves the adjustment of the mechanical drilling parameters, is therefore of prime importance in ensuring efficient drilling. However, drilling formations with assorted types of lithology necessitate the involvement of more parameters to reduce uncertainty and enhance confidence when predicting the ROP. The objective of this paper is to introduce an ensemble model based on random forest (RF), in which artificial neural network (ANN), and adaptive neuro-fuzzy inference system (ANFIS) are the base learner models, to predict the ROP across different lithological formations In this study, two types of actual field data of Well-1 were employed to build the model: (i) mechanical drilling parameters collected from real-time sensors allocated at the rig site, and (ii) petrophysical properties obtained from conventional well logs. Well-2 with more than 2300 unseen data points was used to compare the capability of the base learners (i.e., ANN and ANFIS), standalone RF, and RF-meta model in predicting the ROP with two of the earliest published ROP empirical models (Maurer's and Bingham's models). The results showed that the RF-meta model outperformed the base learners and Maurer's and Bingham's empirical models in predicting the ROP in Well-2 with a low absolute average percentage error (AAPE) of 7.8 % and a high coefficient of determination (R2) of 0.94.}
}
@article{DAVILADELGADO2021101332,
title = {Digital Twins for the built environment: learning from conceptual and process models in manufacturing},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101332},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101332},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000859},
author = {Juan Manuel {Davila Delgado} and Lukumon Oyedele},
keywords = {Digital Twin, BIM, Cyber-Physical Systems, Conceptual Models, Process Models, Maturity Models},
abstract = {The overall aim of this paper is to contribute to a better understanding of the Digital Twin (DT) paradigm in the built environment by drawing inspiration from existing DT research in manufacturing. The DT is a Product Life Management information construct that has migrated to the built environment while research on the subject has grown intensely in recent years. Common to early research phases, DT research in the built environment has developed organically, setting the basis for mature definitions and robust research frameworks. As DT research in manufacturing is the most developed, this paper seeks to advance the understanding of DTs in the built environment by analysing how the DT systems reported in manufacturing literature are structured and how they function. Firstly, this paper presents a thorough review and a comparison of DT, cyber-physical systems (CPS), and building information modelling (BIM). Then, the results of the review and categorisation of DT structural and functional descriptions are presented. Fifty-four academic publications and industry reports were reviewed, and their structural and functional descriptions were analysed in detail. Three types of structural models (i.e. conceptual models, system architectures, and data models) and three types of functional models (process and communication models) were identified. DT maturity models were reviewed as well. From the reviewed descriptions, four categories of DT conceptual models (prototypical, model-based, interface-oriented, and service-based) and six categories of DT process models (DT creation, DT synchronisation, asset monitoring, prognosis and simulation, optimal operations, and optimised design) were defined and its applicability to the AECO assessed. While model-based and service-based models are the most applicable to the built environment, amendments are still required. Prognosis and simulation process models are the most widely applicable for AECO use-cases. The main contribution to knowledge of this study is that it compiles the DT’s structural and functional descriptions used in manufacturing and it provides the basis to develop DT conceptual and process models specific to requirements of the built environment sectors.}
}
@article{FENG2022108165,
title = {An adaptive learning approach to determine and update crack sizes from strain relaxation data for welded plate joints},
journal = {Engineering Fracture Mechanics},
volume = {259},
pages = {108165},
year = {2022},
issn = {0013-7944},
doi = {https://doi.org/10.1016/j.engfracmech.2021.108165},
url = {https://www.sciencedirect.com/science/article/pii/S0013794421005701},
author = {Liuyang Feng and Xudong Qian},
keywords = {Crack sizing, Neural network, Modified bootstrap particle filtering, Strain relaxation, Welded plate joints, Digital twin},
abstract = {This paper proposes the framework to determine and update the crack-front profile at the toe of welded plate joints based on the strain relaxation data. This study determines the crack depth by classifying the nodes in the thickness direction as open nodes on the crack surface and closed nodes on the intact ligament. To update the crack size during the loading history, this research employs the modified bootstrap particle filtering approach, which entails enhanced adjustment capabilities by imposing additional uncertainty distributions. This approach improves the crack size prediction by absorbing limited measurement data on the strain values or crack sizes.}
}
@article{WU2021114292,
title = {Similarity based telemetry data recovery for enhancing operating reliability of satellite},
journal = {Microelectronics Reliability},
volume = {126},
pages = {114292},
year = {2021},
note = {Proceedings of ESREF 2021, 32nd European Symposium on Reliability of Electron Devices, Failure Physics and Analysis},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2021.114292},
url = {https://www.sciencedirect.com/science/article/pii/S0026271421002584},
author = {Y. Wu and J. Liang and Y. Peng},
keywords = {Similarity searching, Data recovery, LSTM},
abstract = {Recently, the satellite launching grows rapidly, especially in the commercial aerospace, which produces thousands of in-orbit spacecrafts. The operation and ground management of these satellites is a hotspot issue to ensure the satellites' in-orbit operation reliability. The telemetry data transmission may be delayed by the unstable telemetry link, insufficient ground station sources, interface failures and so on, which exerts a negative influence on the ground management capability and increases the failure risk. Meanwhile, the deep space spacecrafts, space station and Mars exploration expand this transmission delay. Thus, management and intelligent analysis of the telemetry data from Telemetry Ground Station (TGS) gradually become critical for in-orbit satellite operation. To solve above issues, a similarity-based deep learning time series prediction method is proposed to achieve telemetry data recovery. Considering the pseudo-periodic characteristics in telemetry data, this approach extracts similar segments to construct training samples instead of feeding all the observation samples to train the prediction model. As the predicted data is obtained ahead of the actual time-delayed telemetry data, which can provide sufficient analysis time for ground operator to optimize the maintenance strategy. Furthermore, the telemetry prediction data can also guarantee high-complexity digital twin modelling, virtual maintenance and so on. Experimental results with actual satellite telemetry data verify the effectiveness of the proposed method.}
}
@article{ZHANG2022101481,
title = {Implementation path and reference framework for Industrial Internet Platform (IIP) in product service system using industrial practice investigation method},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101481},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101481},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002317},
author = {Xianyu Zhang and Xinguo Ming},
keywords = {Industrial internet, Industrial internet platform, industrial internet of things (IIoT), internet of things (IoT), Smart manufacturing, Intelligent manufacturing},
abstract = {With the development of intelligent sensing, edge computing, fog computing, cloud computing, parallel computing, smart grid, big data, block chain, 5G, cyber-physical systems, digital twins, machine learning and other technologies, the industrial internet has undergone control network stage, sensor network stage, internet stage, Internet of Things (IoT) stage, Industrial Internet of Things (IIoT) stage, and Industrial Internet (II) stage, etc. In the existing research, scholars focus on a local dot, such as: technology, function, elements and application based on industrial internet. However, there is a lack of an overall framework to study the top-level planning of Industrial Internet Platform (IIP) from a systematic perspective. On the other hand, there are few studies on the detailed path and steps for implementing IIP in a specific enterprise in a specific industry. The objective of this paper is to study a reference framework and industrial implementation path for IIP in product service system using industrial practice investigation method, which meets the needs of industry on the basis of existing theory and industrial practice, and to provide reference for government and industry planning, design, implementation and promotion of IIP. In addition, the proposed reference framework and industrial implementation for IIP in product service system can enhance the core value of the enterprise and increase benefits.}
}
@article{WEI2022106977,
title = {Super-learner model realizes the transient prediction of CO2 and NOx of diesel trucks: Model development, evaluation and interpretation},
journal = {Environment International},
volume = {158},
pages = {106977},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2021.106977},
url = {https://www.sciencedirect.com/science/article/pii/S0160412021006024},
author = {Ning Wei and Qijun Zhang and Yanjie Zhang and Jiaxin Jin and Junyu Chang and Zhiwen Yang and Chao Ma and Zhenyu Jia and Chunzhe Ren and Lin Wu and Jianfei Peng and Hongjun Mao},
keywords = {Vehicle transient emission model, Machine learning, Super-learner model, China VI N vehicle},
abstract = {The transient simulation of CO2 and NOX from motor vehicles has essential applications in evaluating vehicular greenhouse gas emissions and pollutant emissions. However, accurately estimating vehicular transient emissions is challenging due to the heterogeneity between different vehicles and the continuous upgrading of vehicle exhaust purification technology. To accurately characterize the transient emissions of motor vehicles, a Super-learner model is used to build CO2 and NOx transient emission models. The actual onboard test data of 9 China VI N2 vehicles were used to train the model, and the test data of another China VI N2 vehicle were selected for further robustness verification. There were significant differences in the emissions between the vehicles, but the constructed transient model could capture the common law of transient emissions from China VI N2 vehicles. The R2 values of CO2 and NOx emission in the test data of the validation vehicle were 0.71 and 0.82, respectively. In addition, to further prove the model's robustness, the training data were synchronously modelled based on the Moves-method. The Super-learner model has a smaller RMSE on the validation set than the model based on the Moves-method, indicating that the Super-learner model has more transient simulation advantages. The marginal contributions of the model characteristics to the model results were analysed by SHapley Additive exPlanation (SHAP) value interpretation, and the marginal contributions of different pollutant characteristic parameters varied. Therefore, when establishing transient models of different pollutants, the selection of the model parameters demands considering the generation and purification process of different pollutants. The present work provides novel insights into the parameter selection, construction, and interpretation of the transient vehicle emission model.}
}
@article{FRIEDERICH2022103586,
title = {A framework for data-driven digital twins of smart manufacturing systems},
journal = {Computers in Industry},
volume = {136},
pages = {103586},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103586},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001937},
author = {Jonas Friederich and Deena P. Francis and Sanja Lazarova-Molnar and Nader Mohamed},
keywords = {Data-driven, Digital twin, Machine learning, Process mining, Reconfigurable manufacturing, Smart factory},
abstract = {Adoption of digital twins in smart factories, that model real statuses of manufacturing systems through simulation with real time actualization, are manifested in the form of increased productivity, as well as reduction in costs and energy consumption. The sharp increase in changing customer demands has resulted in factories transitioning rapidly and yielding shorter product life cycles. Traditional modeling and simulation approaches are not suited to handle such scenarios. As a possible solution, we propose a generic data-driven framework for automated generation of simulation models as basis for digital twins for smart factories. The novelty of our proposed framework is in the data-driven approach that exploits advancements in machine learning and process mining techniques, as well as continuous model improvement and validation. The goal of the framework is to minimize and fully define, or even eliminate, the need for expert knowledge in the extraction of the corresponding simulation models. We illustrate our framework through a case study.}
}
@article{LI2021103961,
title = {Digital twin-driven virtual sensor approach for safe construction operations of trailing suction hopper dredger},
journal = {Automation in Construction},
volume = {132},
pages = {103961},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103961},
url = {https://www.sciencedirect.com/science/article/pii/S092658052100412X},
author = {Mingchao Li and Qiaorong Lu and Shuo Bai and Mengxi Zhang and Huijing Tian and Liang Qin},
keywords = {Trailing suction hopper dredger, Digital twin, Virtual sensor, Machine learning, Data mining, Construction safety},
abstract = {The stable and safe operation of Trailing suction hopper dredger (TSHD) is one of the most crucial considerations for ensuring its high dredging productivity. However, the instability and sudden failure of physical sensors pose challenges to the monitoring of dredging process. To address these issues, we propose a structure of digital twin-driven virtual sensor (DTDVS) for the construction safety of TSHD. Considering the potential internal relations among construction data, we compare the performance of four machine learning algorithms in predicting the torsional vibration in mechanical failure. The results showed that these algorithms provide high prediction accuracy (R2 > 0.9). Then the DBN model with the best performance was selected as a part of the virtual sensors to predict and analyze the status of TSHD. The digital twin technology provides a more stable and environmentally friendly scheme for TSHD construction safety control. On the one hand, the DTDVS assists physical sensors to monitor the construction state, overcoming the limitation of the sensors on detection targets which are difficult or costly to measure directly. On the other hand, by analyzing the residual between the physical sensor and the virtual sensor, the construction behavior can be diagnosed, and the fault situation can be pre-warned accurately. This improves the time utilization of TSHD and provides an important guarantee for the construction safety.}
}
@article{GAO2022252,
title = {Deep cognitive diagnosis model for predicting students’ performance},
journal = {Future Generation Computer Systems},
volume = {126},
pages = {252-262},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003277},
author = {Lina Gao and Zhongying Zhao and Chao Li and Jianli Zhao and Qingtian Zeng},
keywords = {Cognitive diagnosis, Deep learning, Student modeling, Educational data mining, Learning analysis},
abstract = {Cognitive model is playing very important role in predicting students’ performance and recommending learning resources. Thus, it has received a great deal of attention from researchers. However, most of the existing work design models from the aspect of students, ignoring the internal relation between problems and skills. To address this problem, we propose a deep cognitive diagnosis framework to obtain students’ mastery of skills and problems by enhancing traditional cognitive diagnosis methods with deep learning. First, we model the skill proficiency of students according to their responses to objective and subjective problems. Second, students’ mastery on problems is modeled based on attention mechanism and neural network, considering both the importance and the interactions of skills. Finally, considering the facts that students may carelessly select or simply guess the answer, we predict students’ performance via the proposed model. Extensive experiments are carried out on two real-world data sets, and the results have proved the effectiveness and interpretability of this work.}
}
@article{IBRION2021105355,
title = {Learning from failures in cruise ship industry: The blackout of Viking Sky in Hustadvika, Norway},
journal = {Engineering Failure Analysis},
volume = {125},
pages = {105355},
year = {2021},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2021.105355},
url = {https://www.sciencedirect.com/science/article/pii/S1350630721002156},
author = {Michaela Ibrion and Nicola Paltrinieri and Amir R. Nejad},
keywords = {Engine failure, Cruise ship, Blackout, Viking Sky, Hustadvika-Norway, Learning, Marine accidents and Digital Twin},
abstract = {This article brings to attention learning from the failure - blackout, loss of propulsion and near grounding - of Viking Sky cruise ship which occurred in Hustadvika, Norway, in March 2019. Failures and accidents in the cruise ship industry attract the global media and can severely impact reputation and business performance of companies and authorities involved. A system approach investigation and analysis - CAST - was employed with the aim to maximize learning from the Viking Sky’s failure through a systematic approach and to contribute to failure reduction in the cruise ship industry. Three main recommendations emerged from this study: an overview of the accident or failure precursors and resilience indicators; safety recommendations for other cruise ships; lessons and strategies of actions for the increased cruise operations in the Arctic and Antarctic areas. It was found that several accident or failure precursors, for example, a low level of lubricating oil, the failure of a turbocharger, an inoperative large diesel generator, lack of functionality for safety equipment due to bad weather, and others precursors contributed to failure and highly critical situation encountered by Viking Sky in Hustadvika. Resilience indicators such as the master’s immediate decision to launch mayday, the crew preparedness, and the way how the emergency situation was handled were found to have positive impacts on critical situation of Viking Sky. This article highlights also that adaptations and improvement of standards and regulations for harsh environmental conditions can play an important role in prevention of marine accidents. Furthermore, for a better understanding of correlation between environmental loads and their effects on machinery systems, digital solutions such as digital twin for condition monitoring of cruise ships in the Polar areas are seen as possible innovative solutions yet to be fully implemented in the marine industry.}
}
@article{WANG2022101897,
title = {Deep learning for assessment of environmental satisfaction using BIM big data in energy efficient building digital twins},
journal = {Sustainable Energy Technologies and Assessments},
volume = {50},
pages = {101897},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101897},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821009115},
author = {Weixi Wang and Han Guo and Xiaoming Li and Shengjun Tang and Jizhe Xia and Zhihan Lv},
keywords = {Building digital twins, BIM big data, Deep learning, Assessment of environmental satisfaction, Energy efficient building},
abstract = {Energy efficient Building Digital Twins (BDTs) are researched using Building Information Model (BIM) to explore the key techniques of Digital Twins (DTs). DTs in buildings can be regarded as an expression of “BIM+,” born to digital descriptions. Comprehensive perception of physical systems is the preconditions for DTs implementation. BIM’s energy-saving design includes the selection of building orientation and building shape. BIM energy consumption analysis can compare different materials, examine the performance of various materials, and select the most suitable and most energy-efficient materials for building structure maintenance. Data Fusion Algorithm (DFA) in Wireless Sensor Networks (WSNs) is improved. A novel DFA is constructed by combining Backpropagation Neural Network (BPNN) with Dynamic Host Configuration Protocol (DCHP), recorded as BP-DCHP. Simulation experiment proves that BP-DCHP can prolong sensor nodes’ survival time and provide the highest data fusion quality. BP-DCHP runs for about 310 s, 500 s, and 705 s in WSNs consisting of 20, 50, and 100 WSNs, respectively. Moreover, BP-DCHP can provide higher quality given insufficient data fusion degree. Once the WSNs consume 50% of the total initial energy, BP-DCHP presents a shorter network delay, only 0.6 s on average in the 100-sensor-node-WSN. To validate BDTs’ effectiveness, the environmental satisfaction of residents from two Beijing intelligent communities is assessed using Deep Learning (DL) approach. Taking the data as the clue, the study establishes DTs serving the application of urban scene, which plays a certain role in promoting the technological innovation of BDTs, better optimizing the city and managing the city.}
}
@article{PHANDEN202288,
title = {A state-of-the-art review on implementation of digital twin in additive manufacturing to monitor and control parts quality},
journal = {Materials Today: Proceedings},
volume = {56},
pages = {88-93},
year = {2022},
note = {International Conference on Materials, Machines and Information Technology-2022},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.12.217},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321079360},
author = {Rakesh Kumar Phanden and S.V. Aditya and Aaryan Sheokand and Kapil Kumar Goyal and Pardeep Gahlot and Adam Jacso},
keywords = {Digital twin, Additive manufacturing, Review, Parts quality monitoring},
abstract = {Both, Additive Manufacturing (AM) and Digital Twin (DT) are emerging technologies. DT is helping AM in process simulation, monitoring and controlling as well as to develop insights on process parameters relation to achieve high parts quality. Therefore, the implementation of DT technology in AM is highly desirable and fruitful. In the current state, DT application on AM has been explored by various researchers for education, manufacturing, maintenance and quality area from the theoretical and practical viewpoints. This paper presents the state-of-the-art literature review on the implementation of DT in AM to monitor and control the parts quality from theoretical and practical viewpoints. Based on the literature, a representation scheme has been extracted to implement DT in AM successfully, and various future research directions are given.}
}
@article{BLAIR2021100359,
title = {Digital twins of the natural environment},
journal = {Patterns},
volume = {2},
number = {10},
pages = {100359},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100359},
url = {https://www.sciencedirect.com/science/article/pii/S266638992100221X},
author = {Gordon S. Blair},
abstract = {Summary
Digital twins emerged in the field of engineering but are now being applied in many areas of study. This article reflects on the enormous potential of digital twins of the natural environment and proposes an approach that builds on the massive legacy of process model understanding in this area combined with new insights from data understanding, including from AI/machine learning.}
}
@article{LV2021108366,
title = {Beyond 5G for digital twins of UAVs},
journal = {Computer Networks},
volume = {197},
pages = {108366},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108366},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003534},
author = {Zhihan Lv and Dongliang Chen and Hailing Feng and Ranran Lou and Huihui Wang},
keywords = {Unmanned aerial vehicle, B5G, Deep learning, Coordinated multi-point transmission, Physical layer security, Digital twins},
abstract = {The purpose is to explore the application effects and limitations of Unmanned Aerial Vehicle (UAV) in 5G/B5G (Beyond 5G) mobile and wireless communication. Based on 5Gcommunication, the deep learning (DL) algorithm is introduced to construct the UAV Digital Twins (DTs) communication channel model based on DL. The Coordinated Multi-point Transmission (COMP) technology is adopted to study the interference suppression of UAVs. The key algorithm in the physical layer security is employed to ensure information communication security. Finally, the model constructed is simulated and analyzed. The transmission error rates and transmission estimation accuracy of several algorithms, including the proposed algorithm and ordinary Deep Neural Networks (DNNs), are compared under different Signal-to-Noise Ratios (SNRs). Results find that the convergence speed and convergence effect of the proposed algorithm has prominent advantages, presenting strong robustness; the proposed algorithm's estimation accuracy is about 150 times higher than the traditional algorithms. Further analysis reveals that the proposed algorithm's accuracy reaches 82.39%, which increases by at least 3.2% than other classic machine algorithms. The indicators of Precision, Recall, and F1 are compared as well. Apparently, the Precision, Recall, and F1 values of the proposed algorithm are the highest, while the transmission delay is the smallest. Therefore, the constructed UAV DTs wireless communication channel model has strong robustness and further reduces UAV limitations, providing a reference for improving UAV system performance in the later stage.}
}
@article{SCHILLINGER2021103866,
title = {Adaptive heterogeneous multi-robot collaboration from formal task specifications},
journal = {Robotics and Autonomous Systems},
volume = {145},
pages = {103866},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103866},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021001512},
author = {Philipp Schillinger and Sergio García and Alexandros Makris and Konstantinos Roditakis and Michalis Logothetis and Konstantinos Alevizos and Wei Ren and Pouria Tajvar and Patrizio Pelliccione and Antonis Argyros and Kostas J. Kyriakopoulos and Dimos V. Dimarogonas},
keywords = {Robotics, Multi-robot, Temporal logic, HRI, Heterogeneous robots, Task decomposition, Task allocation, Abstraction},
abstract = {Efficiently coordinating different types of robots is an important enabler for many commercial and industrial automation tasks. Here, we present a distributed framework that enables a team of heterogeneous robots to dynamically generate actions from a common, user-defined goal specification. In particular, we discuss the integration of various robotic capabilities into a common task allocation and planning formalism, as well as the specification of expressive, temporally-extended goals by non-expert users. Models for task allocation and execution both consider non-deterministic outcomes of actions and thus, are suitable for a wide range of real-world tasks including formally specified reactions to online observations. One main focus of our paper is to evaluate the framework and its integration of software modules through a number of experiments. These experiments comprise industry-inspired scenarios as motivated by future real-world applications. Finally, we discuss the results and learnings for motivating practically relevant, future research questions.}
}
@article{CHEN202270,
title = {Digital twins to fight against COVID-19 pandemic},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {2},
pages = {70-81},
year = {2022},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2022.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S266734522200013X},
author = {Dongliang Chen and Nojoom A. AlNajem and Mohammad Shorfuzzaman},
keywords = {Digital twins, Novel coronavirus disease, Blockchain, Deep learning, Information security},
abstract = {This study is aimed to explore the anti-epidemic effect of artificial intelligence (AI) algorithms such as digital twins on the COVID-2019 (novel coronavirus disease 2019), so that the information security and prediction accuracy of epidemic prevention and control (P & C) in smart cities can be further improved. It addresses the problems in the current public affairs governance strategy for the outbreak of the COVID-2019 epidemic, and uses digital twins technology to map the epidemic P & C situation in the real space to the virtual space. Then, the blockchain technology and deep learning algorithms are introduced to construct a digital twins model of the COVID-2019 epidemic (the COVID-DT model) based on blockchain combined with BiLSTM (Bi-directional Long Short-Term Memory). In addition, performance of the constructed COVID-DT model is analyzed through simulation. Analysis of network data security transmission performance reveals that the constructed COVID-DT model shows a lower average delay, its data message delivery rate (DMDR) is basically stable at 80%, and the data message disclosure rate (DMDCR) is basically stable at about 10%. The analysis on network communication cost suggests that the cost of this study does not exceed 700 bytes, and the prediction error does not exceed 10%. Therefore, the COVID-DT model constructed shows high network security performance while ensuring low latency performance, enabling more efficient and accurate interaction of information, which can provide experimental basis for information security and development trends of epidemic P & C in smart cities.}
}
@article{VIIRMAN2021100858,
title = {Running to keep up with the lecturer or gradual de-ritualization? Biology students’ engagement with construction and data interpretation graphing routines in mathematical modelling tasks},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100858},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100858},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000195},
author = {Olov Viirman and Elena Nardi},
keywords = {Mathematics in biology, Mathematical modelling, Commognition, Meta-level learning, Graph construction and interpretation routines, De-ritualization},
abstract = {Through a commognitive lens, we examine twelve first-semester biology students’. engagement with graphing routines as they work in groups, during four sessions of Mathematical Modelling (MM). We trace the students’ meta-level learning, particularly as they fluctuate between deploying graphs for mere illustration of data and as sense-making tools. We account for student activity in relation to precedent events in their experiences of graphing and as fluid, if not always productive, interplay between ritualised and exploratory engagement with graph construction and interpretation routines. The students’ construal of the task situations is marked by efforts to keep up with lecturer expectations which allow for changing degrees of student agency but do not factor in the influence of precedent events. Our analysis has pedagogical implications for the way MM problems are formulated and also foregrounds the capacity of the commognitive framework to trace de-ritualization and meta-level learning in students’ MM activity.}
}
@article{XIA2021107938,
title = {Intelligent fault diagnosis of machinery using digital twin-assisted deep transfer learning},
journal = {Reliability Engineering & System Safety},
volume = {215},
pages = {107938},
year = {2021},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2021.107938},
url = {https://www.sciencedirect.com/science/article/pii/S0951832021004531},
author = {Min Xia and Haidong Shao and Darren Williams and Siliang Lu and Lei Shu and Clarence W. {de Silva}},
keywords = {Digital twin, Fault diagnosis, Novel sparse de-noising auto-encoder, Deep transfer learning},
abstract = {Digital twin (DT) is emerging as a key technology for smart manufacturing. The high fidelity DT model of the physical assets can produce system performance data that is close to reality, which provides remarkable opportunities for machine fault diagnosis when the measured fault condition data are insufficient. This paper presents an intelligent fault diagnosis framework for machinery based on DT and deep transfer learning. First, the DT model of the machine is built by establishing the simulation model and with further updating through continuously measured data from the physical asset. Second, all important machine conditions can be simulated from the built DT. Third, a new-type deep structure based on novel sparse de-noising auto-encoder (NSDAE) is developed and pre-trained with condition data from the source domain, as generated from the DT. Then, to achieve accurate machine fault diagnosis with possible variations in working conditions and system characteristics, the pre-trained NSDAE is fine-tuned using parameter transfer with only one sample from the target domain. The presented method is validated through a case study of triplex pump fault diagnosis. The experimental results demonstrate that the proposed method achieves intelligent fault diagnosis with a limited amount of measured data and outperforms other state-of-the-art data-driven methods.}
}
@article{LUNACEK2021102061,
title = {A data-driven operational model for traffic at the Dallas Fort Worth International Airport},
journal = {Journal of Air Transport Management},
volume = {94},
pages = {102061},
year = {2021},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2021.102061},
url = {https://www.sciencedirect.com/science/article/pii/S0969699721000442},
author = {Monte Lunacek and Lindy Williams and Joseph Severino and Karen Ficenec and Juliette Ugirumurera and Matthew Eash and Yanbo Ge and Caleb Phillips},
keywords = {Microsimulation, Congestion, Digital twin, Machine learning, Airport, Traffic},
abstract = {Airports are on the front line of significant innovations, allowing the movement of more people and goods faster, cheaper, and with greater convenience. As air travel continues to grow, airports will face challenges in responding to increasing passenger vehicle traffic, which leads to lower operational efficiency, poor air quality, and security concerns. This paper evaluates methods for traffic demand forecasting combined with traffic microsimulation, which will allow airport operations staff to accurately predict traffic and congestion. Using two years of detailed data describing individual vehicle arrivals and departures, aircraft movements, and weather at Dallas-Fort Worth (DFW) International Airport, we evaluate multiple prediction methods including the Auto Regressive Integrated Moving Average (ARIMA) family of models, traditional machine learning models, and DeepAR, a modern recurrent neural network (RNN). We find that these algorithms are able to capture the diurnal trends in the surface traffic, and all do very well when predicting the next 30 minutes of demand. Longer forecast horizons are moderately effective, demonstrating the challenge of this problem and highlighting promising techniques as well as potential areas for improvement. Traffic demand is not the only factor that contributes to terminal congestion, because temporary changes to the road network, such as a lane closure, can make benign traffic demand highly congested. Combining a demand forecast with a traffic microsimulation framework provides a complete picture of traffic and its consequences. The result is an operational intelligence platform for exploring policy changes, as well as infrastructure expansion and disruption scenarios. To demonstrate the value of this approach, we present results from a case study at DFW Airport assessing the impact of a policy change for vehicle routing in high demand scenarios. This framework can assist airports like DFW as they tackle daily operational challenges, as well as explore the integration of emerging technology and expansion of their services into long term plans.}
}
@article{PRISACARU2021114134,
title = {Towards virtual twin for electronic packages in automotive applications},
journal = {Microelectronics Reliability},
volume = {122},
pages = {114134},
year = {2021},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2021.114134},
url = {https://www.sciencedirect.com/science/article/pii/S0026271421001001},
author = {Alexandru Prisacaru and Ernesto Oquelis Guerrero and Balakrishna Chimmineni and Przemyslaw Jakub Gromala and Yu-Hsiang Yang and Bongtae Han and Guo Qi Zhang},
keywords = {Digital twin, Machine learning, Finite element method, Surrogate modelling, Electronic package, Mechanical stress sensor},
abstract = {The piezoresistive silicon based stress sensor has the potential to be part of the Digital Twin implementation in automotive electronics. One solution to enforce reliability in digital twins is the use of Machine Learning (ML). One or more physical parameters are being monitored, while other parameters are projected with surrogate models, just like virtual sensors. Piezo-resistive stress sensors are employed to measure the internal stresses of electronic packages, an Acquisition Unit (AU) to read out sensor data and a Raspberry Pi to perform evaluation. Accelerated tests in air thermal chamber are performed to get time series data of the stress sensor signals, with which we can know better about how delamination develops inside the package. In this study stress measurements are performed in several electronic packages during the delamination. The delamination is detected by the stress sensor due to the continuous change of the stiffness and the local boundary conditions causing the stresses to change. Although, the stress change in multiple cells can give enough information if it is delaminated or not, its delamination area location is unknown. Surrogate models built upon Neural Networks (NN) and Finite Element Method (FEM) are developed to predict the out of plane stresses at the delaminated layer. FEM simulation models are calibrated with Moiré measurements and validated at the component and PCB level with stress difference measurements. Simulation delamination areas are constructed based on the Scanning Acoustic Microscope (SAM) images, and are also validated with the equivalent stress measurements. In the end the surrogate model is predicting the out of plane stress in the adhesive layer. The results show good correlation when compared to the SAM images.}
}
@article{CONATI2021103503,
title = {Toward personalized XAI: A case study in intelligent tutoring systems},
journal = {Artificial Intelligence},
volume = {298},
pages = {103503},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103503},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000540},
author = {Cristina Conati and Oswald Barral and Vanessa Putnam and Lea Rieger},
keywords = {Explainable artificial intelligence (XAI), Intelligent tutoring systems (ITS), User modeling, Personalization},
abstract = {Our research is a step toward ascertaining the need for personalization in XAI, and we do so in the context of investigating the value of explanations of AI-driven hints and feedback in Intelligent Tutoring Systems (ITS). We added an explanation functionality to the Adaptive CSP (ACSP) applet, an interactive simulation that helps students learn an algorithm for constraint satisfaction problems by providing AI-driven hints adapted to their predicted level of learning. We present the design of the explanation functionality and the results of a controlled study to evaluate its impact on students' learning and perception of the ACPS hints. The study includes an analysis of how these outcomes are modulated by several user characteristics such as personality traits and cognitive abilities, to asses if explanations should be personalized to these characteristics. Our results indicate that providing explanations increase students' trust in the ACPS hints, perceived usefulness of the hints, and intention to use them again. In addition, we show that students' access of the ACSP explanation and learning gains are modulated by three user characteristics, Need for Cognition, Contentiousness and Reading Proficiency, providing insights on how to personalize the ACSP explanations to these traits, as well as initial evidence on the potential value of personalized Explainable AI (XAI) for ITS.}
}
@article{MEIXEDO2022108268,
title = {Online unsupervised detection of structural changes using train–induced dynamic responses},
journal = {Mechanical Systems and Signal Processing},
volume = {165},
pages = {108268},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108268},
url = {https://www.sciencedirect.com/science/article/pii/S088832702100635X},
author = {Andreia Meixedo and João Santos and Diogo Ribeiro and Rui Calçada and Michael D. Todd},
keywords = {Online assessment, Unsupervised learning, Damage detection, Structural health monitoring, Traffic-induced dynamic responses, ARX model, PCA, Cluster analysis},
abstract = {This paper exploits unsupervised data-driven structural health monitoring (SHM) in order to propose a continuous online procedure for damage detection based on train-induced dynamic bridge responses, taking advantage of the large-magnitude loading for enhancing sensitivity to small-scale structural changes. While such large responses induced by trains might create more damage-sensitive information in the measured response, it also amplifies the effects on those measurements from the environment. Thus, one of the biggest contributions herein is a methodology that exploits the large bridge responses induced by train passage while rejecting the confounding influences of the environment in such a way that false positive detections are mitigated. Furthermore, this research work introduces an adaptable confidence decision threshold that further improves damage detection over time. To ensure an online continuous assessment, a hybrid combination of autoregressive exogenous input (ARX) models, principal components analysis (PCA), and clustering algorithms was sequentially applied to the monitoring data, in a moving window process. A comparison between the performance obtained from autoregressive (AR) and ARX models as feature extractors was conducted, and it was concluded that ARX models lead to increased sensitivity to damage due to their ability to capture cross information between the sensors. The PCA proved its importance and effectiveness in removing observable changes induced by variations in train speed or temperature without the need to measure them, and the clustering methods allowed for an automatic classification of the damage-sensitive features. Since it was not possible to introduce damage to the bridge, several structural conditions were simulated with a highly reliable digital twin of the Sado Bridge, tuned with experimental data acquired from a SHM system installed on site, in order to test and validate the efficiency of the proposed procedure. The strategy proved to be robust when detecting a comprehensive set of damage scenarios with a false detection incidence of 2%. Moreover, it showed sensitivity to smaller damage levels (earlier in life), even when it consists of small stiffness reductions that do not impair structural safety and are imperceptible in the original signals.}
}
@article{GUPTA2022105239,
title = {A hybrid partitioned deep learning methodology for moving interface and fluid–structure interaction},
journal = {Computers & Fluids},
volume = {233},
pages = {105239},
year = {2022},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2021.105239},
url = {https://www.sciencedirect.com/science/article/pii/S0045793021003479},
author = {Rachit Gupta and Rajeev Jaiman},
keywords = {Fluid–structure interaction, Deep learning-based reduced-order model, Proper orthogonal decomposition, Convolutional autoencoder, Long short-term memory network, Digital twin},
abstract = {In this work, we present a hybrid partitioned deep learning framework for the reduced-order modeling of moving interfaces and predicting fluid–structure interaction. Using the discretized Navier–Stokes in the arbitrary Lagrangian–Eulerian reference frame, we generate the full-order flow snapshots and point cloud displacements as target physical data for the learning and inference of coupled fluid–structure dynamics. The hybrid operation of this methodology comes by combining two separate data-driven models for fluid and solid subdomains via deep learning-based reduced-order models (DL-ROMs). The proposed multi-level framework comprises the partitioned data-driven drivers for unsteady flow and the moving point cloud displacements. At the fluid–structure interface, the force information is exchanged synchronously between the two partitioned subdomain solvers. The first component of our proposed framework relies on the proper orthogonal decomposition-based recurrent neural network (POD-RNN) as a DL-ROM procedure to infer the point cloud with a moving interface. This model utilizes the POD basis modes to reduce dimensionality and evolve them in time via long short-term memory-based recurrent neural networks (LSTM-RNNs). The second component employs the convolution-based recurrent autoencoder network (CRAN) as a self-supervised DL-ROM procedure to infer the nonlinear flow dynamics at static Eulerian probes. We introduce these probes as spatially structured query nodes in the moving point cloud to treat the Lagrangian-to-Eulerian conflict together with convenience in training the CRAN driver. To determine these Eulerian probes, we construct a novel snapshot-field transfer and load recovery algorithm. They are chosen in such a way that the two components (i.e., POD-RNN and CRAN) are constrained at the interface to recover the bulk force quantities. These DL-ROM-based data-driven drivers rely on the LSTM-RNNs to evolve the low-dimensional states. A popular prototypical fluid–structure interaction problem of flow past a freely oscillating cylinder is considered to assess the efficacy of the proposed methodology for a different set of reduced velocities that lead to vortex-induced vibrations. The proposed framework tracks the interface description with acceptable accuracy and predicts the nonlinear wake dynamics over the chosen test data range. The proposed framework aligns with the development of partitioned digital twin of engineering systems, especially those involving moving boundaries and fluid–structure interactions.}
}
@article{GHIASABADIFARAHANI2022101978,
title = {Adaptive personalized recommender system using learning automata and items clustering},
journal = {Information Systems},
volume = {106},
pages = {101978},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101978},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001654},
author = {Mansoureh {Ghiasabadi Farahani} and Javad {Akbari Torkestani} and Mohsen Rahmani},
keywords = {Recommender system, Personalization, Adaptive user profile, Clustering, Learning automata},
abstract = {The personalized recommender systems provide user-related services based on user preferences; these preferences are recorded in an individual profile. Therefore, the more complete and precise each user profile leads more successful the recommendation process. The people’s interests change over time though traditional researches do not follow these changes regularly. Under such circumstances, designing an efficient user model to track users’ interests is greatly important. In the current study, we suggest an algorithm to create the learning automata-based user profiling. Due to many items and the commonality of features between them, we clustered items. In this technique, a learning automaton is assigned to the active user. The learning automaton adjusts the amount of user interest in each cluster based on user feedback. As the user interactions with the system increase, the internal state of the learning automaton converges towards the user’s genuine interests in the item clusters. The experimental results demonstrate that our algorithm outperforms compared approaches in precision, recall, RMSE, and MAE. In addition, the proposed algorithm for new users has acceptable performance.}
}
@article{GAO2022108233,
title = {Discrepant multiple instance learning for weakly supervised object detection},
journal = {Pattern Recognition},
volume = {122},
pages = {108233},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108233},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004143},
author = {Wei Gao and Fang Wan and Jun Yue and Songcen Xu and Qixiang Ye},
keywords = {Weakly supervised detection, Multiple instance learning, Learner discrepancy, Collaborative learning},
abstract = {Multiple Instance Learning (MIL) is a fundamental method for weakly supervised object detection (WSOD), but experiences difficulty in excluding local optimal solutions and may miss objects or falsely localize object parts. In this paper, we introduce discrepantly collaborative modules into MIL and thereby create discrepant multiple instance learning (D-MIL), pursuing optimal solutions in a simple-yet-effective way. D-MIL adopts multiple MIL learners to pursue discrepant yet complementary solutions indicating object parts, which are fused with a collaboration module for precise object localization. D-MIL implements a new “teachers-students” model, where MIL learners act as “teachers” and object detectors as “students”. Multiple teachers provide rich yet complementary information, which are absorbed by students and transferred back to reinforce the performance of teachers. Experiments show that D-MIL significantly improves the baseline while achieves state-of-the-art performance on the challenging MS-COCO object detection benchmark.}
}
@article{PRIYANKA2022100272,
title = {Digital twin for oil pipeline risk estimation using prognostic and machine learning techniques},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100272},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100272},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000704},
author = {E.B. Priyanka and S. Thangavel and Xiao-Zhi Gao and N.S. Sivakumar},
keywords = {Oil pipeline, Risk probability rate, Digital twin, Prognostic-manifold learning},
abstract = {Digital Twin technology is emerging as the digitization platform to enhance the industrial information processing and management in concern with virtual and physical entities. It paves the path for integrated industrial data analysis by combining IoT and Artificial Intelligence for better data interpretation. At present in oil industry, pipelines prevail to be feasible mode, the risk probability rate is getting increased and maintenance system becomes difficult with attention to the earlier prediction of accidents risks by undertaking entire pipeline. This paper aims to provide the frame structure of Digital Twin based on machine learning and prognostics algorithms model to analyze and predict the risk probability rate of oil pipeline system. Prognostics focuses on the detection of a failure precursor by estimating risk condition with respect to the pressure data towards the evaluation of remaining useful life (RUL). The abnormality of pressure attribute is taken in prognostic analysis for risk probability estimation followed by Dirichlet Process Clustering and Canopy clustering to segregate the abnormal pressure drop and rise. Using multiple oil substation data integration platform, the features are extracted using manifold learning methods and the best feature probability rates are evaluated using kernel based SVM algorithm to provide on-time control action on the entire oil pipeline system through efficient wireless data communication between server and the oil substations. As a result, the proposed work creates Virtual Intelligent Integrated Automated Control System to predict the risk rate in oil industry by integrating entire transmission lines through enhanced wireless information networks in remote locations.}
}