@incollection{RAJ20201,
title = {Chapter One - Stepping into the digitally instrumented and interconnected era},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {1-34},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300518},
author = {Pethuru Raj and Jenn-Wei Lin},
keywords = {The internet of things, Cyber-physical systems, Artificial intelligence, Edge computing, Real-time applications},
abstract = {This chapter is to tell all about the digitization-inspired possibilities and opportunities and how software-defined cloud centers are the best fit for hosting and running digital applications. Also, how the next-generation data analytics can be smartly accomplished through cloud platforms and infrastructures is also explained in detail. We are to describe some of the impactful developments and technological advancements brewing in the IT space, how the tremendous amount of data getting produced and processed through cloud systems is to impact the IT and business domains, and how next-generation IT infrastructures are accordingly getting refactored, remedied and readied for the impending big data-induced challenges, how likely the move of the data analytics discipline toward fulfilling the digital universe requirements of extracting and extrapolating actionable insights for the knowledge-parched is, and finally for the establishment and sustenance of the dreamt smarter planet. In short, the uninhibited explosion of digitized systems and connected devices pour out a tremendous amount of multi-structured data and the impending challenge is to make sense out of the data heaps. Data analytics is the way to go and in the recent past, the overwhelming trend is to empower our everyday systems with machine and deep learning algorithms to automatically learn out of data heaps and streams in order to be distinctively intelligent in their actions and reactions. This chapter is specially prepared to put a stimulating foundation for explaining the nitty-gritty of the Digital Twin paradigm.}
}
@article{WU2019127,
title = {Understanding students’ mimicry, emulation and imitation of genre exemplars: An exploratory study},
journal = {English for Specific Purposes},
volume = {54},
pages = {127-138},
year = {2019},
issn = {0889-4906},
doi = {https://doi.org/10.1016/j.esp.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0889490618300334},
author = {Zhiwei Wu},
keywords = {Genre exemplars, Imitative learning, Imitation, Emulation, Mimicry},
abstract = {Guided by the Vygotskian concepts of mimicry, emulation, and imitation, this study examines how eight Chinese EFL students modeled from genre exemplars when composing their first academic papers in university. The students were enrolled in a “Cultural Tourism Studies” course at a university in south China. The course was delivered in a co-teaching approach with an embedded six-week EAP workshop. Given the short time frame of the workshop, an ESP genre-based approach was adopted, and genre exemplars were used to scaffold the students' understanding of two focal genre features (i.e., citation and organization). At the end of the course, text-based interviews were conducted to understand how the students made sense of and used the genre exemplars in the composing process. The analysis of students' term papers revealed a varying amount of mimicry, emulation, and imitation of the target genre features. The introspective and retrospective accounts of students' engagement with the exemplars suggested that imitative learning was multifaceted, dynamic, and varied within and between individuals. These findings challenge the dichotomous characterization of exemplars as either an enabling scaffold or a constraining shackle. The paper also discusses how the tripartite Vygotskian framework is a useful heuristic for EAP instructors to assess the extent to which genre exemplars are attuned to students’ zone of proximal development, and how genre exemplars offer fluid affordances in the process of object-, other-, and self-regulation.}
}
@article{RALPH2020253,
title = {An Implementation Approach for an Academic Learning Factory for the Metal Forming Industry with Special Focus on Digital Twins and Finite Element Analysis},
journal = {Procedia Manufacturing},
volume = {45},
pages = {253-258},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.103},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920311458},
author = {Benjamin James Ralph and Andreas Schwarz and Martin Stockinger},
keywords = {Learning Factory, Digital Twin, Industry 4.0, Digitalization, Smart Factory},
abstract = {The requirements for the planning, implementation and operation of an academic learning factory vary depending on the specific area of the respective institution. This paper provides an approach for the planning and implementation of such a factory, specifically tailored to the requirements of the metal forming industry. This learning factory will then be operated at the Chair of Metalforming at the Montanuniversität Leoben (MUL). The objective is to monitor and control forming units of different technological maturity in a common system. The industrial software used, ibaPDA for data logging and ibaAnalyzer for automated further processing, is widespread in practice and enables students to learn the required skills as close to practice as possible. In addition, Analog to Digital (A/D) converters and machine hour counters will be implemented to illustrate the retrofitting approach in practice. For the planning and implementation of Digital Shadows and Digital Twins, common Finite Element (FE) simulation programs will be used and the possibilities of connectivity between machines, simulation programs and automation software will be demonstrated. The project presented here should thus make an important contribution to the training of future specialists with special consideration of the increasing interdisciplinarity in manufacturing technology.}
}
@article{ZAMBRANO2020100419,
title = {TWINKLE: A digital-twin-building kernel for real-time computer-aided engineering},
journal = {SoftwareX},
volume = {11},
pages = {100419},
year = {2020},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2020.100419},
url = {https://www.sciencedirect.com/science/article/pii/S2352711019300664},
author = {V. Zambrano and R. Rodríguez-Barrachina and S. Calvo and S. Izquierdo},
keywords = {Model order reduction, PARAFAC, Machine learning, Data analysis, Tensor decomposition},
abstract = {TWINKLE is a library for building families of solvers to perform Canonical Polyadic Decomposition (CPD) of tensors. The common characteristic of these solvers is that the data structure supporting the tuneable solution strategy is based on a Galerkin projection of the phase space. This allows processing and recovering tensors described by highly sparse and unstructured data. For achieving high performance, TWINKLE is written in C++ and uses the Armadillo open source library for linear algebra and scientific computing, based on LAPACK (Linear Algebra PACKage) and BLAS (Basic Linear Algebra Subprograms) routines. The library has been implemented keeping in mind its future extensibility and adaptability to fulfil the different users’ needs in academia and industry regarding Reduced Order Modelling (ROM) and data analysis by means of tensor decomposition. It is especially focused on post-processing data from Computer-Aided-Engineering (CAE) simulation tools.}
}
@incollection{SAINI2020273,
title = {Chapter 14 - Language learnability analysis of Hindi: a comparison with ideal and constrained learning approaches},
editor = {G.R. Sinha and Jasjit S. Suri},
booktitle = {Cognitive Informatics, Computer Modelling, and Cognitive Science},
publisher = {Academic Press},
pages = {273-290},
year = {2020},
isbn = {978-0-12-819445-4},
doi = {https://doi.org/10.1016/B978-0-12-819445-4.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012819445400014X},
author = {Sandeep Saini and Vineet Sahula},
keywords = {Second language acquisition, language processing, cognitive model for language},
abstract = {Native language acquisition is one of the first tasks undertaken by the human brain in the infant stage of life. The linguist community has always been interested in finding the method adopted by the human brain to acquire the native language. Word segmentation is one of the most important tasks in language acquisition. Statistical learning has been proposed to be one of the earliest strategies that an infant can adapt to segment a lot of different words. The language learnability theories are supposed to be universal in nature and work on all the languages. In this work, we have analyzed the learnability of the most popular Indian language, Hindi, based on ideal (universal) and constrained Bayesian learner models. We have analyzed the learnability of the language using unigram and bigram approaches by considering word, syllables, and phonemes as the smallest unit of the language. We demonstrate that Bayesian inference is indeed a viable crosslinguistic strategy and can be used on Hindi as well.}
}
@article{KHAN202013,
title = {On the requirements of digital twin-driven autonomous maintenance},
journal = {Annual Reviews in Control},
volume = {50},
pages = {13-28},
year = {2020},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2020.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578820300560},
author = {Samir Khan and Michael Farnsworth and Richard McWilliam and John Erkoyuncu},
keywords = {Digital twin, Autonomous systems, Maintenance, Fault detection and isolation, Reinforcement learning},
abstract = {Autonomy has become a focal point for research and development in many industries. Whilst this was traditionally achieved by modelling self-engineering behaviours at the component-level, efforts are now being focused on the sub-system and system-level through advancements in artificial intelligence. Exploiting its benefits requires some innovative thinking to integrate overarching concepts from big data analysis, digitisation, sensing, optimisation, information technology, and systems engineering. With recent developments in Industry 4.0, machine learning and digital twin, there has been a growing interest in adapting these concepts to achieve autonomous maintenance; the automation of predictive maintenance scheduling directly from operational data and for in-built repair at the systems-level. However, there is still ambiguity whether state-of-the-art developments are truly autonomous or they simply automate a process. In light of this, it is important to present the current perspectives about where the technology stands today and indicate possible routes for the future. As a result, this effort focuses on recent trends in autonomous maintenance before moving on to discuss digital twin as a vehicle for decision making from the viewpoint of requirements, whilst the role of AI in assisting with this process is also explored. A suggested framework for integrating digital twin strategies within maintenance models is also discussed. Finally, the article looks towards future directions on the likely evolution and implications for its development as a sustainable technology.}
}
@article{DAVID2019349,
title = {Attaining Learning Objectives by Ontological Reasoning using Digital Twins},
journal = {Procedia Manufacturing},
volume = {31},
pages = {349-355},
year = {2019},
note = {Research. Experience. Education. 9th Conference on Learning Factories 2019 (CLF 2019), Braunschweig, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.03.055},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919304196},
author = {Joe David and Andrei Lobov and Minna Lanz},
keywords = {Digital Twin, Learning Outcomes, ontology, Reasoning, Web Ontology Language (OWL), Learning, Pedagogy},
abstract = {Learning Factories provide a propitious learning environment for nurturing production related competencies. However, several problems continue to plague their widespread adoption. This study mentions these issues before proposing the use of digital twins as an alternative. The study presents an approach towards modelling such a digital twin and proposes a solution that uses ontologies to develop a formal representation of the domain (a flexible manufacturing system) and the learning that occurs in the environment. A reasoning mechanism is used deduce inferences from the ontology to facilitate automated assessment of the learner. A use-case for the pedagogic digital twin is presented and discussed before proposing future directions for work.}
}
@article{MIAO20202154,
title = {Humming-Query and Reinforcement-Learning based Modeling Approach for Personalized Music Recommendation},
journal = {Procedia Computer Science},
volume = {176},
pages = {2154-2163},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.252},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920321566},
author = {Dezhuang Miao and Xuesong Lu and Qiwen Dong and Daocheng Hong},
keywords = {Playlist Prediction, Recommendation, User Model, Interactive Learning},
abstract = {Music recommendation is a prominent application of recommender systems, which has been attracting more and more attentions. There are two research streams of music recommender systems: one is static recommendation based on learning user’s preference according to historical data, and the other is dynamic recommendation considering user’s feedback. But the individual music preference for a certain moment is closely related to personal experience of the music and music literacy, as well as temporal scenario with diversity. Thus, it’s necessary to design a new music recommendation framework by integrating static recommendation and dynamic recommendation. Therefore, we propose a novel approach for music recommendation HRRS (Humming-Query and Reinforcement-Learning based Recommender Systems) by integrating prior two research streams. This novel recommendation framework HRRS based on humming query and reinforcement learning is learning and adapting to user’s current preference continually by collecting interactive data in real time. This preliminary recommendation framework captures song characters, personal dynamic preferences, and yields a better listening experience with proper interaction.}
}
@article{BORDATCHEV2020159,
title = {Preliminary experimental analysis of the surface topography formation during laser polishing H13 tooling steel using statistical characteristics of the surface amplitude distribution},
journal = {Procedia Manufacturing},
volume = {48},
pages = {159-164},
year = {2020},
note = {48th SME North American Manufacturing Research Conference, NAMRC 48},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920314840},
author = {Evgueni V. Bordatchev and Srdjan J. Cvijanovic and Remus O. Tutunea-Fatan},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Surface finish is one of the most important quality characteristics of fabricated components. To complement that, laser polishing (LP) is one of the advanced manufacturing surface finishing techniques that has been recently developed and successfully employed for improving surface quality without deteriorating the overall structural form through surface smoothing by melting and redistributing a thin layer of molten material. This paper proposes a statistical digital twin of the LP process and demonstrates the applicability of amplitude distribution statistical characteristics in the experimental analysis of surface topography formation during LP process. Initially, the thermodynamic transformation of the initial surface topography is considered by means of technical cybernetics and machine learning approaches to describe two of the most critical LP process components, namely: thermodynamic melting and solidification of both solid material and surface topography. To exemplify the effective application of statistical amplitude distribution characteristics, LP experiments were conducted with two different laser powers (25 W and 100 W) on flat and ground initial surfaces and resulting surface topographies were measured. Several amplitude distribution characteristics, such as roughness average value, averaged transverse profile as a W-shape, averaged transverse roughness profile, and probability distribution function were calculated. After that, actual molten material area, volume redistribution and final surface quality were comparatively analyzed. It was shown that the proportion between two components of the LP thermodynamic transformation and surface topography is critically dependent on laser power. As such, during low-power conditions (< 25 W), surface quality is predominantly determined by the thermodynamic transformation of initial surface topography and therefore only this component can be used for statistically reliable LP process modelling and digital identification. In summary, amplitude distribution characteristics have several advantages in building a comprehensive understanding of the molten material redistributing along and across LP line.}
}
@article{BAKLIWAL20181237,
title = {A Multi Agent System architecture to implement Collaborative Learning for social industrial assets⁎⁎This research was funded by the Royal Academy of Engineering under the Newton Bhabha scheme (Project No. HEPI—1516—10). This research was supported by SustainOwner, a project sponsored by the EU Framework Programme Horizon 2020, MSCA-RISE-2014: Marie Skodowska-Curie Research and Innovation Staff Exchange (Rise) (grant agreement number 645733 Sustain-owner H2020-MSCA-RISE-2014).},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1237-1242},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.421},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318315477},
author = {Kshitij Bakliwal and Maharshi Harshadbhai Dhada and Adrià Salvador Palau and Ajith Kumar Parlikad and Bhupesh Kumar Lad},
keywords = {Cyber-Physical Systems, Industrial Internet of Things, Digital Twins, Collaborative Learning, Industry Automation, Multi Agent Systems, Distributed Computing},
abstract = {The ‘Industrial Internet of Things’ aims to connect industrial assets with one another and benefit from the data that is generated, and shared, among these assets. In recent years, the extensive instrumentation of machines and the advancements in Information Communication Technologies are re-shaping the role of assets in our industrial systems. An emerging concept here is that of ‘social assets’: assets that collaborate with each other in order to improve system optimisation. Cyber-Physical Systems (CPSs) are formed by embedding the assets with computers, or microcontrollers, which run real-time decision-making algorithms over the data originating from the asset. These are known as the ‘Digital Twins’ of the assets, and form the backbone of social assets. It is essential to have an architecture which enables a seamless integration of these technological advances for an industry. This paper proposes a Multi Agent System (MAS) architecture for collaborative learning, and presents the findings of an implementation of this architecture for a prognostics problem. Collaboration among assets is performed by calculating inter-asset similarity during operating condition to identify ‘friends’ and sharing operational data within these clusters of friends. The architecture described in this paper also presents a generic model for the Digital Twins of assets. Prognostics is demonstrated for the C-MAPSS turbofan engine degradation simulated data-set (Saxena and Goebel (2008)).}
}
@article{ERTVELDT2020456,
title = {MiCLAD as a platform for real-time monitoring and machine learning in laser metal deposition},
journal = {Procedia CIRP},
volume = {94},
pages = {456-461},
year = {2020},
note = {11th CIRP Conference on Photonic Technologies [LANE 2020]},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.09.164},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120313524},
author = {Julien Ertveldt and Patrick Guillaume and Jan Helsen},
keywords = {Laser Metal Deposition (LMD), Machine learning, Real-time monitoring, Feed-back control},
abstract = {The MiCLAD machine designed at the VUB, Belgium, allows for closed-loop controlled laser metal deposition including various in-situ optical based measurement systems. These integrated sensors collect information on deposition geometry and temperature during the building process. Hence, each cubic millimeter of material that is either added or removed is mapped to its digital twin with a millisecond temporal resolution in the machines database. This paper introduces the platform and its capabilities by focusing on the procedure of obtaining the necessary training data for the future application of machine learning algorithms, with the goal of controlling the geometry and temperature history during additive manufacturing.}
}
@article{TROUSSAS2020103698,
title = {Collaboration and fuzzy-modeled personalization for mobile game-based learning in higher education},
journal = {Computers & Education},
volume = {144},
pages = {103698},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2019.103698},
url = {https://www.sciencedirect.com/science/article/pii/S0360131519302519},
author = {Christos Troussas and Akrivi Krouska and Cleo Sgouropoulou},
keywords = {Advice generator, Collaborative learning, Fuzzy logic, Mobile game-based learning, Personalized learning},
abstract = {Mobile game-based learning constitutes a hot issue in the related scientific literature since it promotes learning through an entertaining way and fosters student motivation to increase engagement in the educational process. As such, it can enhance the learning process and improve student participation. Towards this direction, this paper investigates how mobile learning and game-based learning can be utilized in higher education settings and analyzes the pedagogical affordance of their adoption. As a testbed for our research, we designed and implemented Quiz Time! which is an intelligent mobile game-based learning application for assessing and advancing learners' knowledge in the programming language C#. Quiz Time! employs an assessing knowledge module for testing the knowledge of learners, a vectorial-based recommendation module for proposing personalized collaboration in group playing, a dynamic fuzzy logic-based advice generator for tailored assistance to learners' profile and misconceptions, and a cognitive learner modeler supporting the aforementioned modules. Quiz Time! was used in a higher education institution for an academic semester and was evaluated by students and computer science experts using an established framework and the statistical hypothesis test. Regarding the evaluation results, the computer science experts validated the pedagogical adequacy of the application and the students highlighted its positive impact on learning and its usefulness. A major conclusion is that incorporating personalization and collaboration in mobile game-based learning can further assist students in higher education towards advancing their knowledge level.}
}
@article{RIERA202017610,
title = {Experience feedback and innovative pedagogical applications with HOME I/O},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17610-17615},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2676},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320334388},
author = {B. Riera and T. Ranger and R. Saddem and F. Emprin and J.-P. Chemla and A. Philippot},
keywords = {simulation tool, control education, innovative pedagogical approaches, STEM education, virtual house, digital twin},
abstract = {In the previous IFAC World Congress at Toulouse in 2017, HOME I/O was introduced as an innovative pedagogical tool. This software is the result of a R&D project between CReSTIC lab from the University of Reims Champagne-Ardenne (URCA) and Real Games, partially founded by the French Ministry of National Education, in order to design a virtual house adapted to control and STEM (Science, Technology, Engineering and Mathematics) education. The main idea, from the beginning, has been to bring a virtual house into the classroom, adapted to learners and teachers and usable from middle schools to universities. To perform this goal, a free soft PLC called CONNECT I/O, enables to connect HOME I/O to external automation technologies (e.g. PLC, Modbus TCP, OPC DA, microcontrollers...). All over the world, around 800 middle, high schools and universities use HOME I/O. Teachers and students’ feed-back have been taken into account by updating HOME I/O with new add-ons and features like: a simpler licensing system (2016), Scratch 2.0 (2017) integration and Python 3.x integration (2019). This paper presents these new features and a selection of some innovative pedagogical applications performed by teachers from middle school, high school, university, and even primary school! HOME I/O seems to prove that it is possible to have one simulation tool adapted to different levels of training and enabling original pedagogical approaches: flipped classroom, pedagogic digital twin, learning from errors, projects…}
}
@article{VILORIA20191225,
title = {An intelligent approach for the design and development of a personalized system of knowledge representation},
journal = {Procedia Computer Science},
volume = {151},
pages = {1225-1230},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.176},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930643X},
author = {Amelec Viloria and Omar Bonerge {Pineda Lezama}},
keywords = {adaptive hypermedia, ontologies, knowledge representation, user modeling, interface design tools, teaching on the web, algorithm for advanced cluster vector page ranking},
abstract = {This article proposes a generic presentation system for hypermedia systems of adaptive teaching that is highly independent from the representation of domain knowledge and the application state maintenance. Generality is achieved by providing an application framework for the definition of ontologies that best fit a domain or a specific author. The presentation of the pages to be generated is described in terms of classes and relationships of the ontology. For this purpose, a web page ranking algorithm based on automatic learning is used, specifically, the algorithm for Advanced Cluster Vector Page Ranking (ACVPR). This algorithm provides the user a powerful meta-search tool that presents a ranking order of the web page to quickly meet custom needs, especially when the search is erroneous or incomplete.}
}
@article{BAUER2018147,
title = {Integration of Industrie 4.0 in Lean Manufacturing Learning Factories},
journal = {Procedia Manufacturing},
volume = {23},
pages = {147-152},
year = {2018},
note = {“Advanced Engineering Education & Training for Manufacturing Innovation”8th CIRP Sponsored Conference on Learning Factories (CLF 2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918304803},
author = {Harald Bauer and Felix Brandl and Christopher Lock and Gunther Reinhart},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Industrie 4.0 is referred to as an umbrella term for various digital concepts e.g. IoT, CPS, Big Data, Data Analytics, Digital Twin, Digital Shadow, HRC, etc. Said concepts promise new potentials for production planning and steering (PPS) optimization. In particular, data availability is an enabler for an efficiency increase in PPS. Managers of lean manufacturing systems question how to integrate these new possibilities into the existing philosophy and optimization projects. It is currently uncertain whether Industrie 4.0 approaches replace or revive lean manufacturing. Within the iwb’s learning factory, we illustrate lean and Industrie 4.0 as complementary approaches by postulating five theories concerning their interaction. This paper presents the introduction of Industrie 4.0 into the program of the learning factory by proposing two integrated teaching. The concept was successfully implemented within the iwb’s learning factory.}
}
@article{LOEKEN201862,
title = {Design Principles Behind the Construction of an Autonomous Laboratory-Scale Drilling Rig},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {8},
pages = {62-69},
year = {2018},
note = {3rd IFAC Workshop on Automatic Control in Offshore Oil and Gas Production OOGP 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.06.356},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318306864},
author = {Erik A. Loeken and Alexander Trulsen and Andrew M. Holsaeter and Ekaterina Wiktorski and Dan Sui and Robert Ewald},
keywords = {Drilling Automation, ROP Optimization, Modeling, Fault Detection, Drill String Dynamics},
abstract = {In recent years, hot topics such as digitalization, machine learning, digital twin and big data have evolved from being envisions on the paper to state of art solutions, expected to revolutionize drilling efficiency in the industry. Drilling automation tomorrow is all about exploiting the current state of technologies available to the entire operation of drilling a well. Not only can drilling automation limit costs and reduce the risk to rig personnel and the environment, but they also give access to locations of considerable potential that previously have been regarded unsafe or uneconomical to operate in. There are however some challenges in keeping up with the ever-increasing pace of the development. For one, testing of novel and innovative solutions is often very expensive because of non-productive rig time during implementation, trial runs and data evaluation. Also, the modern technologies require extensive R&D before on-site testing can even commence. While on land-rigs, some of these costs and risks can be greatly minimized, many offshore solutions lack that luxury. This paper presents an overview of the design principles that go into the construction of a fully autonomous laboratory-scale drilling rig at the University of Stavanger. It aims at describing 1) the engineering principles involved to resemble full-scale drilling operations on the laboratory scale, 2) design considerations and components, 3) component requirements for the rig, 4) control system algorithms for real-time optimization of drilling parameters and detection and handling of drilling anomalies, 5) development of drilling models (drill string dynamics, bit-vibration, etc.) and 6) benefits and future work with the laboratory-scale system. Some of the concepts that are presented in this paper have yet to be implemented during 2018.}
}
@article{UMEDA2020325,
title = {Exercise of digital kaizen activities based on ‘digital triplet’ concept},
journal = {Procedia Manufacturing},
volume = {45},
pages = {325-330},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920310635},
author = {Yasushi Umeda and Jun Ota and Shouhei Shirafuji and Fumio Kojima and Masahiro Saito and Hiroki Matsuzawa and Takuji Sukekawa},
keywords = {learning factory, digital triple, kaizen, cyber physical systems, manufacturing system},
abstract = {For supporting kaizen (continuous improvement) activities typically observed in Japanese manufacturing companies, we are developing the concept of ‘Digital Triple’ as an extension of Digital Twin. Digital Triplet contains intelligent activity world in addition to the cyber world and the physical world and emphasizes the ability of knowledge-based support for manufacturing system engineers. At the same time, we are developing an education program based on Digital Triplet. This program contains an exercise in which students execute kaizen with a prototype CPS system of a learning factory. This paper reports the contents of this exercise in relation with Digital Triplet and describes the results of two trial sessions of this exercise.}
}
@article{GAO2018107,
title = {On the teaching complexity of linear sets},
journal = {Theoretical Computer Science},
volume = {716},
pages = {107-123},
year = {2018},
note = {Special Issue on ALT 2015},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2017.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S030439751730854X},
author = {Ziyuan Gao and Hans Ulrich Simon and Sandra Zilles},
keywords = {Teaching complexity, Teaching dimension, Recursive teaching dimension, Linear sets},
abstract = {Linear sets are the building blocks of semilinear sets, which are in turn closely connected to automata theory and formal languages. Prior work has investigated the learnability of linear sets and semilinear sets in three models – Valiant's PAC-learning model, Gold's learning in the limit model, and Angluin's query learning model. This paper considers teacher–learner models of learning families of linear sets, in which a benevolent teacher presents a set of labelled examples to the learner. First, we study the classical teaching model, in which a teacher must successfully teach any consistent learner. Second, we will apply a generalisation of the recently introduced recursive teaching model to several infinite classes of linear sets, and show that thus the maximum sample complexity of teaching these classes can be drastically reduced compared to classical teaching. To this end, a major focus of the paper will be on determining two relevant teaching parameters, the teaching dimension and recursive teaching dimension, for various families of linear sets.}
}
@article{GARRIDO20191814,
title = {Integration of automatic generated simulation models, machine control projects and management tools to support whole life cycle of industrial digital twins.},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1814-1819},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.465},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319314466},
author = {J. Garrido and J. Sáez},
keywords = {Simulation, Industry Automation, Integration, CASE, Maintenance},
abstract = {The paper presents a framework of automatic generation of industrial digital twins. These digital twins will be suitable to support preliminary design phases of systems development, but also to support next phases of detailed designs implementation and systems running phases. These digital twin allow, from the preliminary designing phase, to generate a complete simulation of the target industrial system. But, at the same time, and without the need to develop and add any subsequent code, they should be a valuable support for the phases and tasks of exploitation: maintenance, machine or system learning, etc. The problem is that the requirements for first development phases are much more generic than those for later phases. For this reason, instead of incorporating specificities in the simulation system, the framework takes advantage of the applications which are being developed for the implementation of the real system. In these applications (the control program and the decisions and the high level management system), the specificities have had to be taken into account. The system has been specialized in industrial transportation and warehouse systems which, although have a finite number or building objects, they have an infinite set of final configurations, very different one from each other. The paper presents an evaluation of current simulation platforms suitable to be used as part of the framework, and the digital twin industrial system generation framework itself. An example of application is as well presented.}
}
@article{LUNDQVIST201924,
title = {The burden of smartness: Teacher's pet and classmates’ teasing in a Danish classroom},
journal = {Linguistics and Education},
volume = {52},
pages = {24-32},
year = {2019},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2019.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0898589817303145},
author = {Ulla Lundqvist},
keywords = {Classroom discourse, Social inequity, Student identities, Smartness, Linguistic ethnography, Social identification},
abstract = {Schools are sites of negotiation of what it means to be ''smart", and which students are viewed as smart. This is a pertinent problem for educational scholars, teacher educators, and teachers, because struggles concerning smartness foster social inequity. While much research accentuates the inequity that occurs when those students who do not fit the “smart” category are marginalized, the inequities that emerge when teachers prefer the smart student have not received much scholarly attention. Drawing on linguistic ethnographic fieldwork in a primary school in Copenhagen, Denmark, this paper explores how one student, over the course of two years and two months, comes to inhabit the “smart” role, and must then cope with being favoured by the teachers and ostracised by peers. Dual pressures such as these have implications for education and research.}
}
@article{CATAL201999,
title = {Aligning Education for the Life Sciences Domain to Support Digitalization and Industry 4.0},
journal = {Procedia Computer Science},
volume = {158},
pages = {99-106},
year = {2019},
note = {3rd WORLD CONFERENCE ON TECHNOLOGY, INNOVATION AND ENTREPRENEURSHIP"INDUSTRY 4.0 FOCUSED INNOVATION, TECHNOLOGY, ENTREPRENEURSHIP AND MANUFACTURE" June 21-23, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311901},
author = {Cagatay Catal and Bedir Tekinerdogan},
keywords = {Education, Internet of Things (IoT), Industry 4.0, data analytics, machine learning},
abstract = {Emerging technologies like Internet of Things, Data Science, Deep Learning, Augmented Reality, Edge Computing, and Digital Twins are bringing new opportunities, challenges, and solutions for many domains including agriculture, plant sciences, animal sciences, food sciences, and social sciences. These disruptive technologies are at the center of the fourth industrial revolution, but are we ready yet to educate and prepare new generations to help society, science, and humanity adapt them? How can we change the current curriculum to reflect these technological innovations? How can we help the new generation to develop not only left-brain skills but also right-brain skills? The Netherlands is the second largest food exporter in the world after the United States and the agricultural related exports generated €45 Billion in 2018 for the economy. R&D in Dutch companies and innovation in universities in the Netherlands play an important and active role in this context. In this paper, we provide a general framework for supporting education in the context of Industry 4.0. We adopt the case study of Wageningen University at which we were actively involved in designing and customizing academic courses related to Industry 4.0. Wageningen University, which has the highest rank in the field of Agriculture & Forestry according to influential university rankings and has a rank 59 according to Times Higher Education, is traditionally a life science university but has taken also an active strategy for aligning with the developments in IT and Artificial Intelligence. Apart from the content-wise shift, skills such as critical thinking, creativity, and problem-solving are addressed by applying project-based evaluations. We discuss the lessons learned and address the issues related to Industry 4.0 and education.}
}
@article{DING2019106957,
title = {Compressing CNN-DBLSTM models for OCR with teacher-student learning and Tucker decomposition},
journal = {Pattern Recognition},
volume = {96},
pages = {106957},
year = {2019},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0031320319302547},
author = {Haisong Ding and Kai Chen and Qiang Huo},
keywords = {Optical character recognition, CNN-DBLSTM Character model, Model compression, Teacher-student learning, Tucker decomposition},
abstract = {Integrated convolutional neural network (CNN) and deep bidirectional long short-term memory (DBLSTM) based character models have achieved excellent recognition accuracies on optical character recognition (OCR) tasks, along with large amount of model parameters and massive computation cost. To deploy CNN-DBLSTM model in products with CPU server, there is an urgent need to compress and accelerate it as much as possible, especially the CNN part, which dominates both parameters and computation. In this paper, we study teacher-student learning and Tucker decomposition methods to reduce model size and runtime latency for CNN-DBLSTM based character model for OCR. We use teacher-student learning to transfer the knowledge of a large-size teacher model to a small-size compact student model, followed by Tucker decomposition to further compress the student model. For teacher-student learning, we design a novel learning criterion to bring in the guidance of succeeding LSTM layer when matching the CNN-extracted feature sequences of the large teacher and small student models. Experimental results on large scale handwritten and printed OCR tasks show that, using teacher-student learning alone achieves 9.90 ×  footprint reduction and 15.23 ×  inference speedup yet without degrading recognition accuracy. Combined with Tucker decomposition method, we can compress and accelerate the model further. The decomposed model achieves 11.89 ×  footprint reduction and 22.16 ×  inference speedup while suffering no or only a small recognition accuracy degradation against the large-size baseline model.}
}
@article{BORANGIU2019150,
title = {Digital transformation of manufacturing through cloud services and resource virtualization},
journal = {Computers in Industry},
volume = {108},
pages = {150-162},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519300107},
author = {Theodor Borangiu and Damien Trentesaux and André Thomas and Paulo Leitão and Jose Barata},
keywords = {Digital manufacturing, Cloud services, Resource virtualization, Cloud manufacturing, Holonic manufacturing control, Multi-agent system, Real-time data analysis, Machine learning, Digital twin, Cyber physical production system, Industrial internet of things},
abstract = {This editorial introduces the special issue in the Elsevier journal Computers in Industry that analyses how the digital transformation of manufacturing is speeded up by two important drivers: cloud services and resource virtualization, which are vital for implementing the main building blocks - Cyber Physical Production Systems and Industrial Internet of Things - in the “Industry of the future” framework. The context of this special issue is firstly presented, with a specific focus on the federative concept of Industry 4.0. A framework characterizing research activities led in the field of the digital transformation of manufacturing processes and systems is then introduced. This framework is used to present and position the 12 papers composing the special issue. Perspectives are finally introduced as a guideline for future work in the digital transformation of manufacturing through cloud services and resource virtualization.}
}
@article{NIELSEN2019824,
title = {Low-Cost 3D Scanning in a Smart Learning Factory},
journal = {Procedia Manufacturing},
volume = {38},
pages = {824-831},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.163},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920301645},
author = {Christian P. Nielsen and Ali A. Malik and David G. Hansen and Arne Bilberg},
keywords = {Digital Twins, Point Cloud, Learning Factory, Laser Scanning},
abstract = {With the increased focus over the recent years on digitalizing the factory, 3D scanning has become more and more popular. Acquiring a point cloud of a given factory has shown several benefits, such as better documentation, realistic simulation models, collision detection of materials, visualization of factory development, and more. This paper investigates the testing of a developed low‐cost 3D scanner in a Smart Learning Factory based on parameters identified in literature. Furthermore, the paper compares the developed solution to a commercially available solution. This comparison indicates possible application areas for the developed low-cost solution. The 3D scanner is based on the Microsoft Kinect and a developed hardware platform combined with custom software for acquiring 360° point clouds. A discussion on the acquired results, as well as future works on the developed solution finalizes the paper.}
}
@article{AFININORMADHI2019168,
title = {Identification of personal traits in adaptive learning environment: Systematic literature review},
journal = {Computers & Education},
volume = {130},
pages = {168-190},
year = {2019},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0360131518303026},
author = {Nur Baiti {Afini Normadhi} and Liyana Shuib and Hairul Nizam {Md Nasir} and Andrew Bimba and Norisma Idris and Vimala Balakrishnan},
keywords = {Cooperative/collaborative learning, Intelligent tutoring systems, Interactive learning environments, Navigation},
abstract = {An adaptive learning environment provides personalised information to the learner through self-directed study. An adaptive learning environment model can be subdivided into a learner model, domain model, instructional model and adaptive engine. Personal traits comprise part of the components in a learner model and can be identified either explicitly or implicitly in an adaptive learning environment. In such an environment, the e-learning system should adapt to a learner's needs. However, even though academic research on adaptive learning environments has increased, the field lacks a comprehensive literature analysis of learners' personal traits in these environments. This study conducts a systematic literature review to identify the most commonly used personal traits in modelling the learner and the existing techniques suitable for identifying personal traits in an adaptive learning environment. A total of 140 articles spanning the years 2010–2017 are initially reviewed, from which 78 are selected based on the inclusion and exclusion criteria relevant to this study. This study provides an overview of learners' personal traits and the techniques used to identify them to provide a basis for improving adaptive learning environments. The findings indicate that most of the previous works used a learning style from the cognition learning domain category to model individual personal traits, while the computer-based detection technique was commonly applied to identify a learner's personal traits in adaptive learning environments. This study reveals the common learner characteristics used to develop learner models and the techniques for implementing such models. The findings of this paper can guide other researchers to recognise various personal traits and the identification technique for further studies, as well as assist developers in the development of the adaptive learning system.}
}
@article{SHAN2020113198,
title = {Learn#: A Novel incremental learning method for text classification},
journal = {Expert Systems with Applications},
volume = {147},
pages = {113198},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113198},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420300245},
author = {Guangxu Shan and Shiyao Xu and Li Yang and Shengbin Jia and Yang Xiang},
keywords = {Learn#, Incremental learning, Reinforcement learning},
abstract = {Deep learning is an effective method for extracting the underlying information in text. However, it performs better on closed datasets and is less effective in real-world scenarios for text classification. As the data is updated and the amount of data increases, the models need to be retrained, in what is often a long training process. Therefore, we propose a novel incremental learning strategy to solve these problems. Our method, called Learn#, includes four components: a Student model, a reinforcement learning (RL) module, a Teacher model, and a discriminator model. The Student models first extract the features from the texts, then the RL module filters the results of multiple Student models. After that, the Teacher model reclassifies the filtered results to obtain the final texts category. To avoid increasing the Student models unlimitedly as the number of samples increases, the discriminator model is used to filter the Student models based on their similarity. The Learn# method has the advantage of a shorter training time than the One-Time model, because it only needs to train a new Student model each time, without changing the existing Student models. Furthermore, it can also obtain feedback during application and tune the models parameters over time. Experiments on different datasets show that our method for text classification outperforms many traditional One-Time methods, reducing training time by nearly 80%.}
}
@incollection{WANG202033,
title = {Chapter 2 - Digital twin driven conceptual design},
editor = {Fei Tao and Ang Liu and Tianliang Hu and A.Y.C. Nee},
booktitle = {Digital Twin Driven Smart Design},
publisher = {Academic Press},
pages = {33-66},
year = {2020},
isbn = {978-0-12-818918-4},
doi = {https://doi.org/10.1016/B978-0-12-818918-4.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128189184000026},
author = {Yuchen Wang and Ang Liu and Fei Tao and A.Y.C. Nee},
keywords = {Digital twin, conceptual design, concept generation, design evaluation},
abstract = {In comparison to conventional conceptual design, capability of big data analysis, machine learning ability, and physical–virtual simulations, the digital twin (DT) assists designers to obtain earlier stage feedbacks, more reliable concept generations and evaluations. This chapter will envision the evolution of conceptual design supported by DT technology. Conceptual design methodologies are introduced first, including axiomatic design, systematic design, and function–behavior–structure ontology. Mainly based on the axiomatic design theory, the chapter clarifies how DT technology assists the functional modeling, concept generation, and concept evaluation. In addition, contradiction resolution and constraints management are also part of the discussion. The DT-driven conceptual design is illustrated with a robot vacuum cleaner as an example in each procedure.}
}
@article{JUNMIN201888,
title = {Research on the Construction and Application of Individual Learner Model},
journal = {Procedia Computer Science},
volume = {131},
pages = {88-92},
year = {2018},
note = {Recent Advancement in Information and Communication Technology:},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918305647},
author = {Ye Jun-min and Xu Song and Luo Da-Xiong and Wang Zhi-Feng and Huang Peng-Wei and Xu Chen},
keywords = {individual learner model, learning evaluation, evaluation model},
abstract = {In the background of extensive attention and research on education big data and learning analysis. The model construction method, the statistical analysis method, the empirical method, the inductive method and the deductive method are adopted in this research to probe in depth the problem of learning effect evaluation based on individual learner model: the learning assessment framework for individual learner model is studied, and the evaluation model of learning effect and evaluation index system are summarized of each sub-model, an empirical study is conducted. Compared with the actual learning effect of the learner, the method proposed in this paper is effective [1].}
}
@article{IRIONDO202017592,
title = {A proposal to introduce digitalization technologies within the automation learning process},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17592-17597},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2674},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320334352},
author = {N. Iriondo and D. Orive and O. Casquero and M. Marcos},
keywords = {education, digital factory, product lifecycle management, digital twin, virtual commissioning},
abstract = {Although the digital factory (DF) concept has raised high expectations since its inception, it is still missing industrial impact. One of the problems attributed to this issue is the lack of education curricula for enhancing the related digital competences of the future professionals. Higher education institutions, as major stakeholders in education, should introduce the new technologies for DF in practical courses. However, it is difficult to deal with the complexity of those technologies in a time-limited environment such us a bachelor or a master course. Instead of providing complete knowledge, this paper proposes to focus on the methodological aspects that allow students to acquire the skills needed to handle those technologies. Specifically, this paper illustrates this approach for teaching virtual commissioning (VC) within the automation learning process. The goal is to show the students how to use powerful industrial tools for performing VC through a set of methodological steps that help students manage the complexity of the VC process regardless of the specific tools used for it.}
}
@article{CIANCIOSA2020106671,
title = {Machine learning for analysis of atomic spectral data},
journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
volume = {240},
pages = {106671},
year = {2020},
issn = {0022-4073},
doi = {https://doi.org/10.1016/j.jqsrt.2019.106671},
url = {https://www.sciencedirect.com/science/article/pii/S0022407319302365},
author = {M. Cianciosa and K.J.H. Law and E.H. Martin and D.L. Green},
abstract = {Physics based forward models are the basis on which many experimental diagnostics are interpreted. For some diagnostics, models can be computationally expensive which precludes their use in real time analysis. Reduced models have the potential to capture sufficient physics thereby enabling the desired real time analysis. Using statistical inference and machine learning techniques the application of reduced models for inversion of atomic spectral data used to diagnose magnetic fields in a plasma will be examined. Two approaches are considered, (a) a reduction of the forward model where traditional inversion can be performed on the proxy model, and (b) a reduction of the direct inverse where parameters are a function of measured signal. The resulting inversion is sufficiently fast to be utilized in an online context for digital twinning, and ultimately real-time prediction, design, and control of plasma systems, such as tokamaks. These methods will be demonstrated on both simulated and experimentally measured data.}
}
@incollection{AUGUSTINE202079,
title = {Chapter Four - The industry use cases for the Digital Twin idea},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {79-105},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300610},
author = {Peter Augustine},
keywords = {Digital Twin, Use cases of Digital Twin, IoT, IIot, Smart cities, Virtual replicas, Artificial intelligence, Data analytics},
abstract = {Digital Twin Technology has taken the place in top 10 strategic technology trends in 2017 termed by Gartner Inc. Digital Twin concept brings out the virtual depiction or the digital representation of the real world equipment, device or system whereas the real world and the virtual world gets the highest synchronization. The digital representation of the complete life cycle of a product from its design phase to the maintenance phase will give the prophetic analysis of the problems to the business. This greatest advantage of foreseeing problems in the development of a device will give early warnings, foil downtime, cultivate novel prospects and inventing enhanced devices or gadgets for the later use at the lesser expense by means of digital representations. Indeed, these will devise a larger influence on conveying superior consumer feeling also in the enterprise. The emerging trends such as Artificial Intelligence, Machine Learning, Deep Learning, Internet of Things and Big Data used in Industry 4.0 play a vital role in Digital Twin and they are mostly adopted in the world of manufacturing, Industrial Internet of Things, and automobile business world. The penetration, wide coverage and the advancement of the Internet of Things in real-world have elevated the power of Digital Twins more economical and reachable for the world of various businesses.1.Manufacturing: Digital Twin has brought out the change in the existing manner of the manufacturing segment. Digital Twins have a substantial influence on the design of products and their manufacturing and maintenance. Because of its influence the manufacturing more competent and augmented while dropping throughput times.2.Industrial IoT: Integrating digital twin with industrial firms will facilitate the activities such as monitoring, tracking and controlling industrial systems in digital means. We can potentially experience the power of digital twin since it captures environmental data such as locality, settings of the devices, financial frameworks, etc., other than the operational data, which benefits in foreseeing the forthcoming operations and incongruities.3.Healthcare: Since the healthcare sector demands higher accuracy in diagnosis and treatment, with the important data from IoT, digital twins can play a vital role by reducing the expense for the patient, precautionary alerts to avoid health deterioration and giving tailored health support system. This will be great support especially in developing countries like India.4.Smart cities: Digital Twin coupled with IoT data can augment the efficient planning of the smart city and execution of its building by supplementing financial progress, effectual administration of resources, lessening of environmental impression and escalate the complete worth of a resident's life. The digital twin prototypical can aid city organizers and legislators in the smart city planning by retrieving the visions from numerous sensor networks and smart systems. The information received from the digital twins supports them in reaching well-versed choices concerning the future as well.5.Automobile: Automobile industry can get voluminous benefits out of Digital Twins for producing the simulated framework of a coupled vehicle. It retrieves the behavioral and functional information of the vehicle and services in examining the inclusive performance efficiency of the vehicle as well as the features connected along with it. Digital Twin also supports in supplying a justly enhance support and service for the consumers.6.Retail: Alluring client satisfaction is a fundamental factor in the merchandising world. Digital twin employment can play a key role in supplementing the retail customer experience by forming virtual twins for customers and modeling fashions for them on it. Digital Twins also supports enhanced planning of stock maintenance, safekeeping procedures, and human resource administration in an augmented means.}
}
@article{ZHANG2020105247,
title = {Deep learning-enabled intelligent process planning for digital twin manufacturing cell},
journal = {Knowledge-Based Systems},
volume = {191},
pages = {105247},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105247},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119305611},
author = {Chao Zhang and Guanghui Zhou and Junsheng Hu and Jing Li},
keywords = {Intelligent process planning, Deep learning, Residual networks, Evaluation twin, Digital twin manufacture cell},
abstract = {The transition to intelligent manufacturing provides a fulcrum for the revolution of product lifecycle like design, manufacturing and maintenance, so does it for process planning. Specifically, digital twin manufacturing cell (DTMC) is regarded as a new means of and also a basic unit for implementing intelligent manufacturing. Incorporating process planning in DTMC could improve the integrity of DTMC and enhance the feasibility of process planning. Consequently, this paper proposes a deep learning-enabled framework for intelligent process planning towards DTMC. Firstly, a process knowledge reuse network (PKR-Net) that takes deep residual networks as base architecture is embedding into the framework, which could understand design intents expressed in a drawing or a 3D computer-aided design (CAD) model via its views and automatically retrieve relevant knowledge for the quick generation of theorical processes. Then, an evaluation twin is constructed to transform the theorical processes into practical operations and produce an optimal process plan. Finally, a test bed of the framework is constructed and the experimental results demonstrate the feasibility and effectiveness of the approach.}
}
@article{QIAO20191388,
title = {Digital Twin for Machining Tool Condition Prediction},
journal = {Procedia CIRP},
volume = {81},
pages = {1388-1393},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.049},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119306638},
author = {Qianzhe Qiao and Jinjiang Wang and Lunkuan Ye and Robert X. Gao},
keywords = {tool system, digital twin, deep learning},
abstract = {Digital twin introduces new opportunities for predictive maintenance of manufacturing machines which can consider the influence of working condition on cutting tool and contribute to the understanding and application of the predicted results. This paper presents a data-driven model for digital twin, together with a hybrid model prediction method based on deep learning that creates a prediction technique for enhanced machining tool condition prediction. First, a five-dimensional digital twin model is introduced that highlights the performance of the data analytics in model construction. Next, a deep learning technique, termed Deep Stacked GRU (DSGRU), is demonstrated that enables system identification and prediction. Experimental studies using vibration data measured on milling machine tool have shown the effectiveness of the presented digital twin model for tool wear prediction.}
}
@article{ZANKER2019160,
title = {Measuring the impact of online personalisation: Past, present and future},
journal = {International Journal of Human-Computer Studies},
volume = {131},
pages = {160-168},
year = {2019},
note = {50 years of the International Journal of Human-Computer Studies. Reflections on the past, present and future of human-centred technologies},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2019.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S107158191930076X},
author = {Markus Zanker and Laurens Rook and Dietmar Jannach},
keywords = {Web personalisation, Adaptive systems, Recommender systems},
abstract = {Research on understanding, developing and assessing personalisation systems is spread over multiple disciplines and builds on methodologies and findings from several different research fields and traditions, such as Artificial Intelligence (AI), Machine Learning (ML), Human–Computer Interaction (HCI), and User Modelling based on (applied) social and cognitive psychology. The fields of AI and ML primarily focus on the optimisation of personalisation applications, and concentrate on creating ever more accurate algorithmic decision makers and prediction models. In the fields of HCI and Information Systems, scholars are primarily interested in the phenomena around the use and interaction with personalisation systems, while Cognitive Science (partly) delivers the theoretical underpinnings for the observed effects. The aim and contribution of this work is to put together the pieces about the impact of personalisation and recommendation systems from these different backgrounds in order to formulate a research agenda and provide a perspective on future developments.}
}
@incollection{JUNG2020721,
title = {The Role of Process Engineering in the Digital Transformation},
editor = {Sauro Pierucci and Flavio Manenti and Giulia Luisa Bozzano and Davide Manca},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {48},
pages = {721-726},
year = {2020},
booktitle = {30th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-823377-1.50121-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823377150121X},
author = {Norbert Jung},
keywords = {Process Simulation, Digital Transformation, Unified Engineering},
abstract = {The process engineering discipline has been largely excluded from Digital Transformation trends. The objective of this presentation is to provide an overview of the obstacles to Digital Transformation for the process discipline and explain how these can be overcome. A special emphasis will be given on the role of the process simulation tool (white box modelling) as a catalyst for transformational change. The presentation will examine several challenges specific to process engineering:1)Process simulators are divided into single-purpose point solutions. Separate models may be created for process design, control strategy design, operator training simulation, performance monitoring and online optimization.2)Process simulators are typically poorly integrated into engineering workflows beyond the process world, and if so, with a single directional information flow.3)Legacy process simulators are overloaded with niche features and functions only usable by experts.4)The potential benefits of Artificial Intelligence and Machine Learning for process engineering are not widely understood. Industry stakeholders see the Digital Twin as the most important building block for Digital Transformation of the process industries. While legacy simulators are well-suited to accurately simulate processes, their decades-old architectures mean they are not ideal to serve the entire plant lifecycle and support the Digital Transformation. We will use the AVEVA SimCentral Simulation Platform4 as an example of how the identified obstacles can be overcome with a next generation process simulator. Case examples from leading companies will be outlined.}
}
@incollection{PUSHPA202051,
title = {Chapter Three - Using fog computing/edge computing to leverage Digital Twin},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {51-77},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300464},
author = {J. Pushpa and S.A. Kalyani},
keywords = {Financial benefit, Tangible benefits, Connectivity model, Product management, Digital twin, Gateway, Load balancer, Propeller, Projection, Analyzer},
abstract = {Recreating a real-world entity as a virtual object has already been studied in most of the field, but making the cloned object more intelligent and healer of real-time physical object will give a new vision on technology. Digital Twin is fitting into the above statement. It uses a combination of machine learning, artificial intelligence, the IoT, and big data to evolve as a ubiquitous solution for all kinds of issues. Digital Twin can build on many form based on the requirement which is basically designed to resolve the challenges of the real world entity. Digital Twin is not limited to solving issues with standalone systems, single entities and machinery problems; it is also suitable for all kinds of data management and controlling issues. Digital Twin can extend its reach by embedding with edge or fog computing which can reduce connectivity and latency issues in networks. In this chapter, methodologies for leveraging Digital Twin using fog/edge computing will be discussed along with suitable use cases, such as wind turbines, product management, healthcare centers, and so on. We also discuss the financial benefits, tangible benefits, and connectivity model.}
}
@article{CORADDU2019106063,
title = {Data-driven ship digital twin for estimating the speed loss caused by the marine fouling},
journal = {Ocean Engineering},
volume = {186},
pages = {106063},
year = {2019},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2019.05.045},
url = {https://www.sciencedirect.com/science/article/pii/S0029801819302616},
author = {Andrea Coraddu and Luca Oneto and Francesco Baldi and Francesca Cipollini and Mehmet Atlar and Stefano Savio},
keywords = {Hull and propeller maintenance, Fouling, Condition based maintenance, ISO 19030, Digital twin, Data-Driven Models, Deep learning},
abstract = {Shipping is responsible for approximately the 90% of world trade leading to significant impacts on the environment. As a consequence, a crucial issue for the maritime industry is to develop technologies able to increase the ship efficiency, by reducing fuel consumption and unnecessary maintenance operations. For example, the marine fouling phenomenon has a deep impact, since to prevent or reduce its growth which affects the ship consumption, costly drydockings for cleaning the hull and the propeller are needed and must be scheduled based on a speed loss estimation. In this work a data driven Digital Twin of the ship is built, leveraging on the large amount of information collected from the on-board sensors, and is used for estimating the speed loss due to marine fouling. A thorough comparison between the proposed method and ISO 19030, which is the de-facto standard for dealing with this task, is carried out on real-world data coming from two Handymax chemical/product tankers. Results clearly show the effectiveness of the proposal and its better speedloss prediction accuracy with respect to the ISO 19030, thus allowing reducing the fuel consumption due to fouling.}
}
@article{XU201997,
title = {Formulating a learner model for evaluating construction workers’ learning ability during safety training},
journal = {Safety Science},
volume = {116},
pages = {97-107},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518313201},
author = {Sheng Xu and Mengge Zhang and Lei Hou},
keywords = {Safety training, Personalised training, Learner model, Learning process},
abstract = {The improvement of safety performance of construction workers heavily lies in safety training, and training technologies, materials and organisations. However, the traditional one-size-fit-all safety training does not cater for the needs of heterogeneous workers. Personalised training may proffer a better solution for heterogeneous workers in the construction sector. To understand the rationale of personalised training, this study formulated a learner model that can capture and evaluate the learning abilities of individual workers. Methodically, a survey on 170 construction workers was conducted, and evidenced that they were heterogeneous in safety training motivation, established knowledge, and emotions during the knowledge acquisition process; and were vulnerable to the model effect and convenience effect during the knowledge application process. The results also showed that workers generally perceived safety training as a mandatory requirement, rather than inherently motivated; emotional changes was the most influencing factor in the knowledge acquisition process; about 40% of the workers were strongly vulnerable to the model effect and convenience effect; and 18% of the workers needed to improve their ability of knowledge acquisition and knowledge application. The correlation analysis and t-test indicated that age, year of experience, trade, project type, organisation type and site environment influenced workers’ learning characteristics and abilities; which lead to the varied levels of safety understanding, awareness and performance. It was also concluded that the construction workers had unique characteristics in their safety learning process and the concept of adapted safety learning could potentially improve the efficiency of safety training.}
}
@article{UMEDA2019363,
title = {Development of an education program for digital manufacturing system engineers based on ‘Digital Triplet’ concept},
journal = {Procedia Manufacturing},
volume = {31},
pages = {363-369},
year = {2019},
note = {Research. Experience. Education. 9th Conference on Learning Factories 2019 (CLF 2019), Braunschweig, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.03.057},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919304214},
author = {Yasushi Umeda and Jun Ota and Fumio Kojima and Masahiro Saito and Hiroki Matsuzawa and Takuji Sukekawa and Akihide Takeuchi and Kazuya Makida and Shouhei Shirafuji},
keywords = {Cyber Physical Systems, digital manufacturing system engineers, education program, digital triplet},
abstract = {In the Industry 4.0 era, cyber physical manufacturing systems (CPPS) has started to change activities of manufacturing system engineers into CPS based ones. In typical Japanese factories, manufacturing system engineers are always stationed at the shop floor and continuously improve manufacturing systems with workers. For supporting such engineers’ activities, we are developing the concept of ‘Digital Triplet’ as an extension of Digital Twin. Digital Triplet consists of intelligent activity world in addition to the cyber world and the physical world and supports manufacturing system engineers in developing engineering processes with the cyber and physical worlds. Based on this, we are developing an education program. In this paper, we first describe the concept of Digital Triplet. Next, after explaining the overview of the education program, we introduce a course in which trainees (mainly novice engineers) execute ‘Kaizen’ with a prototype CPS system of a learning factory.}
}
@article{LAUZERAL201995,
title = {A model order reduction approach to create patient-specific mechanical models of human liver in computational medicine applications},
journal = {Computer Methods and Programs in Biomedicine},
volume = {170},
pages = {95-106},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718314676},
author = {Nathan Lauzeral and Domenico Borzacchiello and Michael Kugler and Daniel George and Yves Rémond and Alexandre Hostettler and Francisco Chinesta},
keywords = {Real-time simulation, Patient-specific modeling, Data-based modeling, Statistical shape analysis, Finite element modeling, Human liver},
abstract = {Background and objective
This paper focuses on computer simulation aspects of Digital Twin models in the medical framework. In particular, it addresses the need of fast and accurate simulators for the mechanical response at tissue and organ scale and the capability of integrating patient-specific anatomy from medical images to pinpoint the individual variations from standard anatomical models.
Methods
We propose an automated procedure to create mechanical models of the human liver with patient-specific geometry and real time capabilities. The method hinges on the use of Statistical Shape Analysis to extract the relevant anatomical features from a database of medical images and Model Order Reduction to compute an explicit parametric solution for the mechanical response as a function of such features. The Sparse Subspace Learning, coupled with a Finite Element solver, was chosen to create low-rank solutions using a non-intrusive sparse sampling of the feature space.
Results
In the application presented in the paper, the statistical shape model was trained on a database of 385 three dimensional liver shapes, extracted from medical images, in order to create a parametrized representation of the liver anatomy. This parametrization and an additional parameter describing the breathing motion in linear elasticity were then used as input in the reduced order model. Results show a consistent agreement with the high fidelity Finite Element models built from liver images that were excluded from the training dataset. However, we evidence in the discussion the difficulty of having compact shape parametrizations arising from the extreme variability of the shapes found in the dataset and we propose potential strategies to tackle this issue.
Conclusions
A method to represent patient-specific real-time liver deformations during breathing is proposed in linear elasticity. Since the proposed method does not require any adaptation to the direct Finite Element solver used in the training phase, the procedure can be easily extended to more complex non-linear constitutive behaviors - such as hyperelasticity - and more general load cases. Therefore it can be integrated with little intrusiveness to generic simulation software including more sophisticated and realistic models.}
}
@article{FAN2021102049,
title = {Disaster City Digital Twin: A vision for integrating artificial and human intelligence for disaster management},
journal = {International Journal of Information Management},
volume = {56},
pages = {102049},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.102049},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219302956},
author = {Chao Fan and Cheng Zhang and Alex Yahja and Ali Mostafavi},
keywords = {Digital twin, Machine learning, Information flow, Disaster management},
abstract = {This paper presents a vision for a Disaster City Digital Twin paradigm that can: (i) enable interdisciplinary convergence in the field of crisis informatics and information and communication technology (ICT) in disaster management; (ii) integrate artificial intelligence (AI) algorithms and approaches to improve situation assessment, decision making, and coordination among various stakeholders; and (iii) enable increased visibility into network dynamics of complex disaster management and humanitarian actions. The number of humanitarian relief actions is growing due to the increased frequency of natural and man-made crises. Various streams of research across different disciplines have focused on ICT and AI solutions for enhancing disaster management processes. However, most of the existing research is fragmented without a common vision towards a converging paradigm. Recognizing this, this paper presents the Disaster City Digital Twin as a unifying paradigm. The four main components of the proposed Digital Twin paradigm include: multi-data sensing for data collection, data integration and analytics, multi-actor game-theoretic decision making, and dynamic network analysis. For each component, the current state of the art related to AI methods and approaches are examined and gaps are identified.}
}
@article{YERA2019395,
title = {Modelling the interactive behaviour of users with a medication safety dashboard in a primary care setting},
journal = {International Journal of Medical Informatics},
volume = {129},
pages = {395-403},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619301662},
author = {Ainhoa Yera and Javier Muguerza and Olatz Arbelaitz and Iñigo Perona and Richard N. Keers and Darren M. Ashcroft and Richard Williams and Niels Peek and Caroline Jay and Markel Vigo},
keywords = {Patient safety, Primary health care, Supervised machine learning, User modelling, Human-Computer interaction},
abstract = {Objective
To characterise the use of an electronic medication safety dashboard by exploring and contrasting interactions from primary users (i.e. pharmacists) who were leading the intervention and secondary users (i.e. non-pharmacist staff) who used the dashboard to engage in safe prescribing practices.
Materials and methods
We conducted a 10-month observational study in which 35 health professionals used an instrumented medication safety dashboard for audit and feedback purposes in clinical practice as part of a wider intervention study. We modelled user interaction by computing features representing exploration and dwell time through user interface events that were logged on a remote database. We applied supervised learning algorithms to classify primary against secondary users.
Results
We observed values for accuracy above 0.8, indicating that 80% of the time we were able to distinguish a primary user from a secondary user. In particular, the Multilayer Perceptron (MLP) yielded the highest values of precision (0.88), recall (0.86) and F-measure (0.86). The behaviour of primary users was distinctive in that they spent less time between mouse clicks (lower dwell time) on the screens showing the overview of the practice and trends. Secondary users exhibited a higher dwell time and more visual search activity (higher exploration) on the screens displaying patients at risk and visualisations.
Discussion and conclusion
We were able to distinguish the interactive behaviour of primary and secondary users of a medication safety dashboard in primary care using timestamped mouse events. Primary users were more competent on population health monitoring activities, while secondary users struggled on activities involving a detailed breakdown of the safety of patients. Informed by these findings, we propose workflows that group these activities and adaptive nudges to increase user engagement.}
}
@article{GOMISPORQUERAS2018329,
title = {Teaching technologies, attendance, learning and the optimal level of access to online materials},
journal = {Economic Modelling},
volume = {73},
pages = {329-342},
year = {2018},
issn = {0264-9993},
doi = {https://doi.org/10.1016/j.econmod.2018.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0264999317308593},
author = {Pedro Gomis-Porqueras and José A. Rodrigues-Neto},
keywords = {Absenteeism, Attendance, Education, Technology, Online policies},
abstract = {A stylized game-theoretic model explores the relationship between a lecturer and a representative student in a university class. The lecturer moves first, choosing the student's level of access to online class materials, from zero to 100%. The student observes the lecturer's choice, and then chooses whether to attend or skip class. The student has a valuable outside option during class time, but she also values learning. Out-of-class learning cannot perfectly substitute in-class learning. The student's decision depends on her outside option and the level of access to online materials chosen by the lecturer. For extreme parameter values, the student's optimal action does not depend on the lecturer's choice; in these cases there is full access to online materials, as the lecturer anticipates the student's choice. If parameters lie in a range where the lecturer's action may influence the student, then the lecturer provides the maximum level of access to online materials that still incentivizes the student to attend class. Online policies adopted by universities may affect learning outcomes, the intensity of use of online technologies, and attendance. This paper analyzes two types of university policies about the access to online materials.}
}
@incollection{LAI2020109,
title = {Chapter 4 - Digital twin driven virtual verification},
editor = {Fei Tao and Ang Liu and Tianliang Hu and A.Y.C. Nee},
booktitle = {Digital Twin Driven Smart Design},
publisher = {Academic Press},
pages = {109-138},
year = {2020},
isbn = {978-0-12-818918-4},
doi = {https://doi.org/10.1016/B978-0-12-818918-4.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818918400004X},
author = {Yiling Lai and Yuchen Wang and Robert Ireland and Ang Liu},
keywords = {Digital twin, virtual verification, product life cycle, product design},
abstract = {The creation of high-fidelity, dynamic, and self-learning virtual models, known as digital twin (DT), will play a critical role in Industry 4.0 by reflecting the whole product life cycle. This ability enables designers to determine product functionality and configuration during the initial product design stages. The integration of virtual and physical elements of the product life cycle pioneers allows designers to model performance and detect issues of real-world function in a virtual space. This process, known as DT-driven virtual verification, will facilitate a significant leap in design efficiency. This chapter proposes a DT-driven virtual verification framework model to improve product design with respect to the five stages of a product life cycle (including design, manufacturing, usage, maintenance, and end-of-life). Furthermore, as demonstration of the framework’s efficacy two case studies are conducted: the first on a commercial espresso coffee machine and the second on a 3D printer. Development and application of the framework aim to highlight the revolutionary potential and expedite development of DT technology into real-world design.}
}
@article{YE2018233,
title = {Building feedforward neural networks with random weights for large scale datasets},
journal = {Expert Systems with Applications},
volume = {106},
pages = {233-243},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418302318},
author = {Hailiang Ye and Feilong Cao and Dianhui Wang and Hong Li},
keywords = {Large scale data, Neural networks, Learning, Approximate Newton-type method},
abstract = {With the explosive growth in size of datasets, it becomes more significant to develop effective learning schemes for neural networks to deal with large scale data modelling. This paper proposes an iterative approximate Newton-type learning algorithm to build neural networks with random weights (NNRWs) for problem solving, where the whole training samples are divided into some small subsets under certain assumptions, and each subset is employed to construct a local learner model for integrating a unified classifier. The convergence of the output weights of the unified learner model is given. Experimental results on UCI datasets with comparisons demonstrate that the proposed algorithm is promising for large scale datasets.}
}
@incollection{AMBIKA2020321,
title = {Chapter Thirteen - Machine learning and deep learning algorithms on the Industrial Internet of Things (IIoT)},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {321-338},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300609},
author = {P. Ambika},
keywords = {Machine learning, Deep learning, IIoT, Digital twin, Supervised, Unsupervised, Industry 4.0, Analytics, Interoperability, Contextual analysis},
abstract = {Deep transformation and human progress is a new industrial revolution that makes “Automation of Everything.” It connects all digital interfaces, data analysis and control of the physical world through networks of computers. This key revolution promises everyone to unlock trillions of opportunities in the next decade. Human could feel massive improvements in productivity in physical and digital industries that enhances quality life of a human healthier and more sustainable community. In the world of IIoT, the creation of massive amounts of data from a various sensors is common and there is lot of challenges. This goal of this chapter is to provide a comprehensive review about Machine learning and deep learning techniques, popular algorithms, and their impact on Industrial Internet of Things. This chapter also delves use cases where machine learning is used and to gain insights from IoT data.}
}
@article{DRUSHKU201979,
title = {Interest-based recommendations for business intelligence users},
journal = {Information Systems},
volume = {86},
pages = {79-93},
year = {2019},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917307032},
author = {Krista Drushku and Julien Aligon and Nicolas Labroche and Patrick Marcel and Verónika Peralta},
keywords = {User interest, Feature construction, Clustering, BI analyses, Collaborative recommender systems},
abstract = {It is quite common these days for experts, casual analysts, executives and data enthusiasts, to analyze large datasets through user-friendly interfaces on top of Business Intelligence (BI) systems. However, current BI systems do not adequately detect and characterize user interests, which may lead to tedious and unproductive interactions. In this paper, we propose a collaborative recommender system for BI interactions, specifically designed to take advantage of identified user interests. Such user interests are discovered by characterizing the intent of the interaction with the BI system. Building on user modeling for proactive search systems, we identify a set of features for an adequate description of intents, and a similarity measure for grouping intents into coherent clusters. On top of these automatically identified interests, we build a collaborative recommender system based on a Markov model that represents the probability for a user to switch from one interest to another. We validate our approach experimentally with an in-depth user study, where we analyze traces of BI navigation. Our results are two-fold. First, we show that our similarity measure outperforms a state-of-the-art query similarity measure and yields a very good precision with respect to expressed user interests. Second, we compare our recommender system to two state-of-the-art systems to demonstrate the benefit of relying on user interests.}
}
@article{YANG201897,
title = {Study on student performance estimation, student progress analysis, and student potential prediction based on data mining},
journal = {Computers & Education},
volume = {123},
pages = {97-108},
year = {2018},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2018.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0360131518300861},
author = {Fan Yang and Frederick W.B. Li},
keywords = {Evaluation methodologies, Intelligent tutoring systems, Teaching/learning strategies, Applications in subject areas, Simulations},
abstract = {Student performance, student progress and student potential are critical for measuring learning results, selecting learning materials and learning activities. However, existing work doesn't provide enough analysis tools to analyze how students performed, which factors would affect their performance, in which way students can make progress, and whether students have potential to perform better. To solve those problems, we have provided multiple analysis tools to analyze student performance, student progress and student potentials in different ways. First, this paper formulates student model with performance related attributes and non-performance related attributes by Student Attribute Matrix (SAM), which quantifies student attributes, so that we can use it to make further analysis. Second, this paper provides a student performance estimation tools using Back Propagation Neural Network (BP-NN) based on classification, which can estimate student performance/attributes according to students' prior knowledge as well as the performance/attributes of other students who have similar characteristics. Third, this paper proposes student progress indicators and attribute causal relationship predicator based on BP-NN to comprehensively describe student progress on various aspects together with their causal relationships. Those indicators and predicator can tell how much a factor would affect student performance, so that we can train up students on purpose. Finally, this paper proposes a student potential function that evaluates student achievement and development of such attributes. We have illustrated our analysis tools by using real academic performance data collected from 60 high school students. Evaluation results show that the proposed tools can give correct and more accurate results, and also offer a better understanding on student progress.}
}
@article{EPPERLEIN2019116,
title = {Recovering Markov models from closed-loop data},
journal = {Automatica},
volume = {103},
pages = {116-125},
year = {2019},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2019.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0005109819300299},
author = {Jonathan P. Epperlein and Sergiy Zhuk and Robert Shorten},
abstract = {Situations in which recommender systems are used to augment decision making are becoming prevalent in many application domains. Almost always, these prediction tools (recommenders) are created with a view to affecting behavioural change. Clearly, successful applications actuating behavioural change, affect the original model underpinning the predictor, leading to an inconsistency. This feedback loop is often not considered in standard machine learning techniques which rely upon machine learning/statistical learning machinery. The objective of this paper is to develop tools that recover unbiased user models in the presence of recommenders. More specifically, we assume that we observe a time series which is a trajectory of a Markov chain R modulated by another Markov chain S, i.e. the transition matrix of R is unknown and depends on the current state of S. The transition matrix of the latter is also unknown. In other words, at each time instant, S selects a transition matrix for R within a given set which consists of known and unknown matrices. The state of S, in turn, depends on the current state of R thus introducing a feedback loop. We propose an Expectation–Maximisation (EM) type algorithm, which estimates the transition matrices of S and R. Experimental results are given to demonstrate the efficacy of the approach.}
}
@article{DAI2018437,
title = {Cross-modal deep discriminant analysis},
journal = {Neurocomputing},
volume = {314},
pages = {437-444},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.09.059},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217315783},
author = {Xue-mei Dai and Sheng-Gang Li},
keywords = {Cross-modal analysis, Cross-media retrieval, Discriminant analysis, Deep learning},
abstract = {Cross-modal analysis has widespread applications ranging from cross-media retrieval to heterogeneous face recognition. The critical problem in cross-modal analysis is to correlate heterogeneous features originating from different modalities. Extensive studies have been focused on discovering shared feature space between modalities, while largely overlooked the discriminant information contained in the cross-modal data. Leveraging the discriminant information has been found effective in discovering the underlying semantic structure to facilitate the end applications. Considering this, we propose a deep learning-based method to simultaneously consider the cross-modal correlation and intra-modal discriminant information. Specifically, a unified objective function is introduced which consists of a LDA-like discriminant part and a CCA-like correlation part. The proposed method can be easily generalized to exploiting the unpaired samples. Extensive experiments are conducted on three representative cross-modal analysis problems: cross-media retrieval, cross-OSN user modeling and heterogeneous face recognition. By comparing with existing state-of-the-art algorithms, the results show that the proposed algorithm is robust to the feature dimension and achieves the best performance in all experiments.}
}
@article{BARTELT2020337,
title = {Automated production of individualized products for teaching I4.0 concepts},
journal = {Procedia Manufacturing},
volume = {45},
pages = {337-342},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920310659},
author = {Matthias Bartelt and Jannis Stecken and Bernd Kuhlenkötter},
keywords = {Automated production concepts, Individualization, I4.0},
abstract = {Many trends, such as cyber-physical systems, Internet of Things or digital twins are currently predominant in research and development. However, the teaching and training of these topics is often neglected. Especially, students or participants of a training course cannot develop and apply new technologies on their own. In order to meet this demand, this paper presents an approach that segments an automated production system into modules. The complexity of the system can be varied depending on the needs of the students or participants. With this, classical automation topics as well as state-of-the-art topics can be touched and trained. Additionally, the concept is designed in such a way that it is not only applicable in university education, but also that individual topics can be taught in half-day or one-day seminars with appropriate previous experience.}
}
@article{NIE2020539,
title = {3D Model classification based on few-shot learning},
journal = {Neurocomputing},
volume = {398},
pages = {539-546},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.03.105},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219310471},
author = {Jie Nie and Ning Xu and Ming Zhou and Ge Yan and Zhiqiang Wei},
keywords = {Few-shot, Meta-learner, 3D model classification},
abstract = {With the development of multimedia technology, 3D model has been applied in many fields such as mechanical design, construction industry, entertainment industry, medical treatment and so on. The number of 3D model is becoming more and more in our lives. Therefore, effective automatic management and classification of 3D models become more and more important. In this paper, we propose a dual-meta-learner model based on LSTM to learn the exact optimization algorithm used to train another two learner neural network classifier in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while it can also achieve a general initialization of the learner (classifier) network that allows for quick convergence of training. Our method attains state-of-the-art performance by significant margins.}
}
@article{STROER2018714,
title = {Combined development and test of product-service systems in early product development stages for customized, availability-oriented business models in the capital goods industry},
journal = {Procedia CIRP},
volume = {72},
pages = {714-719},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.246},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304177},
author = {Felix Ströer and Paaranan Sivasothy and Karl-G. Faißt and Hristo Apostolov and Thomas Eickhoff and Dani Bechev and Georgis Bulun and Jörg Seewig and Martin Eigner and Bernd Sauer},
keywords = {Product-Service Systems, Model-Based Systems Engineering, Digital Twin, Conditon Monitoring, Physical Modeling},
abstract = {Availability-Oriented Product-Service Systems (AOPSS) give manufacturers of capital goods the opportunity to expand their offerings while fulfilling the customers’ needs for product availability. In turn, they generate special requirements for the early product development phase. Condition monitoring has to be taken into account, which results in a contradiction since learning-based condition monitoring approaches require large amounts of data which isn’t available in that particular stage. Techniques from model-based systems engineering allow solving problems newly arising when developing parts or products in AOPSS. The approach proposed in the paper is being illustrated based on a use case from the agricultural industry.}
}
@article{ZHANG2019345,
title = {A data- and knowledge-driven framework for digital twin manufacturing cell},
journal = {Procedia CIRP},
volume = {83},
pages = {345-350},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.084},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119306985},
author = {Chao Zhang and Guanghui Zhou and Jun He and Zhi Li and Wei Cheng},
keywords = {digital twin, digital twin manufacturing cell, intelligent manufacturing, smart product-service systems},
abstract = {Intelligent manufacturing is regarded as the next generation manufacturing mode with powerful learning and cognitive capacities enabled by new generation information technologies such as Internet of Things, big data analytics, edge computing and artificial intelligence. To provide an insight into intelligent manufacturing, this paper takes autonomous manufacturing cell as implementation scenario and proposes a data- and knowledge-driven framework for digital twin manufacturing cell (DTMC), which could support autonomous manufacturing by an intelligent perceiving, simulating, understanding, predicting, optimizing and controlling strategy. In addition, three key enabling technologies including digital twin model, dynamic knowledge bases and knowledge-based intelligent skills for supporting the above strategy are analyzed. Then, the implementing methods of DTMC are introduced through a thus constructed digital twin robot, and the usage of data and knowledge for supporting the automous operations of DTMC is also discussed. Finally, benefits of DTMC in smart product-service systems (PSS) and its current challenges are summarized.}
}
@article{GRUBE2019219,
title = {SMEs can touch Industry 4.0 in the Smart Learning Factory},
journal = {Procedia Manufacturing},
volume = {31},
pages = {219-224},
year = {2019},
note = {Research. Experience. Education. 9th Conference on Learning Factories 2019 (CLF 2019), Braunschweig, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.03.035},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919303993},
author = {David Grube and Ali A. Malik and Arne Bilberg},
keywords = {I4.0, SME, Collaborative simulation},
abstract = {This paper describes how a Smart Learning Factory enables manufacturing SMEs of capturing the benefits of highly complex tools and enablers such as virtual simulation and the Digital Twin. The collaborative factory design approach is enabled by embedding the use of discrete event simulation connected with physical objects placed on a Digital Twin Module (DTM). The users can manipulate the physical objects as physical counterparts to the machines and equipment in the virtual space and visualize the designed factory and make further analysis. The method combines the use of dynamic discrete event simulation seamlessly connected with physical objects placed on the DTM to enable collaborative design. The bridging between simulation and physical objects is done by using a digital integration platform. Using physical artifacts as counterparts of the virtual objects in the simulation, participants confidently interact with the simulation regardless level of skills and competencies. The Smart Learning Factory is helping SMEs to get inspired using physical and virtual simulations for factory design and re-design, and develop solutions in a collaborative environment. This is in-line with the theory of lean automation, that suggests making simple and cheap automation and automating the correct value-adding processes. A demonstration case of designing a production setup in cooperation with a SME is developed and documented.}
}
@article{JAAKMA2019e02622,
title = {Auto-assessment tools for mechanical computer aided design education},
journal = {Heliyon},
volume = {5},
number = {10},
pages = {e02622},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e02622},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019362826},
author = {Kaur Jaakma and Panu Kiviluoma},
keywords = {Mechanical engineering, Education, Machine design, Computer-aided engineering, Pedagogy, Teaching research, Evaluation methodologies, Improving classroom teaching, STEP, Auto-assessment},
abstract = {Traditionally Computer Aided Design (CAD) courses have been carried out in computer classrooms requiring great amount of teaching personnel. Assessment of students' modeling exercises has been both time consuming and error prone. Utilization of the teaching resources could clearly benefit from online auto-assessment. The auto-assessment tools are widely in use in programming and language courses, but suitable tools for assessing 3D models used in CAD are lacking. This paper presents two new online auto-assessment tools to support the development of both command (“what steps are needed to create this shape?”) and strategic (“how should I model this shape?”) knowledge while learning CAD. The first tool is based on neutral file format (in this case STEP) and can recognize surface differences between student's model and reference model. This tool can assess student's skill to create certain predefined shape (i.e. command knowledge). The second auto-assessment tool utilizes commercial CAD software's API (Application Programming Interface) to test how student's model behaves when modeling parameters are changed. This tool assess student's capabilities to build and design a CAD model's design intent (i.e. strategic knowledge). Developed tools were tested on three mechanical engineering courses. This paper presents both the tools and the feedback received from the students and teachers. Overall, the auto-assessment tools functioned well and feedback from both students and teachers were positive. The most appreciated tool functionality was time and place independent submission and assessment of exercise works. These new tools able focusing teachers' workload from checking the basic exercises to guiding the learning process.}
}
@article{LI201973,
title = {Robust stochastic configuration networks with maximum correntropy criterion for uncertain data regression},
journal = {Information Sciences},
volume = {473},
pages = {73-86},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518307278},
author = {Ming Li and Changqin Huang and Dianhui Wang},
keywords = {Stochastic configuration networks, Robust data regression, Randomized algorithms, Maximum correntropy criterion, Alternating optimization},
abstract = {This paper develops a robust stochastic configuration network (RSCN) framework to cope with data modelling problems when the given samples contain noises or outliers. Technically, RSCNs are built by generalizing the objective function used in our original stochastic configuration networks with maximum correntropy criterion (MCC) induced losses (the proposed algorithm is termed as RSC-MCC). The half-quadratic (HQ) technique is employed to optimize the penalty weights for each training sample, aiming to weaken the impacts caused by the noisy data or outliers throughout the training session. Alternating optimization (AO) methodology is used to renew the RSCN model in company with updated penalty weights determined by HQ methods. The performance of RSC-MCC algorithm is compared with some existing methods, such as the probabilistic robust learning algorithm for neural networks with random weights (PRNNRW), RVFL networks, improved RVFL networks (Imp-RVFL), and our recent work RSCNs with kernel density estimation (RSC-KDE), on two synthetic function approximation examples, four benchmark datasets and one educational data modelling case study (for student learning performance prediction). The experimental results show that RSC-MCC performs more favourably in robust data analytics, and further indicate that our proposed RSCN framework (both RSC-KDE and RSC-MCC) has a good potential for real-world applications.}
}
@article{AISSAOUI201987,
title = {Combining supervised and unsupervised machine learning algorithms to predict the learners’ learning styles},
journal = {Procedia Computer Science},
volume = {148},
pages = {87-96},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919300122},
author = {Ouafae EL AISSAOUI and Yasser EL ALAMI {EL MADANI} and Lahcen OUGHDIR and Youssouf EL ALLIOUI},
keywords = {Web usage mining techniques, machine learning algorithms, K-modes clustering algorithm, Naive Bayes classifier, adaptive e-learning systems, Learning style model;},
abstract = {The implementation of an efficient adaptive e-learning system requires the construction of an effective student model that represents the student’s characteristics, among those characteristics, there is the learning style that refers to the way in which a student prefers to learn. Knowing learning styles helps adaptive E-learning systems to improve the learning process by providing customized materials to students. In this work, we have proposed an approach to identify the learning style automatically based on the existing learners’ behaviors and using web usage mining techniques and machine learning algorithms. The web usage mining techniques were used to pre-process the log file extracted from the E-learning environment and capture the learners’ sequences. The captured learners’ sequences were given as an input to the K-modes clustering algorithm to group them into 16 learning style combinations based on the Felder and Silverman learning style model. Then the naive Bayes classifier was used to predict the learning style of a student in real time. To perform our approach, we used a real dataset extracted from an e-learning system’s log file, and in order to evaluate the performance of the used classifier, the confusion matrix method was used. The obtained results demonstrate that our approach yields excellent results.}
}
@article{DERYABIN20203210,
title = {About some issues of developing Digital Twins for the intelligent process control in quarries},
journal = {Procedia Computer Science},
volume = {176},
pages = {3210-3216},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.128},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320287},
author = {Sergey A. Deryabin and Igor O. Temkin and Sergey V. Zykov},
keywords = {Digital Twin, Industry 4.0, software architecting, functional modeling, software prototyping},
abstract = {The present work is devoted to the problems of Digital Twin development of industrial enterprises in the field of mining. The main goal of this article is to formulate the principles of designing platform solutions for the integration of the most important functional elements that ensure the implementation of technological processes of a full production cycle. Various classification schemes for heterogeneous, poorly structured information spaces that form the distributed digital environment of a mining enterprise are proposed. Based on the results of structural and functional modeling, a number of principles and requirements are formulated for the implementation of the Digital Twin technology of the transport and technological process in quarry. A conceptual diagram of the functional structure of the Digital Twin platform is proposed, taking into account the need to include Industry 4.0 technologies such as Industrial Internet of Things, Big Data (including Predictive Analytics and Machine Learning), Autonomous Haulage Systems and Dynamics 3D Optimization Modeling. Some aspects of the implementation and functioning of the prototype version of the Digital Twin platform are considered in terms of the use of instrumental solutions based on the Unity visual modeling environment.}
}
@article{FRANCIOSA2020369,
title = {Deep learning enhanced digital twin for Closed-Loop In-Process quality improvement},
journal = {CIRP Annals},
volume = {69},
number = {1},
pages = {369-372},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.04.110},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620301323},
author = {Pasquale Franciosa and Mikhail Sokolov and Sumit Sinha and Tianzhu Sun and Dariusz Ceglarek},
keywords = {Digital Manufacturing System, Assembly, Digital Twin},
abstract = {A digital twin framework is presented for assembly systems with compliant parts fusing sensors with deep learning and CAE simulations. Its underlying concept, ‘process capability space,’ updates iteratively during evolving tasks of new product introduction with resulting model fidelity able to simulate dimensional, geometric and weld quality of parts and assemblies; isolate root causes of quality defects; and, suggest corrective actions for automatic defects mitigation; thereby, enabling ‘Closed-Loop In-Process (CLIP) quality improvement’ during assembly system development. Results, using the first fully digitally developed remote laser welding process for aluminium doors, yielded a right-first-time rate of >96% for door assembly cell development.}
}
@article{CALAFATO2019102700,
title = {The non-native speaker teacher as proficient multilingual: A critical review of research from 2009–2018},
journal = {Lingua},
volume = {227},
pages = {102700},
year = {2019},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0024384119300828},
author = {Raees Calafato},
keywords = {Multilingualism, Teacher identity, Non-native speaker teacher, Language awareness, Foreign language teaching},
abstract = {With many countries now implementing multilingual educational initiatives in schools, there is a need to reconfigure training programs and encourage teachers to develop a multilingual identity so that they can better promote multilingualism among their students. At present, monolingual ideologies dominate the language education landscape, particularly disadvantaging non-native speaker teachers (NNSTs), who are proficient multilinguals by default and who might best embody the successful language learner model for emergent multilinguals. Studies on NNSTs, however, have traditionally focused on their status as deficient native speakers instead of exploring their multilinguality, although this has started to change in recent years. This study represents a critical review of empirical studies (n=84) from 2009–2018 that reflect this change. The review indicates that a growing number of studies have started to document NNSTs’ unique affordances and multilingual practices in the classroom globally, that NNSTs can develop a reactive multilingual identity in response to native-speakerism, and that learners, when engaged by researchers, do recognize the multilingual affordances of NNSTs. The study's findings should contribute to the growing body of research on multilingual teacher identity, affordances, and practices, especially as this concerns NNSTs.}
}
@article{MCPAKE2019220,
title = {Radiographers' and students' experiences of undergraduate radiotherapy practice placement in the United Kingdom},
journal = {Radiography},
volume = {25},
number = {3},
pages = {220-226},
year = {2019},
issn = {1078-8174},
doi = {https://doi.org/10.1016/j.radi.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1078817418302104},
author = {M. McPake},
keywords = {Radiotherapy, Practice educator, Student, Practice placement model, Peer-assisted learning},
abstract = {Introduction
A three-phased, mixed-methods study was conducted to explore the experiences of undergraduate radiotherapy students and their supervising practice educators within U.K. radiotherapy practice placement.
Methods
Qualitative data were gathered from focus groups/interviews with volunteer participants to elicit in-depth perceptions about experiences of practice placement. Data were transcribed, verbatim, and manually coded and analysed by the researcher using the applied research methodology of framework analysis, enabling the investigation of the a priori theme ‘practice placement model’, and recommendations were made for improvement.
Results
Two radiotherapy placement models are confirmed, i.e. the single student model, and the paired student model, and advantages and disadvantages are identified for each. Study findings suggest that neither radiotherapy model is superior to the other in terms of placement education and experience. Previous and current experience of either model appears to bias students and practice educators towards that model, despite recognition of its disadvantages.
Conclusion
The experiences of students and practice educators using the radiotherapy models are consistent with the experiences of other AHPs and nursing using similar practice placement models. It is recommended that all students should have access to peer-assisted learning on placement to improve critical thinking skills, to enable time for reflection, and to consolidate learning.}
}
@article{DAMGRAVE2019341,
title = {Student driven learning in Synthetic Environments},
journal = {Procedia CIRP},
volume = {84},
pages = {341-346},
year = {2019},
note = {29th CIRP Design Conference 2019, 08-10 May 2019, Póvoa de Varzim, Portgal},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.270},
url = {https://www.sciencedirect.com/science/article/pii/S221282711930575X},
author = {R.G.J. Damgrave and E. Lutters},
keywords = {Digital twin, Virtual Reality, Virtual factory, Engineering education},
abstract = {113 master students of multiple engineering study backgrounds were challenged to develop solutions for the Fraunhofer Project Center at the University of Twente (FPC@UT). In 20 groups the students had to develop a synthetic environment to monitor, manage and control a pilot plant or virtual factory. The assignment was carried out in the context of the study Industrial Design Engineering, in the course ‘Virtual Reality’. During the course the students had to provide a strategic, yet concrete proposal and demonstrator on how to realize a solution using virtual and augmented reality technology and to find balance between generic tools and specific applications. The students had the Virtual Reality Lab and Smart Industry Lab to their disposal during this 10 week, 5 ECTS, course. Based on a brief introduction by the FPC@UT the students had to set their own goals and deliverables and convince the client that their envisaged solution would be beneficial. The course is based on student driven learning, in combination with project led education. This resulted in a situation where the students were in charge of their education and had to decide for themselves which knowledge and feedback they would need in order to achieve their deliverables. Eventually the resulted solutions could be used by FPC@UT to further integrate in their (future) clients.}
}
@article{FANG2018149,
title = {A generalized stereotype learning approach and its instantiation in trust modeling},
journal = {Electronic Commerce Research and Applications},
volume = {30},
pages = {149-158},
year = {2018},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2018.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1567422318300644},
author = {Hui Fang and Jie Zhang and Murat Şensoy},
keywords = {User modeling, Stereotype trust model, Fuzzy semantic framework, E-commerce},
abstract = {Owing to the lack of historical data regarding an entity in online communities, a user may rely on stereotyping to estimate its behavior based on historical data about others. However, these stereotypes cannot accurately reflect the user’s evaluation if they are based on limited historical data about other entities. In view of this issue, we propose a novel generalized stereotype learning approach: the fuzzy semantic framework. Specifically, we propose a fuzzy semantic process, incorporated with traditional machine-learning techniques to construct stereotypes. It consists of two sub-processes: a fuzzy process that generalizes over non-nominal attributes (e.g., price) by splitting their values in a fuzzy manner, and a semantic process that generalizes over nominal attributes (e.g., location) by replacing their specific values with more general terms according to a predefined ontology. We also implement the proposed framework on the traditional decision tree method to learn users’ stereotypes and validate the effectiveness of our framework for computing trust in e-marketplaces. Experiments on real data confirm that our proposed model can accurately measure the trustworthiness of sellers with which buyers have limited experience.}
}
@article{PROMYOO20191043,
title = {Innovative Digital Manufacturing Curriculum for Industry 4.0},
journal = {Procedia Manufacturing},
volume = {34},
pages = {1043-1050},
year = {2019},
note = {47th SME North American Manufacturing Research Conference, NAMRC 47, Pennsylvania, USA.},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.06.092},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919308194},
author = {Rapeepan Promyoo and Shashank Alai and Hazim El-Mounayri},
keywords = {Digital manufacturing, Digital thread, Industry 4.0, SDPD},
abstract = {Manufacturing companies across all major industries are facing serious challenges trying to competitively design and manage modern products, which are becoming increasingly complex multi-domain systems or “systems of systems”. Model-based systems driven product development (or SDPD, for Systems Driven Product Development) has been proposed as a solution based on driving the product lifecycle from the systems requirements and tracing back performance to stakeholders’ needs through a RFLP (Requirement, Functional, Logical, Physical) traceability process. The SDPD framework integrates system behavioral modeling with downstream product design and manufacturing process practices to support the verification/validation of the systems behavior as products progress through all phases of the lifecycle, as well as the optimization of trade-offs decisions by maintaining the cross-product digital twin and thread for global decision optimization in an efficient and effective way. We have developed an innovative digital manufacturing curriculum (designed around the SDPD paradigm) that is based on the digitalization of the SE (Systems Engineering) process through the integration of modelling and simulation continuum, in the form of Model-based Systems Engineering (MBSE), with Product lifecycle management (PLM). At the core of this curriculum is a shift of focus from theory to implementation and practice, through an applied synthesis of engineering fundamentals and systems engineering, that is driven by a state-of-the-art digital innovation platform for product (or system) development consisting of integrated software (digital) tools spanning the complete lifecycle. The curriculum consists of three key components, namely, modelling and simulation continuum, traceability, and digital thread. The curriculum provides a foundation for implementing the digital twin and supports the training of the next generation of engineers for Industry 4.0. The digital manufacturing (or SDPD) framework is applied in the design and optimization of an electric skateboard. The implementation demonstrates: 1) The benefits of digitalization/model-based engineering when developing complex multi-domain products or systems; 2) The ability of students to effectively complete a real-life modern product development within the time line of one semester; 3) The provision of MBSE curriculum for Engineering Education 4.0, characterized by key, integrated skills for the digital enterprise and Industry 4.0.}
}
@article{DAI2019367,
title = {Stochastic configuration networks with block increments for data modeling in process industries},
journal = {Information Sciences},
volume = {484},
pages = {367-386},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.01.062},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519300738},
author = {Wei Dai and Depeng Li and Ping Zhou and Tianyou Chai},
keywords = {Stochastic configuration networks, Process industries, Randomized learner model, Block incremental approach, Simulated annealing algorithm},
abstract = {Stochastic configuration networks (SCNs) that employ a supervisory mechanism to automatically and fast construct universal approximators can achieve promising performance for resolving regression problems. This paper develops an extension of the original SCNs with block increments to enhance learning efficiency, which has received considerable attention in industrial process modeling. This extension allows the learner model to add multiple hidden nodes (termed hidden node block) simultaneously to the network during construction process. To meet industrial demands, two block incremental implementations of SCNs are presented by adopting different strategies for setting the block size. Specifically, the first one adds the hidden node blocks with a fixed block size, which achieves the acceleration of convergence rate at the cost of model compactness; the second one can automatically set the block size by incorporating simulated annealing algorithm, achieving a good balance between efficiency and complexity. The two algorithms are suitable for industrial data modeling with distinct requirements on modeling speed and memory space. The improved methods for building SCNs are evaluated by two function approximations, four benchmark datasets and two real world applications in process industries. Experimental results with comparisons indicate that the proposed schemes perform favorably.}
}
@article{ROTGANS2019294,
title = {A Students’ Model of Team-based Learning},
journal = {Health Professions Education},
volume = {5},
number = {4},
pages = {294-302},
year = {2019},
issn = {2452-3011},
doi = {https://doi.org/10.1016/j.hpe.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2452301118301706},
author = {Jerome I. Rotgans and Preman Rajalingam and Michael A. Ferenczi and Naomi Low-Beer},
keywords = {Team-based learning, Medical education, Path analysis, Students׳ model},
abstract = {Background
Team-based learning (TBL) combines direct instruction with active, collaborative small group learning. This study aimed to elucidate-from the students’ perspective-the relations between different elements of TBL. This is expected to provide a better understanding of the inner workings of TBL in education.
Method
Three hundred and thirteen first- and second-year medical students participated in the study. Data about TBL were collected at the end of six teaching blocks, by means of a questionnaire. The data were then combined and subjected to path analysis, which enabled testing of hypothesised relations between three layers of TBL-relevant variables. These were (1) input variables: prior knowledge, teamwork, challenging application exercise, content expert and facilitator; (2) process variables: preparation materials, individual readiness assurance test (iRAT), team readiness assurance test (tRAT); and (3) output variables: learning and topic interest.
Results
Initial analysis resulted in amendments to the hypothesised model. An amended model fitted the data well and explained 43% of the variance in learning and 32% of the variance in topic interest. Content expert had a direct effect on topic interest, as did prior knowledge, teamwork, iRAT and application exercise. Learning was directly influenced by tRAT, application exercise and facilitator, but not content expert.
Conclusions
The results of this study demonstrate the inter-relationships of different elements of TBL. The results provide new insights in how TBL works from a students’ perspective. Implications of these findings are discussed.}
}
@article{BAZAZ2020288,
title = {The prediction method of tool life on small lot turning process – Development of Digital Twin for production},
journal = {Procedia Manufacturing},
volume = {51},
pages = {288-295},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.041},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920318965},
author = {Sara Moghadaszadeh Bazaz and Mika Lohtander and Juha Varis},
keywords = {Tool life, turning process, artificial intelligence, mathematical modelling, digital twin},
abstract = {Saving resources is one of the most significant factors in the manufacturing industry. There are in the factory, several different products under processing at the same time, therefore the handling of production conditions could be hard every now and then. Changing tools during operation might causes interruption and prolong production time. Estimation of a tool life during turning process is one of the key factors to avoid unnecessary unfinished parts and waste of resources. Overall research aiming to develop a machine learning method to predict tool life for any work-piece or tool material in the general turning process. The addressed method is important in modern small lot production when parts and materials changed constantly. The Purpose of this particular paper is to find out suitable machine learning method or several methods to evaluate tool-life in different turning conditions and circumstances. As a hypothesis of this research, we assume machine learning combine mathematical modelling is a proper method to estimate tool life in small-lot production with reasonable cost and operation time.}
}
@article{BURGUN2019913,
title = {Intelligence artificielle et radiothérapie : quelles bases et quelles perspectives ?},
journal = {Cancer/Radiothérapie},
volume = {23},
number = {8},
pages = {913-916},
year = {2019},
issn = {1278-3218},
doi = {https://doi.org/10.1016/j.canrad.2019.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1278321819303865},
author = {A. Burgun},
keywords = {Intelligence artificielle, Oncologie, Radiothérapie, Médecine de précision, Algorithme, Artificial intelligence, Precision oncology, Radiation therapy, Algorithm},
abstract = {Résumé
L’intelligence artificielle est une notion hautement polysémique. Pour réaliser un raisonnement complexe dans la vie réelle, et s’adapter à des connaissances et des situations nouvelles, deux grandes approches sont développées en informatique : les réseaux de neurones basés sur le modèle connexionniste (deep learning) pour l’apprentissage, et les méthodes symboliques et logiques capables de travailler à un niveau abstrait de description et de raisonnement. Les algorithmes d’intelligence artificielle reproduisant les processus de déduction, induction et abduction ont des applications en radiothérapie. Combinés à la radiomique, les réseaux de neurones ont obtenu de bons résultats en classification d’images, traitement du langage naturel, phénotypage à partir des dossiers patients, adaptation des traitements. Les approches logiques ont produit des ontologies formelles, des algorithmes déterministes pour la décision et des méthodes de vérification de cohérence des systèmes complexes. Une intelligence artificielle hybride conjuguant apprentissage et logique est nécessaire pour réaliser des tâches complexes allant au delà de l’intelligence artificielle qui réalise des tâches restreintes et spécialisées. Combinée à des modèles formalisant les connaissances physicobiologiques, l’intelligence artificielle est au cœur de nouveaux outils comme les jumeaux numériques (digital twins) nécessaires à la médecine de précision en oncologie.
Artificial intelligence is a highly polysemic term. In computer science, with the objective of being able to solve totally new problems in new contexts, artificial intelligence includes connectionism (neural networks) for learning and logics for reasoning. Artificial intelligence algorithms mimic tasks normally requiring human intelligence, like deduction, induction, and abduction. All apply to radiation oncology. Combined with radiomics, neural networks have obtained good results in image classification, natural language processing, phenotyping based on electronic health records, and adaptive radiation therapy. General adversial networks have been tested to generate synthetic data. Logics based systems have been developed for providing formal domain ontologies, supporting clinical decision and checking consistency of the systems. Artificial intelligence must integrate both deep learning and logic approaches to perform complex tasks and go beyond the so-called narrow artificial intelligence that is tailored to perform some highly specialized task. Combined together with mechanistic models, artificial intelligence has the potential to provide new tools such as digital twins for precision oncology.}
}
@article{ELJANATI2018436,
title = {SMART Education Framework for Adaptation Content Presentation},
journal = {Procedia Computer Science},
volume = {127},
pages = {436-443},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.141},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918301534},
author = {Salma {El Janati} and Abdelilah Maach and Driss {El Ghanami}},
keywords = {E-learning, ES, ALS, Physical disability, Media Adaptation, DAHS, Learner Model, Transcoding},
abstract = {The rapid growth of the Educational System (ES) has changed traditional learning behavior and presented a new situation to learners. To address the ever increasing needs and challenges associated with ES, a strategic methods and techniques of Adaptive Learning System (ALS) is adapted, which the primary objective is the adaptation of content, presentation, and navigation. In this paper, we suggest a new framework to adapt the Content Presentation to the preferences of learners and physical disability with learners who suffer from the visual and hearing limitations to stimulate the content. This framework is intended to be integrated in Dynamic Adaptive Hypermedia System (DAHS)in a manner that increases the learner’s perceived quality, high level of adaptivity and reliability.}
}
@article{YANG2020106598,
title = {A comparative analysis of bubble point pressure prediction using advanced machine learning algorithms and classical correlations},
journal = {Journal of Petroleum Science and Engineering},
volume = {185},
pages = {106598},
year = {2020},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2019.106598},
url = {https://www.sciencedirect.com/science/article/pii/S0920410519310198},
author = {Xi Yang and Birol Dindoruk and Ligang Lu},
keywords = {Bubble point pressure, PVT, Fluid properties, Machine learning methods (XGBoost, LightGBM, random forest regressor, MLP neural network, and super learner), Correlations},
abstract = {The need for fluid properties or PVT (Pressure-Volume-Temperature) properties, is part of the entire Exploration and Production (E&P) lifecycle from exploration to mature asset management to the typical later life events such as, Improved Oil Recovery (IOR). As the projects mature, the need for such data and its integration for various discipline-specific workflows and its interpretation in the light of reservoir performance varies. Among all the key PVT properties, bubble point pressure is probably the most important parameter. Bubble point pressure is important because it is the point at which constant composition and variable composition portions of the depletion paths merge. Geometrically, bubble point pressure appears to be a discontinuity. In addition, it dictates the existence (or not) of the incipient phase (i.e., gas phase) leading to the changes in the flow characteristics both in porous media and as well as within the wellbore and the facilities. Furthermore, it is also a good indicative of a possible gas cap when the reservoir is at saturation (reservoir pressure is equal to the bubble point pressure) or near-saturated. Among the highlighted uses, there are many more used such as the determination of the elements of miscibility, gas lift design, etc. Therefore, it is very important to estimate the bubble point pressure accurately. In this study, tree-based advanced machine learning algorithm including XGBoost, LightGBM, and random forest regressor, and multi-layer perceptron (neural network) regressor are implemented to predict bubble point pressure (Pbp). A novel super learner model which is also known as stacking ensemble is used to enhance base machine learning model performance on predicting bubble point pressure. Three datasets with different predictors are prepared to study machine learning algorithms' performance for three situations: only compositional data are available; only bulk properties (Gas-Oil-Ratio, gas gravity, API gravity and reservoir Temperature) are available; both compositional data and bulk properties are available. Through literature review, there is no research on using only compositional data and temperature to predict bubble point pressure. Our super learner model offers an accurate solution for oil bubble point pressure when only compositional data and temperature are available. Machine learning models perform better than empirical correlations with limited input data (i.e., bulk properties). When compositional data and bulk properties are all used as predictors, super learner reaches about 5.146% mean absolute relative error on predicting the bubble point pressure from global samples with bubble point pressures in the range of 100 to 10,000 psi, which is a wider range compared to most ANN models published in literature.}
}
@article{ZHANG2020105958,
title = {Knowledge distilling based model compression and feature learning in fault diagnosis},
journal = {Applied Soft Computing},
volume = {88},
pages = {105958},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105958},
url = {https://www.sciencedirect.com/science/article/pii/S1568494619307392},
author = {Wenfeng Zhang and Gautam Biswas and Qi Zhao and Hongbo Zhao and Wenquan Feng},
keywords = {Fault diagnosis, Neural networks, Knowledge distilling, Feature learning},
abstract = {Recently, there has been interest in developing diagnosis methods that combine model-based and data-driven diagnosis. In both approaches, selecting the relevant measurements or extracting important features from historical data is a key determiner of the success of the algorithm. Recently, deep learning methods have been effective in automating the feature selection process. Autoencoders have been shown to be an effective neural network configuration for extracting features from complex data, however, they may also learn irrelevant features. In addition, end-to-end classification neural networks have also been used for diagnosis, but like autoencoders, this method may also learn unimportant features thus making the diagnostic inference scheme inefficient. To rapidly extract significant fault features, this paper employs end-to-end networks and develops a new feature extraction method based on importance analysis and knowledge distilling. First, a set of cumbersome neural network models are trained to predict faults and some of their internal values are defined as features. Then an occlusion-based importance analysis method is developed to select the most relevant input variables and learned features. Finally, a simple student neural network model is designed based on the previous analysis results and an improved knowledge distilling method is proposed to train the student model. Because of the way the cumbersome networks are trained, only fault features are learned, with the importance analysis further pruning the relevant feature set. These features can be rapidly generated by the student model. We discuss the algorithms, and then apply our method to two typical dynamic systems, a communication system and a 10-tank system employed to demonstrate the proposed approach.}
}
@article{SUN202030,
title = {A comprehensive hybrid first principles/machine learning modeling framework for complex industrial processes},
journal = {Journal of Process Control},
volume = {86},
pages = {30-43},
year = {2020},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2019.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S095915241930592X},
author = {Bei Sun and Chunhua Yang and Yalin Wang and Weihua Gui and Ian Craig and Laurentz Olivier},
keywords = {Comprehensive state space, Descriptive system, Modeling, Machine learning},
abstract = {The selection of an appropriate descriptive system and modeling framework to capture system dynamics and support process control applications is a fundamental problem in the operation of industrial processes. In this study, to account for the highly complex dynamics of industrial process and additional requirements imposed by smart and optimal manufacturing systems, an extended state space descriptive system, named comprehensive state space, is first designed. Then, based on the descriptive system, a hybrid first principles/machine learning modeling framework is proposed. The hybrid model is formulated as a combination of a nominal term and a deviation term. The nominal term covers the underlying physicochemical principles. The deviation term handles the effects of high-dimensional influence factors using regression of low-dimensional deep process features. To handle the multimodal and time-varying properties of process dynamics, the comprehensive state space is divided into subspaces indicating different operating conditions. The model parameters are identified and trained for each operating condition to form the sub-models. Then the system dynamics are formulated as a weighted sum of sub-models, with the weights being the probabilities that the current operating point belongs to different operating conditions. The weights update with the movement of the operating point in the comprehensive state space. Moreover, the descriptive system provides a platform for visualization, and can act as a digital twin of the physical process. A case study illustrates the feasibility and performance of the proposed descriptive system.}
}
@incollection{LAZZARI2020187,
title = {Chapter Six - Toward a digital polymer reaction engineering},
editor = {Davide Moscatelli and Mattia Sponchioni},
series = {Advances in Chemical Engineering},
publisher = {Academic Press},
volume = {56},
number = {1},
pages = {187-227},
year = {2020},
booktitle = {Advances in Polymer Reaction Engineering},
issn = {0065-2377},
doi = {https://doi.org/10.1016/bs.ache.2020.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065237720300211},
author = {Stefano Lazzari and Andree Lischewski and Yury Orlov and Peter Deglmann and Andreas Daiss and Eduard Schreiner and Hugo Vale},
keywords = {Polymer, Digitalization, Digital twin, Modeling, Kinetics, Thermodynamics, Computational fluid dynamics, Quantum chemistry, Molecular dynamics, Machine learning},
abstract = {What is digitalization, and why do we need it? What does digitalization mean for research and development in polymer reaction engineering (PRE)? In this chapter, we address these questions, starting from a global perspective, briefly analyzing the current situation. We then illustrate our views on how the virtual representation of a polymerization process, i.e., a digital twin, would enable us to widen the spectrum of research and development in PRE. The main tasks of the digital twin are, in our opinion, to approach two yet unsolved problems: linking reaction conditions to final material properties and producing materials in an optimal way. Our strategy to tackle these issues is to combine physical and data science models. We analyze how the predictive ability of deterministic approaches, such as kinetic, thermodynamic and fluid mechanics models is improved by parameters and mechanistic understanding provided by quantum chemistry and molecular dynamics. While we constantly aim for a deeper physicochemical understanding to improve these physical models, we recognize the power of statistics and machine learning, as they enable us to approach highly complex problems, thanks to their large flexibility. When analyzing the current limitations of each technique, we believe that concrete progress can only be achieved if we systematically centralize our knowledge. Therefore, we conclude our perspective on digitalization in PRE by advocating for initiatives that foster an open source sharing of published experimental data, parameters, models, and algorithms.}
}
@article{TACOMA2020106276,
title = {Enhancing learning with inspectable student models: Worth the effort?},
journal = {Computers in Human Behavior},
volume = {107},
pages = {106276},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106276},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220300327},
author = {Sietske Tacoma and Corine Geurts and Bert Slof and Johan Jeuring and Paul Drijvers},
keywords = {Feedback-seeking behavior, Higher education, Inspectable student model, Log file analysis, Statistics education},
abstract = {In electronic learning environments, information about a student's performance can be provided to the student in the form of an inspectable student model. While relatively easy to implement, little is known about whether students use the feedback provided by such models and whether they benefit from it. In this study, the use of inspectable student models in an introductory university statistics course by 599 first-year social science students was monitored. Research questions focused on whether students sought feedback from the student models, which decisions for subsequent study steps they made, and how this feedback seeking and decision making related to results on their statistics exams. Results showed a large variety among students in feedback-seeking and decision-making behavior. Lower student model scores seemed to encourage students to practice more on the same topic and higher scores seemed to evoke the decision to move to a different topic. Viewing frequency and amount of variety in decision making were positively related to exam results, even when controlling for total time students worked. These findings imply that inspectable student models can be a valuable addition to electronic learning environments and suggest that more intensive use of inspectable student models may contribute to learning.}
}
@article{JING2020644,
title = {A Learner Model Integrating Cognitive and Metacognitive And Its Application on Scratch Programming Projects},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {5},
pages = {644-649},
year = {2020},
note = {3rd IFAC Workshop on Cyber-Physical & Human Systems CPHS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.154},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321002913},
author = {Sifeng Jing and Ying Tang and Xiwei Liu and Xiaoyan Gong},
keywords = {learner model, cognitive state, metacognitive ability, individualized teaching},
abstract = {learner’s cognitive and metacognitive are key personal profile for individualized teaching. To evaluate learner’s comprehensive characteristics, existing learner model were reviewed. Two challenges of constructing an accurate and comprehensive learner model integrating cognitive and metacognitive were summarized. A plan of constructing a comprehensive learner model was made based on analysis of existing massive online learning environment, sensor information technology and educational data-mining. As a case study, a method of how to map learning data onto learners’ cognitive and metacognitive was proposed based on an analysis of a number of pupils’ Scratch projects. Three mapping table were established. Pupil’s cognitive skill could be evaluated from technology shown from Scratch project, namely, data structure, algorithm, computational practices and overall evaluation. Content shown from Scratch project were used to infer pupil’s cognitive style. Meta-cognitive ability can be measured from computational practices and behavior in programming process.}
}
@article{WU2020101695,
title = {Multi-teacher knowledge distillation for compressed video action recognition based on deep learning},
journal = {Journal of Systems Architecture},
volume = {103},
pages = {101695},
year = {2020},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2019.101695},
url = {https://www.sciencedirect.com/science/article/pii/S1383762119305028},
author = {Meng-Chieh Wu and Ching-Te Chiu},
keywords = {Deep convolutional model compression, Action recognition, Knowledge distillation, Transfer learning},
abstract = {Recently, Convolutional Networks have great progress in classifying images. While action recognition is different from still image classification, video data contains temporal information which plays an important role in video understanding. Currently most CNN-based approaches for action recognition has excessive computational costs, an explosion of parameters and computation time. The most efficient method currently trained a deep network directly on the compressed video contains the motion information. However, this method has a large number of parameters. We propose a multi-teacher knowledge distillation framework for compressed video action recognition to compress this model. With this framework, the model is compressed by transferring the knowledge from multiple teachers to a single small student model. With multi-teacher knowledge distillation, students learn better than single-teacher knowledge distillation. Experiments show that we can reach a 2.4 ×  compression rate in number of parameters and 1.2 ×  computation reduction with 1.79% loss of accuracy on the UCF-101 dataset and 0.35% loss of accuracy on the HMDB51 dataset.}
}
@article{MIN2019502,
title = {Machine Learning based Digital Twin Framework for Production Optimization in Petrochemical Industry},
journal = {International Journal of Information Management},
volume = {49},
pages = {502-519},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218311484},
author = {Qingfei Min and Yangguang Lu and Zhiyong Liu and Chao Su and Bo Wang},
keywords = {digital twin, machine learning, internet of things, petrochemical industry, production control optimization},
abstract = {Digital twins, along with the internet of things (IoT), data mining, and machine learning technologies, offer great potential in the transformation of today’s manufacturing paradigm toward intelligent manufacturing. Production control in petrochemical industry involves complex circumstances and a high demand for timeliness; therefore, agile and smart controls are important components of intelligent manufacturing in the petrochemical industry. This paper proposes a framework and approaches for constructing a digital twin based on the petrochemical industrial IoT, machine learning and a practice loop for information exchange between the physical factory and a virtual digital twin model to realize production control optimization. Unlike traditional production control approaches, this novel approach integrates machine learning and real-time industrial big data to train and optimize digital twin models. It can support petrochemical and other process manufacturing industries to dynamically adapt to the changing environment, respond in a timely manner to changes in the market due to production optimization, and improve economic benefits. Accounting for environmental characteristics, this paper provides concrete solutions for machine learning difficulties in the petrochemical industry, e.g., high data dimensions, time lags and alignment between time series data, and high demand for immediacy. The approaches were evaluated by applying them in the production unit of a petrochemical factory, and a model was trained via industrial IoT data and used to realize intelligent production control based on real-time data. A case study shows the effectiveness of this approach in the petrochemical industry.}
}
@article{NAFEE2020230,
title = {Machine learning to predict venous thrombosis in acutely ill medical patients},
journal = {Research and Practice in Thrombosis and Haemostasis},
volume = {4},
number = {2},
pages = {230-237},
year = {2020},
issn = {2475-0379},
doi = {https://doi.org/10.1002/rth2.12292},
url = {https://www.sciencedirect.com/science/article/pii/S2475037922019690},
author = {Tarek Nafee and C. Michael Gibson and Ryan Travis and Megan K. Yee and Mathieu Kerneis and Gerald Chi and Fahad AlKhalfan and Adrian F. Hernandez and Russell D. Hull and Ander T. Cohen and Robert A. Harrington and Samuel Z. Goldhaber},
keywords = {acute medically ill, machine learning, personalized medicine, super learner, venous thromboembolism},
abstract = {Background
The identification of acutely ill patients at high risk for venous thromboembolism (VTE) may be determined clinically or by use of integer‐based scoring systems. These scores demonstrated modest performance in external data sets.
Objectives
To evaluate the performance of machine learning models compared to the IMPROVE score.
Methods
The APEX trial randomized 7513 acutely medically ill patients to extended duration betrixaban vs. enoxaparin. Including 68 variables, a super learner model (ML) was built to predict VTE by combining estimates from 5 families of candidate models. A “reduced” model (rML) was also developed using 16 variables that were thought, a priori, to be associated with VTE. The IMPROVE score was calculated for each patient. Model performance was assessed by discrimination and calibration to predict a composite VTE end point. The frequency of predicted risks of VTE were plotted and divided into tertiles. VTE risks were compared across tertiles.
Results
The ML and rML algorithms outperformed the IMPROVE score in predicting VTE (c‐statistic: 0.69, 0.68 and 0.59, respectively). The Hosmer‐Lemeshow goodness‐of‐fit P‐value was 0.06 for ML, 0.44 for rML, and <0.001 for the IMPROVE score. The observed event rate in the lowest tertile was 2.5%, 4.8% in tertile 2, and 11.4% in the highest tertile. Patients in the highest tertile of VTE risk had a 5‐fold increase in odds of VTE compared to the lowest tertile.
Conclusion
The super learner algorithms improved discrimination and calibration compared to the IMPROVE score for predicting VTE in acute medically ill patients.}
}
@article{FERRARIO2019663,
title = {A Multipurpose Small-Scale Smart Factory For Educational And Research Activities},
journal = {Procedia Manufacturing},
volume = {38},
pages = {663-670},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.085},
url = {https://www.sciencedirect.com/science/article/pii/S235197892030086X},
author = {Andrea Ferrario and Matteo Confalonieri and Andrea Barni and Gabriele Izzo and Giuseppe Landolfi and Paolo Pedrazzoli},
keywords = {Smart Factories, Industrial Internet of Thing, Engineering Education, Production Planning, Scheduling, Inventory Control, Engineering Design Methods, Tools for Industry 4.0},
abstract = {As manufacturing industry is moving its steps towards a more digital, smart and flexible scenario, the changes required to achieve the expectations of the Industry 4.0 (I4.0) framework are numerous and extensive. Though, a general lack of understanding of how new technologies could be integrated and shall be implemented is present, limiting the rate of adoption of such changes and the related beneficial impacts. This paper describes the implementation of a smart-factory at the University of Applied Sciences and Arts of Southern Switzerland (SUPSI) aiming at filling the gap in I4.0 related skills development thorough a learning by doing approach, and providing a research platform that could foster collaboration of practitioners and academia on the development and testing of new technologies. The developed factory integrates different production technologies such as additive manufacturing, laser processing and milling, in order to produce a highly customizable item consisting in a TANGRAM game-set, packaged into personalized boxes. The entire factory is coupled with its digital twin, which is fed by an exhaustive monitoring infrastructure composed by vision systems and high precision measurement instruments, allowing to track in real time plant processes. The design has been carried out in order to make the smart factory serving as a mean to face both educational and research challenges at many different levels. As an educational mean, students and professionals have the chance to dive into manufacturing history experiencing both classical automation topics (PLC, MES and SCADA programming, precision axes control and pneumatics), as well as more advanced technologies, typical of the most advanced smart-factories (IoT, vision systems, simulation and digital twin, advanced measuring methods and smart production management systems). From a research point of view, the factory functions as a pilot plant for internal research and applied industrial projects, on the top of which applications, manufacturing methods and technologies are developed, tested and integrated.}
}
@article{MUKHERJEE201959,
title = {A digital twin for rapid qualification of 3D printed metallic components},
journal = {Applied Materials Today},
volume = {14},
pages = {59-65},
year = {2019},
issn = {2352-9407},
doi = {https://doi.org/10.1016/j.apmt.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2352940718304931},
author = {T. Mukherjee and T. DebRoy},
keywords = {Additive manufacturing, Digital twin, Machine learning, Big data, Mechanistic model},
abstract = {The customized production of complex components by 3D printing has been hailed as a potentially transformative tool in manufacturing with important applications in health care, automotive and aerospace industries. However, after about a quarter of a century of research and development, only a handful of commercial alloys can be printed and the market value of all 3D printed products now amounts to a negligible portion of the manufacturing economy. This difficulty is attributable to a remarkable diversity in structure and properties of the printed components and susceptibility to defects. In addition, the current practice of qualifying components by prolonged trial and error with expensive printing equipment and feed stock material confine the printed products to a niche market where the high product cost and the delay in the qualification are not critical factors. Here we explain how a digital twin or a digital replica of the printing machine will reduce the number of trial and error tests to obtain desired product attributes and reduce the time required for part qualification to make the printed components cost effective. It is shown that a comprehensive digital twin of 3D printing machine consisting of mechanistic, control and statistical models of 3D printing, machine learning and big data can reduce the volume of trial and error testing, reduce defects and shorten time between the design and production.}
}
@article{XIONG2018481,
title = {Vehicle grid integration for demand response with mixture user model and decentralized optimization},
journal = {Applied Energy},
volume = {231},
pages = {481-493},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.09.139},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918314508},
author = {Yingqi Xiong and Bin Wang and Chi-cheng Chu and Rajit Gadh},
keywords = {Electric vehicle, Demand response, Distributed optimization, Machine learning, Topic mining},
abstract = {With the rapidly growing electric vehicle adoption rate and increasing number of public electric vehicle charging stations in recent years, electric vehicle becomes more and more critical in the demand response programs. Development in Vehicle to Grid technology has converted electric vehicle to distributed energy resources. Using the Electric Vehicle Smart Charging Infrastructures on UCLA campus and city of Santa Monica as testbeds, we have collected real-world datasets of electric vehicle usage, based on which, we proposed optimal bi-directional charging control strategies to integrate electric vehicle in commercial and public parking facilities into the power grid as distributed energy resources for demand response programs by two-stage distributed optimization and water-filling algorithm. Driver behavioral uncertainties have been considered in our approach. Specifically, electric vehicle users are clustered by their behavioral patterns using a modified Latent Semantic Analysis. The first-stage optimization is performed to minimize energy cost using day-ahead wholesale energy price with predictions on energy demand and electric vehicle availability which generated by a mixture user model. Decentralized optimization (second-stage) is carried out on the next day in real-time to control individual electric vehicle so that the aggregated load can follow the first-stage optimal profile. As an alternative, a fast converging water filling algorithm is proposed and compared with two-stage optimization. Extensive simulation results show that proposed charging controls can utilize electric vehicle as distributed energy resource to accommodate demand response program while satisfying electric vehicle energy demands and providing significant energy cost savings.}
}
@article{YEN2019547,
title = {Design of a computational model for social learning support and analystics},
journal = {Computers in Human Behavior},
volume = {92},
pages = {547-561},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.07.042},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218303649},
author = {Neil Y. Yen and Jason C. Hung and Chia-Chen Chen and Qun Jin},
keywords = {Social learning, Social network analytics, Social knowledge, Crowdsourcing, Human-centered computing, User modeling},
abstract = {Conventional online learning typically allows an instructor to deliver instruction to students via a predefined curriculum and within a fixed knowledge structure (i.e., explaining the instructional subject). With the dramatic growth of social media technology and correlated data aggregation, some sort of instant knowledge is obtained by daily users. An emerging type of knowledge (i.e., social knowledge) has been identified and may lead to self-paced learning from social networks, which is simply defined as social learning. This article points out three important issues for social learning, namely, knowledge retrieval via temporal social factors, and the connection between social network and the knowledge domain. Two significant automation mechanisms, lecture generation for self-regulated learning and influencing domain computation for opportunity finding, are suggested to facilitate the process of social learning. A prototype system based on Elgg was implemented, sourced by a federated repository that has stored and shared more than 1.5 millions transactions (e.g., content, interactions, etc.). We conclude that timely social knowledge (or crowdsourcing results) can be widely applied in the next era of online learning environment. Findings through the statistical analysis are prospective to support understanding of phenomenon of social learning and design of future learning platform for followup researchers.}
}
@article{BHATT2018144,
title = {iABC: Towards a hybrid framework for analyzing and classifying behaviour of iOS applications using static and dynamic analysis},
journal = {Journal of Information Security and Applications},
volume = {41},
pages = {144-158},
year = {2018},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S2214212617303812},
author = {Arpita Jadhav Bhatt and Chetna Gupta and Sangeeta Mittal},
keywords = {iOS applications, Reverse engineering, Machine learning, Static analysis, Dynamic analysis, Static risk score},
abstract = {Is this app safe to use? - A wrong decision can result in privacy breach in iOS devices. In this digital era users extensively use smart devices to store their personal and important information. To ease users’ tasks, thousands of free or paid apps are available in app store. However, recent studies reveal startling facts about various attacks and data harvesting incidents through these apps, where personal data is put at risk. Through this paper, we propose a permission induced risk model- iOS Application analyzer and Behavior Classifier (iABC), for iOS devices to detect privacy violations arising due to granting permissions during installation of applications. It is a two-layer process comprising of static and dynamic analysis. It uses reverse engineering to extract permission variables from applications and computes a risk score for each application using ranking algorithms. The approach considers application's category as a key feature for detecting malicious applications while computing static risk score. Different machine learning classifiers were employed to evaluate 1,150 applications. The empirical results show that our proposed model gives detection rate of 97.04%. Furthermore, to assess privacy breaches by applications at run time, dynamic analysis on 50 applications has been performed to obtain dynamic risk scores of installed apps.}
}
@article{ALGEDDAWY20201799,
title = {A Digital Twin Creation Method for an Opensource Low-cost Changeable Learning Factory},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1799-1805},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.250},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321284},
author = {Tarek Al-Geddawy},
keywords = {Learning Factory, Digital Twin, Changeable Manufacturing},
abstract = {Learning factories demonstrate applications and technology to students in a real industrial environment. While turnkey changeable learning factories are supplied by many vendors, with some digital twin capabilities, they are mostly a closed box, with very little flexibility to change the underlying architecture or technology, hindering the maximum benefit of student hands-on experience. This paper presents a method to build a simulated changeable learning factory and link it to the physical system to create a digital twin. The studied learning factory (LEAF) is an opensource low-cost changeable automated system. The suggested digital environment is ‘RoboDK’, which is a 3D simulation and offline/online programming environment, mainly for industrial robots, but it also offers an open source ‘Python’ programming library, allowing the extension of the capabilities of the software to adapt to LEAF. The method is also using the opensource Modbus TCP and OPC UA industrial communication protocols to establish the connection between the physical modules and the digital objects. The results show a capable digital system that is accurately mirroring the physical system layout and material flow, with a flexible structure to allow future extensions.}
}
@article{WEI2019102915,
title = {A vision and learning-based indoor localization and semantic mapping framework for facility operations and management},
journal = {Automation in Construction},
volume = {107},
pages = {102915},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102915},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519301219},
author = {Yujie Wei and Burcu Akinci},
keywords = {Facility operation and management, Image, Indoor localization, Semantic understanding, Multi-task deep learning, Convolutional neural network},
abstract = {Recent research on facility management has focused on leveraging location-based services (LBS) to assist on-demand access to model information and on-site documentation. Fast and robust indoor localization is of great importance for location-based facility management services, such as the ones used in mobile computing settings. However, there are several challenges in achieving fast and robust indoor localization: 1) Signal-based indoor localization methods, such as WIFI, RFID, Bluetooth and Ultrasound, require installation of extra infrastructures in a building to support localization; 2) Visual-based indoor localization methods, such as LiDAR and camera, depend on feature point detection and matching, which require heavy computation and can also be impacted by environmental conditions, such as lighting and texture richness. In addition to these, existing localization methods do not support semantic understanding which is of great importance when associating a component with its digital twin. To address the stated problems above, this paper presents a vision and learning-based framework that utilizes a shared convolutional neural network to perform localization and semantic segmentation simultaneously. The proposed framework can support facility management by locating facility components within a building and associate them with their digital twins in an information repository. Compared to conventional methods, the developed image-based indoor localization and semantic mapping framework has the following advantages: 1) It only requires image as input to support localization, semantic understanding, and association, which eliminates the need for extra infrastructure, such as deployment of RFID tags, etc.; 2) It reuses the feature extraction network for simultaneous localization and semantic understanding, which saves computing resources; 3) With 6-DoF poses and semantic labels, it supports component-level association. The authors evaluated the proposed framework on publicly available data sets using three metrics: localization accuracy, semantic segmentation accuracy, and association success rate. The results show that the proposed image-based method can achieve 6-DoF localization and semantic segmentation concurrently. Also, formalized experiments on a synthetic data set with different noise levels introduced to localization and semantic segmentation showed that a main factor affecting the performance of association of an image to its digital twin is the accuracy of its localization.}
}
@article{MOCHIZUKI201936,
title = {The lived experience of thesis writers in group writing conferences: The quest for “perfect” and “critical”},
journal = {Journal of Second Language Writing},
volume = {43},
pages = {36-45},
year = {2019},
note = {Special Issue: Thesis and dissertation writing in a second language: Context, identity, genre},
issn = {1060-3743},
doi = {https://doi.org/10.1016/j.jslw.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1060374317302539},
author = {Naoko Mochizuki},
keywords = {Thesis writing, Perezhivanie (lived experience), Writing conferences, Social interactions, Sociocultural theory},
abstract = {While group writing conferences have become a popular means of encouraging social interactions among doctoral students, little is known about how these group interactions influence a writer’s learning of the thesis genre. Taking a genre as social practice perspective (Tardy, 2009), this study uses the analytical lens of perezhivanie (lived experience) (Vygotsky, 1994) to investigate how doctoral students perceive group writing conferences. Previous studies of thesis writers’ experiences have suggested the interconnectedness of cognition, emotions, and the social contexts in their learning processes. The study focuses on the role that the oral interactions around the text play within doctoral students’ social situations of learning thesis writing. The participants are two L2 doctoral students in group writing conferences run by the learning centre at an Australian university. Data were collected through observation and audio-recording of group discussions, interviews with students and facilitators, and students’ writing drafts. The findings reveal the students’ ‘models’ in their minds as drivers of their learning thesis writing. Underlying those perceived ‘models’ were some social and ideological forces related to ‘native-speaker’ English. The study illuminates the role of writing conferences in assisting students’ co-constructing processes of the thesis genre in their social situations.}
}
@article{HOSE201938,
title = {Cardiovascular models for personalised medicine: Where now and where next?},
journal = {Medical Engineering & Physics},
volume = {72},
pages = {38-48},
year = {2019},
note = {Special issue to commemorate the 40th anniversary of Medical Engineering & Physics},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1350453319301626},
author = {D. Rodney Hose and Patricia V. Lawford and Wouter Huberts and Leif Rune Hellevik and Stig W. Omholt and Frans N. {van de Vosse}},
keywords = {Cardiovascular modelling, Model personalisation, Model, Uncertainity, Physiological modelling, Clinical descision support},
abstract = {The aim of this position paper is to provide a brief overview of the current status of cardiovascular modelling and of the processes required and some of the challenges to be addressed to see wider exploitation in both personal health management and clinical practice. In most branches of engineering the concept of the digital twin, informed by extensive and continuous monitoring and coupled with robust data assimilation and simulation techniques, is gaining traction: the Gartner Group listed it as one of the top ten digital trends in 2018. The cardiovascular modelling community is starting to develop a much more systematic approach to the combination of physics, mathematics, control theory, artificial intelligence, machine learning, computer science and advanced engineering methodology, as well as working more closely with the clinical community to better understand and exploit physiological measurements, and indeed to develop jointly better measurement protocols informed by model-based understanding. Developments in physiological modelling, model personalisation, model outcome uncertainty, and the role of models in clinical decision support are addressed and ‘where-next’ steps and challenges discussed.}
}
@article{MERZ2020215,
title = {A Cloud-Based Research and Learning Factory for Industrial Production},
journal = {Procedia Manufacturing},
volume = {45},
pages = {215-221},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.097},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920311392},
author = {Robert Merz and Ralph Hoch and Damian Drexel},
keywords = {Cloud-based research, Learning Factory, Digital Factory},
abstract = {The Digital Factory Vorarlberg is the youngest Research Center of Vorarlberg University of Applied Sciences. In the lab of the research center a research and learning factory has been established for educating students and employees of industrial partners. We devised learning scenarios and developed courses addressing a wide variety of topics related to Industry 4.0 and showcase best practice scenarios for various topics of digitalization. In addition, novel methods and technologies for digital production, cloud-based manufacturing, data analytics, IT- and OT-security or digital twins are being developed. A centralized SCADA (Supervisory Control and Data Acquisition)-System is the core data hub for the factory. As an alternative to on premise manufacturing, orders can be pushed into a cloud-based manufacturing platform, which has been developed at the Digital Factory. In this paper, we present the basic concept of the Digital Factory Vorarlberg, some of the newly developed topics as well as learning scenarios for students and industry staff.}
}
@article{YAGO201848,
title = {ON-SMMILE: Ontology Network-based Student Model for MultIple Learning Environments},
journal = {Data & Knowledge Engineering},
volume = {115},
pages = {48-67},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17301945},
author = {Hector Yago and Julia Clemente and Daniel Rodriguez and Pedro Fernandez-de-Cordoba},
keywords = {Ontological engineering, Student modeling, Ontology network, Learning supervision, Semantic web},
abstract = {Currently, many educational researchers focus on the extraction of information about the learning progress to properly assist students. We present ON-SMMILE, a student-centered and flexible student model which is represented as an ontology network combining information related to (i) students and their knowledge state, (ii) assessments that rely on rubrics and different types of objectives, (iii) units of learning and (iv) information resources previously employed as support for the student model in intelligent virtual environment for training/instruction and here extended. The aim of this work is to design and build methodologically, throughout ontological engineering, the ON-SMMILE model to be used as support of future works closely linked to supervision of student's learning as competence-based recommender system. For this purpose, our model is designed as a set of ontological resources that have been extended, standardized, interrelated and adapted to be used in multiple learning environments. In this paper, we also analyze the available approaches based on instructional design which can be added to ontology network to build the proposed model. As a case study, a chemical experiment in a virtual environment and its instantiation are described in terms of ON-SMMILE.}
}
@article{SYKES201866,
title = {Reasoning about ideal interruptible moments: A soft computing implementation of an interruption classifier in free-form task environments},
journal = {International Journal of Human-Computer Studies},
volume = {120},
pages = {66-93},
year = {2018},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918303471},
author = {Edward R. Sykes},
keywords = {Human computer interaction, Interruption, Machine learning, Soft computing applications, Situationally appropriate interaction, Workload},
abstract = {Current trends in society and technology make the concept of interruption a central human computer interaction problem. In this work, a novel soft computing implementation for an Interruption Classifier was designed, developed and evaluated that draws from a user model and real-time observations of the user's actions as s/he works on computer-based tasks to determine ideal times to interact with the user. This research is timely as the number of interruptions people experience daily has grown considerably over the last decade. Thus, systems are needed to manage interruptions by reasoning about ideal timings of interactions. This research shows: (1) the classifier incorporates a user model in its’ reasoning process. Most of the research in this area has focused on task-based contextual information when designing systems that reason about interruptions; (2) the classifier performed at 96% accuracy in experimental test scenarios and significantly outperformed other comparable systems; (3) the classifier is implemented using an advanced machine learning technology—an Adaptive Neural-Fuzzy Inference System—this is unique since all other systems use Bayesian Networks or other machine learning tools; (4) the classifier does not require any direct user involvement—in other systems, users must provide interruption annotations while reviewing video sessions so the system can learn; and (5) a promising direction for reasoning about interruptions for free-form tasks–this is largely an unsolved problem.}
}
@article{HARRIS2018103,
title = {Spatial evolution of regularization in learned behavior of animals},
journal = {Mathematical Biosciences},
volume = {299},
pages = {103-116},
year = {2018},
issn = {0025-5564},
doi = {https://doi.org/10.1016/j.mbs.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0025556417306648},
author = {Dakari Harris and Dominik Wodarz and Natalia L. Komarova},
abstract = {Stochastic population dynamics of learned traits are studied, where individual learners behave according to a reinforcement learner model, which is a nonlinear version of the Bush–Mosteller model. Depending on a regularization parameter (parameter a), the learners may possess different degrees of overmatching (regularization behavior, 0 ≤ a < 1), frequency matching (corresponding to a=1), or undermatching behavior (a > 1). Both non-spatial and spatial models are considered, to study the interplay of individual heterogeneity of behavior, spatial and temporal effects of learning, and the possibility of emergence of regional culture. In non-spatial models, we observe that populations of individuals learning from each other converge to a universally shared, deterministic rule (either rule “1” or rule “0”), only if they to some extent possess the ability to generalize (a < 1). Otherwise, a low-coherence solution where both rules are used intermittently by everyone, is achieved. If the evolution of the regularization ability is included, then we find that a initially evolves toward lower values, and a shared solution is established when everyone reliably uses the same rule. The spatial (2D) model has two well known limiting cases: if a=0 (the strongest degree of regularization), the model converges to a threshold voter model, and if a=1 (frequency matching), it is equivalent to the discrete diffusion equation. If 0 < a < 1 (the case where individuals regularize), spatial patterns emerge, where patches of different usage of the rule are formed. Smaller values of a lead to sharper and longer lived patches. Values of a < 1 close to unity result in probabilistic outcomes where patches only survive if they are attached to the boundary. Analytical treatment of the 1D case reveals the existence of approximate equilibria that have front structure, where spatially intermittent deterministic usage of one and the other rule are separated by interfaces whose analytical form is derived.}
}
@incollection{AGAPAKI202065,
title = {Chapter 3 - Scene understanding and model generation},
editor = {Ioannis Brilakis and Carl Haas},
booktitle = {Infrastructure Computer Vision},
publisher = {Butterworth-Heinemann},
pages = {65-167},
year = {2020},
isbn = {978-0-12-815503-5},
doi = {https://doi.org/10.1016/B978-0-12-815503-5.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128155035000036},
author = {Eva Agapaki and Mohammad Nahangi},
keywords = {3D object fitting, BIM, Data management, Deep learning, Digital twins, IFC, Segmentation, Shape classification},
abstract = {ICV scene understanding provides an overview about various object detection and fitting techniques that can produce accurate information for the digital twin (DT) generation of existing assets. The aim of this chapter is to acquire digital twins of existing infrastructure with the least amount of human intervention and give readers the appropriate tools to apply ICV scene understanding and fitting techniques for specific domain applications. The chapter gives an overview of scene understanding for infrastructure applications and user requirements and explores deep learning methods used so far on shape detection and the potential for its use in ICV scene understanding.}
}
@incollection{SUSILA2020247,
title = {Chapter Ten - Impact of cloud security in digital twin},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {247-263},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300488},
author = {Nagarajan Susila and Anand Sruthi and Sakthivel Usha},
keywords = {Cloud security, Digital twin prototype, Digital twin instance, Prognostics, Skyhigh security, Virtualization, Internet of things},
abstract = {Digital Twin is a way to virtually represent or model a physical object using the real time data. This innovation sets up a way to deal with industries and organizations to supervise their products, consequently bridging the gap between design and implementations. As the name suggests, “Digital Twin” infers that a reproduction of the product is made in order to have a nearby relationship with the live item. The procedure of computerized twin begins by gathering real time data, processed data, and operational data and performs distinctive investigation which helps in anticipating the future. This additionally enhances the customer experiences by giving a digital feel of their product. The objective behind all these is the job of gathering information and putting them in a place, i.e., the cloud which could store exorbitant data. The user experience gets enhanced by the intervention of digital twin technology which could help in the successful working of the products geographically distributed. The impact of Internet of Things and Cloud Computing lifts up the digital twin. The information gathered from the sources can be arranged in terms of utilization and prospect to change on a timely basis. These data, as they are stored require proper coordination and a legitimate use. Digital Twin innovation assumes incredible opportunities in the field of manufacturing, healthcare, smart cities, automobile and so on. The effect of having a digital twin for the product makes it simple for activities and recognize the blemishes, if any happened. This approach can help reduce the workload and furthermore can get trained on the virtual machine without the need of a specific training. With the most prevailing technologies of today, like Artificial Intelligence, Machine Learning and Internet of Things more prominent approach to train and monitor products, taking care of its own execution, collaborating to different frameworks, performing self-repairs are made possible. Hence the future is getting unfolded with the emerging DIGITAL TWIN era. The massive data utilized in the field of digital twin is prone to severe security breaches. Thus digital twin technology should be handled with extreme care so as to protect the data. Hence, this chapter identifies the ways and means of collecting, organizing and storing the data in a secured cloud environment. The data is filtered according to the use and priority and pushed into the cloud. It is determined to implement an exclusive algorithm for a secured cloud which would greatly benefit the users and the providers to handle and process it effectively.}
}
@article{TVENGE202036,
title = {Added value of a virtual approach to simulation-based learning in a manufacturing learning factory},
journal = {Procedia CIRP},
volume = {88},
pages = {36-41},
year = {2020},
note = {13th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 17-19 July 2019, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S221282712030322X},
author = {Nina Tvenge and Olga Ogorodnyk and Niels Peter Østbø and Kristian Martinsen},
keywords = {learning, simulation-based learning, manufacturing learning factory, cognitive processes, VR, AR},
abstract = {More and more learning factories (LF) are set up supporting the vision of Industry 4.0 connectivity and automation levels; thus, digital twins, virtual and augmented reality are emerging tools. Literature review is the basis for the discourse on possible constraints and opportunities these tools have for the cognitive learning processes in a simulation. State of the art give insight in the lack of research on value of digital learning activities in learning factory setting. This paper is a concept description, giving input to the community on aspects to be considered regarding the use VR/AR/digital twins in a learning factory context.}
}
@article{TOIVONEN2018135,
title = {The FMS Training Center - a versatile learning environment for engineering education},
journal = {Procedia Manufacturing},
volume = {23},
pages = {135-140},
year = {2018},
note = {“Advanced Engineering Education & Training for Manufacturing Innovation”8th CIRP Sponsored Conference on Learning Factories (CLF 2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918304785},
author = {Ville Toivonen and Minna Lanz and Hasse Nylund and Harri Nieminen},
keywords = {Production system design, digital twin},
abstract = {Digital twins are created by combining a design model with the use phase data of a product or a system. They are developed in order to observe and test the behavior of the target over its life cycle. This paper introduces a versatile learning environment focusing on a Flexible Manufacturing System (FMS). The FMS Training center phenomenon based learning environment consists of a physical training system and its digital twin. The realization of the digital twin is a virtual model linked to a system simulator and an industrial controller. Students can thus become familiar with the fully automated production system and develop and test programs in a virtual environment before visiting the production facility. The digital twin also meets an industrial need as it facilitates virtual commissioning of new system installations. Evaluation of the benefits in educational context, and the largest obstacles, of developing system level digital twins are described. A full system description of the learning environment is presented with the pedagogical objectives of the associated exercises. Future development and extendibility of the learning environment are also discussed.}
}
@article{BUISSON2019110197,
title = {Towards an integrated machine-learning framework for model evaluation and uncertainty quantification},
journal = {Nuclear Engineering and Design},
volume = {354},
pages = {110197},
year = {2019},
note = {Special Issue on TRENDS AND PERSPECTIVES IN NUCLEAR THERMAL-HYDRAULICS},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2019.110197},
url = {https://www.sciencedirect.com/science/article/pii/S0029549319302067},
author = {Bertrand Buisson and Djamel Lakehal},
keywords = {Fluid flow simulation, Wall boiling, Data analytics, Digital Twin, Machine-learning, Data-driven models (DDM)},
abstract = {We introduce a new paradigm for treating and exploiting simulation data, serving in parallel as an alternative workflow for model evaluation and uncertainty quantification. Instead of reporting simulations of base-case and specific variations scenarios, databases covering a wide spectrum of operational conditions are built by means of machine-learning using sophisticated mathematical algorithms. While the approach works for all sorts of computer-aided engineering applications, the present contribution addresses the CFD/CMFD sub-branch, with application to a widely used benchmark of convective flow boiling. In addition to comparing simulation and experimental results on a case-by-case basis, machine-learning is used to create their respective (CFD and experiment) data-driven models (DDM), which will in a later stage serve for assessing the predictive performance of the CFD models over a wider range of experimental conditions, hence providing a high-level classification of their range of applicability.}
}
@incollection{VIJAYAKUMAR2020265,
title = {Chapter Eleven - Digital twin in consumer choice modeling},
editor = {Pethuru Raj and Preetha Evangeline},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {117},
number = {1},
pages = {265-284},
year = {2020},
booktitle = {The Digital Twin Paradigm for Smarter Systems and Environments: The Industry Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300531},
author = {D. Sudaroli Vijayakumar},
keywords = {Digital Twin, Retail, Consumer choice modeling, Tarantool},
abstract = {“Digital twin” more often perceived as a twin terminology along with industry virtualization of physical assets. The usage of digital twin on physical asset is well known, such as to predict when the individual parts of a machine must be replaced. However, digital twin technology in non-physical modeling is a vibrant research area. One area where digital twin can be effective is predicting the customer's needs. Most businesses to predict the customer's needs uses risk analysis and profitability assessment which holds its own pitfalls. One of the major downfalls arises during the analysis on historical data is the time consumed. Time is one of the crucial factors that determines the profit a company makes, holding of customers, satisfying the customers' needs at the right time, fails because of the static behavior. This can be made more effective by enforcing Digital twin to track the customer behavior dynamically such as the products they consume, their satisfaction. So instead of relying on the historical data, the data for digital twin will be from CRMs, logs, order processing info etc. Right product at right time can be achieved by creating suitable machine learning models on this dynamic dataset and this trained model are held in the digital twin, which runs them in real time. For achieving this approach, a specific technology called Tarantool Data Grid is very useful. In this chapter, we will explore how this technology can be used to create consumer choice modeling using Digital Twin with suitable use cases.}
}
@article{YILDIZ2020216,
title = {Virtual Factory: Digital Twin Based Integrated Factory Simulations},
journal = {Procedia CIRP},
volume = {93},
pages = {216-221},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120306077},
author = {Emre Yildiz and Charles Møller and Arne Bilberg},
keywords = {virtual factory, digital twin, modelling, simulation, virtual reality, industry 4.0},
abstract = {The co-evolution problem, which is known as the concurrent evolution of products, processes and production systems, along with increased complexity and shorter manufacturing operation lifecycles, makes modelling, simulation and evaluation of such operations challenging activities for industry players. This paper presents the concept of a digital twin-based virtual factory (VF) and its architecture to support modelling, simulation and evaluation of manufacturing systems while employing multi-user (collaborative and coordinated) virtual reality (VR) learning/training scenarios. This paper also addresses how digital twin-based virtual factory can support factory lifecycle processes by demonstrating the concept in a wind turbine manufacturing plant, including preliminary evaluation by industry experts.}
}
@article{DRODER2018187,
title = {A Machine Learning-Enhanced Digital Twin Approach for Human-Robot-Collaboration},
journal = {Procedia CIRP},
volume = {76},
pages = {187-192},
year = {2018},
note = {7th CIRP Conference on Assembly Technologies and Systems (CATS 2018)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118300295},
author = {Klaus Dröder and Paul Bobka and Tomas Germann and Felix Gabriel and Franz Dietrich},
keywords = {Human-Robot-Collaboration, Safety, Machine Learning},
abstract = {A key problem in human robot collaboration is a safe movement of the robot. The reason for this lies mainly in the variety of possible different events that can occur in an unstructured environment. Especially the description of a variable working space and the movements of humans are difficult to represent deterministically. In this paper, an approach to machine learning to enable industrial robots to bypass obstacles or people in the workspace is presented. First, a machine learning-enhanced robot control strategy is presented, which combines a nearest neighbor approach for path planning, clustering analysis and artificial neural networks for obstacle detection. Finally, a proof of concept is presented describing adaptive path planning for the protection of a human being.}
}