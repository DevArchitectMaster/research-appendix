@article{LIU2022,
title = {A survey on blockchain-enabled federated learning and its prospects with digital twin},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001626},
author = {Kangde Liu and Zheng Yan and Xueqin Liang and Raimo Kantola and Chuangyue Hu},
keywords = {Digital twin, Artificial intelligence, Federated learning, Blockchain},
abstract = {Digital Twin (DT) supports real time analysis and provides a reliable simulation platform in the Internet of Things (IoT). The creation and application of DT hinges on amounts of data, which poses pressure on the application of Artificial Intelligence (AI) for DT descriptions and intelligent decision-making. Federated Learning (FL) is a cutting-edge technology that enables geographically dispersed devices to collaboratively train a shared global model locally rather than relying on a data center to perform model training. Therefore, DT can benefit by combining with FL, successfully solving the ”data island” problem in traditional AI. However, FL still faces serious challenges, such as enduring single-point failures, suffering from poison attacks, lacking effective incentive mechanisms. Before the successful deployment of DT, we should tackle the issues caused by FL. Researchers from industry and academia have recognized the potential of introducing Blockchain Technology (BT) into FL to overcome the challenges faced by FL, where BT acting as a distributed and immutable ledger, can store data in a secure, traceable, and trusted manner. However, to the best of our knowledge, a comprehensive literature review on this topic is still missing. In this paper, we review existing works about blockchain-enabled FL and visualize their prospects with DT. To this end, we first propose evaluation requirements with respect to security, fault-tolerance, fairness, efficiency, cost-saving, profitability, and support for heterogeneity. Then, we classify existing literature according to the functionalities of BT in FL and analyze their advantages and disadvantages based on the proposed evaluation requirements. Finally, we discuss open problems in the existing literature and the future of DT supported by blockchain-enabled FL, based on which we further propose some directions for future research.}
}
@article{TANG2022,
title = {Analyzing topics in social media for improving digital twinning based product development},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000657},
author = {Wenyi Tang and Ling Tian and Xu Zheng and Ke Yan},
keywords = {Digital twinning, Product development, Topic analysis, Social media},
abstract = {Digital twinning enables manufacturers to create digital representations of physical entities, thus implementing virtual simulations for product development. Previous efforts of digital twinning neglect the decisive consumer feedback in product development stages, failing to cover the gap between physical and digital spaces. This work mines real-world consumer feedbacks through social media topics, which is significant to product development. We specifically analyze the prevalent time of a product topic, giving an insight into both consumer attention and the widely-discussed time of a product. The primary body of current studies regards the prevalent time prediction as an accompanying task or assumes the existence of a preset distribution. Therefore, these proposed solutions are either biased in focused objectives and underlying patterns or weak in the capability of generalization towards diverse topics. To this end, this work combines deep learning and survival analysis to predict the prevalent time of topics. We propose a specialized deep survival model which consists of two modules. The first module enriches input covariates by incorporating latent features of the time-varying text, and the second module fully captures the temporal pattern of a rumor by a recurrent network structure. Moreover, a specific loss function different from regular survival models is proposed to achieve a more reasonable prediction. Extensive experiments on real-world datasets demonstrate that our model significantly outperforms the state-of-the-art methods.}
}
@article{GAO2022102515,
title = {Segmentation only uses sparse annotations: Unified weakly and semi-supervised learning in medical images},
journal = {Medical Image Analysis},
volume = {80},
pages = {102515},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102515},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522001621},
author = {Feng Gao and Minhao Hu and Min-Er Zhong and Shixiang Feng and Xuwei Tian and Xiaochun Meng and Ma-yi-di-li Ni-jia-ti and Zeping Huang and Minyi Lv and Tao Song and Xiaofan Zhang and Xiaoguang Zou and Xiaojian Wu},
keywords = {Weakly supervised learning, Semi-supervised learning, Medical image, Semantic segmentation},
abstract = {Since segmentation labeling is usually time-consuming and annotating medical images requires professional expertise, it is laborious to obtain a large-scale, high-quality annotated segmentation dataset. We propose a novel weakly- and semi-supervised framework named SOUSA (Segmentation Only Uses Sparse Annotations), aiming at learning from a small set of sparse annotated data and a large amount of unlabeled data. The proposed framework contains a teacher model and a student model. The student model is weakly supervised by scribbles and a Geodesic distance map derived from scribbles. Meanwhile, a large amount of unlabeled data with various perturbations are fed to student and teacher models. The consistency of their output predictions is imposed by Mean Square Error (MSE) loss and a carefully designed Multi-angle Projection Reconstruction (MPR) loss. Extensive experiments are conducted to demonstrate the robustness and generalization ability of our proposed method. Results show that our method outperforms weakly- and semi-supervised state-of-the-art methods on multiple datasets. Furthermore, our method achieves a competitive performance with some fully supervised methods with dense annotation when the size of the dataset is limited.}
}
@article{LI2022119030,
title = {Towards unified machine learning characterization of lithium-ion battery degradation across multiple levels: A critical review},
journal = {Applied Energy},
volume = {316},
pages = {119030},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119030},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922004354},
author = {Alan G. Li and Alan C. West and Matthias Preindl},
keywords = {Battery management systems, Machine learning, Lithium batteries},
abstract = {Lithium-ion battery (LIB) degradation is often characterized at three distinct levels: mechanisms, modes, and metrics. Recent trends in diagnostics and prognostics have been heavily influenced by machine learning (ML). This review not only provides a unique multi-level perspective on characterizing LIB degradation, but also highlights the role of ML in achieving higher accuracies with accelerated computation times. We survey the state-of-the-art in degradation research and show that existing techniques lay the foundations for a unified ML method – a single tool for characterizing degradation at multiple levels. This could inform optimal management of lithium-ion systems, thus extending lifetimes and reducing costs. We propose a framework for the hypothesized technique using pulse injection, digital-twinning, and neural networks, and identify the challenges and future trends in degradation research.}
}
@article{LEE2022102833,
title = {Control framework for collaborative robot using imitation learning-based teleoperation from human digital twin to robot digital twin},
journal = {Mechatronics},
volume = {85},
pages = {102833},
year = {2022},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2022.102833},
url = {https://www.sciencedirect.com/science/article/pii/S0957415822000691},
author = {Hyunsoo Lee and Seong Dae Kim and Mohammad Aman Ullah Al Amin},
keywords = {Collaborative robot, Teleoperation framework, Imitation learning, Digital twin, Bezier curve-based smooth pose mapping, Convolutional encoder-decoder},
abstract = {Despite the deployment of collaborative robots for various industrial processes, their teaching and control remain comparatively difficult tasks compared with general industrial robots. Various imitation learning methods involving the transfer of human poses to a collaborative robot have been proposed. However, most of these methods depend heavily on deep learning-based human recognition algorithms that fail to recognize complicated human poses. To address this issue, we propose an automated/semi-automated vision-based teleoperation framework using human digital twin and a collaborative robot digital twin models. First, a human pose is recognized and reasoned to a human skeleton model using a convolution encoder-decoder architecture. Next, the developed human digital twin model is taught using the skeletons. As human and collaborative robots have different joints and rotation architectures, pose mapping is achieved using the proposed Bezier curve-based smooth approximation. Then, a real collaborative robot is controlled using the developed robot digital twin. Furthermore, the proposed framework works successfully using a human digital twin in the case of recognition failures of human poses. To verify the effectiveness of the proposed framework, transfers of several human poses to a real collaborative robot are tested and analyzed.}
}
@article{LI2022677,
title = {Predicting hot-strip finish rolling thickness using stochastic configuration networks},
journal = {Information Sciences},
volume = {611},
pages = {677-689},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.07.173},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522008684},
author = {Xu Li and Yaodong He and Jingguo Ding and Feng Luan and Dianhua Zhang},
keywords = {Stochastic configuration networks, Industrial data modeling, Thickness prediction, Metal forming process},
abstract = {In the hot-rolling metal forming process, the consistency and accuracy of the thickness of the metal strip are the most important factors for the product quality control. The current method of utilizing a mechanism prediction model with pre-defined parameters does not perform well due to some limits on the model assumptions and environmental interference. Manually tuning these parameters of the mechanism model may even result in worse performance. To resolve this problem, an advanced randomized learner model, termed stochastic configuration network (SCN), is employed to build a data-driven prediction model which can be trained by using a dataset collected from a real-world hot-rolling production site. Based on the rolling theory and gray relational analysis (GRA), 36 features are selected as the inputs of the prediction model. Experimental results with comparisons show that our proposed method is feasible and outperforms other machine learning methods, such as deep learning models and the random vector functional link (RVFL) model.}
}
@article{VANDINTER2022107008,
title = {Predictive maintenance using digital twins: A systematic literature review},
journal = {Information and Software Technology},
volume = {151},
pages = {107008},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001331},
author = {Raymon {van Dinter} and Bedir Tekinerdogan and Cagatay Catal},
keywords = {Systematic literature review, Active learning, Digital twin, Predictive maintenance},
abstract = {Context
Predictive maintenance is a technique for creating a more sustainable, safe, and profitable industry. One of the key challenges for creating predictive maintenance systems is the lack of failure data, as the machine is frequently repaired before failure. Digital Twins provide a real-time representation of the physical machine and generate data, such as asset degradation, which the predictive maintenance algorithm can use. Since 2018, scientific literature on the utilization of Digital Twins for predictive maintenance has accelerated, indicating the need for a thorough review.
Objective
This research aims to gather and synthesize the studies that focus on predictive maintenance using Digital Twins to pave the way for further research.
Method
A systematic literature review (SLR) using an active learning tool is conducted on published primary studies on predictive maintenance using Digital Twins, in which 42 primary studies have been analyzed.
Results
This SLR identifies several aspects of predictive maintenance using Digital Twins, including the objectives, application domains, Digital Twin platforms, Digital Twin representation types, approaches, abstraction levels, design patterns, communication protocols, twinning parameters, and challenges and solution directions. These results contribute to a Software Engineering approach for developing predictive maintenance using Digital Twins in academics and the industry.
Conclusion
This study is the first SLR in predictive maintenance using Digital Twins. We answer key questions for designing a successful predictive maintenance model leveraging Digital Twins. We found that to this day, computational burden, data variety, and complexity of models, assets, or components are the key challenges in designing these models.}
}
@article{LIU2022102365,
title = {A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {78},
pages = {102365},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102365},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000539},
author = {Yongkui Liu and He Xu and Ding Liu and Lihui Wang},
keywords = {Deep reinforcement learning, Sim-to-real transfer, Digital twin, Robot grasping},
abstract = {Deep reinforcement learning (DRL) has proven to be an effective framework for solving various complex control problems. In manufacturing, industrial robots can be trained to learn dexterous manipulation skills from raw pixels with DRL. However, training robots in the real world is a time-consuming, high-cost and of safety concerns process. A frequently adopted approach for easing this is to train robots through simulations first and then deploy algorithms (or policies) on physical robots. How to transfer policies of robot learning from simulation to the real world is a challenging issue. Digital twin that is able to create a dynamic, up-to-date representation of a physical robotic grasping system provides an effective approach for addressing this issue. In this paper, we focus on the scenario of DRL-based assembly-oriented industrial grasping and propose a digital twin-enabled approach for achieving effective transfer of DRL algorithms to a physical robot. Two parallel training systems, i.e., the physical robotic system and corresponding digital twin system, respectively, are established, which take virtual and real images as inputs. The output of the digital twin system is used to correct the real grasping point so that accurate grasping can be achieved. Experimental results verify the effectiveness of the intelligent grasping algorithm and the digital twin-enabled sim-to-real transfer approach and mechanism.}
}
@article{CHIACHIO2022104333,
title = {Structural digital twin framework: Formulation and technology integration},
journal = {Automation in Construction},
volume = {140},
pages = {104333},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104333},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002060},
author = {Manuel Chiachío and María Megía and Juan Chiachío and Juan Fernandez and María L. Jalón},
keywords = {Digital twin, Petri nets, Bayesian learning, Internet of things, Structural health monitoring},
abstract = {This work presents a digital twin framework for structural engineering. The digital twin is conceptualised and mathematically idealised within the context of structural integrity, and includes the main attributes to behave as a functional digital twin, namely simulation, learning, and management. The manuscript makes special emphasis on the autonomous interactions between the physical and digital counterparts along with on the workflow modelling of the digital twin, which are both missing aspects in the majority of use cases found in the literature, specially within the civil and structural engineering domain. The proposed framework is demonstrated in a proof of concept using a laboratory scale test structure monitored using internet-of-things-based sensors and actuators. The results reveal that the virtual counterpart can respond in real-time with self-adaptability in liaison to the performance of the physical counterpart. Moreover, the tests show that the digital twin is able to provide automated decision making for structural integrity.}
}
@article{CLEMENTE2022118171,
title = {A proposal for an adaptive Recommender System based on competences and ontologies},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118171},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118171},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422013392},
author = {Julia Clemente and Héctor Yago and Javier {de Pedro-Carracedo} and Javier Bueno},
keywords = {Recommender system, , Ontology network, Methodological development, Student modeling},
abstract = {Context:
Competences represent an interesting pedagogical support in many processes like diagnosis or recommendation. From these, it is possible to infer information about the progress of the student to provide help targeted both, trainers who must make adaptive tutoring decisions for each learner, and students to detect and correct their learning weaknesses. For the correct development of any of these tasks, it is important to have a suitable student model that allows the representation of the most significant information possible about the student. Additionally, it would be very advantageous for this modeling to incorporate mechanisms from which it would be possible to infer more information about the student’s state of knowledge.
Objective:
To facilitate this goal, in this paper a new approach to develop an adaptive competence-based recommender system is proposed.
Method:
We present a methodological development guide as well as a set of ontological and non-ontological resources to develop and adapt the prototype of the proposed recommender system.
Results:
A modular flexible ontology network previously built for this purpose has been extended, which is responsible for recording the instructional design and student information. Furthermore, we describe a case study based on a first aid learning experience to assess the prototype with the proposed methodology.
Conclusions:
We highlight the relevance of flexibility and adaptability in learning modeling and recommendation processes. In order to promote improvement in the personalized learning of students, we present a Recommender System prototype taking advantages of ontologies, with a methodological guide, a broad taxonomy of recommendation criteria and the nature of competences. Future lines of research lines, including a more comprehensive evaluation of the system, will allow us to demonstrate in depth its adaptability according to the characteristics of the student, flexibility and extensibility for its integration in various environments and domains.}
}
@article{XU2022230,
title = {Knowledge distillation guided by multiple homogeneous teachers},
journal = {Information Sciences},
volume = {607},
pages = {230-243},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.05.117},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522005576},
author = {Quanzheng Xu and Liyu Liu and Bing Ji},
keywords = {Deep learning, Knowledge distillation (KD), Teacher–student model, Network compression},
abstract = {Knowledge distillation (KD) transfers knowledge from a heavy teacher network to a lightweight student network while maintaining the student’s performance closely to that of the teacher. However, the large gap between the teacher and the student in terms of capacity is not conducive to KD. Consequently, a large teacher network is not necessarily the most suitable teacher to guide the student. Therefore, this study proposes a multiple homogeneous teacher-guided KD method. First, multiple networks with the same structure as that of the student are pretrained to act as a teacher group, which is different from utilizing a large teacher network in traditional KD, to alleviate the capacity gap between the teacher and student. Second, a confidence-adaptive initialization strategy is developed to initialize the student network, which learns knowledge from the pretrained teacher group. Experiments are performed on CIFAR10, CIFAR100, and Tiny-ImageNet using three networks. The experimental results demonstrate that the proposed KD method outperforms existing advanced KD methods. Furthermore, a similarity loss function is introduced to optimize the parameters of the classifier in the student network. The experimental results indicate that this loss function improves the performance for basic classification tasks without KD and efficiently works in the proposed KD method.}
}
@article{PHUA2022103667,
title = {A digital twin hierarchy for metal additive manufacturing},
journal = {Computers in Industry},
volume = {140},
pages = {103667},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103667},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000641},
author = {A. Phua and C.H.J. Davies and G.W. Delaney},
keywords = {Digital twin, Additive manufacturing, Part qualification, Artificial intelligence, Control policy, Machine learning, Industry 4.0, Smart manufacturing},
abstract = {Digital twins present a conceptual framework for product life-cycle monitoring and control using a simulated replica of the physical system. Since their emergence, they have garnered particular attention as a shift away from costly physical testing and towards the use of high fidelity simulations, sensor data and intelligent control. Metal additive manufacturing (AM), a 3D printing technology prone to defects, requires a digital twin capable of tackling issues of printed part qualification, certification and optimisation. In this paper, we evaluate the key features specific to metal AM and review the current literature of modelling, sensing, control and machine intelligence. We find that the body of research toward the development of an metal additive manufacturing (AM) digital twin can be organised logically into a hierarchy of four levels of increasing complexity. The elements composing each level require deep integration and we highlight the key enabling technologies: surrogate modelling, in-situ sensing, hardware control systems and intelligent control policies. Our proposed digital twin hierarchy for AM provides a developer framework for engineering digital twins, both for AM and other intelligent manufacturing systems.}
}
@article{PHOON2022967,
title = {Unpacking data-centric geotechnics},
journal = {Underground Space},
volume = {7},
number = {6},
pages = {967-989},
year = {2022},
issn = {2467-9674},
doi = {https://doi.org/10.1016/j.undsp.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2467967422000514},
author = {Kok-Kwang Phoon and Jianye Ching and Zijun Cao},
keywords = {Data-centric geotechnics, Bayesian machine learning, Data-driven site characterization (DDSC), Project DeepGeo, Data-informed decision support index},
abstract = {The purpose of this paper (presented online as a keynote lecture at the 25th Annual Indonesian Geotechnical Conference on 10 Nov 2021) is to broadly conceptualize the agenda for data-centric geotechnics, an emerging field that attempts to prepare geotechnical engineering for digital transformation. The agenda must include (1) development of methods that make sense of all real-world data (not selective input data for a physical model), (2) offering insights of significant value to critical real-world decisions for current or future practice (not decisions for an ideal world or decisions of minor concern to geotechnical engineers), and (3) sensitivity to the physical context of geotechnics (not abstract data-driven analysis connected to geotechnics in a peripheral way, i.e., engagement with the knowledge and experience base should be substantial). These three elements are termed “data centricity”, “fit for (and transform) practice”, and “geotechnical context” in the agenda. Given that a knowledge of the site is central to any geotechnical engineering project, data-driven site characterization (DDSC) must constitute one key application domain in data-centric geotechnics, although other infrastructure lifecycle phases such as project conceptualization, design, construction, operation, and decommission/reuse would benefit from data-informed decision support as well. One part of DDSC that addresses numerical soil data in a site investigation report and soil property databases is pursued under Project DeepGeo. In principle, the source of data can also go beyond site investigation, and the type of data can go beyond numbers, such as categorical data, text, audios, images, videos, and expert opinion. The purpose of Project DeepGeo is to produce a 3D stratigraphic map of the subsurface volume below a full-scale project site and to estimate relevant engineering properties at each spatial point based on actual site investigation data and other relevant Big Indirect Data (BID). Uncertainty quantification is necessary, as current real-world data is insufficient, incomplete, and/or not directly relevant to construct a deterministic map. The value of a deterministic map for decision support is debatable. The computational cost to do this for a 3D true scale subsurface volume must be reasonable. Ultimately, geotechnical structures need to be a part of a completely smart infrastructure that fits the circular economy and need to focus on delivering service to end-users and the community from project conceptualization to decommission/reuse with full integration to smart city and smart society. Although current geotechnical practice has been very successful in taking “calculated risk” informed by limited data, imperfect theories, prototype testing, observations, among others and exercising judicious caution and engineering judgment, there is no clear pathway forward to leverage on big data and digital technologies such as machine learning, BIM, and digital twin to meet more challenging needs such as sustainability and resilience engineering.}
}
@article{LIU2022102390,
title = {Adaptive reconstruction of digital twins for machining systems: A transfer learning approach},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {78},
pages = {102390},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102390},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000771},
author = {Shimin Liu and Yuqian Lu and Pai Zheng and Hui Shen and Jinsong Bao},
keywords = {Digital twin, Machining system, Intelligent machining, Adaptability, Transfer learning},
abstract = {Digital twin technology has been gradually explored and applied in the machining process. A digital twin machining system creates high-fidelity virtual entities of physical entities to observe, analyze, and control the machining process in real-time. However, the current digital twin machining systems lack sufficient adaptability because they are usually customized for specific scenes. Usually, if a decision model is directly reused in a different working condition, the accuracy of the decision model is often poor and difficult to work effectively. Meanwhile, the decision model remodeled from scratch will cause a waste of resources and low modeling efficiency. This paper proposes an adaptive reconstruction method to adjust the decision model in the digital twin machining system to enhance adaptability. The proposed method can ensure the rapid development of the digital twin decision model under new working conditions. Finally, taking the drilling process as an example, this paper establishes the experimental drilling platform and verifies the feasibility of this method in the burr prediction task.}
}
@article{NIELSEN2022111579,
title = {Machine learning enhancement of manoeuvring prediction for ship Digital Twin using full-scale recordings},
journal = {Ocean Engineering},
volume = {257},
pages = {111579},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.111579},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822009453},
author = {Rasmus E. Nielsen and Dimitrios Papageorgiou and Lazaros Nalpantidis and Bugge T. Jensen and Mogens Blanke},
keywords = {Digital Twin, Machine learning, Ship motion prediction, Ship performance prediction, System identification, Manoeuvring simulation, Manoeuvring model},
abstract = {Digital Twins have much attention in the shipping industry, attempting to support all phases of a vessel’s life cycle. With several tools appearing in Digital Twin software suites, high-quality manoeuvring and performance prediction remain cornerstones. Propulsion efficiency is in focus while in service. Simulator-based training is in focus to ensure safety of manoeuvring in confined waters and harbours. Prediction of ships’ velocity and turn rate are essential for correct look and feel during training, but phenomena like dynamic inflow to propellers, bank and shallow water effects limit simulators’ accuracy, and master mariners often comment that simulations could be in better agreement with actual behaviours of their vessel. This paper focuses on digital twin enhancements to better match reality. Using data logged during in-service operation, we first consider a system identification perspective, employing a first-principles model structure. Showing that a complete first-principles model is not identifiable under the excitation met in service, we employ a Recurrent Neural Network to predict deviations between measured velocities and the model output. The outcome is a hybrid of a first-principles model with a machine learning generic approximator add-on. The paper demonstrates significant improvements in prediction accuracy of both in-harbour manoeuvring and shallow water passage conditions.}
}
@article{WANG2022102447,
title = {Semi-supervised medical image segmentation via a tripled-uncertainty guided mean teacher model with contrastive learning},
journal = {Medical Image Analysis},
volume = {79},
pages = {102447},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102447},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522000925},
author = {Kaiping Wang and Bo Zhan and Chen Zu and Xi Wu and Jiliu Zhou and Luping Zhou and Yan Wang},
keywords = {Semi-supervised segmentation, Mean teacher, Multi-task learning, Tripled-uncertainty, Contrastive learning},
abstract = {Due to the difficulty in accessing a large amount of labeled data, semi-supervised learning is becoming an attractive solution in medical image segmentation. To make use of unlabeled data, current popular semi-supervised methods (e.g., temporal ensembling, mean teacher) mainly impose data-level and model-level consistency on unlabeled data. In this paper, we argue that in addition to these strategies, we could further utilize auxiliary tasks and consider task-level consistency to better excavate effective representations from unlabeled data for segmentation. Specifically, we introduce two auxiliary tasks, i.e., a foreground and background reconstruction task for capturing semantic information and a signed distance field (SDF) prediction task for imposing shape constraint, and explore the mutual promotion effect between the two auxiliary and the segmentation tasks based on mean teacher architecture. Moreover, to handle the potential bias of the teacher model caused by annotation scarcity, we develop a tripled-uncertainty guided framework to encourage the three tasks in the student model to learn more reliable knowledge from the teacher. When calculating uncertainty, we propose an uncertainty weighted integration (UWI) strategy for yielding the segmentation predictions of the teacher. In addition, following the advance of unsupervised learning in leveraging the unlabeled data, we also incorporate a contrastive learning based constraint to help the encoders extract more distinct representations to promote the medical image segmentation performance. Extensive experiments on the public 2017 ACDC dataset and the PROMISE12 dataset have demonstrated the effectiveness of our method.}
}
@article{LIU2022,
title = {Design of modified model of intelligent assembly digital twins based on optical fiber sensor network},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001353},
author = {Zhichao Liu and Jinhua Yang and Juan Wang and Lin Yue},
keywords = {Digital twins, Intelligent manufacturing, Intelligent assembly, Optical fiber sensor network, Assembly condition monitoring algorithm},
abstract = {Intelligent assembly of large-scale, complex structures using an intelligent manufacturing platform represents the future development direction for industrial manufacturing. During large-scale structural assembly processes, several bottleneck problems occur in the existing auxiliary assembly technology. First, the traditional LiDAR-based assembly technology is often limited by the openness of the manufacturing environment, in which there are blind spots, and continuous online assembly adjustment thus cannot be realized. Second, for assembly of large structures, a single-station LiDAR system cannot achieve complete coverage, which means that a multi-station combination method must be used to acquire the complete three-dimensional data; many more data errors are caused by the transfer between stations than by the measurement accuracy of a single station, which means that the overall system's measurement and adjustment errors are increased greatly. Third, because of the large numbers of structural components contained in a large assembly, the accumulated errors may lead to assembly interference, but the LiDAR-assisted assembly process does not have a feedback perception capability, and thus assembly component loss can easily be caused when assembly interference occurs. Therefore, this paper proposes to combine an optical fiber sensor network with digital twin technology, which will allow the test data from the assembly entity state in the real world to be applied to the “twin” model in the virtual world and thus solve the problems with test openness and data transfer. The problem of station and perception feedback is also addressed and represents the main innovation of this work. The system uses an optical fiber sensor network as a flexible sensing medium to monitor the strain field distribution within a complex area in real time, and then completes real-time parameter adjustment of the virtual assembly based on the distributed data. Complex areas include areas that are laser-unreachable, areas with complex contact surfaces, and areas with large-scale bending deformations. An assembly condition monitoring system is designed based on the optical fiber sensor network, and an assembly condition monitoring algorithm based on multiple physical quantities is proposed. The feasibility of use of the optical fiber sensor network as the real-state parameter acquisition module for the digital twin intelligent assembly system is discussed. The offset of any position in the test area is calculated using the convolutional neural network of a residual module to provide the compensation parameters required for the virtual model of the assembly structure. In the model optimization parameter module, a correction data table is obtained through iterative learning of the algorithm to realize state prediction from the test data. The experiment simulates a large-scale structure assembly process, and performs virtual and real mapping for a variety of situations with different assembly errors to enable correction of the digital twin data stream for the assembly process through the optical fiber sensor network. In the plane strain field calibration experiment, the maximum error among the test values for this system is 0.032 mm, and the average error is 0.014 mm. The results show that use of visual calibration can correct the test error to within a very small range. This result is equally applicable to gradient curvature surfaces and freeform surfaces. Statistics show that the average measurement accuracy error for regular surfaces is better than 11.2%, and the average measurement accuracy error for irregular surfaces is better than 14.8%. During simulation of large-scale structure assembly experiments, the average position deviation accuracy is 0.043 mm, which is in line with the designed accuracy.}
}
@article{BELLAVISTA2022101646,
title = {Digital twin oriented architecture for secure and QoS aware intelligent communications in industrial environments},
journal = {Pervasive and Mobile Computing},
volume = {85},
pages = {101646},
year = {2022},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2022.101646},
url = {https://www.sciencedirect.com/science/article/pii/S1574119222000736},
author = {Paolo Bellavista and Carlo Giannelli and Marco Mamei and Matteo Mendula and Marco Picone},
keywords = {Internet of things, Industry 4.0, Zones and conduits, Digital twin, Intelligent reconfiguration},
abstract = {In modern networking industrial environments, characterized by the integration of Operation Technology and Information Technology, there is a strong need to ensure both safety and security of operations and communications. In this regard, IEC 62443 zones and conduits represent powerful high-level abstractions stressing the importance of clearly separating machines in relation to safety requirements and of clearly defining inter-machine communication security requirements. However, their actual implementation is still demanded to human-centric error-prone procedures performed by technicians directly on network elements, without any integrated plant-wide point of view. To overcome these issues, first of all we originally state the need of applying the Digital Twin approach to zones and conduits, making easier the definition and management of inter-machine security requirements. For instance, industrial technicians can specify that communication among two zones should always flows through a ciphered conduit with a given algorithm and key length, at the cost of increased latency. Secondly, we state the need of exploiting an intelligent reasoner to monitor the current state of the environment (represented by asset and network Digital Twins), actively reconfiguring them in case desired requirements are not satisfied. Then, the reasoner allows to enforce requirements while also considering the fulfillment of a proper trade-off between security and performance, e.g., by reducing the ciphering complexity to ensure prompt packet dispatching whenever required. Performance results based on our working prototype demonstrate the feasibility and efficiency of the proposed solution under stringent requirements typical of industrial environments. In particular, in terms of better flexibility we proved that our orchestrator is able to create a new Digital Twin in less than 2.5 s in a typical edge node with a medium load. In addition, proposed routing policies based on our machine learning reasoner led to the satisfaction of well-defined low latency requirements (250 ms) while avoiding packet dropping.}
}
@article{OSHO2022370,
title = {Four Rs Framework for the development of a digital twin: The implementation of Representation with a FDM manufacturing machine},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {370-380},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000656},
author = {John Osho and Anna Hyre and Minas Pantelidakis and Allison Ledford and Gregory Harris and Jia Liu and Konstantinos Mykoniatis},
keywords = {Digital twin, Cyber-physical systems, Industry 4.0, Additive manufacturing, Fused deposition modeling},
abstract = {This work considers the conceptualization and design of a 4 Rs framework for creating a general purpose, modular Digital Twin. The 4 Rs, correspond to the 4 different phases of a Digital Twin implementation, namely Representation, Replication, Reality, and Relational. Representation is about understanding the physical system, its behavior, actions, components, relationships and describing the significant features for the identified use case as data and algorithms. Replication duplicates the chosen components/variables in a virtual environment from a set of inputs identified in Representation. Reality employs machine learning to produce a virtual device that runs independent of the physical device with the ability to make predictions, enhance models, provide alternative scenarios and optimizations. Reality enhances the virtual system to become autonomous and self-aware with the ability to make decisions and take corrective actions. We introduce these phases and outline their core elements and principles. We showcase the implementation of phase 1, Representation, using a Fused Deposition Modeling (FDM) additive manufacturing machine via temperature and position sensors. We evaluate their precision in representing the actual FDM machine and lay the foundation work for the implementation of the 4R framework in our next work.}
}
@article{ZHANG2022238,
title = {A digital twin dosing system for iron reverse flotation},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {238-249},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000413},
author = {Dingsen Zhang and Xianwen Gao},
keywords = {Digital twin, Dosage of reagent, Iron reverse flotation, Soft sensor},
abstract = {Flotation is an essential process in beneficiation production. The amount of flotation reagent has a significant influence on the quality of the product. An inappropriate dosing system will lead to metal loss and reagent waste, especially when the nature of the raw ore changes frequently. If the dosing system is not updated in time, it will cause economic losses. Based on digital twin technology and machine learning algorithms, this research designed a digital twin system for iron reverse flotation reagents. Based on the flotation froth image and transformer algorithm, a soft sensor model of tailings grade is established to monitor the product quality in real-time. The flotation dosing model established based on the ELM algorithm automatically updates the reagent system and intelligently assigns the controller. On the basis of stabilizing product quality, this research avoids the waste of reagents and improves the economic benefits of production efficiency. The system was applied in an iron flotation plant, and industrial operation effect verified the method.}
}
@article{PURCELL2023100094,
title = {Digital Twins in Agriculture: A State-of-the-art review},
journal = {Smart Agricultural Technology},
volume = {3},
pages = {100094},
year = {2023},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2022.100094},
url = {https://www.sciencedirect.com/science/article/pii/S2772375522000594},
author = {Warren Purcell and Thomas Neubauer},
keywords = {Digital Twin, Agriculture},
abstract = {The Digital Twin enables the distinctions between state sensing, entity understanding and physical automation to be eliminated, through high-fidelity modelling and bi-directional data streams. The concept of real-time virtual representation places the Digital Twin in a unique position to enable digitization in agriculture. The union of data, modelling and what-if simulation can provide an approach to overcome current limitations in decision-making support and automation, across a diverse range of agricultural enterprises. This paper conducts a Systematic Literature Review of Digital Twins in agriculture, identifying current trends and open questions with the goal of increasing awareness and understanding of the Digital Twin and its possibilities.}
}
@article{PANTOJAROSERO2022104430,
title = {Generating LOD3 building models from structure-from-motion and semantic segmentation},
journal = {Automation in Construction},
volume = {141},
pages = {104430},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104430},
url = {https://www.sciencedirect.com/science/article/pii/S092658052200303X},
author = {B.G. Pantoja-Rosero and R. Achanta and M. Kozinski and P. Fua and F. Perez-Cruz and K. Beyer},
keywords = {Digital twin, LOD models, Deep learning, Structure from motion, 3D building models, Masonry buildings},
abstract = {This paper describes a pipeline for automatically generating level of detail (LOD) models (digital twins), specifically LOD2 and LOD3, from free-standing buildings. Our approach combines structure from motion (SfM) with deep-learning-based segmentation techniques. Given multiple-view images of a building, we compute a three-dimensional (3D) planar abstraction (LOD2 model) of its point cloud using SfM techniques. To obtain LOD3 models, we use deep learning to perform semantic segmentation of the openings in the two-dimensional (2D) images. Unlike existing approaches, we do not rely on complex input, pre-defined 3D shapes or manual intervention. To demonstrate the robustness of our method, we show that it can generate 3D building shapes from a collection of building images with no further input. For evaluating reconstructions, we also propose two novel metrics. The first is a Euclidean–distance-based correlation of the 3D building model with the point cloud. The second involves re-projecting 3D model facades onto source photos to determine dice scores with respect to the ground-truth masks. Finally, we make the code, the image datasets, SfM outputs, and digital twins reported in this work publicly available in github.com/eesd-epfl/LOD3_buildings and doi.org/10.5281/zenodo.6651663. With this work we aim to contribute research in applications such as construction management, city planning, and mechanical analysis, among others.}
}
@article{WU2022104252,
title = {Real-time mixed reality-based visual warning for construction workforce safety},
journal = {Automation in Construction},
volume = {139},
pages = {104252},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104252},
url = {https://www.sciencedirect.com/science/article/pii/S092658052200125X},
author = {Shaoze Wu and Lei Hou and Guomin (Kevin) Zhang and Haosen Chen},
keywords = {Construction safety, Mixed reality, Digital twin, Deep learning, Wearable device, Visualisation},
abstract = {Spatial locations of personnel, equipment, and materials are constantly changing as construction projects progress. The dynamic nature of the construction industry affects workers' performance of identifying hazards. Even though a great deal of effort has been made to improve construction safety, the construction industry still witnesses a high accident rate. In order to complement the existing body of knowledge relating to construction safety, this paper integrates Digital Twin (DT), Deep Learning (DL), and Mixed Reality (MR) technologies into a newly developed real-time visual warning system, which enables construction workers to proactively determine their safety status and avoid accidents. Next, system tests were conducted under three quasi-on-site scenarios, and the feasibility was proven in terms of synchronising construction activities over a large area and visually representing hazard information to its users. These evidenced merits of the development testing scenarios can improve workers' risk assessment accuracy, reinforce workers' safety behaviour, and provide a new perspective for construction safety managers to analyse construction safety status.}
}
@article{ALRIGE20221124,
title = {Utilizing geospatial intelligence and user modeling to allow for a customized health awareness campaign during the pandemic: The case of COVID-19 in Saudi Arabia},
journal = {Journal of Infection and Public Health},
volume = {15},
number = {10},
pages = {1124-1133},
year = {2022},
issn = {1876-0341},
doi = {https://doi.org/10.1016/j.jiph.2022.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S1876034122002337},
author = {Mayda Alrige and Hind Bitar and Maram Meccawy and Balakrishnan Mullachery},
keywords = {COVID-19, Geospatial intelligence, Space-time cube (STC), User modeling, Customization, Health awareness campaign},
abstract = {Background
As of 2022, people are getting better at learning how to coexist with the Covid-19 global pandemic. In Saudi Arabia, many attempts have been made to raise public health awareness. However, most health awareness campaigns are generic and might not influence the desired behavior among individuals.
Objectives
This study aims to apply geospatial intelligence and user modeling to profile the districts of the city of Jeddah. This customized map can provide a baseline for a customized health awareness campaign that targets the locals of each district individually based on the virus spread level.
Methodology
It is ongoing research, which has resulted in the creation of a health messages library in the first phase [1]. This paper focuses on a second phase of the research study, which aims to provide a customized baseline for this campaign by applying the geospatial artificial intelligence technique known as space-time cube (STC). STC was applied to create a local map of the Saudi city of Jeddah, representing three different profiles for the city’s districts. The model is built using valid COVID-19 clinical data obtained from one of Jeddah’s general hospitals.
Results and implications
When applied, STC displays three profiles for the districts of Jeddah city: high infection, moderate infection, and low infection. To assess the geo-intelligent map, a new instrument was created and validated. The usability and practicality of this map were quantitatively evaluated in a cross-sectional survey using the goal-question-metric measurement framework, and a total of 43 participants filled out the questionnaire. The results indicate that the geo-intelligent map is suitable for everyday use, as evidenced by the participants’ responses. We argue that the developed instrument can also be used to assess any geo-intelligence map. This research provides a legitimate approach to customizing health awareness messages during pandemics.}
}
@article{PANG2022102859,
title = {3D building reconstruction from single street view images using deep learning},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102859},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102859},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222000619},
author = {Hui En Pang and Filip Biljecki},
keywords = {3D geoinformation, GeoAI, Urban morphology, Digital twin, Google Street View, 3D GIS},
abstract = {3D building models are an established instance of geospatial information in the built environment, but their acquisition remains complex and topical. Approaches to reconstruct 3D building models often require existing building information (e.g.their footprints) and data such as point clouds, which are scarce and laborious to acquire, limiting their expansion. In parallel, street view imagery (SVI) has been gaining currency, driven by the rapid expansion in coverage and advances in computer vision (CV), but it has not been used much for generating 3D city models. Traditional approaches that can use SVI for reconstruction require multiple images, while in practice, often only few street-level images provide an unobstructed view of a building. We develop the reconstruction of 3D building models from a single street view image using image-to-mesh reconstruction techniques modified from the CV domain. We regard three scenarios: (1) standalone single-view reconstruction; (2) reconstruction aided by a top view delineating the footprint; and (3) refinement of existing 3D models, i.e.we examine the use of SVI to enhance the level of detail of block (LoD1) models, which are common. The results suggest that trained models supporting (2) and (3) are able to reconstruct the overall geometry of a building, while the first scenario may derive the approximate mass of the building, useful to infer the urban form of cities. We evaluate the results by demonstrating their usefulness for volume estimation, with mean errors of less than 10% for the last two scenarios. As SVI is now available in most countries worldwide, including many regions that do not have existing footprint and/or 3D building data, our method can derive rapidly and cost-effectively the 3D urban form from SVI without requiring any existing building information. Obtaining 3D building models in regions that hitherto did not have any, may enable a number of 3D geospatial analyses locally for the first time.}
}
@article{TARIQ2022133517,
title = {Computational intelligence for empirical modeling and optimization of methylene blue adsorption phenomena using available local zeolites and clay of Morocco},
journal = {Journal of Cleaner Production},
volume = {370},
pages = {133517},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133517},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622030979},
author = {Rasikh Tariq and Mohamed Abatal and A. Bassam},
keywords = {Artificial neural network, Particle swarm optimization, Genetic algorithm, Simulated annealing, Support vector machine, Sensitivity analysis, Methylene blue},
abstract = {The present study investigated the performance of natural Moroccan zeolites and clay in the removal of methylene blue (MB) from aqueous solutions. The deposits of the samples are extracted from the Teteoune (sample 1), Kenitra (sample 2), Khroubga (sample 3), and Benghrir (sample 4) regions of Morocco. The samples were characterized by the X-ray diffraction technique. Sorption experiments were carried out by batch experimental to examine the effects of contact time, solution pH, initial concentration of MB, and biosorbent dosage of samples. In the next step, an integrated methodology is adapted by applying the computational intelligence packages to this problem to develop empirical models and to conduct an optimization to maximize the removal percentage. The empirical or digital twin models are generated using machine learning techniques including regression tree (RT), support vector machine, ensemble, and gaussian process regression (GPR), a statistical technique including multivariate regression (MVR), and neural network techniques including artificial neural network (ANN) and group method of data handling (GMDH). The results have shown that the ANN technique having 10 hidden layers trained using the Bayesian regularization algorithm can accurately predict the removal percentage with an RMSE = 0.88, R2 = 0.9961, MSE = 0.67, and MAE = 0.43. The second-best technique is GPR followed by GMDH. The worst techniques are MVR and RT. The single-objective optimization was applied using simulated annealing (SA), particle swarm optimization (PSO), and genetic algorithm (GA) to maximize the removal percentage for each material to find the optimal mass, initial concentration, contact time, and pH. The results have revealed that the SA technique is not fully capable to find the true optima, whereas PSO and GA have shown conformity on the optimal removal percentage. The optimized indicator came out to be 85.32%, 90.23%, 93.25%, and 97.21% for material 1, 2, 3, and 4 which was 49%, 14.69%, 20.16%, and 45.11% for material 1, 2, 3, and 4, respectively. It is concluded that the sample from the region of Benghrir has the best removal performance of methylene blue from aqueous solutions marking up to a high digit of 97.21%.}
}
@article{FELICETTI2022819,
title = {Deep stochastic configuration networks with different random sampling strategies},
journal = {Information Sciences},
volume = {607},
pages = {819-830},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522006120},
author = {Matthew J. Felicetti and Dianhui Wang},
keywords = {Stochastic configuration networks, Weight distributions, Random sampling, Generalization},
abstract = {Stochastic configuration networks (SCNs) are a class of randomized learner models that ensure the universal approximation property, whereby random weights and biases are drawn from the uniform distribution and selected by a supervisory mechanism. This paper looks into the impact of the distribution of random weights on the performance of SCNs. In the light of a fundamental principle in machine learning, that is, a model with smaller parameters holds improved generalization, we recommend using symmetric zero-centered distributions in constructing SCNs to improve the generalization performance. Further, we introduce a scalar in the distributions to make the SCN model adaptively feasible to different datasets. Simulation results are reported for both regression and classification tasks over twenty-one benchmark datasets using SCN. Results are also presented on ten regression datasets using a deep implementation of SCN, known as deep stochastic configuration networks (DeepSCN).}
}
@article{RITTO2022109485,
title = {Reinforcement learning and approximate Bayesian computation for model selection and parameter calibration applied to a nonlinear dynamical system},
journal = {Mechanical Systems and Signal Processing},
volume = {181},
pages = {109485},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109485},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022005970},
author = {T.G. Ritto and S. Beregi and D.A.W. Barton},
keywords = {Nonlinear dynamics, Parameter identification, Model selection, Reinforcement learning, ABC, Decision under uncertainty},
abstract = {In the context of digital twins and integration of physics-based models with machine learning tools, this paper proposes a new methodology for model selection and parameter identification. It combines (i) reinforcement learning (RL) for model selection through a Thompson-like sampling with (ii) approximate Bayesian computation (ABC) for parameter identification and uncertainty quantification. These two methods are applied together to a nonlinear mechanical oscillator with periodic forcing. Experimental data are used in the analysis and two different nonlinear models are tested. The initial Beta distribution that represents the likelihood of the model is updated depending on how successful the model is at reproducing the reference data (reinforcement learning strategy). At the same time, the prior distribution of the model parameters is updated using a likelihood-free strategy (ABC). In the end, the rewards and the posterior distribution of the parameters of each model are obtained. The results show that the combined methodology (RL-ABC) is promising for model selection from bifurcation diagrams. Prior parameter distribution was successfully updated, correlations between parameters were found, probabilistic envelopes of the posterior model are consistent with the available data, the most rewarded model was selected, and the reinforcing strategy allows to speed up the selection process.}
}
@article{FU2022118973,
title = {The role of deep learning in urban water management: A critical review},
journal = {Water Research},
volume = {223},
pages = {118973},
year = {2022},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2022.118973},
url = {https://www.sciencedirect.com/science/article/pii/S0043135422009204},
author = {Guangtao Fu and Yiwen Jin and Siao Sun and Zhiguo Yuan and David Butler},
keywords = {Artificial intelligence, Data analytics, Deep learning, Digital twin, Water management},
abstract = {Deep learning techniques and algorithms are emerging as a disruptive technology with the potential to transform global economies, environments and societies. They have been applied to planning and management problems of urban water systems in general, however, there is lack of a systematic review of the current state of deep learning applications and an examination of potential directions where deep learning can contribute to solving urban water challenges. Here we provide such a review, covering water demand forecasting, leakage and contamination detection, sewer defect assessment, wastewater system state prediction, asset monitoring and urban flooding. We find that the application of deep learning techniques is still at an early stage as most studies used benchmark networks, synthetic data, laboratory or pilot systems to test the performance of deep learning methods with no practical adoption reported. Leakage detection is perhaps at the forefront of receiving practical implementation into day-to-day operation and management of urban water systems, compared with other problems reviewed. Five research challenges, i.e., data privacy, algorithmic development, explainability and trustworthiness, multi-agent systems and digital twins, are identified as key areas to advance the application and implementation of deep learning in urban water management. Future research and application of deep learning systems are expected to drive urban water systems towards high intelligence and autonomy. We hope this review will inspire research and development that can harness the power of deep learning to help achieve sustainable water management and digitalise the water sector across the world.}
}
@article{ZHAO2022107247,
title = {Digital twin for rapid damage detection of a fixed net panel in the sea},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107247},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107247},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005610},
author = {Yun-Peng Zhao and Likai Lian and Chun-Wei Bi and Zhijing Xu},
keywords = {Aquaculture net, Artificial neural network, Digital twin, Damage detection},
abstract = {The fishing net is the most important component of aquaculture net cages. Once damaged, it can cause substantial economic losses and ecological problems to the aquaculture industry. To avoid these subsequent issues caused by the damage to the fishing net, seeking a computerized, labor-saving approach and detecting damage in real time becomes the primary task of this paper. Inspired by recent development of the artificial neural networks and machine learning, this study proposes a fast and accurate approach for fishing net damage detection based on digital twin. Firstly, time-domain numerical simulations of the fishing net are conducted in a series of wave and current conditions to develop the artificial neural network-based digital twin. Then, the significant wave height, the spectral peak period, and tensions of vertical and horizontal ropes are used as input variables during the artificial neural network training; the intact and damaged states of the fishing net are considered outputs. Alongside this, the back-propagation learning algorithm is used for training to maximize damage detection performance. The results highlight that the digital twin model can effectively identify the fishing net damage using the sensor data, and the accuracy of the damage detection reaches above 93%. When the damage occurs at different net positions, the prediction accuracies of the artificial neural network model for training and testing sets are 93.97% and 93.00%, respectively. Regarding different wave-current directions, the prediction accuracies of the artificial neural network model are 99.87 % and 99.34 % for training and testing sets. Moreover, the developed digital twin can accurately detect the damage of the net even sea conditions and sensor data are not included in the training. The digital twin model proposed in the present study can potentially be used for damage detection in other aquaculture components and structures.}
}
@article{ZHAN2022112278,
title = {Calibrating building simulation models using multi-source datasets and meta-learned Bayesian optimization},
journal = {Energy and Buildings},
volume = {270},
pages = {112278},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112278},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822004492},
author = {Sicheng Zhan and Gordon Wichern and Christopher Laughman and Adrian Chong and Ankush Chakrabarty},
keywords = {Meta learning, Deep learning, Parameter estimation, Probabilistic machine learning, Bayesian methods, Digital twin},
abstract = {Reliable building simulation models are key to optimizing building performance and reducing greenhouse gas emissions. Informed decision making requires simulation models to be accurate, extrapolatable, and interpretable, all of which require calibrating model simulations to ground truth. Complicated building dynamics and highly uncertain exogenous disturbances make the model calibration process challenging and expensive; hence, a scalable and efficient calibration approach is needed to enable actual application. Current automatic calibration algorithms do not leverage data collected from multiple sources: for example, data obtained from previous calibration tasks on other buildings. In this paper, we employ probabilistic deep learning to meta-learn a distribution using multi-source data acquired during previous calibration. Subsequently, the meta-learned Bayesian optimizer accelerates calibration of new, unseen tasks. The few-shot (that is, requiring few model simulations) nature of the proposed algorithm is demonstrated on a Modelica library of residential buildings validated by the United States Department of Energy (USDoE). The proposed algorithm is compared against classical Bayesian optimization-based calibration, and it is shown that ANP significantly sped up the calibration procedure: the optimal model parameters are identified with 40–60% less simulations compared to the baseline.}
}
@article{PAN2022104375,
title = {Enriching geometric digital twins of buildings with small objects by fusing laser scanning and AI-based image recognition},
journal = {Automation in Construction},
volume = {140},
pages = {104375},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104375},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002485},
author = {Yuandong Pan and Alexander Braun and Ioannis Brilakis and André Borrmann},
keywords = {Digital twin, Deep learning, Object detection, Text recognition, 3D reconstruction},
abstract = {This paper addresses the challenge of enriching geometric digital twins of buildings, with a particular emphasis on capturing small but important entities from the electrical and the fire-safety domain, such as signs, sockets, switches, smoke alarms, etc. Unlike most previous research that focussed on structural elements and processed laser point clouds and images separately, we propose a novel method that fuses laser scanning and photogrammetry methods to capture the relevant objects, recognise them in 2D images and then map these to a 3D space. The considered object classes include electrical elements (light switch, light, speaker, socket, elevator button), safety elements (emergency switch, smoke alarm, fire extinguisher, escape sign), plumbing system elements (pipes), and other objects with useful information (door sign, board). Semantic information like class labels is extracted by applying AI-based image segmentation and then mapped to the 3D point cloud, segmenting the point cloud into point clusters. We subsequently fit geometric primitives to the point clusters and extract text information by AI-based text detection and recognition. The final output of our proposed method is an information-rich digital twin of buildings that contains geometric information, semantic information such as object categories and useful text information which is valuable in many aspects, like condition monitoring, facility maintenance and management. In summary, the paper presents a nearly fully-automated pipeline to enrich a geometric digital twin of buildings with details and provides a comprehensive case study.}
}
@article{KANDASAMY2022108061,
title = {An electric power digital twin for cyber security testing, research and education},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {108061},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108061},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622003196},
author = {Nandha Kumar Kandasamy and Sarad Venugopalan and Tin Kit Wong and Nicholas Junming Leu},
keywords = {Cyber Physical System, Smartgrid, Security, Digital twin, Test-bed},
abstract = {Cyber–Physical Systems (CPS) rely on communication and control technologies to efficiently manage devices in the system. However, a wide variety of potential security challenges has emerged due to the evolution of critical infrastructures (CI) from siloed sub-systems into integrated networks, including smart grid (SG). SG security studies are carried out on physical test-beds to train and test cyber attacks in a safe/controlled environment. However, it has limitations w.r.t modifying physical configuration and scalability. To overcome these shortcomings, we built a digital power twin for a physical test-bed that is used for cyber security studies on SGs. The twin enable users to deploy real world attacks and countermeasures, and study its effectiveness. In the twin users may easily modify the components and configurations unlike in a physical test-bed. Further, reproducing the twin for advancing the research is significantly cheaper. We illustrate the typical use case with an example case study.}
}
@article{XIE2022105972,
title = {Semi-supervised region-connectivity-based cerebrovascular segmentation for time-of-flight magnetic resonance angiography image},
journal = {Computers in Biology and Medicine},
volume = {149},
pages = {105972},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105972},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522007004},
author = {Lei Xie and Zan Chen and Xuanshuo Sheng and Qingrun Zeng and Jiahao Huang and Caiyun Wen and Liang Wen and Guoqiang Xie and Yuanjing Feng},
keywords = {TOF-MRA, Cerebrovascular segmentation, Deep learning, Semi-supervised learning, Region-connectivity-based},
abstract = {Deep-learning-based methods have achieved state-of-the-art results in cerebrovascular segmentation. However, it is costly and time-consuming to acquire labeled data because of the complex structure of cerebral vessels. In this paper, we propose a novel semi-supervised cerebrovascular segmentation with a region-connectivity-based mean teacher model (RC-MT) from time-of-flight magnetic resonance angiography (TOF-MRA), where unlabeled data is introduced into the training. Concretely, the RC-MT framework consists of a mean teachers (MT) model and a region-connectivity-based model. The region-connectivity-based model dynamically controls the balance between the supervised loss and unsupervised consistency loss by taking into account that the predicted vessel voxels should be continuous in the underlying anatomy of the brain. Meanwhile, we design a novel multi-scale channel attention fusion Unet (MSCAF-Unet) as a backbone for the student model and the teacher model. The MSCAF-Unet is a multi-scale channel attention fusion layer used to construct an image pyramid input and achieve multi-level receptive field fusion. The proposed method is evaluated on diverse TOF-MRA datasets (three clinical datasets and a public dataset). Experimental results show that the proposed method achieves high-performance gains by incorporating the unlabeled data and outperforms competing semi-supervised-based methods. The code will be openly available at https://github.com/IPIS-XieLei/RC-MT.}
}
@article{YANG2022109156,
title = {A novel quantitative relationship neural network for explainable cognitive diagnosis model},
journal = {Knowledge-Based Systems},
volume = {250},
pages = {109156},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109156},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122005755},
author = {Haowen Yang and Tianlong Qi and Jin Li and Longjiang Guo and Meirui Ren and Lichen Zhang and Xiaoming Wang},
keywords = {Cognitive diagnosis, Cognitive status, Learner modeling, Quantitative relationships, Implicit concepts},
abstract = {Cognitive diagnosis is a fundamental task to assist personalized learning in education, and aims to discover learners’ proficiency in knowledge concepts. Because cognitive diagnosis models play a very important role in predicting learner performance and recommending personalized learning resources such as exercises, course videos, and course audio, they have received great attention from researchers. However, existing cognitive diagnosis models mostly start from the interactive perspective of learners’ answers, ignoring the internal quantitative relationship between exercises and knowledge concepts. This study proposes a novel quantitative relationship-based explainable cognitive diagnosis model called QRCDM. First, learners’ concept proficiency was defined based on their answers to objective and subjective questions. Correlation hypotheses are then proposed, which include the explicit correlation between exercises and their corresponding knowledge concepts, as well as the implicit correlation between exercises and the non-inclusive concept. Finally, two contribution matrices of exercises and knowledge concepts through a neural network designed in this study are calculated based on the above hypotheses, which can predict the learner’s concept proficiency and answer score. To reduce the noisy data, the learners’ faults and guessing factors were also considered. In the experiments, the proposed QRCDM was compared with two classical models, DINA, FuzzyCDF and three latest state-of-the-art models, DeepCDM, NeuralCDM and RCD on five real datasets, and the most experimental results on the majority metrics show the effectiveness and interpretability of this work.}
}
@article{NASER2022104640,
title = {Digital twin for next gen concretes: On-demand tuning of vulnerable mixtures through Explainable and Anomalous Machine Learning},
journal = {Cement and Concrete Composites},
volume = {132},
pages = {104640},
year = {2022},
issn = {0958-9465},
doi = {https://doi.org/10.1016/j.cemconcomp.2022.104640},
url = {https://www.sciencedirect.com/science/article/pii/S0958946522002335},
author = {M.Z. Naser},
keywords = {Machine learning, Digital twin, Concrete, Explainability, Clustering},
abstract = {This paper presents a framework for integrating Explainable and Anomalous Machine Learning (EAML) into a digital twin to enable finetuning of mixtures as a mean to realize next-gen concretes with favorable performance. In this framework, both anomalous unsupervised and explainable supervised ML algorithms are joined to create a virtual assistant capable of exploring the influence of mixture materials and proportions on the required performance of concrete. This virtual assistant is not only trained to detect inherent vulnerabilities within mixtures but can also finetune such mixtures to overcome potential weaknesses – especially when concrete is expected to serve under extreme loading conditions. The proposed framework has been rigorously examined on three case studies to identify vulnerable mixtures to: 1) fire-induced spalling, 2) chloride penetration, and 3) failing to attain full design strength in job sites, using small and large datasets comprised from actual measurements. Results from our analysis show how the proposed framework was capable of identifying vulnerable concrete mixtures and of satisfying various performance metrics. While the proposed framework is designed to be algorithm-independent and hence can be scalable across multiple platforms, this work showcases the application of anomaly detecting and clustering algorithms, together with an ensemble of classifiers encompassing extreme and light gradient boosted trees (GBT), generalized additive models (GAM), and keras deep residual neural network (KDP).}
}
@article{YANG2022109367,
title = {Assessment of reactor flow field prediction based on deep learning and model reduction},
journal = {Annals of Nuclear Energy},
volume = {179},
pages = {109367},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109367},
url = {https://www.sciencedirect.com/science/article/pii/S0306454922004029},
author = {Jun Yang and Xi Sui and Yanping Huang and Ling Zhao and Minyun Liu},
keywords = {Digital twin, Computational fluid dynamics, Model reduction, Singular value decomposition, Deep learning},
abstract = {All-around and full-cycle digital simulation technology can improve the safety and economy of the reactor in research, development, operation, and maintenance processes. However, for the local three-dimensional fluid dynamic problems in complex reactor system, traditional computational fluid dynamic (CFD) methods are severely limited by the solution efficiency and accuracy. This study proposed a rapid flow field prediction method based on singular value decomposition (SVD) and deep learning. The raw flow field snapshot data was compressed using SVD to extract low-dimensional features, and the deep neural network was used to construct a flow field reduced-order model to achieve rapid flow field prediction. A benchmark problem of flow around a cylinder was selected to assess the efficacy of this method. Furtherly, this method was applied in the three-dimensional flow field simulations of the fuel assembly. The results demonstrated that the reduced-order model (ROM) error was less than 6% compared with that of the CFD model, and the time consumption was less than 1% of that of the CFD model. This exploration illustrates that high-fidelity ROMs based on order reduction and deep learning are a viable route to developing engineering-ready digital twins of nuclear reactors.}
}
@article{ARUMUGARAJA2022111603,
title = {Design and development of foot worn piezoresistive sensor for knee pain analysis with supervised machine learning algorithms based on gait pattern},
journal = {Measurement},
volume = {200},
pages = {111603},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111603},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122008144},
author = {M. Arumugaraja and B. Padmapriya and S. Poornachandra},
keywords = {Gait monitoring, Wearable sensorized insole, Piezoresistive sensor, Gaussian noise filter, Supervised machine learning, Knee pain detection},
abstract = {Gait monitoring and analysis have garnered more attention in gait analysis to verify the potential improvements of lower limb disorders like arthritis, trauma, and degenerative disorders. The irregularities in gait often manifest as knee pain or physical discomfort experienced by the patient. Existing vision and floor sensor-based systems have the limitations of operational complexity and high cost that make them uncomfortable for individual use. These limitations led to research interest in the design and development of insole embedded with 102 sensors to detect foot pressure distribution image for detecting lower limb disorder-based problems. The quality of these heat images is enhanced by a hybrid filter (RMSE = 2.748 and PSNR = 39.35) and feature extraction technique is utilized on the enhanced foot pressure images for classification. The KNN learner model yields accuracy of 99.4% in the detection of knee pain.}
}
@article{GAO2022102857,
title = {SEDML: Securely and efficiently harnessing distributed knowledge in machine learning},
journal = {Computers & Security},
volume = {121},
pages = {102857},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102857},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822002516},
author = {Yansong Gao and Qun Li and Yifeng Zheng and Guohong Wang and Jiannan Wei and Mang Su},
keywords = {Distributed learning, Knowledge transfer, Privacy protection, Secure computation, Differential privacy},
abstract = {Training high-performing machine learning models require a rich amount of data which is usually distributed among multiple data sources in practice. Simply centralizing these multi-sourced data for training would raise critical security and privacy concerns, and might be prohibited given the increasingly strict data regulations. To resolve the tension between privacy and data utilization in distributed learning, a machine learning framework called private aggregation of teacher ensembles (PATE) has been recently proposed. PATE harnesses the knowledge (label predictions for an unlabeled dataset) from distributed teacher models to train a student model, obviating access to distributed datasets. Despite being enticing, PATE does not offer protection for the individual label predictions from teacher models, which still entails privacy risks. In this paper, we propose SEDML, a new protocol which allows to securely and efficiently harness the distributed knowledge in machine learning. SEDML builds on lightweight cryptography and provides strong protection for the individual label predictions, as well as differential privacy guarantees on the aggregation results. Extensive evaluations show that while providing privacy protection, SEDML preserves the accuracy as in the plaintext baseline. Meanwhile, SEDML outperforms the state-of-the-art work of Xiang et al. (ICDCS’20) by 43× in computation and 1.23× in communication.}
}
@article{ZOHDI2022115315,
title = {Machine-learning and digital-twins for rapid evaluation and design of injected vaccine immune-system responses},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {401},
pages = {115315},
year = {2022},
note = {A Special Issue on Computational Modeling and Simulation of Infectious Diseases},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.115315},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522004169},
author = {T.I. Zohdi},
keywords = {Vaccine design, Digital-twins, Machine-learning},
abstract = {A computational framework is developed that researchers in the field can easily implement and subsequently use as an efficient tool to study the immune-response to a vaccine injection. There are three main components to this work: Part I-Digital-twin construction: An approach is developed that efficiently simulates the time-transient proliferation of cells/antibodies (proteins) and regulator/antigens (deactivated toxin) to an injected vaccine within tissue possessing complex heterogeneous microstructure. Here, we use the terms “cells” and “antibodies”, as well as “regulator” and “antigen” interchangeably. The approach utilizes two strongly-coupled conservation laws: (a) Conservation Law 1: comprises (a) rate of change of cells/antibodies, (b) cellular/antibody migration, (c) cellular/antibody proliferation controlled by a cell/antibody mitosis regulating chemical (antigen), (d) cell/antibody apoptosis and (b) Conservation Law 2: comprises (a) rate of change of the cell/antibody mitosis chemical regulator/antigen, (b) regulator/antigen diffusion, (c) regulator production by cells/antibody and (d) regulator/antigen decay. Part II-Efficient computation: A technique based on a voxel (3D “volume pixels”) representation of tissue microstructures and corresponding digital solution methods is developed for the calculations, which avoids computationally expensive steps involved in usual Finite Element procedures such as topologically conforming meshing, mapping, volume integration, stiffness matrix generation and matrix-based solution methods. The process proceeds by converting the tissue microstructure into voxels. The problem then becomes “digital” on a regular “voxel-grid”, directly manipulating voxel values, allowing extremely fast methods to be used to construct derivatives and to iteratively solve the system with minimal memory requirements. Part III-Machine-learning: The rapid and efficient computation allows for many vaccines to be tested quickly and uses a genomic-based Machine-Learning Algorithm to optimize the system. This is particularly useful for rapid design of next-generation vaccines and boosters for disease strain mutations. Numerical examples are provided to illustrate the results, with the overall goal being to provide a computational framework to rapidly design and deploy a vaccine for a targeted response.}
}
@article{ZHAO2022108454,
title = {Digital twin-enabled dynamic spatial-temporal knowledge graph for production logistics resource allocation},
journal = {Computers & Industrial Engineering},
volume = {171},
pages = {108454},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108454},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222004880},
author = {Zhiheng Zhao and Mengdi Zhang and Jian Chen and Ting Qu and George Q. Huang},
keywords = {Digital twin, Production logistics, Knowledge graph},
abstract = {Production logistics (PL) is increasingly receiving attention from supply chain research. The spatial disorder and temporal asynchrony of the PL resources due to the uncertainty and dynamicity pose great challenges to efficient resource allocation. The inability to obtain and rational use of PL resource spatial–temporal values causes unnecessary long travelling distances and excessive waiting time, which impede the sustainable performance of PL operations. In response, this research proposes a PL resource allocation approach based on the dynamic spatial–temporal knowledge graph (DSTKG). Internet of Things(IoT) signals data generated from large-scale deployed IoT devices are investigated and analysed to spatial–temporal values through deep neural networks. The DSTKG model is established for representing the digital twin replica with spatial–temporal consistency, followed by reasoning and completion of relationships based on PL task information. The PL resources are allocated efficiently through the graph algorithm from the directed and weighted graph. The case study is conducted to verify the feasibility and practicality of the proposed solution based on large-scale deployment. Finally, the result demonstrates the effectiveness of the proposed methodology.}
}
@article{KHAN2022105581,
title = {Knowledge distillation approach towards melanoma detection},
journal = {Computers in Biology and Medicine},
volume = {146},
pages = {105581},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105581},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522003730},
author = {Md Shakib Khan and Kazi Nabiul Alam and Abdur Rab Dhruba and Hasib Zunair and Nabeel Mohammed},
keywords = {Melanoma detection, Knowledge distillation, Skin lesion analysis, Deep learning},
abstract = {Melanoma is regarded as the most threatening among all skin cancers. There is a pressing need to build systems which can aid in the early detection of melanoma and enable timely treatment to patients. Recent methods are geared towards machine learning based systems where the task is posed as image recognition, tag dermoscopic images of skin lesions as melanoma or non-melanoma. Even though these methods show promising results in terms of accuracy, they are computationally quite expensive to train, that questions the ability of these models to be deployable in a clinical setting or memory constraint devices. To address this issue, we focus on building simple and performant models having few layers, less than ten compared to hundreds. As well as with fewer learnable parameters, 0.26 million (M) compared to 42.5 M using knowledge distillation with the goal to detect melanoma from dermoscopic images. First, we train a teacher model using a ResNet-50 to detect melanoma. Using the teacher model, we train the student model known as Distilled Student Network (DSNet) which has around 0.26 M parameters using knowledge distillation achieving an accuracy of 91.7%. We compare against ImageNet pre-trained models such MobileNet, VGG-16, Inception-V3, EfficientNet-B0, ResNet-50 and ResNet-101. We find that our approach works well in terms of inference runtime compared to other pre-trained models, 2.57 s compared to 14.55 s. We find that DSNet (0.26 M parameters), which is 15 times smaller, consistently performs better than EfficientNet-B0 (4 M parameters) in both melanoma and non-melanoma detection across Precision, Recall and F1 scores.}
}
@article{SHI2022104493,
title = {Data-driven construction of Three-dimensional subsurface geological models from limited Site-specific boreholes and prior geological knowledge for underground digital twin},
journal = {Tunnelling and Underground Space Technology},
volume = {126},
pages = {104493},
year = {2022},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2022.104493},
url = {https://www.sciencedirect.com/science/article/pii/S088677982200133X},
author = {Chao Shi and Yu Wang},
keywords = {3D geological model, Machine learning, Convolutional neural networks, XGBoost, Sparse measurements, Point cloud},
abstract = {A digital twin is a digital replica or virtual representations of 3D physical entities in the real world. In practice, it is challenging for 3D modelling of subsurface stratigraphy in an underground digital twin due to insufficient site-specific measurements and a lack of efficient 3D spatial prediction tools. In this study, a data-driven and deep learning method, called IC-XGBoost3D, is proposed to build a 3D geological model from limited site-specific boreholes and 2D training images reflecting prior geological knowledge. Anisotropic stratigraphic relationships are learned from two perpendicular 2D training images, and the extracted stratigraphic statistics serve as the input for pre-training a 2D simulation slice. A sequence of 2D simulation slices is then simulated with constraints by site-specific boreholes and subsequently reassembled into a 3D geological model. Note that the parameters of the deep learning method are calibrated with site-specific data and prior training images before being applied to develop the geological model. The model performance is demonstrated and validated using an illustrative example. The proposed method can efficiently generate an anisotropic 3D geological model as a point cloud from two perpendicular training images and sparse boreholes with a high prediction accuracy. More importantly, the proposed method not only infers the most probable 3D geological domain, but also provides a quantitative evaluation of associated 3D stratigraphic uncertainty. Effects of irregular borehole spacing and single training image on the simulation performance are also investigated.}
}
@article{GARCIA2022108463,
title = {Towards a connected Digital Twin Learning Ecosystem in manufacturing: Enablers and challenges},
journal = {Computers & Industrial Engineering},
volume = {171},
pages = {108463},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108463},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222004922},
author = {Álvaro García and Anibal Bregon and Miguel A. Martínez-Prieto},
keywords = {Digital twin, Learning ecosystem, Manufacturing, Human–machine collaboration, Learning factory, Cyber–physical system},
abstract = {The evolution of digital twin, leveraged by the progressive physical–digital convergence, has provided smart manufacturing systems with knowledge-generation ecosystems based on new models of collaboration between the workforce and industrial processes. Digital twin is expected to be a decision-making solution underpinned by real-time communication and data-driven enablers, entailing close cooperation between workers, systems and processes. But industry will need to face the challenges of building and supporting new technical and digital infrastructures, while workers’ skills development eventually manages to include the increased complexity of industrial processes. This paper is intended to reach a better understanding of learning opportunities offered by emerging Industry 4.0 digital twin ecosystems in manufacturing. Diverse learning approaches focused on the potential application of the digital twin concept in theoretical and real manufacturing ecosystems are reviewed. In addition, we propose an original definition of Digital Twin Learning Ecosystem and the conceptual layered architecture. Existing key enablers of the digital twin physical–digital convergence, such as collaborative frameworks, data-driven approaches and augmented interfaces, are also described. The role of the Learning Factory concept is highlighted, providing a common understanding between academia and industry. Academic applications and complex demonstration scenarios are combined in line with the enablement of connected adaptive systems and the empowerment of workforce skills and competences. The adoption of digital twin in production is still at an initial stage in the manufacturing industry, where specific human and technological challenges must be addressed. The research priorities presented in this work are considered as a recognised basis in industry, which should help digital twin with the objective of its progressive integration as a future learning ecosystem.}
}
@article{TEEKARAMAN2022108203,
title = {Abridged design with demand conventions for health care applications},
journal = {Computers and Electrical Engineering},
volume = {102},
pages = {108203},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108203},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200444X},
author = {Yuvaraja Teekaraman and Hariprasath Manoharan and Irina Kirpichnikova and Ramya Kuppusamy},
keywords = {Digital twins, Exploration units, Health care, Virtual representation},
abstract = {This article focuses on the process of improving health care in real-time experimental settings. Despite the fact that there are numerous solutions for health-care issues, no remote solution has been devised. As a result, a novel model has been devised in the suggested technique, in which all hospital administration processes are simplified through the use of a virtual representation digital twin process. When such technologies are used, a discrete protocol is followed, resulting in the identification and integration of a security model based on demand movement. A machine learning discriminant analysis has been combined with the process of digital twin functions efficiently based on input signals and state vectors. The cohesive model is put to the test in four case studies, including delay monitoring, predictive score analysis, communication units, and battery charge rate, with all case study outcomes proving to be more effective than existing systems by an average of 74%.}
}
@article{ZHOU2022103448,
title = {Uncertainty-aware consistency regularization for cross-domain semantic segmentation},
journal = {Computer Vision and Image Understanding},
volume = {221},
pages = {103448},
year = {2022},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2022.103448},
url = {https://www.sciencedirect.com/science/article/pii/S1077314222000625},
author = {Qianyu Zhou and Zhengyang Feng and Qiqi Gu and Guangliang Cheng and Xuequan Lu and Jianping Shi and Lizhuang Ma},
keywords = {Domain adaptation, Semantic segmentation, Transfer learning, Consistency regularization},
abstract = {Unsupervised domain adaptation (UDA) aims to adapt existing models of the source domain to a new target domain with only unlabeled data. Most existing methods suffer from noticeable negative transfer resulting from either the error-prone discriminator network or the unreasonable teacher model. Besides, the local regional consistency in UDA has been largely neglected, and only extracting the global-level pattern information is not powerful enough for feature alignment due to the abuse use of contexts. To this end, we propose an uncertainty-aware consistency regularization method for cross-domain semantic segmentation. Firstly, we introduce an uncertainty-guided consistency loss with a dynamic weighting scheme by exploiting the latent uncertainty information of the target samples. As such, more meaningful and reliable knowledge from the teacher model can be transferred to the student model. We further reveal the reason why the current consistency regularization is often unstable in minimizing the domain discrepancy. Besides, we design a ClassDrop mask generation algorithm to produce strong class-wise perturbations. Guided by this mask, we propose a ClassOut strategy to realize effective regional consistency in a fine-grained manner. Experiments demonstrate that our method outperforms the state-of-the-art methods on four domain adaptation benchmarks, i.e., GTAV → Cityscapes, SYNTHIA → Cityscapes, Virtual KITTI ⟶ KITTI and Cityscapes ⟶ KITTI.}
}
@article{SUN2022117680,
title = {Ensemble Knowledge Tracing: Modeling interactions in learning process},
journal = {Expert Systems with Applications},
volume = {207},
pages = {117680},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117680},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422009769},
author = {Jianwen Sun and Rui Zou and Ruxia Liang and Lu Gao and Sannyuya Liu and Qing Li and Kai Zhang and Lulu Jiang},
keywords = {Knowledge Tracing, Deep neural network, Learning interactions, User modeling},
abstract = {Knowledge Tracing (KT) aims to continuously estimate students’ evolving knowledge state during their learning process, which has attracted much research attention due to its potential for delivering personalized and optimal experiences to students in intelligent learning systems. The learning process is essentially the pairwise interactions of Students, Concepts, and Questions (S–C, S–Q, C–Q for short). Modeling all these interactions will improve the performance of KT. However, existing KT methods hardly exploit all the interactions in a single model. Specifically, Bayesian Knowledge Tracing (BKT) and most of its variants neglect C–Q; Deep Knowledge Tracing (DKT) and other deep neural network approaches mostly neglect S–Q and C–Q. We propose the Ensemble Knowledge Tracing (EnKT), which models all three types of interactions. The base model of EnKT is a hybrid of BKT and DKT. We also present an ensemble algorithm Recurrent Boosting (RB), which extends AdaBoost to deal with KT sequential data. Inspired by BKT, EnKT represents S–C and S–Q using learning and performance parameters, respectively. Besides, EnKT defines C–Q as the correlation complexity among the concepts involved in a question. Experiments show EnKT significantly outperforms state-of-the-art methods (by up to 6% in AUC in some cases) on four real-world benchmark datasets and illustrate better interpretability by several typical case studies.}
}
@article{XIAO2022107099,
title = {Efficient Combination of CNN and Transformer for Dual-Teacher Uncertainty-guided Semi-supervised Medical Image Segmentation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {226},
pages = {107099},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107099},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722004801},
author = {Zhiyong Xiao and Yixin Su and Zhaohong Deng and Weidong Zhang},
keywords = {Magnetic resonance imaging (MRI), Deep learning, Transformer, Semi-supervised learning, Image segmentation},
abstract = {Background and objective: Deep learning-based methods for fast target segmentation of magnetic resonance imaging (MRI) have become increasingly popular in recent years. Generally, the success of deep learning methods in medical image segmentation tasks relies on a large amount of labeled data. The time-consuming and labor-intensive problem of data annotation is a major challenge in medical image segmentation tasks. The aim of this work is to enhance the segmentation of MR images using a semi-supervised learning-based method using a small amount of labeled data and a large amount of unlabeled data. Methods: To utilize the effective information of the unlabeled data, we designed the method of guiding the Student segmentation model simultaneously by the Dual-Teacher structure of CNN and transformer forming the subject network. Both Teacher A and Student models are CNNs, and the TA-S module they form is a mean teacher structure with added data noise. In the TB-S module formed by the combination of Student and Teacher B models, their backbone networks CNN and transformer capture the local and global information of the image at the same time, respectively, to create pseudo labels for each other and perform cross-supervision. The Dual-Teacher guides the Student through synchronous training and performs knowledge rectification and communication with each other through consistent regular constraints, which better utilizes the valid information in the unlabeled data. In addition, the segmentation predictions of Teacher A and Student and Teacher A and Teacher B are screened for uncertainty assessment during the training process to enhance the prediction accuracy and generalization of the model. This method uses the mechanism of simultaneous training of the synthetic structure composed of TA-S and TB-S modules to jointly guide the optimization of the Student model to obtain better segmentation ability. Results: We evaluated the proposed method on a publicly available MRI dataset from a cardiac segmentation competition organized by MICCAI in 2017. Compared with several existing state-of-the-art semi-supervised segmentation methods, the method achieves better segmentation results in terms of Dice coefficient and HD distance evaluation metrics of 0.878 and 4.9 mm and 0.886 and 5.0 mm, respectively, using a training set containing only 10% and 20% of labeled data. Conclusion: This method fuses CNN and transformer to design a new Teacher-Student semi-supervised learning optimization strategy, which greatly improves the utilization of a large number of unlabeled medical images and the effectiveness of model segmentation results.}
}
@article{JI2022103533,
title = {Defending against attacks tailored to transfer learning via feature distancing},
journal = {Computer Vision and Image Understanding},
volume = {223},
pages = {103533},
year = {2022},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2022.103533},
url = {https://www.sciencedirect.com/science/article/pii/S107731422200114X},
author = {Sangwoo Ji and Namgyu Park and Dongbin Na and Bin Zhu and Jong Kim},
keywords = {Robust transfer learning, Adversarial example, Triplet loss, Mimic attack, Target-agnostic attack},
abstract = {Transfer learning is preferable for training a deep neural network with a small training dataset by leveraging a pre-trained teacher model. However, transfer learning opens a door for new attacks that generate adversarial examples using the pre-trained teacher model. In this paper, we propose a novel method called feature distancing to defend against adversarial attacks tailored to transfer learning. The method aims to train a student model with a distinct feature representation from the teacher model. We generate adversarial examples of the mimic attack with the teacher model, and the examples are used to train the student model. We use triplet loss to put the mimic attack examples close to their source images and far from their target images in the feature space of the student model. The proposed method is evaluated on three different transfer learning tasks with diverse attack configurations. It is the only method that achieves high “robust accuracy” and high “test accuracy” on every task we evaluate.}
}
@article{KARIMIALAVIJEH2022100040,
title = {Digitally enabled approaches for the scale up of mammalian cell bioreactors},
journal = {Digital Chemical Engineering},
volume = {4},
pages = {100040},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100040},
url = {https://www.sciencedirect.com/science/article/pii/S277250812200031X},
author = {Masih {Karimi Alavijeh} and Irene Baker and Yih Yean Lee and Sally L. Gras},
keywords = {Machine learning, Mechanistic modelling, Biomanufacturing, Bioreactor},
abstract = {With recent advances in digitisation and big data analytics, more pharmaceutical firms are adopting digital tools to achieve modernisation. The biological phenomena within bioreactors are a key target for such digital approaches, as these processes are often complicated and difficult to scale. Historically, rules of thumb have been used to match performance indicators across bioreactor scales. Although such methods are well-established and frequently employed by industry, no universal solution has been developed to overcome the many challenges faced in process development and scale-up. Several computer-based methodologies can potentially be applied to bioreactor scale-up, including knowledge driven and data-driven techniques. This review assesses the state of the art in digital advances in scaling bioreactors and the advantages and limitations of scaling techniques. Traditional approaches and their constraints are outlined. The application of knowledge-based techniques is then considered and compared to data-driven models. The ability to transfer processes across bioreactor scales, to compare data and predict process indicators across scales are then examined. Finally, the role of hybrid modelling and digital twins and their potential in bioprocess development are explored.}
}
@article{HUGHES2022109502,
title = {On robust risk-based active-learning algorithms for enhanced decision support},
journal = {Mechanical Systems and Signal Processing},
volume = {181},
pages = {109502},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109502},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022006124},
author = {A.J. Hughes and L.A. Bull and P. Gardner and N. Dervilis and K. Worden},
keywords = {Decision-making, Active learning, Value of information, Structural health monitoring, Sampling bias, Digital twins, Risk},
abstract = {Classification models are a fundamental component of physical-asset management technologies such as structural health monitoring (SHM) systems and digital twins. Previous work introduced risk-based active learning, an online approach for the development of statistical classifiers that takes into account the decision-support context in which they are applied. Decision-making is considered by preferentially querying data labels according to expected value of perfect information (EVPI). Although several benefits are gained by adopting a risk-based active learning approach, including improved decision-making performance, the algorithms suffer from issues relating to sampling bias as a result of the guided querying process. This sampling bias ultimately manifests as a decline in decision-making performance during the later stages of active learning, which in turn corresponds to lost resource/utility. The current paper proposes two novel approaches to counteract the effects of sampling bias: semi-supervised learning, and discriminative classification models. These approaches are first visualised using a synthetic dataset, then subsequently applied to an experimental case study, specifically, the Z24 Bridge dataset. The semi-supervised learning approach is shown to have variable performance; with robustness to sampling bias dependent on the suitability of the generative distributions selected for the model with respect to each dataset. In contrast, the discriminative classifiers are shown to have excellent robustness to the effects of sampling bias. Moreover, it was found that the number of inspections made during a monitoring campaign, and therefore resource expenditure, could be reduced with the careful selection of the statistical classifiers used within a decision-supporting monitoring system.}
}
@article{WAN2022109551,
title = {A dual learning-based recommendation approach},
journal = {Knowledge-Based Systems},
volume = {254},
pages = {109551},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109551},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122007791},
author = {Shanshan Wan and Ying Liu and Dongwei Qiu and James Chambua and Zhendong Niu},
keywords = {Recommender system, Dual learning, Data sparsity, Duality, Hybrid filtering recommendation},
abstract = {Data sparsity and cold start are two critical issues which need to be addressed in recommender systems (RSs). Currently, most methods address these issues by applying user history files or some side information to improve the user model and complete the rating matrix. However, such methods cannot perform well when labeled data is scarce or unavailable. In this paper, we propose a dual learning-based recommendation approach (DLRA). DLRA can trigger initial recommendation and improve the quality of recommendations by using the duality characteristics of RSs, even when the available labeled information is scarce. Specifically, DLRA regards the recommendation task as two independent subtasks — primal task and dual task, and these two tasks show strong duality in DLRA. The primal task is item-centered which aims to find users who can rate high for items, while the dual task is user-centered that aims to recommend the most favorite items to users. These two tasks have strong dualities in terms of the recommendation space, selection probability and recommendation basis. Based on these dualities, we design three dual learning strategies to couple the whole recommendation process and realize the self-tuning and self-improvement of each task model, and finally optimize the whole recommendation model. Based on the dataset of Movielens and BookCrossing, we simulate data sparsity and cold start recommendation scenarios, the experimental results show that DLRA achieves substantial improvement when the labeled data is scare, and it outperforms other hybrid recommendation approaches and deep learning strategies with a smaller predictive error as well as better recommendation accuracy.}
}
@article{BANNOUR2022104073,
title = {Privacy-preserving mimic models for clinical named entity recognition in French},
journal = {Journal of Biomedical Informatics},
volume = {130},
pages = {104073},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104073},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000892},
author = {Nesrine Bannour and Perceval Wajsbürt and Bastien Rance and Xavier Tannier and Aurélie Névéol},
keywords = {Confidentiality, Datasets as topic, Electronic health records, Mimic learning, Natural language processing, Neural networks, Computer},
abstract = {A vast amount of crucial information about patients resides solely in unstructured clinical narrative notes. There has been a growing interest in clinical Named Entity Recognition (NER) task using deep learning models. Such approaches require sufficient annotated data. However, there is little publicly available annotated corpora in the medical field due to the sensitive nature of the clinical text. In this paper, we tackle this problem by building privacy-preserving shareable models for French clinical Named Entity Recognition using the mimic learning approach to enable the knowledge transfer through a teacher model trained on a private corpus to a student model. This student model could be publicly shared without any access to the original sensitive data. We evaluated three privacy-preserving models using three medical corpora and compared the performance of our models to those of baseline models such as dictionary-based models. An overall macro F-measure of 70.6% could be achieved by a student model trained using silver annotations produced by the teacher model, compared to 85.7% for the original private teacher model. Our results revealed that these privacy-preserving mimic learning models offer a good compromise between performance and data privacy preservation.}
}
@article{FERDAUS2022108818,
title = {Significance of activation functions in developing an online classifier for semiconductor defect detection},
journal = {Knowledge-Based Systems},
volume = {248},
pages = {108818},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108818},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122003872},
author = {Md Meftahul Ferdaus and Bangjian Zhou and Ji Wei Yoon and Kain Lu Low and Jieming Pan and Joydeep Ghosh and Min Wu and Xiaoli Li and Aaron Voon-Yew Thean and J. Senthilnath},
keywords = {Leaky ReLU, Online learning, Defect detection, Prequential, Semiconductors},
abstract = {In anomaly detection problems for advanced semiconductor devices, non-visual defects occur frequently. Machine learning (ML) algorithms have the advantage of identifying such defects. However, in this real-world problem, data comes sequentially in a streaming fashion, thus, we may not have sufficient data to train an ML model in batch mode. In such a scenario, online ML models are useful to detect defects immediately since they work in a single-pass mode. Besides, when data is collected from more realistic non-stationary monitoring environments, online ML models with evolving architecture are more practical. Thus, evolving and online ML models are developed in this work to detect defects in technology computer-aided design (TCAD)-based digital twin model of advanced nano-scaled semiconductor devices such as a fin field-effect transistor (FinFET) and a gate-all-around field-effect transistor (GAA-FET). Activation functions (AFs) in deep neural networks (DNNs) and membership functions (MFs) in neuro-fuzzy systems (NFSs) play an important role in the performance of those ML models. This work focuses on analyzing the effects of various AFs/MFs in our developed online ML models while detecting defects in real-world nano-scaled semiconductor devices, where significant training samples are not available. From various semiconductor datasets having fewer samples, it has been observed that the proposed evolving neuro-fuzzy system (ENFS) with Leaky-ReLU MF performs better (improvement in the range of 1.9% to 30.8% considering overall classification accuracy) than the other DNN or ENFS-based online ML models. Having an evolving architecture and online learning mechanism, besides anomaly detection, the proposed model’s performance has also been evaluated for handling large data streams problems with concept drift. The performance of the proposed method has been compared with some recently developed baselines under the prequential test-then-train protocol. The classification rates of the proposed method has an improvement in the range of 1.1% to 65.9% than the existing methods. The code of this work has been made publicly available at https://github.com/MdFerdaus/LREC.}
}
@article{SERRANORUIZ2022185,
title = {Development of a multidimensional conceptual model for job shop smart manufacturing scheduling from the Industry 4.0 perspective},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {185-202},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000462},
author = {Julio C. Serrano-Ruiz and Josefa Mula and Raúl Poler},
keywords = {Industry 4.0, Job shop, Smart manufacturing scheduling, Digital twin, Zero-defect manufacturing},
abstract = {Based on a scientific literature review in the conceptual domain defined by smart manufacturing scheduling (SMS), this article identifies the benefits and limitations of the reviewed contributions, establishes and discusses a set of criteria with which to collect and structure its main synergistic attributes, and devises a conceptual framework that models SMS around three axes: a semantic ontology context, a hierarchical agent structure, and the deep reinforcement learning (DRL) method. The main purpose of such a modelling research is to establish a conceptual and structured relationship framework to improve the efficiency of the job shop scheduling process using the approach defined by SMS. The presented model orients the job shop scheduling process towards greater flexibility, through enhanced rescheduling capability, and towards autonomous operation, mainly supported by the use of machine learning technology. To the best of our knowledge, there are no other similar conceptual models in the literature that synergistically combine the potential of the specific set of Industry 4.0 principles and technologies that model SMS. This research can provide guidance for practitioners and researchers’ efforts to move toward the digital transformation of job shops.}
}
@article{SUN2022119739,
title = {Deep learning method based on graph neural network for performance prediction of supercritical CO2 power systems},
journal = {Applied Energy},
volume = {324},
pages = {119739},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119739},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922010273},
author = {Lei Sun and Tianyuan Liu and Ding Wang and Chengming Huang and Yonghui Xie},
keywords = {S-CO power system, Performance prediction, Thermodynamic characteristics, Digital twin, Graph neural network},
abstract = {Considering the increasing energy consumption and greenhouse gas emissions, the Supercritical CO2 (S-CO2) power system has attracted more and more attention. Due to the expensive computation resource and time cost, data-based solutions for performance prediction are urged. The surrogate model by machine learning is a promising alternative, but it only focuses on the objective functions and ignores the importance of topological structures and physical states of cycles. Aiming at providing a comprehensive model to predict physical states as well as thermodynamic characteristics, a deep learning method based on graph neural network (GNN) are devised in this paper. With the modeling calculation results as training dataset, a well-trained model can accurately reconstruct the physical states consisting of temperature, pressure, enthalpy, entropy (relative error of most samples <5 %) and exergy as well as thermal and exergy efficiency (relative error of most samples <5 %). Moreover, this model shows superior performance compared with traditional machine learning models including Regression Tree, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Gaussian Process Regression (GPR). Finally, the comparison between different training sizes demonstrate the model can help reduce sampling costs for complex systems. Overall, the presented deep learning model can provide a reliable and competitive choice for the digital twin of S-CO2 power system and other power systems.}
}
@article{HERRMANN202214,
title = {Hands-on kinetic measurements and simulation for chemical process engineering students},
journal = {Education for Chemical Engineers},
volume = {41},
pages = {14-21},
year = {2022},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772822000185},
author = {Stefan Herrmann and Daniel Felder and Maria Padligur and Sebastian Brosch and Matthias Geiger and Felix Stockmeier and Kristina Baitalow and Deniz Rall and Robert Femmer and Florian Roghmans and Martin Hauser and Jannik Mehlis and John Linkhorst and Matthias Wessling},
keywords = {Clock reaction, Laboratory class, Kinetic modeling, Computational fluid dynamics, Academic teaching, Hands-on learning},
abstract = {Hands-on experience in the laboratory is essential in chemical engineering education to enhance the understanding of abstract theories and their effect on chemical processes. In this work, we describe a laboratory class, which combines some of the main engineering concepts into a set of hands-on experiments and simulations. Students are introduced to an iodine clock reaction performed in multiple different reactor types and are instructed to determine the reaction kinetics. Subsequent analysis of the experimental data in Python teaches basic programming skills and the concepts of numeric integration and optimization. Finally, a digital twin of one of the reactors is developed in COMSOL Multiphysics to give the students an application-focused introduction to more-dimensional multiphysics modeling. The students thereby get practical insights into the different methods and stages of reactor and reaction engineering. Based on the students’ assignments, we consistently see a deeper understanding of reaction kinetics and reactor engineering than in the accompanying traditional lecture.}
}
@article{BAEK2022100097,
title = {Current state-of-the-art and utilities of machine learning for detection, monitoring, growth prediction, rupture risk assessment, and post-surgical management of abdominal aortic aneurysms},
journal = {Applications in Engineering Science},
volume = {10},
pages = {100097},
year = {2022},
issn = {2666-4968},
doi = {https://doi.org/10.1016/j.apples.2022.100097},
url = {https://www.sciencedirect.com/science/article/pii/S2666496822000152},
author = {Seungik Baek and Amirhossein Arzani},
keywords = {Data-driven approaches, EVAR, Circulating biomarkers, Pulse wave imaging, Physics-based machine learning, Digital twin},
abstract = {Ultrasound imaging has long been playing a central role in detecting abdominal aortic aneurysms (AAAs). With a recent trend of reducing prevalence of AAAs, ultrasound screening is only recommended for men aged 65 to 75 years with previous smoking history, and a national level of a screening program for women is currently not recommended in the US. In the 2000s, several research groups demonstrated the utility of finite element stress analysis using patient-specific images, which was promising for an accurate assessment of the rupture risk, but physical models remain to be enhanced by considering patient variability and multi-physical characteristics. This review aims to provide a survey of emerging and alternative technologies and new methodologies, such as personalized medicine and data-driven approaches, that may make potential breakthroughs on detection of small AAAs, monitoring of patients during the follow-ups, prediction of AAA growth, assessment of the rupture risk, and post-surgical prognosis for AAA patient management.}
}
@article{CHEN2022102974,
title = {A self-attention based global feature enhancing network for semantic segmentation of large-scale urban street-level point clouds},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {113},
pages = {102974},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102974},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222001662},
author = {Qi Chen and Zhenxin Zhang and Siyun Chen and Siyuan Wen and Hao Ma and Zhihua Xu},
keywords = {Urban street, Large-scale point clouds, Semantic segmentation, Attention mechanism, Global feature},
abstract = {Point clouds of large-scale urban street scenes contain large quantities of object categories and rich semantic information. The semantic segmentation is the basis and key to subsequent essential applications, such as digital twin engineering and city information model. The global feature of point clouds in large-scale scenes can provide long-range context information, which is critical to high-quality semantic segmentation. However, the learning of global spatial saliency considering class label constraints is often ignored in the feature representation of some deep learning models. With regard to this, we propose a Global Feature Self-Attention Encoding (GFSAE) module and a Weighted Semantic Mapping (WSM) module to make the semantic segmentation model of point clouds in large-scale urban street scene focus more on the global salient feature expression by self-attention enhancement channel by channel and take into account the constraints of semantic categories to learn a better semantic segmentation model for urban street scenes. The experiments are performed on the Semantic3D dataset and our own collected vehicle Mobile Laser Scanning (MLS) point cloud dataset. The segmentation results show that the GFSAE and the WSM proposed by us can improve the semantic segmentation of point clouds in large-scale urban street scenes and prove the effectiveness of our model compared with other state-of-the-art methods.}
}
@article{HAMID2022100044,
title = {Hybrid modelling for remote process monitoring and optimisation},
journal = {Digital Chemical Engineering},
volume = {4},
pages = {100044},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100044},
url = {https://www.sciencedirect.com/science/article/pii/S2772508122000333},
author = {Anuar Hamid and Anton Heryanto Hasan and Siti Nurfaqihah Azhari and Zalina Harun and Zulfan A. Putra},
keywords = {First principle modeling, Machine learning, Digital process engineering, Process modeling, Process simulation, Process optimization, Multi objective optimization, Dehydration unit, Natural gas, Water dew point, Reboiler duty},
abstract = {Process simulation is used to develop a digital twin representation of chemical processes typically for process optimization or what-if scenarios. However, it is known to be computationally expensive and there is an increasing need for process remote monitoring and optimization. This is where machine learning models shine, where they can run up to several orders of magnitude faster than their equivalent first principle process simulation models. Most of the previous work in this area tend to demonstrate the ability of machine learning models to accurately capture complex, non-linear relationships between process parameters in various chemical processes. Two important aspects are rarely discussed, namely the construction of machine learning models using operating data and the deployment of these models. In this paper, we address these two aspects and review the different information silos in industrial settings that need to be considered when working with hybrid models (combining process simulation and machine learning). This is illustrated by a case study for an operating natural gas dehydration unit, covering data management, process simulation, machine learning and visualization. We demonstrate how the hybrid models can be constructed and packaged as an online monitoring and a prediction dashboard. Several unique challenges are also highlighted including the reliability of field data and operational deviations due to operability and controllability - all of which need to be understood in order to successfully translate operating systems to process simulation and machine learning models that are reliable and accurate. While the exact implementation may vary from project to project, the current work serves as an example and highlights the important considerations to make when working with such systems.}
}
@article{CHRYSAFIADI2022109111,
title = {Cognitive-based adaptive scenarios in educational games using fuzzy reasoning},
journal = {Knowledge-Based Systems},
volume = {250},
pages = {109111},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109111},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122005482},
author = {Konstantina Chrysafiadi and Spyros Papadimitriou and Maria Virvou},
keywords = {Serious game, Educational game, Adaptivity, Fuzzy logic, Cognitive states},
abstract = {Nowadays, the use of educational games is gaining substantial popularity. Games offer immersive and fascinating environments that render them a powerful tool for achieving high-quality learning outcomes, like advancing students’ education through engaging activities. However, one crucial enhancement is that they should take into consideration important aspects of each individual student and adapt to them accordingly for improving the educational results. Given the above, the present work considers an educational adventure game that offers cognitive-based adaptivity in its educational content and scenarios. More specifically, it adapts both the difficulty level of the educational content and its plot dynamically to the knowledge level and learning needs of each individual student. The adaptation is realized using a fuzzy student model that detects the current cognitive state of the trainee and decides about the way in which the game’s plot has to be modified. It determines if its scenario will be dynamically extended or not, aiming to assist the trainee overcome her/his weaknesses and improve her/his educational performance. The gain of this is that through the plot’s adaptation, the game motivates the learners/players to be involved actively in the learning process and provides educational content tailored to their knowledge level and weaknesses, enhancing the educational results. The contribution of the game to the educational process and outcomes was thoroughly evaluated. The evaluation results indicate a high acceptance rate of the game by learners and teachers and underline its effectiveness in the educational results.}
}
@article{REJA2022104245,
title = {Computer vision-based construction progress monitoring},
journal = {Automation in Construction},
volume = {138},
pages = {104245},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104245},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522001182},
author = {Varun Kumar Reja and Koshy Varghese and Quang Phuc Ha},
keywords = {Progress monitoring, Computer vision, Automated construction, Data acquisition, 3D reconstruction, As-built modelling, Point cloud, Scan to BIM, Literature review, Digital Twin},
abstract = {Automating the process of construction progress monitoring through computer vision can enable effective control of projects. Systematic classification of available methods and technologies is necessary to structure this complex, multi-stage process. Using the PRISMA framework, relevant studies in the area were identified. The various concepts, tools, technologies, and algorithms reported by these studies were iteratively categorised, developing an integrated process framework for Computer-Vision-Based Construction Progress Monitoring (CV-CPM). This framework comprises: data acquisition and 3D-reconstruction, as-built modelling, and progress assessment. Each stage is discussed in detail, positioning key studies, and concurrently comparing the methods used therein. The four levels of progress monitoring are defined and found to strongly influence all stages of the framework. The need for benchmarking CV-CPM pipelines and components are discussed, and potential research questions within each stage are identified. The relevance of CV-CPM to support emerging areas such as Digital Twin is also discussed.}
}
@article{STERGIOU2022279,
title = {Digital twin intelligent system for industrial internet of things-based big data management and analysis in cloud environments},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {4},
pages = {279-291},
year = {2022},
note = {Virtual-reality and intelligent hardware in digital twins A)},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000444},
author = {Christos L. Stergiou and Kostas E. Psannis},
keywords = {Machine learning, IoT, Big data, Cloud computing, Management, Analytics, Digital twin Scenario, Energy efficiency},
abstract = {This work surveys and illustrates multiple open challenges in the field of industrial Internet of Things (IoT)-based big data management and analysis in cloud environments. Challenges arising from the fields of machine learning in cloud infrastructures, artificial intelligence techniques for big data analytics in cloud environments, and federated learning cloud systems are elucidated. Additionally, reinforcement learning, which is a novel technique that allows large cloud-based data centers, to allocate more energy-efficient resources is examined. Moreover, we propose an architecture that attempts to combine the features offered by several cloud providers to achieve an energy-efficient industrial IoT-based big data management framework (EEIBDM) established outside of every user in the cloud. IoT data can be integrated with techniques such as reinforcement and federated learning to achieve a digital twin scenario for the virtual representation of industrial IoT-based big data of machines and room temperatures. Furthermore, we propose an algorithm for determining the energy consumption of the infrastructure by evaluating the EEIBDM framework. Finally, future directions for the expansion of this research are discussed.}
}
@article{CHEN2022124040,
title = {Digital twins model and its updating method for heating, ventilation and air conditioning system using broad learning system algorithm},
journal = {Energy},
volume = {251},
pages = {124040},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124040},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222009434},
author = {Kang Chen and Xu Zhu and Burkay Anduv and Xinqiao Jin and Zhimin Du},
keywords = {Digital twins, Broad learning system, Incremental learning, Online model updating, HVAC system},
abstract = {Digital Twins (DT) can be used for the energy efficiency management of entire life cycle of HVAC systems. The existing chiller models usually can not update in real-time, so they are not suitable for real-time interactions between DT models and real physical systems. In this paper, an intelligent DT framework is proposed for HVAC systems, which includes the equipment, data, simulation, and application layers. Broad learning system (BLS) is presented to build the simulation layer of the chiller and its DT platform. The basic BLS model is optimized to reach the best performance by choosing linear rectification function as activation function and setting batch size to 64 by enumeration method. The real HVAC system located in Zhejiang province is selected to verify the proposed method. For the first half year operation, the average mean absolute error, root mean square error and coefficient of determination (R2) of Multi-BLS model for nine chillers can reach 9.04, 15.20 and 0.98 respectively. For the second half year operation, the proposed method can be updated in 4.63s and its R2 is 0.95. Compared with conventional models, the proposed Multi-BLS model has better prediction precision and can be updated in real-time within a shorter time.}
}
@article{WANG2022104515,
title = {Object verification based on deep learning point feature comparison for scan-to-BIM},
journal = {Automation in Construction},
volume = {142},
pages = {104515},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104515},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003880},
author = {Boyu Wang and Qian Wang and Jack C.P. Cheng and Chao Yin},
keywords = {Building information model (BIM), LiDAR point clouds, As-built modeling, Object verification, Deep learning},
abstract = {Building information models (BIMs) have been widely adopted in current construction projects to enhance the efficiency of facility maintenance operations. As-built BIMs can reflect the actual conditions of facilities and thus as-built BIM reconstruction has shown great significance in digital twin generation, building health monitoring, facility management and urban renewal. Laser scanners are capable to capture dense 3D measurements of the environment in a fast and highly accurate way. Therefore, laser scanning data have been widely used for as-built BIM generation. Although research efforts have been made on how to automatically achieve “Scan-to-BIM”, there are still gaps from applying current solutions to real scenarios. One of the challenges is that some irrelevant point clusters may be wrongly recognized as the desired object in the detection stage. This study presents a novel object verification approach based on deep learning point feature comparison to improve the accuracy of automated BIM reconstruction process. Firstly, a KPConv-based deep neural network is developed and trained to perform 3D point feature computation. Then through comparing point features calculated for extracted point clusters and as-designed BIM generated point clouds, point feature distance maps are generated. Afterwards, to automatically analyze the generated feature distance maps, a dataset including simulated positive and negative instances is created based on ModelNet40. And a tiny neural network is established and trained on the prepared dataset to acquire ability of distinguishment. To validate the feasibility of the proposed technique, experiments were conducted on both artificial point clouds and real scan data collected in one MEP room in a water treatment work in Hong Kong. It is demonstrated that the proposed technique can successfully filter out all the false positives in the Scan-to-BIM process, improving reconstruction accuracy significantly.}
}
@article{QIU2022102895,
title = {Low-cost mobile mapping system solution for traffic sign segmentation using Azure Kinect},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102895},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102895},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222000978},
author = {Zhouyan Qiu and Joaquín Martínez-Sánchez and Víctor Manuel Brea and Paula López and Pedro Arias},
keywords = {Time of Flight camera, Traffic sign detection and segmentation, Multi sensor system, Mobile mapping system},
abstract = {The mobile mapping system (MMS) could become the foundation of digital twins and 3D modeling, and is widely applicable in a variety of fields, such as infrastructure management, intelligent transportation systems, and smart cities. However, data collected by MMS is extensive and complex, making data processing difficult. We present a novel method for segmenting urban assets (specifically in this case study traffic signs) with a lower-cost Azure Kinect and automatic data processing workflows. First, it was necessary to verify the reliability of this approach using the Time of Flight (ToF) camera from Azure Kinect to detect road signs outdoors. Using the data generated by the ToF camera, we then extracted the Region of Interest (ROI) quickly and efficiently. After transforming the ROI to the RGB image, we obtained the traffic sign area through a hybrid color-shape based method. In addition, we calculated the distance between the traffic sign and Azure Kinect based on the depth image. The Coefficient of Variation cv averaged 1.1%. It is thus evident that Azure Kinect is reliable for outdoor traffic sign segmentation. Our algorithm has been compared with deep learning algorithms. According to our analysis, our algorithm has an accuracy of 0.8216, while the accuracy of deep learning is 0.7466, which indicates that our solution is more flexible and cost-effective.}
}
@article{SHARMA2022100383,
title = {Digital Twins: State of the art theory and practice, challenges, and open research questions},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100383},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100383},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000516},
author = {Angira Sharma and Edward Kosasih and Jie Zhang and Alexandra Brintrup and Anisoara Calinescu},
keywords = {Digital Twin, Internet of Things, Autonomous systems, Big data, Machine learning},
abstract = {Digital Twin was introduced over a decade ago, as an innovative all-encompassing tool, with perceived benefits including real-time monitoring, simulation, optimisation and accurate forecasting. However, the theoretical framework and practical implementations of digital twin (DT) are yet to fully achieve this vision at scale. Although an increasing number of successful implementations exist in research and industrial works, sufficient implementation details are not publicly available, making it difficult to fully assess their components and effectiveness, to draw comparisons, identify successful solutions, share lessons, and thus to jointly advance and benefit from the DT methodology. This work first presents a review of relevant DT research and industrial works, focusing on the key DT features, current approaches in different domains, and successful DT implementations, to infer the key DT components and properties, and to identify current limitations and reasons behind the delay in the widespread implementation and adoption of digital twin. This work identifies that the major reasons for this delay are: the fact the DT is still a fast evolving concept; the lack of a universal DT reference framework, e.g. DT standards are scarce and still evolving; problem- and domain-dependence; security concerns over shared data; lack of DT performance metrics; and reliance of digital twin on other fast-evolving technologies. Advancements in machine learning, Internet of Things (IoT) and big data have led to significant improvements in DT features such as real-time monitoring and accurate forecasting. Despite this progress and individual company-based efforts, certain research and implementation gaps exist in the field, which have so far prevented the widespread adoption of the DT concept and technology; these gaps are also discussed in this work. Based on reviews of past work and the identified gaps, this work then defines a conceptualisation of DT which includes its components and properties; these also validate the uniqueness of DT as a concept, when compared to similar concepts such as simulation, autonomous systems and optimisation. Real-life case studies are used to showcase the application of the conceptualisation. This work discusses the state-of-the-art in DT, addresses relevant and timely DT questions, and identifies novel research questions, thus contributing to a better understanding of the DT paradigm and advancing the theory and practice of DT and its allied technologies.}
}
@article{CONE2022103441,
title = {Learning with unobserved regimes},
journal = {Journal of Macroeconomics},
volume = {73},
pages = {103441},
year = {2022},
issn = {0164-0704},
doi = {https://doi.org/10.1016/j.jmacro.2022.103441},
url = {https://www.sciencedirect.com/science/article/pii/S0164070422000398},
author = {Thomas E. Cone},
keywords = {Adaptive learning, Economic dynamics, Expectations, New Keynesian model, Misspecification, Regime switching},
abstract = {This paper introduces new economic dynamics induced by learning. Under monetary policy with regime switches, private sector agents learn about output and inflation, which are functions of their beliefs about those variables. The learners’ models are misspecified because they do not observe the regimes. The novel dynamics occur if the regime switches are slow relative to the learning speed: Learners’ beliefs do not converge to the single process that usually describes their asymptotic beliefs, but to a Markov switching process. Checking for this possibility is crucial: When it occurs, the standard analysis gives incorrect results about the equilibria to which beliefs and other endogenous variables will converge. In an extension in which the learners choose their learning speed optimally, the result is strengthened.}
}
@article{HU2022102371,
title = {A grasps-generation-and-selection convolutional neural network for a digital twin of intelligent robotic grasping},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {77},
pages = {102371},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102371},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000588},
author = {Weifei Hu and Chuxuan Wang and Feixiang Liu and Xiang Peng and Pengwen Sun and Jianrong Tan},
keywords = {Intelligent robotic grasping, Digital twin, Convolutional neural network, Deep learning, RGB-D image},
abstract = {Robotic grasping plays an essential role in human-machine cooperation in various household and industrial applications. Although humans can instinctively execute grasps in an accurate, stable, and rapid way even under a constantly changing environment, intelligent grasping remains a challenging task for robots. As a prerequisite for grasping, robots need to correctly identify the best grasping location of unknown objects often based on an artificial intelligence approach, which is still a challenging problem. This paper proposes a new grasps-generation-and-selection convolutional neural network (GGS-CNN), which is trained and implemented in a digital twin of intelligent robotic grasping (DTIRG). By defining a grasp with 3-D position, rotation angle, and gripper width, the GGS-CNN generates grasp candidates by transforming the red–green-blue-depth images (RGB-D images) into feature maps and evaluating the quality of selected grasps. The GGS-CNN is trained in the virtual environment and the real world of the DTIRG to detect accurate grasps. In the grasping tests, the proposed GGS-CNN achieves grasping success rates of 96.7% and 93.8% for grasping single objects and cluttered objects, respectively, and obtains the best grasp from the RGB-D image in less than 40 ms.}
}
@article{YANG2022104519,
title = {Automated semantic segmentation of bridge components from large-scale point clouds using a weighted superpoint graph},
journal = {Automation in Construction},
volume = {142},
pages = {104519},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104519},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003922},
author = {Xiaofei Yang and Enrique {del Rey Castillo} and Yang Zou and Liam Wotherspoon and Yi Tan},
keywords = {Bridge component recognition, Deep learning, Semantic segmentation, Large-scale point clouds, Weighted Superpoint Graph},
abstract = {Deep learning techniques have the potential to provide versatile solutions for automated semantic segmentation of bridge point clouds, but previous studies were limited to small-scale bridge point clouds and focused on limited bridge component categories due to training sample scarcity. Additionally, no prior work considered the intrinsic data imbalance problem in the bridge dataset, with the points unequally distributed between the various components. This paper presents a weighted superpoint graph (WSPG) method, where bridge point clouds were firstly clustered into hundreds of semantically homogeneous superpoints that were then classified into different bridge components using PointNet and Graph Neural Networks. The WSPG method can recognize components directly from large-scale bridge point clouds and alleviate the data imbalance by leveraging a novel loss function that assigns weights according to the number of points contained in different bridge components. The effectiveness of the method was validated on both a real-world dataset with 5 categories of bridge components and a synthetic dataset with 8 categories of bridge components. Experiment results on the real-world dataset showed that the WSPG model achieved the best performance on all overall evaluation metrics of overall accuracy (OA: 99.43%), mean class accuracy (mAcc: 98.75%) and mean Intersection over Union (mIoU: 96.49%) compared to the existing cutting edge models such as PointNet, DGCNN and the original SPG. Additionally, the WSPG method also surpassed the cutting edge representatives in terms of mAcc and mIoU on the synthetic dataset, especially increasing the original SPG by 8.5% mAcc and 6.7% mIoU. The successful application of the proposed method will significantly improve upper-level tasks such as digital twining for existing bridges.}
}
@article{LENG2022101676,
title = {Digital twins-based flexible operating of open architecture production line for individualized manufacturing},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101676},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101676},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001379},
author = {Jiewu Leng and Ziying Chen and Weinan Sha and Zisheng Lin and Jun Lin and Qiang Liu},
keywords = {Individualized manufacturing, Digital twin, Open architecture production line, Flexible operating},
abstract = {Individualized manufacturing implies high flexibility of both the hardware and software of the production lines based on a fast physical and logical system (de)commissioning. This paper proposes an open architecture production line (OAPL) design together with a digital twins-based flexible operating approach for individualized manufacturing. Firstly, an OAPL is designed and implemented with physical reconfigurability by orchestrating different open architectural platforms together with open architecture machine tools (OAMTs). Secondly, an open architectural style modeling and configuration method is presented to enable the software reconfigurability of the controls of the OAPL. Thirdly, a digital twin-based online process emulating and multi-physics simulation is integrated to aid the comprehensive characterizing of the operation status of the OAPL. Based on the system reconfigurability and digital twins system, a triple-layer Learning-Optimization-Reacting approach together with an ensemble algorithm for flexible operating of the OAPL is proposed. The digital twins are formed with the ability to flexibly operate the OAPL for catering to different individualized requirements. A demonstrative implementation of a stepping-motor assembly OAPL is presented finally.}
}
@article{LI202233,
title = {Intelligent Drilling and Completion: A Review},
journal = {Engineering},
volume = {18},
pages = {33-48},
year = {2022},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S2095809922006257},
author = {Gensheng Li and Xianzhi Song and Shouceng Tian and Zhaopeng Zhu},
keywords = {Intelligent drilling and completion, Artificial intelligence, Intelligent application scenarios, Literature review, Systematic discuss},
abstract = {The application of artificial intelligence (AI) has become inevitable in the petroleum industry. In drilling and completion engineering, AI is regarded as a transformative technology that can lower costs and significantly improve drilling efficiency (DE). In recent years, numerous studies have focused on intelligent algorithms and their application. Advanced technologies, such as digital twins and physics-guided neural networks, are expected to play roles in drilling and completion engineering. However, many challenges remain to be addressed, such as the automatic processing of multi-source and multi-scale data. Additionally, in intelligent drilling and completion, methods for the fusion of data-driven and physics-based models, few-sample learning, uncertainty modeling, and the interpretability and transferability of intelligent algorithms are research frontiers. Based on intelligent application scenarios, this study comprehensively reviews the research status of intelligent drilling and completion and discusses key research areas in the future. This study aims to enhance the berthing of AI techniques in drilling and completion engineering.}
}
@article{UDUGAMA202294,
title = {Digitalisation in chemical engineering: Industrial needs, academic best practice, and curriculum limitations},
journal = {Education for Chemical Engineers},
volume = {39},
pages = {94-107},
year = {2022},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1749772822000112},
author = {Isuru A. Udugama and Christoph Bayer and Saeid Baroutian and Krist V. Gernaey and Wei Yu and Brent R. Young},
keywords = {Digitalisation, Digital twins, Big data analytics, internet of things, Industry survey, Expert survey, Staff survey},
abstract = {The digitalisation megatrend is currently changing the way whole sectors of the economy are operated and in the manufacturing sector, digitalisation promises productivity improvements with seemingly marginal investments in “steel and concrete”. These sector wide shifts will inevitably influence how a chemical engineering graduate would perform their job functions, irrespective if they are working in an area such as petrochemical manufacturing or in the financial and management sector of the economy. This manuscript presents the results from three targeted surveys carried out to get a better understanding of (1) what alumni from a chemical and materials engineering degree think these changes will mean for them, (2) expert opinions on the level of detail key topics of digital twins, big data and Internet of Things (IOT) should be covered by an undergraduate chemical engineering curriculum and (3) what staff from a chemical and materials engineering degree think the best way to embed these concepts into an undergraduate level chemical engineering education. Analysing the results from the survey highlighted the following aspects. (1) An overwhelming number of alumni reported that elements of digitalisation are already influencing their industry and job function and these influences are likely to get stronger overtime (2) The experts identified that chemical engineering graduates will likely drive the development of digital twins hence requiring a high level of understanding in this subject. The graduates would only need to apply Big Data analytics and likely not be involved with the IOT developments. (3) The staff identified that these requirements identified by the experts can be satisfied by taking a two-pronged approach of adding modules to current core courses while developing standalone elective courses to cover the more advanced concepts.}
}
@article{LI2022124440,
title = {Integrated graph deep learning framework for flow field reconstruction and performance prediction of turbomachinery},
journal = {Energy},
volume = {254},
pages = {124440},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124440},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222013433},
author = {Jinxing Li and Tianyuan Liu and Yuqi Wang and Yonghui Xie},
keywords = {Deep learning, Graph neural network, Field reconstruction, Performance prediction, Arbitrary structured/unstructured grids, Digital twin},
abstract = {The performance and reliability of turbomachinery directly affect the efficiency and safety of energy conversion systems. A dual graph neural network (DGNN) for turbomachinery flow field reconstruction and performance prediction is proposed, which utilizes flow field data at grid vertices and characterizes the neighborhood relationships of grids through the adjacency matrix. Different from previous work, this work extends deep learning methods to the reconstruction of global turbomachinery fields defined on arbitrary structured/unstructured grids. The flow field reconstruction and performance prediction of low aspect ratio rotors are used to verify the generalizability of DGNN. The proposed method can not only accurately predict performance parameters, but also achieve excellent performance in flow field reconstruction. The superiority of DGNN over the artificial neural network (ANN) in flow field reconstruction is clarified. The optimal GNN operator with the highest accuracy and lowest computation costs is obtained as SAGE. A well-trained DGNN can give the flow field distribution as well as the performance of turbines within 0.05 s. The proposed approach could be a real-time simulation and analysis approach to assist turbomachinery design and optimization. It may provide an efficient solution for establishing the digital twin system of turbomachinery in the future.}
}
@article{KIM2022109471,
title = {Digital twin approach for on-load tap changers using data-driven dynamic model updating and optimization-based operating condition estimation},
journal = {Mechanical Systems and Signal Processing},
volume = {181},
pages = {109471},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109471},
url = {https://www.sciencedirect.com/science/article/pii/S088832702200588X},
author = {Wongon Kim and Sunuwe Kim and Jingyo Jeong and Hyunjae Kim and Hyeonchan Lee and Byeng D. Youn},
keywords = {On-load tap changer (OLTC), Digital Twin, Prognostics and Health Management (PHM), Vibration Signal},
abstract = {On-load tap changers (OLTCs), which are found in power transformers, are mechanically operating components. The vibration signal of an OLTC can provide effective observed data for estimation of the mechanical state transition and a faulty operating condition. Data-driven methods (e.g., deep learning, machine learning) and physics-based methods (e.g., the finite element method, the lumped parameter model) for health-state estimation require sufficient prior knowledge – such as various observed data about fault states, modeling information (including geometry, material properties), and operating conditions – to build a valid digital twin approach. However, prior knowledge for various OLTCs and transformer models is hard to obtain. To begin to address the shortcomings of existing methods, this study proposes a digital twin approach for OLTCs using 1) pre-processing of the vibration signal, 2) data-driven dynamic model updating, and 3) optimization-based operating condition estimation. First, the time–frequency domain features are extracted from the reference signal using a minimum entropy deconvolution (MED) filter, to extract the impulsive vibration signal from OLTC operation. The initial operating conditions that arise from tap changing and diverter switching are assumed as impulsive force using extracted features. The dynamic model is driven by the numerical algorithm for subspace state-space system identification (N4SID), the reference signal, and the excitation impulse force. Next, the phase and magnitude modulation are updated to estimate uncertain operating condition and refine the dynamic model using optimization-based parameter tuning. Analysis results from the proposed digital twin approach are demonstrated for both a numerical and experimental example to verify the effectiveness of the proposed approach. In the numerical case study, i) the simplified physics-based modelling and ii) the joint-input state estimation method were compared with the proposed method. In the experimental case study, the proposed method was applied to an OLTC vibration signal of both inactive and active power transformers. The inflection points in the Dynamic Resistance Measurement (DRM) graph show synchronization with the experimental results of the estimated excitation forces derived using the proposed method.}
}
@article{BERGER2022113198,
title = {Multi-sensor spectral synergies for crop stress detection and monitoring in the optical domain: A review},
journal = {Remote Sensing of Environment},
volume = {280},
pages = {113198},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.113198},
url = {https://www.sciencedirect.com/science/article/pii/S003442572200308X},
author = {Katja Berger and Miriam Machwitz and Marlena Kycko and Shawn C. Kefauver and Shari {Van Wittenberghe} and Max Gerhards and Jochem Verrelst and Clement Atzberger and Christiaan {van der Tol} and Alexander Damm and Uwe Rascher and Ittai Herrmann and Veronica Sobejano Paz and Sven Fahrner and Roland Pieruschka and Egor Prikaziuk and Ma. Luisa Buchaillot and Andrej Halabuk and Marco Celesti and Gerbrand Koren and Esra Tunc Gormus and Micol Rossini and Michael Foerster and Bastian Siegmann and Asmaa Abdelbaki and Giulia Tagliabue and Tobias Hank and Roshanak Darvishzadeh and Helge Aasen and Monica Garcia and Isabel Pôças and Subhajit Bandopadhyay and Mauro Sulis and Enrico Tomelleri and Offer Rozenstein and Lachezar Filchev and Gheorghe Stancile and Martin Schlerf},
keywords = {Precision agriculture multi-modal solar-induced fluorescence satellite hyperspectral multispectral biotic and abiotic stress},
abstract = {Remote detection and monitoring of the vegetation responses to stress became relevant for sustainable agriculture. Ongoing developments in optical remote sensing technologies have provided tools to increase our understanding of stress-related physiological processes. Therefore, this study aimed to provide an overview of the main spectral technologies and retrieval approaches for detecting crop stress in agriculture. Firstly, we present integrated views on: i) biotic and abiotic stress factors, the phases of stress, and respective plant responses, and ii) the affected traits, appropriate spectral domains and corresponding methods for measuring traits remotely. Secondly, representative results of a systematic literature analysis are highlighted, identifying the current status and possible future trends in stress detection and monitoring. Distinct plant responses occurring under short-term, medium-term or severe chronic stress exposure can be captured with remote sensing due to specific light interaction processes, such as absorption and scattering manifested in the reflected radiance, i.e. visible (VIS), near infrared (NIR), shortwave infrared, and emitted radiance, i.e. solar-induced fluorescence and thermal infrared (TIR). From the analysis of 96 research papers, the following trends can be observed: increasing usage of satellite and unmanned aerial vehicle data in parallel with a shift in methods from simpler parametric approaches towards more advanced physically-based and hybrid models. Most study designs were largely driven by sensor availability and practical economic reasons, leading to the common usage of VIS-NIR-TIR sensor combinations. The majority of reviewed studies compared stress proxies calculated from single-source sensor domains rather than using data in a synergistic way. We identified new ways forward as guidance for improved synergistic usage of spectral domains for stress detection: (1) combined acquisition of data from multiple sensors for analysing multiple stress responses simultaneously (holistic view); (2) simultaneous retrieval of plant traits combining multi-domain radiative transfer models and machine learning methods; (3) assimilation of estimated plant traits from distinct spectral domains into integrated crop growth models. As a future outlook, we recommend combining multiple remote sensing data streams into crop model assimilation schemes to build up Digital Twins of agroecosystems, which may provide the most efficient way to detect the diversity of environmental and biotic stresses and thus enable respective management decisions.}
}
@article{BADUGE2022104440,
title = {Artificial intelligence and smart vision for building and construction 4.0: Machine and deep learning methods and applications},
journal = {Automation in Construction},
volume = {141},
pages = {104440},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104440},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003132},
author = {Shanaka Kristombu Baduge and Sadeep Thilakarathna and Jude Shalitha Perera and Mehrdad Arashpour and Pejman Sharafi and Bertrand Teodosio and Ankit Shringi and Priyan Mendis},
keywords = {Artificial intelligence, Machine learning, Deep learning, Automation, Internet of things, Building information modelling, Smart vision, Convolution neural network, Generative adversarial network, Artificial neural network},
abstract = {This article presents a state-of-the-art review of the applications of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) in building and construction industry 4.0 in the facets of architectural design and visualization; material design and optimization; structural design and analysis; offsite manufacturing and automation; construction management, progress monitoring, and safety; smart operation, building management and health monitoring; and durability, life cycle analysis, and circular economy. This paper presents a unique perspective on applications of AI/DL/ML in these domains for the complete building lifecycle, from conceptual stage, design stage, construction stage, operational and maintenance stage until the end of life. Furthermore, data collection strategies using smart vision and sensors, data cleaning methods (post-processing), data storage for developing these models are discussed, and the challenges in model development and strategies to overcome these challenges are elaborated. Future trends in these domains and possible research avenues are also presented.}
}
@article{KHAWANDAZOULAI2022934,
title = {The Patient Story in Palliative Care: A Thematic Analysis of Medical Students’ Reflections on a Narrative Medicine Assignment (S551)},
journal = {Journal of Pain and Symptom Management},
volume = {63},
number = {5},
pages = {934},
year = {2022},
issn = {0885-3924},
doi = {https://doi.org/10.1016/j.jpainsymman.2022.02.174},
url = {https://www.sciencedirect.com/science/article/pii/S0885392422002597},
author = {Mariana Khawand-Azoulai and Amanda Alladin and Kyra Lipman and Sarah Bland and Corinne Ferrari and Maria {van Zuilen}},
abstract = {Outcomes
1. Identify the benefits of a “patient story” exercise with a reflection component for medical students on a palliative care rotation 2. Describe the impact of a patient story exercise by reviewing common themes that were identified among medical students’ reflections 3. Discuss feasibility of integration of a patient story assignment with reflection as a routine part of palliative medicine rotations for learners of all stages
Original Research Background
Communication with seriously ill patients is a skill that requires practice; the traditional medical student model of learning to interview patients overlooks in-depth exploration of patients’ social background and personal history. This information is vital to determining goals of care and facilitates an authentic care partnership by building empathy and trust. We implemented a narrative medicine activity for students during their hospice and palliative medicine (HPM) rotation with the intent of highlighting the impact of humanism in the care of patients.
Research Objectives
Todetermine the impact of a narrative medicine exercise by a thematic analysis of students’ reflections on a “patient story” assignment.
Methods
Studentsreceived a 5-minute introduction to the “patient story” assignment. They invited patients to share their story and elicited “what matters most.” Students transcribed and shared this story with the HPM team and penned a personal reflection. We analyzed 100 reflections and used an inductive and iterative approach to thematic analysis that started with the development of codes from which broader themes were constructed.
Results
Four main themes were analyzed as being predominant with associated subthemes constructed and shown in parentheses: getting to know the patient (background, values, family, seeing the patient as more than a disease, resilience). student reaction to the experience (positive, negative, challenging), building blocks of the patient-physician relationship (empathy, trust, connection, listening), and student personal insights (personal reflection, lasting impact)
Conclusion
This activity demonstrated to students the value of uncovering patients’ backgrounds and values, reinforcing the multidomain approach of palliative medicine. The majority of students felt this was a positive, impactful exercise that would change their approach to patients in future practice.
Implications for Research, Policy, or Practice
A patient storyassignment is a feasible, well-received activity that creates an opportunity for students to reflect on the physician-patient relationship beyond the medical domain and enriches the care of those with serious illness.}
}
@article{WANG2022104464,
title = {Construction and maintenance of urban underground infrastructure with digital technologies},
journal = {Automation in Construction},
volume = {141},
pages = {104464},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104464},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003375},
author = {Mingzhu Wang and Xianfei Yin},
keywords = {Underground infrastructure, Literature review, Digital technologies, Inspection and maintenance, Condition assessment, Underground construction, Infrastructure operation & maintenance},
abstract = {Urban underground infrastructure is a critical component in cities to provide essential services to residents. Research efforts have been made to facilitate different activities of underground infrastructure projects using various methods, particularly digital technologies. To obtain deeper insights from existing research and provide directions for future research, this study conducts a comprehensive review of research on underground infrastructure construction and Operation & Maintenance (O&M) with a focus on digital technologies. The in-depth review was conducted based on 145 publications from the perspective of locating and mapping, construction and coordination, as well as O&M. Consequently, critical limitations and challenges are revealed, such as the lack of as-built and as-is information, the requirement of data quality and quantity for deep learning methods, the lack of fully automated robotic systems, etc. Afterwards, a status matrix was presented to identify the level of different digital technologies being studied and their future application potential for key activities of underground infrastructure projects. In the end, future research trends are proposed, including (1) digital twinning of underground infrastructure, (2) quality and uncertainty of inspection data, (3) data generation and semi-supervised learning, (4) predictive maintenance, and (5) fully automated robotic systems for inspection and maintenance. This study contributes to the body of knowledge by identifying the challenges and limitations of existing studies through a systematic review, providing a clear view of the achievements and potentials of digital technologies for underground infrastructure, and proposing future research directions to facilitate digital transformation in this area.}
}
@article{WANG2022108033,
title = {Digital twins supported equipment maintenance model in intelligent water conservancy},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {108033},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108033},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622002968},
author = {Zhoukai Wang and Weina Jia and Kening Wang and Yichuan Wang and Qiaozhi Hua},
keywords = {Digital twins, Transfer learning, Fault diagnose, Equipment maintenance, Intelligent water conservancy},
abstract = {With the rapid growth of China’s economy, applied research on hydropower engineering has received an increasing amount of attention. However, since hydraulic electromechanical devices often work in actual industrial manufacturing environments at high loads for a long time, their health status is hardly predicted. By introducing digital twins technique, this paper proposed a predictive maintenance model for electromechanical devices to solve the problems. Firstly, multiple sensors are implemented on critical parts of the hydraulic electromechanical devices to collect devices’ physical and spatial signals. Secondly, constructing the digital twins model of electromechanical devices with the sensing data and the devices’ structural characteristics. Finally, by transfer learning, a comprehensive and reliable fault diagnosis method is designed to predict the remaining life of the devices and make decisions for facilities maintenance. Experiments show that the proposed model performs the best accuracy rate compared with the other methods.}
}
@article{GUARINO2022118302,
title = {Adaptive talent journey: Optimization of talents’ growth path within a company via Deep Q-Learning},
journal = {Expert Systems with Applications},
volume = {209},
pages = {118302},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118302},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422014348},
author = {Alfonso Guarino and Delfina Malandrino and Francesco Marzullo and Antonio Torre and Rocco Zaccagnino},
keywords = {Talent journey, Deep Q-Learning, Digital Twin, User evaluation},
abstract = {In enterprise context, companies constantly aim to optimize their human resources and acquire new ones. Employees, also called talents, are required to achieve new skills for the company to stay competitive in the business. The talents’ ability to productively improve is a crucial factor for the success of a company. We propose Adaptive Talent Journey, a novel method for optimizing the growth path of talents within a company. The ultimate goal of Adaptive Talent Journey is to hold talent back inside the company. It exploits the notion of “digital twin” to define a digital representation of the talent, namely Talent Digital Twin, built on the basis of skills level and personal traits. Given a target company’s role, Adaptive Talent Journey proposes the most suitable path of work experiences (journey) to improve the skills of a talent so to achieve the target role requirements. Such a mechanism resonates with the Reinforcement Learning paradigm, and specifically with Deep Q-Learning. Specifically, the proposed method exploits: (i) two double Deep Q-Networks (DDQNs) for selecting the work experiences to be made; (ii) a transition module to support the DDQNs training and ensure good performance despite the limited availability of data. We implemented and deployed Adaptive Talent Journey in an intuitive Web application, namely ATJWeb. We evaluated both the effectiveness and efficiency of our proposal and the users’ satisfaction in using it, adopting, as a testbed, an IT company with its employees. Results proved that the Adaptive Talent Journey can optimize the growth path of talents, and that ATJWeb is pleasant and useful.}
}
@article{LEE2022101710,
title = {Digital twin-driven deep reinforcement learning for adaptive task allocation in robotic construction},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101710},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101710},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001689},
author = {Dongmin Lee and SangHyun Lee and Neda Masoud and M.S. Krishnan and Victor C. Li},
keywords = {Digital Twin, Proximal Policy Optimization (PPO), Deep Reinforcement Learning (DRL), Autonomous Robot, Adaptive Task Allocation},
abstract = {In order to accomplish diverse tasks successfully in a dynamic (i.e., changing over time) construction environment, robots should be able to prioritize assigned tasks to optimize their performance in a given state. Recently, a deep reinforcement learning (DRL) approach has shown potential for addressing such adaptive task allocation. It remains unanswered, however, whether or not DRL can address adaptive task allocation problems in dynamic robotic construction environments. In this paper, we developed and tested a digital twin-driven DRL learning method to explore the potential of DRL for adaptive task allocation in robotic construction environments. Specifically, the digital twin synthesizes sensory data from physical assets and is used to simulate a variety of dynamic robotic construction site conditions within which a DRL agent can interact. As a result, the agent can learn an adaptive task allocation strategy that increases project performance. We tested this method with a case project in which a virtual robotic construction project (i.e., interlocking concrete bricks are delivered and assembled by robots) was digitally twinned for DRL training and testing. Results indicated that the DRL model’s task allocation approach reduced construction time by 36% in three dynamic testing environments when compared to a rule-based imperative model. The proposed DRL learning method promises to be an effective tool for adaptive task allocation in dynamic robotic construction environments. Such an adaptive task allocation method can help construction robots cope with uncertainties and can ultimately improve construction project performance by efficiently prioritizing assigned tasks.}
}
@article{AREFI2022101389,
title = {Intelligent potato frying: Time to say goodbye to the “good old” processing strategies},
journal = {Thermal Science and Engineering Progress},
volume = {34},
pages = {101389},
year = {2022},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2022.101389},
url = {https://www.sciencedirect.com/science/article/pii/S2451904922001950},
author = {Arman Arefi and Oliver Hensel and Barbara Sturm},
keywords = {Acrylamide, Artificial Intelligence, Deep-fat frying, Digital Twin, Hyperspectral imaging, Machine learning, Intelligent processes, Optimal frying conditions, Smart frying},
abstract = {Potato chips production is a traditional food process. To achieve uniform product quality, raw materials are usually rigorously sorted. Traditionally, the process is conducted in a single stage approach leading to high quality losses. Recently, dynamically optimized frying processes have been found to result in higher product quality. Consequently, industrial continuous deep-fat fryers convey potato disks through several zones pre-set at different temperatures. However, these improved systems still do not take the variabilities in frying kinetics among potatoes into consideration. To address this issue and decrease uncertainties in end-product quality, frying conditions of each zone must be optimized, physiochemical properties of the various raw tubers and their frying kinetics taking into account. This paper, therefore, presents a novel approach for an intelligent frying process with embedded computer vision systems providing continuous monitoring of product quality and, therefore, facilitate dynamic control of frying conditions in order to meet desired quality attributes in the final product. An extensive literature review of the key physiochemical attributes of raw potato tubers is presented, followed by an introduction to novel pre-treatment technologies, and the importance of optimal frying conditions. An overview of the potentials for using computer vision systems for the assessment of said quality criteria is given, followed by a detailed description of the envisioned frying process. The paper concludes that the realization of intelligent frying processes necessitates the development of fully fledged digital twins of the process and the products, combining physics based and data driven modelling with real time sensing and control. Terminology: Chips refer to thin slices of potato while French fries refers to wedges/stripes.}
}
@article{KIM2022168,
title = {Cross-modal distillation with audio–text fusion for fine-grained emotion classification using BERT and Wav2vec 2.0},
journal = {Neurocomputing},
volume = {506},
pages = {168-183},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222008931},
author = {Donghwa Kim and Pilsung Kang},
keywords = {Multi-class emotion classification, Knowledge distillation, Transformer, BERT, Wav2Vec 2.0, Contrastive learning},
abstract = {Fine-grained emotion classification for mood- and emotion-related physical-characteristics detection and its application to computer technology using biometric sensors has been extensively researched in the field of affective computing. Although text modality has achieved a considerably high performance from the perspective of sentiment analysis, which simply classifies a positive or negative label, fine-grained emotion classification requires additional information besides text. An audio feature can be adopted as the additional information as it is closely associated with text, and the characteristics of the changes in sound pulses can be employed in fine-grained emotion classification. However, the multimodal datasets related to fine-grained emotion are limited, and the scalability and efficiency are insufficient for multimodal training to be applied extensively via the self-supervised learning (Self-SL) approach, which can adequately represent modality. To address these limitations, we propose cross-modal distillation (CMD), which induces the feature spaces of student models with a few parameters while receiving those of the teacher models that can adequately express each modality based on Self-SL. The proposed CMD performs the mapping of a feature space between teacher-student models based on contrastive learning, while two attention mechanisms—cross-attention between audio and text features and self-attention for features in modality—are performed during knowledge distillation. Wav2vec 2.0 and BERT, which are already adequately trained for audio and text via Self-SL, were adopted as teacher models; audio–text transformer models were used as student models. Accordingly, the CMD-based representation learning applies a lightweight model for IEMOCAP, MELD, and CMU–MOSEI datasets with the task of multi-class emotion classification, while exhibiting better fine-grained emotion classification performance than benchmark models with a considerably low uncertainty for prediction.}
}
@article{BONGKIM20221,
title = {A digital twin implementation architecture for wire + arc additive manufacturing based on ISO 23247},
journal = {Manufacturing Letters},
volume = {34},
pages = {1-5},
year = {2022},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2022.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S2213846322001808},
author = {Duck {Bong Kim} and Guodong Shao and Guejong Jo},
keywords = {Digital Twin, Wire + Arc Additive Manufacturing, ISO 23247, Data Analytics},
abstract = {Digital twin (DT) is an enabling technology characterized by integrating cyber and physical spaces. It is well-fitted to additive manufacturing since it can benefit from digitalized assets and data analytics for the process control. Wire + arc additive manufacturing (WAAM) is being increasingly recognized due to its fabrication of large-scale parts. This paper proposes a generalized DT implementation architecture for WAAM based on ISO 23247 to address integration and interoperability issues. It will enable manufacturers to leverage DT for the real-time decision-making and control. An application scenario of machine learning-based anomaly detection for WAAM is used to explain the architecture.}
}
@article{XIAO2022,
title = {AFSTGCN: Prediction for multivariate time series using an adaptive fused spatial-temporal graph convolutional network},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001419},
author = {Yuteng Xiao and Kaijian Xia and Hongsheng Yin and Yu-Dong Zhang and Zhenjiang Qian and Zhaoyang Liu and Yuehan Liang and Xiaodan Li},
keywords = {Adaptive adjacency matrix, Digital twin, Graph convolutional network, Multivariate time series prediction, Spatial-temporal graph},
abstract = {The prediction for Multivariate Time Series (MTS) explores the interrelationships among variables at historical moments, extracts their relevant characteristics, and is widely used in finance, weather, complex industries and other fields. Furthermore, it is important to construct a digital twin system. However, existing methods do not take full advantage of the potential properties of variables, which results in poor predicted accuracy. In this paper, we propose the Adaptive Fused Spatial-Temporal Graph Convolutional Network (AFSTGCN). First, to address the problem of the unknown spatial-temporal structure, we construct the Adaptive Fused Spatial-Temporal Graph (AFSTG) layer. Specifically, we fuse the spatial-temporal graph based on the interrelationship of spatial graphs. Simultaneously, we construct the adaptive adjacency matrix of the spatial-temporal graph using node embedding methods. Subsequently, to overcome the insufficient extraction of disordered correlation features, we construct the Adaptive Fused Spatial-Temporal Graph Convolutional (AFSTGC) module. The module forces the reordering of disordered temporal, spatial and spatial-temporal dependencies into rule-like data. AFSTGCN dynamically and synchronously acquires potential temporal, spatial and spatial-temporal correlations, thereby fully extracting rich hierarchical feature information to enhance the predicted accuracy. Experiments on different types of MTS datasets demonstrate that the model achieves state-of-the-art single-step and multi-step performance compared with eight other deep learning models.}
}
@article{HUO2022102508,
title = {Automatic Grading Assessments for Knee MRI Cartilage Defects via Self-ensembling Semi-supervised Learning with Dual-Consistency},
journal = {Medical Image Analysis},
volume = {80},
pages = {102508},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102508},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522001554},
author = {Jiayu Huo and Xi Ouyang and Liping Si and Kai Xuan and Sheng Wang and Weiwu Yao and Ying Liu and Jia Xu and Dahong Qian and Zhong Xue and Qian Wang and Dinggang Shen and Lichi Zhang},
keywords = {Knee cartilage defect, Semi-supervised learning, Dual consistency, Attention mechanism},
abstract = {Knee cartilage defects caused by osteoarthritis are major musculoskeletal disorders, leading to joint necrosis or even disability if not intervened at early stage. Deep learning has demonstrated its effectiveness in computer-aided diagnosis, but it is time-consuming to prepare a large set of well-annotated data by experienced radiologists for model training. In this paper, we propose a semi-supervised framework to effectively use unlabeled data for better evaluation of knee cartilage defect grading. Our framework is developed based on the widely-used mean-teacher classification model, by designing a novel dual-consistency strategy to boost the consistency between the teacher and student models. The main contributions are three-fold: (1) We define an attention loss function to make the network focus on the cartilage regions, which can both achieve accurate attention masks and boost classification performance simultaneously; (2) Besides enforcing the consistency of classification results, we further design a novel attention consistency mechanism to ensure the focusing of the student and teacher networks on the same defect regions; (3) We introduce an aggregation approach to ensemble the slice-level classification outcomes for deriving the final subject-level diagnosis. Experimental results show that our proposed method can significantly improve both classification and localization performances of knee cartilage defects. Our code is available on https://github.com/King-HAW/DC-MT.}
}
@article{YANG2022,
title = {A digital twins enabled underwater intelligent internet vehicle path planning system via reinforcement learning and edge computing},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000967},
author = {Jiachen Yang and Meng Xi and Jiabao Wen and Yang Li and Houbing Herbert Song},
keywords = {Digital twins, Reinforcement learning, Edge computing, Underwater intelligent internet vehicle, Path planning},
abstract = {The Autonomous Underwater Glider (AUG) is a kind of prevailing underwater intelligent internet vehicle and occupies a dominant position in industrial applications, in which path planning is an essential problem. Due to the complexity and variability of the ocean, accurate environment modeling and flexible path planning algorithms are pivotal challenges. The traditional models mainly utilize mathematical functions, which are not complete and reliable. Most existing path planning algorithms depend on the environment and lack flexibility. To overcome these challenges, we propose a path planning system for underwater intelligent internet vehicles. It applies digital twins and sensor data to map the real ocean environment to a virtual digital space, which provides a comprehensive and reliable environment for path simulation. We design a value-based reinforcement learning path planning algorithm and explore the optimal network structure parameters. The path simulation is controlled by a closed-loop model integrated into the terminal vehicle through edge computing. The integration of state input enriches the learning of neural networks and helps to improve generalization and flexibility. The task-related reward function promotes the rapid convergence of the training. The experimental results prove that our reinforcement learning based path planning algorithm has great flexibility and can effectively adapt to a variety of different ocean conditions.}
}
@article{DEON2022109578,
title = {Digital twin and machine learning for decision support in thermal power plant with combustion engines},
journal = {Knowledge-Based Systems},
volume = {253},
pages = {109578},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109578},
url = {https://www.sciencedirect.com/science/article/pii/S095070512200795X},
author = {B. Deon and K.P. Cotta and R.F.V. Silva and C.B. Batista and G.T. Justino and G.C. Freitas and A.M. Cordeiro and A.S. Barbosa and F.L. Loução and T. Simioni and A.M. Morais and I.E.A. Medeiros and R.J.S. Almeida and C.A.A. {Araújo Jr.} and C. Soares and N. Padoin},
keywords = {Digital twin, Machine learning, Predictive maintenance, Thermal power plant, Decision support},
abstract = {The reliability and performance of the generating machines in a thermal power plant are crucial to ensure agility and assertiveness in decision-making, maximize economic results, and ensure meeting the electricity sector demands. In this work, a decision support system (DSM) was developed to predict trends and operational deviations in thermal power plants with combustion engines in an automated and reliable way. It is based on digital twin models for thermoelectric generation engines and their subsystems associated with models of machine learning for predictive maintenance, allowing the classification of failures in the generating units of the plant. The models represent the mechanical, thermal, and electrical conditions and parameters of each piece of equipment under normal operating conditions, and the tool generates alerts when deviations from the base model occur. The benefits from event forecasting range from a reduction in operational issues to the company’s strategic objectives due to the reduction in corrective maintenance downtimes, resulting in reduced operation and maintenance costs. Considering the real-time execution character of the models, it is essential for the tool to meet the operation’s decision-making needs; so an on-premises application is necessary. The proposed architecture can be applied to any industrial sector that uses SCADA supervisors and can be adapted, expanded, and evolved to other generation technologies, such as thermal plants that use different fuels and small hydroelectric, wind, and thermonuclear plants. The techniques used in conjunction with the developed architecture can be replicated in other systems and energy sectors, such as distribution and transmission, and can also be applied to industry in general: chemical, petrochemical, oil and gas, and others.}
}
@article{KHANMOHAMMADI2022499,
title = {A framework of data modeling and artificial intelligence for environmental-friendly energy system: Application of Kalina cycle improved with fuel cell and thermoelectric module},
journal = {Process Safety and Environmental Protection},
volume = {164},
pages = {499-516},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0957582022005602},
author = {Shoaib Khanmohammadi and Farayi Musharavati and Rasikh Tariq},
keywords = {Artificial neural network, Computational intelligence, Scenario-based optimization, Efficient energy systems, Sensitivity analysis},
abstract = {Geothermal energy-driven systems with integrated waste heat recovery units such as the use of fuel cells and thermoelectric module can help to improve the renewable energy contribution in the energy mix. Data-driven optimization can improve their economic and environmental performance and their macro-projection can help in the achievement of net-zero plans. This article extends the use of a framework containing the usage of data modeling and artificial intelligence to conduct different optimization scenarios of the geothermal-driven energy system. It includes the improvement of the economic, exergetic, energetic, and environmental performance through the development of various optimization scenarios. This is done through the development of an extensive thermodynamic model and validation based upon energy, exergy, economic, and environmental evaluations. Different machine learning techniques are adapted for digital twinning of the six performance indicators as a function of nine design variables including operational, source, and economic variables. It is shown that the artificial neural network offers the best statistical fit as compared to the other machine learning techniques including RMSE: 0.1768, R2:0.9999, MSE:0.0312, and MAE:0.1107 for the total work output. Energy-efficient design has yielded a total work output of 1044.86 kW, with a first law efficiency of 0.3322. The economic design offers the lowest cost of electricity at only 34.004 $/hr. The sensitivity analysis has shown that the following parameters are the most sensitivity: turbine inlet temperature (18.19%) and pressure (18.23%), geothermal inlet temperature (16.34%) and pressure (18.00%), and the ammonia water concentration at the inlet of separator (15.96%).}
}
@article{AHMED2022292,
title = {Integrating digital twins and deep learning for medical image analysis in the era of COVID-19},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {4},
pages = {292-305},
year = {2022},
note = {Virtual-reality and intelligent hardware in digital twins A)},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000183},
author = {Imran Ahmed and Misbah Ahmad and Gwanggil Jeon},
keywords = {Digital twins, Deep learning, Healthcare, COVID-19, Chest X-rays, Artificial intelligence},
abstract = {Background
Digital twins are virtual representations of devices and processes that capture the physical properties of the environment and operational algorithms/techniques in the context of medical devices and technologies. Digital twins may allow healthcare organizations to determine methods of improving medical processes, enhancing patient experience, lowering operating expenses, and extending the value of care. During the present COVID-19 pandemic, various medical devices, such as X-rays and CT scan machines and processes, are constantly being used to collect and analyze medical images. When collecting and processing an extensive volume of data in the form of images, machines and processes sometimes suffer from system failures, creating critical issues for hospitals and patients.
Methods
To address this, we introduce a digital-twin-based smart healthcare system integrated with medical devices to collect information regarding the current health condition, configuration, and maintenance history of the device/machine/system. Furthermore, medical images, that is, X-rays, are analyzed by using a deep-learning model to detect the infection of COVID-19. The designed system is based on the cascade recurrent convolution neural network (RCNN) architecture. In this architecture, the detector stages are deeper and more sequentially selective against small and close false positives. This architecture is a multi-stage extension of the RCNN model and sequentially trained using the output of one stage for training the other. At each stage, the bounding boxes are adjusted to locate a suitable value of the nearest false positives during the training of the different stages. In this manner, the arrangement of detectors is adjusted to increase the intersection over union, overcoming the problem of overfitting. We train the model by using X-ray images as the model was previously trained on another dataset.
Results
The developed system achieves good accuracy during the detection phase of COVID-19. The experimental outcomes reveal the efficiency of the detection architecture, which yields a mean average precision rate of 0.94.}
}
@article{HAO2022102366,
title = {A self-training teacher-student model with an automatic label grader for abdominal skeletal muscle segmentation},
journal = {Artificial Intelligence in Medicine},
volume = {132},
pages = {102366},
year = {2022},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2022.102366},
url = {https://www.sciencedirect.com/science/article/pii/S0933365722001282},
author = {Degan Hao and Maaz Ahsan and Tariq Salim and Andres Duarte-Rojo and Dadashzadeh Esmaeel and Yudong Zhang and Dooman Arefan and Shandong Wu},
keywords = {Image segmentation, Semi-supervised learning, Self-attention, Teacher-student model, Skeletal muscle},
abstract = {Deep learning on a limited number of labels/annotations is a challenging task for medical imaging analysis. In this paper, we propose a novel self-training segmentation pipeline (Self-Seg in short) for segmenting skeletal muscle in CT images. Self-Seg starts with a small set of annotated images and then iteratively learns from unlabeled datasets to gradually improve the segmentation performance. Self-Seg follows a semi-supervised teacher-student learning scheme and there are two contributions: 1) we construct a self-attention UNet to improve segmentation over the classical UNet model, and 2) we implement an automatic label grader to implicitly incorporate medical knowledge for quality assurance of pseudo labels, from which good quality pseudo labels are identified to enhance learning of the segmentation model. We perform extensive experiments on three CT image datasets and show promising results on five evaluation settings, and we also compared our method to several baseline and related methods and achieved superior performance.}
}
@article{COLLINS2022100518,
title = {Review: Smart agri-systems for the pig industry},
journal = {animal},
volume = {16},
pages = {100518},
year = {2022},
note = {Manipulating Pig Production XVIII: Proceedings of the Eighteenth Biennial Conference of the Australasian Pig Science Association (APSA), 15-18 November 2021, Brisbane, Australia},
issn = {1751-7311},
doi = {https://doi.org/10.1016/j.animal.2022.100518},
url = {https://www.sciencedirect.com/science/article/pii/S1751731122000696},
author = {L.M. Collins and L.M. Smith},
keywords = {Digital farming, Pork, Precision Livestock Farming, Production, Systems approach},
abstract = {The projected rise in the global human population and the anticipated increase in demand for meat and animal products, albeit with a greatly reduced environmental footprint, offers a difficult set of challenges to the livestock sector. Primarily, how do we produce more, but in a way that is healthier for the animals, public, and the environment? Implementing a smart agri-systems approach, utilising multiplatform precision technologies, internet of things, data analytics, machine learning, digital twinning and other emerging technologies can support a more informed decision-making and forecasting position that will allow us to move towards greater sustainability in future. If we look to precision agronomy, there are a wide range of technologies available and examples of how digitalisation and integration of platform outputs can lead to advances in understanding the agricultural system and forecasting upcoming events and performance that have hitherto been impossible to achieve. There is much for the livestock sector and animal scientists to learn from the developments of precision technologies and smart agri-system approaches in the arable and horticultural contexts. However, there are several barriers the livestock sector must overcome: (i) the development and implementation of precision livestock farming technologies that can be easily integrated and analysed without the support of a dedicated data analyst in house; (ii) the lack of extensive validation of many developed and available precision livestock farming technologies means that reliability and accuracy are likely to be compromised when applied in commercial practice; (iii) the best smart agri-systems approaches are reliant on large quantities of data from across a wide variety of conditions, but at present the complications of data sharing, commercial sensitivities, data ownership, and permissions make it challenging to obtain or knit together data from different parts of the system into a comprehensive picture; and (iv) the high level of investment needed to develop and scale these technologies is substantial and represents significant risk for companies when a technology is emerging. Using a case study of the National Pig Centre (a flagship pig research facility in the UK) we discuss how a smart agri-systems approach can be applied in practice to investigate alternative future systems for production, and enable monitoring of these systems as a commercial demonstrator site for future pork production.}
}
@article{CATSOULIS2022104221,
title = {Integrating supervised learning and applied computational multi-fluid dynamics},
journal = {International Journal of Multiphase Flow},
volume = {157},
pages = {104221},
year = {2022},
issn = {0301-9322},
doi = {https://doi.org/10.1016/j.ijmultiphaseflow.2022.104221},
url = {https://www.sciencedirect.com/science/article/pii/S0301932222001987},
author = {Sotiris Catsoulis and Joel-Steven Singh and Chidambaram Narayanan and Djamel Lakehal},
keywords = {Computational Fluid Dynamics, Multiphase flow, Machine learning, Data-driven model, Simulation-based Digital Twin},
abstract = {The transition from iterative methods of engineering design towards physics-based modeling has been assisted by the advent of Computer-Aided-Engineering. However, post-processing of simulation results, based on a standard workflow providing base-case simulations complemented by selected operating conditions, has changed little. In this work, we propose a new paradigm for handling simulation data by deploying machine learning to encompass a wide spectrum of operating conditions, bypassing the need for additional simulations. This hybrid physics-based and data-driven modeling procedure yields to what we refer to as a Simulation-based Digital Twin. In this paper, we make the case for Computational Fluid Dynamics in multiphase flow systems, although the workflow can be generalized to any other computational engineering method. We quantify the computational speed-up to conclude that the combination of these two fields generates potential for improvement on the conventional methods used in the broad area of computational engineering.}
}
@article{WEI2022104356,
title = {Panorama-to-model registration through integration of image retrieval and semantic reprojection},
journal = {Automation in Construction},
volume = {140},
pages = {104356},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104356},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002291},
author = {Yujie Wei and Burcu Akinci},
keywords = {Image, Panorama, Image-to-BIM registration, Indoor localization, Semantic understanding, Deep learning, Convolutional neural network},
abstract = {Registering images to a building information model is an effective approach of associating as-built component status to as-designed information. However, registering a single still image to a digital twin is difficult due to pose ambiguities caused by the limited field of view in images. Several recent studies have started focusing on leveraging panoramic images to address problems, such as partial occlusions, repetitive facility components and textureless views, which could cause registration failures in existing image-based registration workflows. Even though having a bigger field of view helps in locating where the image was taken, registering a panoramic image to a building information model is still challenging due to inconsistent visual appearances, such as model vs. image visual differences caused by temporary objects in a scene, lighting condition changes, and different levels of details. In this paper, we present a novel method that registers panoramic images to a digital twin in a hierarchical way: the proposed method first performs rough registration through image retrieval using semantic segmentation, then localizes the image with a fine registration through minimizing semantic reprojection errors. Compared to existing methods, the developed method has the following contributions: 1) semantic-based image retrieval makes the rough registration process robust to lighting condition changes, texture differences, and temporary objects, 2) semantic-based image retrieval allows for bi-directional queries between a model and images, 3) reprojection-based fine registration further reduces the localization error due to the dimension reduction of features during image retrieval. Though the proposed method is developed for panoramic images, it can be generalized to monocular images at the cost of localization accuracy. The developed method was evaluated on a real-world academic building and a synthetic dataset, and the results showed that the proposed method can localize a panorama in less than a second and achieve sub-meter level localization error.}
}
@article{LI202261,
title = {Stochastic configuration networks for self-blast state recognition of glass insulators with adaptive depth and multi-scale representation},
journal = {Information Sciences},
volume = {604},
pages = {61-79},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522004133},
author = {Weitao Li and Qian Zhang and Dianhui Wang and Wei Sun and Qiyue Li},
keywords = {Self-blast state, Adaptive depth and multi-scale representation, Ensemble learning, Stochastic configuration networks, Feedback mechanism},
abstract = {The operating state of insulators is directly related to the stability of power transmission line. The existing methods for insulator state recognition cannot achieve satisfactory performance. In this paper, the self-blast state recognition of glass insulators is investigated by using an adaptive learning representation. To increase the adaptability of the network to different scales, we propose a solution based on multi-scale information throughout the entire process, beginning from a low-scale to high-scale subnetworks. The multi-scale information is aggregated in parallel way to take advantage of rich information representation. Then, an imitation of the human thinking pattern is employed. Utilizing entropy-based cost function, we update the parameters of the learner model in real-time. Based on the constraint of the evaluation index, adaptive depth representation for training glass insulators that are unsatisfied with the reliability evaluation is constructed to realize the self-optimizing regulation of feature space. Correspondingly, a stochastic configuration networks (SCNs) classifier is re-constructed to fit for the update multi-hierarchies knowledge space to carry out the re-recognition process. Finally, fuzzy integration is employed to ensemble multi-hierarchies network to improve the model’s generalization. The recognition results on aerial dataset of insulators images demonstrate the effectiveness of our proposed approach.}
}
@article{LV2022108956,
title = {Memory‐augmented neural networks based dynamic complex image segmentation in digital twins for self‐driving vehicle},
journal = {Pattern Recognition},
volume = {132},
pages = {108956},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108956},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322004368},
author = {Zhihan Lv and Liang Qiao and Shuo Yang and Jinhua Li and Haibin Lv and Francesco Piccialli},
keywords = {Deep learning, Image segmentation, Memory-augmented neural networks, LSTM, Self-driving, Digital twins},
abstract = {With the continuous increase of the amount of information, people urgently need to identify the information in the image in more detail in order to obtain richer information from the image. This work explores the dynamic complex image segmentation of self-driving vehicle under Digital Twins (DTs) based on Memory-augmented Neural Networks (MANNs), so as to further improve the performance of self-driving in intelligent transportation. In view of the complexity of the environment and the dynamic changes of the scene in intelligent transportation, this work constructs a segmentation model for dynamic complex image of self-driving vehicle under DTs based on MANNs by optimizing the Deep Learning algorithm and further combining with the DTs technology, so as to recognize the information in the environment image during the self-driving. Finally, the performance of the constructed model is analyzed by experimenting with different image datasets (PASCALVOC 2012, NYUDv2, PASCAL CONTEXT, and real self-driving complex traffic image data). The results show that compared with other classical algorithms, the established MANN-based model has an accuracy of about 85.80%, the training time is shortened to 107.00 s, the test time is 0.70 s, and the speedup ratio is high. In addition, the average algorithm parameter of the given energy function α=0.06 reaches the maximum value. Therefore, it is found that the proposed model shows high accuracy and short training time, which can provide experimental reference for future image visual computing and intelligent information processing.}
}
@article{SOLOMON2022197,
title = {Predicting application usage based on latent contextual information},
journal = {Computer Communications},
volume = {192},
pages = {197-209},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002079},
author = {Adir Solomon and Bracha Shapira and Lior Rokach},
keywords = {Application predictions, User modeling, Deep learning},
abstract = {Predicting application usage is useful for offering personalized services, improving mobile energy consumption, and mobile system resource management optimization. Currently, however, there are many possible applications, and each user has his/her own preferences and usage patterns, which makes the application prediction task very challenging. In this study we use different representation methods to represent mobile users’ contextual information in order to predict application usage. We focus on the spatial information context (i.e., where the applications are used) and represent it with graph embeddings, which capture the locations users have visited based on their movement. We use multimodal embeddings to represent the temporal context, users’ identifiers, and previously used applications. Then, the contextual information’s latent representation is used in a deep learning framework composed of a GRU (gated recurrent unit), attention layer, and a softmax layer to provide application usage predictions. We evaluate our method on two real-world datasets comprised of data collected from mobile users’ devices. Our results show that the proposed application usage prediction method outperforms various machine learning models and state-of-the-art solutions. We also found that the spatial information’s latent representation derived from graph embeddings outperformed traditional and commonly used representation methods when predicting application usage. Our findings also reveal interesting usage patterns regarding users’ predictability, which can help us better understand users’ behavior.}
}
@article{ZHAO2022108741,
title = {Progressive privileged knowledge distillation for online action detection},
journal = {Pattern Recognition},
volume = {129},
pages = {108741},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108741},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002229},
author = {Peisen Zhao and Lingxi Xie and Jiajie Wang and Ya Zhang and Qi Tian},
keywords = {Online action detection, Knowledge distillation, Privileged information, Curriculum learning},
abstract = {Online Action Detection (OAD) in videos addresses the problem of real-time analysis for streaming videos, i.e., only the observed historical video frames are available at prediction time. Considering the future frames observable only at the training stage as a form of privileged information, this paper adopts the Learning Using Privileged Information (LUPI) paradigm. Knowledge distillation (KD) is employed to transfer the privileged information from the offline teacher to the online student. Note that this setting is different from conventional KD because the difference between the teacher and student models mostly lies in the input data rather than the network architecture. To relieves the input information gap for the LUPI, we propose a simple but effective Privileged Knowledge Distillation (PKD) method that enforce KD loss to partial hidden features of the student model. Moreover, we also schedules a curriculum learning procedure to gradually distill the privileged information. This approach is named as Progressive Privileged Knowledge Distillation (PPKD). Compared to some OAD methods that explicitly predict future frames or feature, our approach avoids predicting stage and achieves state-of-the-art accuracy on two popular OAD benchmarks, TVSeries and THUMOS14.}
}
@article{YAN2022105823,
title = {Digital twin-enabled dynamic scheduling with preventive maintenance using a double-layer Q-learning algorithm},
journal = {Computers & Operations Research},
volume = {144},
pages = {105823},
year = {2022},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2022.105823},
url = {https://www.sciencedirect.com/science/article/pii/S0305054822001046},
author = {Qi Yan and Hongfeng Wang and Fang Wu},
keywords = {Double-resource flexible job shop, Uncertain disturbances, Preventive maintenance, Digital twin, Reinforcement learning},
abstract = {Dynamic scheduling methods are essential and critical to manufacturing systems because of uncertain events in the production process, such as new job insertions, order cancellations, worker absences, and machine breakdowns. Emerging digital twin (DT) technology can help detect disturbances by continuously comparing physical space with virtual space and triggering a rescheduling policy immediately after a disturbance. This enables dynamic scheduling and greatly reduces the deviation between preschedules and actual schedules. This study focuses on a DT-enabled integrated optimisation problem of flexible job shop scheduling and flexible preventive maintenance (PM) considering both machine and worker resources. A double-layer Q-learning algorithm (DLQL) is designed as the underlying key optimisation method to simultaneously learn the selection process of machines and operations to achieve efficient real-time scheduling. The superior solution performance of DLQL was verified by comparing it with two well-known metaheuristic algorithms and a single-layer Q-learning algorithm under several benchmarks. Furthermore, different disturbance settings were designed to illustrate the DLQL-based dynamic scheduling process in detail. The proposed reinforcement learning (RL)-driven DT enables efficient collaborative scheduling between production and maintenance departments and helps manufacturing companies improve the real-time decision-making process under uncertain perturbations.}
}